static inline uint32_t
kcs_get_elem_size(kcdata_subtype_descriptor_t d)
{
	if (d->kcs_flags & KCS_SUBTYPE_FLAGS_ARRAY) {
		
		return (uint32_t)((d->kcs_elem_size & 0xffff) * ((d->kcs_elem_size & 0xffff0000) >> 16));
	}
	return d->kcs_elem_size;
}

static inline uint32_t
kcs_get_elem_count(kcdata_subtype_descriptor_t d)
{
	if (d->kcs_flags & KCS_SUBTYPE_FLAGS_ARRAY) {
		return (d->kcs_elem_size >> 16) & 0xffff;
	}
	return 1;
}

static inline int
kcs_set_elem_size(kcdata_subtype_descriptor_t d, uint32_t size, uint32_t count)
{
	if (count > 1) {
		
		if (size > 0xffff || count > 0xffff) {
			return -1; 
		}
		d->kcs_elem_size = ((count & 0xffff) << 16 | (size & 0xffff));
	} else {
		d->kcs_elem_size = size;
	}
	return 0;
}
















                                                             
                                                             
                                                             
                                                             
                                                             
                                                             
                                                             























enum dyld_shared_cache_flags {
	kSharedCacheSystemPrimary = 0x1, 
	kSharedCacheDriverkit = 0x2, 
	kSharedCacheAOT = 0x4,    
};
@interface KCDBasicTypeDescription : KCDataType

- (id)initWithKCTypeDesc:(kcdata_subtype_descriptor_t)sub_type_desc;
- (id)createDefaultForType:(uint32_t)typeID;
- (BOOL) shouldMergeData;
@interface KCDStructTypeDescription : KCDataType

- (id)initWithType:(unsigned int)typeID withName:(NSString *)name;
- (void)addFieldBasicType:(KCDBasicTypeDescription *)fieldType;
- (void)setFlagsRequestedMerge;
@interface KCDataType : NSObject
- (NSDictionary * _Nullable)parseData:(void * _Nonnull)dataBuffer ofLength:(uint32_t)length NS_RETURNS_RETAINED;
- (NSString * _Nonnull)name;
- (unsigned int)typeID;
- (BOOL) shouldMergeData;
@end


KCDataType * _Nullable getKCDataTypeForID(uint32_t typeID);
NSString * _Nonnull KCDataTypeNameForID(uint32_t typeID) NS_RETURNS_NOT_RETAINED;
NSMutableDictionary * _Nullable parseKCDataArray(kcdata_iter_t iter, NSError * _Nullable * _Nullable error) NS_RETURNS_RETAINED;
NSMutableDictionary * _Nullable parseKCDataContainer(kcdata_iter_t * _Nonnull iter, NSError * _Nullable * _Nullable error) NS_RETURNS_RETAINED;
NSDictionary * _Nullable parseKCDataBuffer(void * _Nonnull dataBuffer, uint32_t size, NSError * _Nullable * _Nullable error) NS_RETURNS_RETAINED;
__BEGIN_DECLS




int     mac_audit_check_postselect(kauth_cred_t cred, unsigned short syscode,
    void *args, int error, int retval, int mac_forced) __result_use_check;
int     mac_audit_check_preselect(kauth_cred_t cred, unsigned short syscode,
    void *args) __result_use_check;
int     mac_cred_check_label_update(kauth_cred_t cred,
    struct label *newlabel) __result_use_check;
int     mac_cred_check_label_update_execve(vfs_context_t ctx,
    struct vnode *vp, off_t offset, struct vnode *scriptvp,
    struct label *scriptvnodelabel, struct label *execlabel,
    proc_t proc, void *macextensions) __result_use_check;
int     mac_cred_check_visible(kauth_cred_t u1, kauth_cred_t u2) __result_use_check;
void    mac_cred_label_associate(kauth_cred_t cred_parent,
    kauth_cred_t cred_child);
void    mac_cred_label_associate_fork(kauth_cred_t cred, proc_t child);
void    mac_cred_label_associate_kernel(kauth_cred_t cred);
void    mac_cred_label_associate_user(kauth_cred_t cred);
void    mac_cred_label_destroy(kauth_cred_t cred);
int     mac_cred_label_externalize_audit(proc_t p, struct mac *mac) __result_use_check;
void    mac_cred_label_free(struct label *label);
void    mac_cred_label_init(kauth_cred_t cred);
void    mac_cred_label_seal(kauth_cred_t cred);
bool    mac_cred_label_is_equal(const struct label *a, const struct label *b) __result_use_check;
uint32_t mac_cred_label_hash_update(const struct label *a, uint32_t hash);
void    mac_cred_label_update(kauth_cred_t cred, struct label *newlabel);
void    mac_cred_label_update_execve(vfs_context_t ctx, kauth_cred_t newcred,
    struct vnode *vp, off_t offset, struct vnode *scriptvp,
    struct label *scriptvnodelabel, struct label *execlabel, u_int *csflags,
    void *macextensions, int *disjoint, int *labelupdateerror);
void    mac_devfs_label_associate_device(dev_t dev, struct devnode *de,
    const char *fullpath);
void    mac_devfs_label_associate_directory(const char *dirname, int dirnamelen,
    struct devnode *de, const char *fullpath);
void    mac_devfs_label_copy(struct label *, struct label *label);
void    mac_devfs_label_destroy(struct devnode *de);
void    mac_devfs_label_init(struct devnode *de);
void    mac_devfs_label_update(struct mount *mp, struct devnode *de,
    struct vnode *vp);
int     mac_execve_enter(user_addr_t mac_p, struct image_params *imgp) __result_use_check;
int     mac_file_check_change_offset(kauth_cred_t cred, struct fileglob *fg) __result_use_check;
int     mac_file_check_create(kauth_cred_t cred) __result_use_check;
int     mac_file_check_dup(kauth_cred_t cred, struct fileglob *fg, int newfd) __result_use_check;
int     mac_file_check_fcntl(kauth_cred_t cred, struct fileglob *fg, int cmd,
    user_long_t arg) __result_use_check;
int     mac_file_check_get(kauth_cred_t cred, struct fileglob *fg,
    char *elements, size_t len) __result_use_check;
int     mac_file_check_get_offset(kauth_cred_t cred, struct fileglob *fg) __result_use_check;
int     mac_file_check_inherit(kauth_cred_t cred, struct fileglob *fg) __result_use_check;
int     mac_file_check_ioctl(kauth_cred_t cred, struct fileglob *fg,
    unsigned long cmd) __result_use_check;
int     mac_file_check_lock(kauth_cred_t cred, struct fileglob *fg, int op,
    struct flock *fl) __result_use_check;
int     mac_file_check_library_validation(struct proc *proc,
    struct fileglob *fg, off_t slice_offset,
    user_long_t error_message, size_t error_message_size) __result_use_check;
int     mac_file_check_mmap(kauth_cred_t cred, struct fileglob *fg,
    int prot, int flags, uint64_t file_pos, int *maxprot) __result_use_check;
void    mac_file_check_mmap_downgrade(kauth_cred_t cred, struct fileglob *fg,
    int *prot);
int     mac_file_check_receive(kauth_cred_t cred, struct fileglob *fg) __result_use_check;
int     mac_file_check_set(kauth_cred_t cred, struct fileglob *fg,
    char *bufp, size_t buflen) __result_use_check;
void    mac_file_notify_close(struct ucred *cred, struct fileglob *fg);
int     mac_iokit_check_open_service(kauth_cred_t cred, io_object_t service, unsigned int user_client_type) __result_use_check;
int     mac_iokit_check_open(kauth_cred_t cred, io_object_t user_client, unsigned int user_client_type) __result_use_check;
int     mac_iokit_check_set_properties(kauth_cred_t cred, io_object_t registry_entry, io_object_t properties) __result_use_check;
int     mac_iokit_check_filter_properties(kauth_cred_t cred, io_object_t registry_entry) __result_use_check;
int     mac_iokit_check_get_property(kauth_cred_t cred, io_object_t registry_entry, const char *name) __result_use_check;
int     mac_iokit_check_hid_control(kauth_cred_t cred) __result_use_check;
int     mac_mount_check_fsctl(vfs_context_t ctx, struct mount *mp,
    unsigned long cmd) __result_use_check;
int     mac_mount_check_getattr(vfs_context_t ctx, struct mount *mp,
    struct vfs_attr *vfa) __result_use_check;
int     mac_mount_check_label_update(vfs_context_t ctx, struct mount *mp) __result_use_check;
int     mac_mount_check_mount(vfs_context_t ctx, struct vnode *vp,
    struct componentname *cnp, const char *vfc_name) __result_use_check;
int     mac_mount_check_mount_late(vfs_context_t ctx, struct mount *mp) __result_use_check;
int     mac_mount_check_quotactl(vfs_context_t ctx, struct mount *mp,
    int cmd, int id) __result_use_check;
int     mac_mount_check_snapshot_create(vfs_context_t ctx, struct mount *mp,
    const char *name) __result_use_check;
int     mac_mount_check_snapshot_delete(vfs_context_t ctx, struct mount *mp,
    const char *name) __result_use_check;
int     mac_mount_check_snapshot_mount(vfs_context_t ctx, struct vnode *rvp,
    struct vnode *vp, struct componentname *cnp, const char *name,
    const char *vfc_name) __result_use_check;
int     mac_mount_check_snapshot_revert(vfs_context_t ctx, struct mount *mp,
    const char *name) __result_use_check;
int     mac_mount_check_remount(vfs_context_t ctx, struct mount *mp, int flags) __result_use_check;
int     mac_mount_check_setattr(vfs_context_t ctx, struct mount *mp,
    struct vfs_attr *vfa) __result_use_check;
int     mac_mount_check_stat(vfs_context_t ctx, struct mount *mp) __result_use_check;
int     mac_mount_check_umount(vfs_context_t ctx, struct mount *mp) __result_use_check;
void    mac_mount_label_associate(vfs_context_t ctx, struct mount *mp);
void    mac_mount_label_destroy(struct mount *mp);
int     mac_mount_label_externalize(struct label *label, char *elements,
    char *outbuf, size_t outbuflen) __result_use_check;
int     mac_mount_label_get(struct mount *mp, user_addr_t mac_p) __result_use_check;
void    mac_mount_label_init(struct mount *);
int     mac_mount_label_internalize(struct label *, char *string) __result_use_check;
void    mac_mount_notify_mount(vfs_context_t ctx, struct mount *mp);
int     mac_necp_check_open(proc_t proc, int flags) __result_use_check;
int     mac_necp_check_client_action(proc_t proc, struct fileglob *fg, uint32_t action) __result_use_check;
int     mac_pipe_check_ioctl(kauth_cred_t cred, struct pipe *cpipe,
    unsigned long cmd) __result_use_check;
int     mac_pipe_check_kqfilter(kauth_cred_t cred, struct knote *kn,
    struct pipe *cpipe) __result_use_check;
int     mac_pipe_check_read(kauth_cred_t cred, struct pipe *cpipe) __result_use_check;
int     mac_pipe_check_select(kauth_cred_t cred, struct pipe *cpipe,
    int which) __result_use_check;
int     mac_pipe_check_stat(kauth_cred_t cred, struct pipe *cpipe) __result_use_check;
int     mac_pipe_check_write(kauth_cred_t cred, struct pipe *cpipe) __result_use_check;
void    mac_pipe_label_associate(kauth_cred_t cred, struct pipe *cpipe);
void    mac_pipe_label_destroy(struct pipe *cpipe);
void    mac_pipe_label_free(struct label *label);
void    mac_pipe_label_init(struct pipe *cpipe);
void    mac_pipe_set_label(struct pipe *cpipe, struct label *label);
void    mac_policy_initbsd(void);
int     mac_posixsem_check_create(kauth_cred_t cred, const char *name) __result_use_check;
int     mac_posixsem_check_open(kauth_cred_t cred, struct pseminfo *psem) __result_use_check;
int     mac_posixsem_check_post(kauth_cred_t cred, struct pseminfo *psem) __result_use_check;
int     mac_posixsem_check_unlink(kauth_cred_t cred, struct pseminfo *psem,
    const char *name) __result_use_check;
int     mac_posixsem_check_wait(kauth_cred_t cred, struct pseminfo *psem) __result_use_check;
void    mac_posixsem_vnode_label_associate(kauth_cred_t cred,
    struct pseminfo *psem, struct label *plabel,
    vnode_t vp, struct label *vlabel);
void    mac_posixsem_label_associate(kauth_cred_t cred,
    struct pseminfo *psem, const char *name);
void    mac_posixsem_label_destroy(struct pseminfo *psem);
void    mac_posixsem_label_init(struct pseminfo *psem);
int     mac_posixshm_check_create(kauth_cred_t cred, const char *name) __result_use_check;
int     mac_posixshm_check_mmap(kauth_cred_t cred, struct pshminfo *pshm,
    int prot, int flags) __result_use_check;
int     mac_posixshm_check_open(kauth_cred_t cred, struct pshminfo *pshm,
    int fflags) __result_use_check;
int     mac_posixshm_check_stat(kauth_cred_t cred, struct pshminfo *pshm) __result_use_check;
int     mac_posixshm_check_truncate(kauth_cred_t cred, struct pshminfo *pshm,
    off_t s) __result_use_check;
int     mac_posixshm_check_unlink(kauth_cred_t cred, struct pshminfo *pshm,
    const char *name) __result_use_check;
void    mac_posixshm_vnode_label_associate(kauth_cred_t cred,
    struct pshminfo *pshm, struct label *plabel,
    vnode_t vp, struct label *vlabel);
void    mac_posixshm_label_associate(kauth_cred_t cred,
    struct pshminfo *pshm, const char *name);
void    mac_posixshm_label_destroy(struct pshminfo *pshm);
void    mac_posixshm_label_init(struct pshminfo *pshm);
int     mac_priv_check(kauth_cred_t cred, int priv) __result_use_check;
int     mac_priv_grant(kauth_cred_t cred, int priv) __result_use_check;
int     mac_proc_check_debug(proc_ident_t tracing_ident, kauth_cred_t tracing_cred, proc_ident_t traced_ident) __result_use_check;
int     mac_proc_check_dump_core(proc_t proc) __result_use_check;
int     mac_proc_check_proc_info(proc_t curp, proc_t target, int callnum, int flavor) __result_use_check;
int     mac_proc_check_get_cs_info(proc_t curp, proc_t target, unsigned int op) __result_use_check;
int     mac_proc_check_set_cs_info(proc_t curp, proc_t target, unsigned int op) __result_use_check;
int     mac_proc_check_fork(proc_t proc) __result_use_check;
int     mac_proc_check_suspend_resume(proc_t proc, int sr) __result_use_check;
int     mac_proc_check_get_task(kauth_cred_t cred, proc_ident_t pident, mach_task_flavor_t flavor) __result_use_check;
int     mac_proc_check_expose_task(kauth_cred_t cred, proc_ident_t pident, mach_task_flavor_t flavor) __result_use_check;
int     mac_proc_check_get_movable_control_port(void) __result_use_check;
int     mac_proc_check_inherit_ipc_ports(struct proc *p, struct vnode *cur_vp, off_t cur_offset, struct vnode *img_vp, off_t img_offset, struct vnode *scriptvp) __result_use_check;
int     mac_proc_check_getaudit(proc_t proc) __result_use_check;
int     mac_proc_check_getauid(proc_t proc) __result_use_check;
int     mac_proc_check_dyld_process_info_notify_register(void) __result_use_check;
int     mac_proc_check_ledger(proc_t curp, proc_t target, int op) __result_use_check;
int     mac_proc_check_map_anon(proc_t proc, kauth_cred_t cred, user_addr_t u_addr,
    user_size_t u_size, int prot, int flags, int *maxprot) __result_use_check;
int     mac_proc_check_memorystatus_control(proc_t proc, uint32_t command, pid_t pid) __result_use_check;
int     mac_proc_check_mprotect(proc_t proc,
    user_addr_t addr, user_size_t size, int prot) __result_use_check;
int     mac_proc_check_run_cs_invalid(proc_t proc) __result_use_check;
void    mac_proc_notify_cs_invalidated(proc_t proc);
int     mac_proc_check_sched(proc_t proc, proc_t proc2) __result_use_check;
int     mac_proc_check_setaudit(proc_t proc, struct auditinfo_addr *ai) __result_use_check;
int     mac_proc_check_setauid(proc_t proc, uid_t auid) __result_use_check;
int     mac_proc_check_seteuid(proc_t curp, kauth_cred_t cred, uid_t euid) __result_use_check;
int     mac_proc_check_setegid(proc_t curp, kauth_cred_t cred, gid_t egid) __result_use_check;
int     mac_proc_check_setuid(proc_t curp, kauth_cred_t cred, uid_t uid) __result_use_check;
int     mac_proc_check_setgid(proc_t curp, kauth_cred_t cred, gid_t gid) __result_use_check;
int     mac_proc_check_setreuid(proc_t curp, kauth_cred_t cred, uid_t ruid, uid_t euid) __result_use_check;
int     mac_proc_check_setregid(proc_t curp, kauth_cred_t cred, gid_t rgid, gid_t egid) __result_use_check;
int     mac_proc_check_settid(proc_t curp, uid_t uid, gid_t gid) __result_use_check;
int     mac_proc_check_signal(proc_t curp, proc_ident_t instigator, proc_ident_t target,
    int signum) __result_use_check;
int     mac_proc_check_syscall_unix(proc_t proc, int scnum) __result_use_check;
int     mac_proc_check_wait(proc_t proc1, proc_t proc2) __result_use_check;
void    mac_proc_notify_exit(proc_t proc);
int     mac_proc_check_launch_constraints(proc_t curp, struct image_params *imgp, os_reason_t *reasonp) __result_use_check;
int     mac_socket_check_accept(kauth_cred_t cred, struct socket *so) __result_use_check;
int     mac_socket_check_accepted(kauth_cred_t cred, struct socket *so) __result_use_check;
int     mac_socket_check_bind(kauth_cred_t cred, struct socket *so,
    struct sockaddr *addr) __result_use_check;
int     mac_socket_check_connect(kauth_cred_t cred, struct socket *so,
    struct sockaddr *addr) __result_use_check;
int     mac_socket_check_create(kauth_cred_t cred, int domain,
    int type, int protocol) __result_use_check;
int     mac_socket_check_ioctl(kauth_cred_t cred, struct socket *so,
    unsigned long cmd) __result_use_check;
int     mac_socket_check_listen(kauth_cred_t cred, struct socket *so) __result_use_check;
int     mac_socket_check_receive(kauth_cred_t cred, struct socket *so) __result_use_check;
int     mac_socket_check_received(kauth_cred_t cred, struct socket *so,
    struct sockaddr *saddr) __result_use_check;
int     mac_socket_check_send(kauth_cred_t cred, struct socket *so,
    struct sockaddr *addr) __result_use_check;
int     mac_socket_check_getsockopt(kauth_cred_t cred, struct socket *so,
    struct sockopt *sopt) __result_use_check;
int     mac_socket_check_setsockopt(kauth_cred_t cred, struct socket *so,
    struct sockopt *sopt) __result_use_check;
int     mac_socket_check_stat(kauth_cred_t cred, struct socket *so) __result_use_check;
void    mac_socket_label_associate(kauth_cred_t cred, struct socket *so);
void    mac_socket_label_associate_accept(struct socket *oldsocket,
    struct socket *newsocket);
void    mac_socket_label_copy(struct label *from, struct label *to);
void    mac_socket_label_destroy(struct socket *);
int     mac_socket_label_get(kauth_cred_t cred, struct socket *so,
    struct mac *extmac) __result_use_check;
int     mac_socket_label_init(struct socket *, int waitok) __result_use_check;
void    mac_socketpeer_label_associate_socket(struct socket *peersocket,
    struct socket *socket_to_modify);
int     mac_socketpeer_label_get(kauth_cred_t cred, struct socket *so,
    struct mac *extmac) __result_use_check;
int     mac_system_check_acct(kauth_cred_t cred, struct vnode *vp) __result_use_check;
int     mac_system_check_audit(kauth_cred_t cred, void *record, int length) __result_use_check;
int     mac_system_check_auditctl(kauth_cred_t cred, struct vnode *vp) __result_use_check;
int     mac_system_check_auditon(kauth_cred_t cred, int cmd) __result_use_check;
int     mac_system_check_host_priv(kauth_cred_t cred) __result_use_check;
int     mac_system_check_info(kauth_cred_t, const char *info_type) __result_use_check;
int     mac_system_check_nfsd(kauth_cred_t cred) __result_use_check;
int     mac_system_check_reboot(kauth_cred_t cred, int howto) __result_use_check;
int     mac_system_check_settime(kauth_cred_t cred) __result_use_check;
int     mac_system_check_swapoff(kauth_cred_t cred, struct vnode *vp) __result_use_check;
int     mac_system_check_swapon(kauth_cred_t cred, struct vnode *vp) __result_use_check;
int     mac_system_check_sysctlbyname(kauth_cred_t cred, const char *namestring, int *name,
    size_t namelen, user_addr_t oldctl, size_t oldlen,
    user_addr_t newctl, size_t newlen) __result_use_check;
int     mac_system_check_kas_info(kauth_cred_t cred, int selector) __result_use_check;
void    mac_sysvmsg_label_associate(kauth_cred_t cred,
    struct msqid_kernel *msqptr, struct msg *msgptr);
void    mac_sysvmsg_label_init(struct msg *msgptr);
void    mac_sysvmsg_label_recycle(struct msg *msgptr);
int     mac_sysvmsq_check_enqueue(kauth_cred_t cred, struct msg *msgptr,
    struct msqid_kernel *msqptr) __result_use_check;
int     mac_sysvmsq_check_msgrcv(kauth_cred_t cred, struct msg *msgptr) __result_use_check;
int     mac_sysvmsq_check_msgrmid(kauth_cred_t cred, struct msg *msgptr) __result_use_check;
int     mac_sysvmsq_check_msqctl(kauth_cred_t cred,
    struct msqid_kernel *msqptr, int cmd) __result_use_check;
int     mac_sysvmsq_check_msqget(kauth_cred_t cred,
    struct msqid_kernel *msqptr) __result_use_check;
int     mac_sysvmsq_check_msqrcv(kauth_cred_t cred,
    struct msqid_kernel *msqptr) __result_use_check;
int     mac_sysvmsq_check_msqsnd(kauth_cred_t cred,
    struct msqid_kernel *msqptr) __result_use_check;
void    mac_sysvmsq_label_associate(kauth_cred_t cred,
    struct msqid_kernel *msqptr);
void    mac_sysvmsq_label_init(struct msqid_kernel *msqptr);
void    mac_sysvmsq_label_recycle(struct msqid_kernel *msqptr);
int     mac_sysvsem_check_semctl(kauth_cred_t cred,
    struct semid_kernel *semakptr, int cmd) __result_use_check;
int     mac_sysvsem_check_semget(kauth_cred_t cred,
    struct semid_kernel *semakptr) __result_use_check;
int     mac_sysvsem_check_semop(kauth_cred_t cred,
    struct semid_kernel *semakptr, size_t accesstype) __result_use_check;
void    mac_sysvsem_label_associate(kauth_cred_t cred,
    struct semid_kernel *semakptr);
void    mac_sysvsem_label_destroy(struct semid_kernel *semakptr);
void    mac_sysvsem_label_init(struct semid_kernel *semakptr);
void    mac_sysvsem_label_recycle(struct semid_kernel *semakptr);
int     mac_sysvshm_check_shmat(kauth_cred_t cred,
    struct shmid_kernel *shmsegptr, int shmflg) __result_use_check;
int     mac_sysvshm_check_shmctl(kauth_cred_t cred,
    struct shmid_kernel *shmsegptr, int cmd) __result_use_check;
int     mac_sysvshm_check_shmdt(kauth_cred_t cred,
    struct shmid_kernel *shmsegptr) __result_use_check;
int     mac_sysvshm_check_shmget(kauth_cred_t cred,
    struct shmid_kernel *shmsegptr, int shmflg) __result_use_check;
void    mac_sysvshm_label_associate(kauth_cred_t cred,
    struct shmid_kernel *shmsegptr);
void    mac_sysvshm_label_destroy(struct shmid_kernel *shmsegptr);
void    mac_sysvshm_label_init(struct shmid_kernel* shmsegptr);
void    mac_sysvshm_label_recycle(struct shmid_kernel *shmsegptr);
int     mac_vnode_check_access(vfs_context_t ctx, struct vnode *vp,
    int acc_mode) __result_use_check;
int     mac_vnode_check_chdir(vfs_context_t ctx, struct vnode *dvp) __result_use_check;
int     mac_vnode_check_chroot(vfs_context_t ctx, struct vnode *dvp,
    struct componentname *cnp) __result_use_check;
int     mac_vnode_check_clone(vfs_context_t ctx, struct vnode *dvp,
    struct vnode *vp, struct componentname *cnp) __result_use_check;
int     mac_vnode_check_copyfile(vfs_context_t ctx, struct vnode *dvp,
    struct vnode *tvp, struct vnode *fvp, struct componentname *cnp,
    mode_t mode, int flags) __result_use_check;
int     mac_vnode_check_create(vfs_context_t ctx, struct vnode *dvp,
    struct componentname *cnp, struct vnode_attr *vap) __result_use_check;
int     mac_vnode_check_deleteextattr(vfs_context_t ctx, struct vnode *vp,
    const char *name) __result_use_check;
int     mac_vnode_check_exchangedata(vfs_context_t ctx, struct vnode *v1,
    struct vnode *v2) __result_use_check;
int     mac_vnode_check_exec(vfs_context_t ctx, struct vnode *vp,
    struct image_params *imgp) __result_use_check;
int     mac_vnode_check_fsgetpath(vfs_context_t ctx, struct vnode *vp) __result_use_check;
int     mac_vnode_check_getattr(vfs_context_t ctx, struct ucred *file_cred,
    struct vnode *vp, struct vnode_attr *va) __result_use_check;
int     mac_vnode_check_getattrlist(vfs_context_t ctx, struct vnode *vp,
    struct attrlist *alist, uint64_t options) __result_use_check;
int     mac_vnode_check_getattrlistbulk(vfs_context_t ctx, struct vnode *dvp,
    struct attrlist *alist, uint64_t options) __result_use_check;
int     mac_vnode_check_getextattr(vfs_context_t ctx, struct vnode *vp,
    const char *name, struct uio *uio) __result_use_check;
int     mac_vnode_check_ioctl(vfs_context_t ctx, struct vnode *vp,
    unsigned long cmd) __result_use_check;
int     mac_vnode_check_kqfilter(vfs_context_t ctx,
    kauth_cred_t file_cred, struct knote *kn, struct vnode *vp) __result_use_check;
int     mac_vnode_check_label_update(vfs_context_t ctx, struct vnode *vp,
    struct label *newlabel);
__result_use_check
int     mac_vnode_check_link(vfs_context_t ctx, struct vnode *dvp,
    struct vnode *vp, struct componentname *cnp) __result_use_check;
int     mac_vnode_check_listextattr(vfs_context_t ctx, struct vnode *vp) __result_use_check;
int     mac_vnode_check_lookup(vfs_context_t ctx, struct vnode *dvp,
    struct componentname *cnp) __result_use_check;
int     mac_vnode_check_lookup_preflight(vfs_context_t ctx, struct vnode *dvp,
    const char *path, size_t pathlen) __result_use_check;
int     mac_vnode_check_open(vfs_context_t ctx, struct vnode *vp,
    int acc_mode) __result_use_check;
int     mac_vnode_check_read(vfs_context_t ctx,
    kauth_cred_t file_cred, struct vnode *vp) __result_use_check;
int     mac_vnode_check_readdir(vfs_context_t ctx, struct vnode *vp) __result_use_check;
int     mac_vnode_check_readlink(vfs_context_t ctx, struct vnode *vp) __result_use_check;
int     mac_vnode_check_rename(vfs_context_t ctx, struct vnode *fdvp,
    struct vnode *fvp, struct componentname *fcnp, struct vnode *tdvp,
    struct vnode *tvp, struct componentname *tcnp) __result_use_check;
int     mac_vnode_check_rename_swap(vfs_context_t ctx, struct vnode *fdvp,
    struct vnode *fvp, struct componentname *fcnp, struct vnode *tdvp,
    struct vnode *tvp, struct componentname *tcnp) __result_use_check;
int     mac_vnode_check_revoke(vfs_context_t ctx, struct vnode *vp) __result_use_check;
int     mac_vnode_check_searchfs(vfs_context_t ctx, struct vnode *vp,
    struct attrlist *returnattrs, struct attrlist *searchattrs) __result_use_check;
int     mac_vnode_check_select(vfs_context_t ctx, struct vnode *vp,
    int which) __result_use_check;
int     mac_vnode_check_setacl(vfs_context_t ctx, struct vnode *vp,
    struct kauth_acl *acl) __result_use_check;
int     mac_vnode_check_setattrlist(vfs_context_t ctxd, struct vnode *vp,
    struct attrlist *alist) __result_use_check;
int     mac_vnode_check_setextattr(vfs_context_t ctx, struct vnode *vp,
    const char *name, struct uio *uio) __result_use_check;
int     mac_vnode_check_setflags(vfs_context_t ctx, struct vnode *vp,
    u_long flags) __result_use_check;
int     mac_vnode_check_setmode(vfs_context_t ctx, struct vnode *vp,
    mode_t mode) __result_use_check;
int     mac_vnode_check_setowner(vfs_context_t ctx, struct vnode *vp,
    uid_t uid, gid_t gid) __result_use_check;
int     mac_vnode_check_setutimes(vfs_context_t ctx, struct vnode *vp,
    struct timespec atime, struct timespec mtime) __result_use_check;
int     mac_vnode_check_signature(struct vnode *vp,
    struct cs_blob *cs_blob, struct image_params *imgp,
    unsigned int *cs_flags, unsigned int *signer_type,
    int flags, unsigned int platform) __result_use_check;
int     mac_vnode_check_supplemental_signature(struct vnode *vp,
    struct cs_blob *cs_blob, struct vnode *linked_vp,
    struct cs_blob *linked_cs_blob, unsigned int *signer_type) __result_use_check;
int     mac_vnode_check_stat(vfs_context_t ctx,
    kauth_cred_t file_cred, struct vnode *vp) __result_use_check;
int     mac_vnode_check_trigger_resolve(vfs_context_t ctx, struct vnode *dvp,
    struct componentname *cnp) __result_use_check;
int     mac_vnode_check_truncate(vfs_context_t ctx,
    kauth_cred_t file_cred, struct vnode *vp) __result_use_check;
int     mac_vnode_check_uipc_bind(vfs_context_t ctx, struct vnode *dvp,
    struct componentname *cnp, struct vnode_attr *vap) __result_use_check;
int     mac_vnode_check_uipc_connect(vfs_context_t ctx, struct vnode *vp, struct socket *so) __result_use_check;
int     mac_vnode_check_unlink(vfs_context_t ctx, struct vnode *dvp,
    struct vnode *vp, struct componentname *cnp) __result_use_check;
int     mac_vnode_check_write(vfs_context_t ctx,
    kauth_cred_t file_cred, struct vnode *vp) __result_use_check;
int     mac_vnode_label_associate(struct mount *mp, struct vnode *vp,
    vfs_context_t ctx) __result_use_check;
void    mac_vnode_label_associate_devfs(struct mount *mp, struct devnode *de,
    struct vnode *vp);
int     mac_vnode_label_associate_extattr(struct mount *mp, struct vnode *vp) __result_use_check;
int     mac_vnode_label_associate_fdesc(struct mount *mp, struct fdescnode *fnp,
    struct vnode *vp, vfs_context_t ctx) __result_use_check;
void    mac_vnode_label_associate_singlelabel(struct mount *mp,
    struct vnode *vp);
void    mac_vnode_label_copy(struct label *l1, struct label *l2);
void    mac_vnode_label_destroy(struct vnode *vp);
int     mac_vnode_label_externalize_audit(struct vnode *vp, struct mac *mac) __result_use_check;
void    mac_vnode_label_free(struct label *label);
void    mac_vnode_label_init(struct vnode *vp);
int     mac_vnode_label_init_needed(struct vnode *vp) __result_use_check;
void    mac_vnode_label_recycle(struct vnode *vp);
void    mac_vnode_label_update(vfs_context_t ctx, struct vnode *vp,
    struct label *newlabel);
void    mac_vnode_label_update_extattr(struct mount *mp, struct vnode *vp,
    const char *name);
int     mac_vnode_notify_create(vfs_context_t ctx, struct mount *mp,
    struct vnode *dvp, struct vnode *vp, struct componentname *cnp) __result_use_check;
void    mac_vnode_notify_deleteextattr(vfs_context_t ctx, struct vnode *vp, const char *name);
void    mac_vnode_notify_link(vfs_context_t ctx, struct vnode *vp,
    struct vnode *dvp, struct componentname *cnp);
void    mac_vnode_notify_open(vfs_context_t ctx, struct vnode *vp, int acc_flags);
void    mac_vnode_notify_rename(vfs_context_t ctx, struct vnode *fvp,
    struct vnode *tdvp, struct componentname *tcnp);
void    mac_vnode_notify_rename_swap(vfs_context_t ctx, struct vnode *fvp,
    struct vnode *fdvp, struct componentname *fcnp, struct vnode *tvp,
    struct vnode *tdvp, struct componentname *tcnp);
void    mac_vnode_notify_setacl(vfs_context_t ctx, struct vnode *vp, struct kauth_acl *acl);
void    mac_vnode_notify_setattrlist(vfs_context_t ctx, struct vnode *vp, struct attrlist *alist);
void    mac_vnode_notify_setextattr(vfs_context_t ctx, struct vnode *vp, const char *name, struct uio *uio);
void    mac_vnode_notify_setflags(vfs_context_t ctx, struct vnode *vp, u_long flags);
void    mac_vnode_notify_setmode(vfs_context_t ctx, struct vnode *vp, mode_t mode);
void    mac_vnode_notify_setowner(vfs_context_t ctx, struct vnode *vp, uid_t uid, gid_t gid);
void    mac_vnode_notify_setutimes(vfs_context_t ctx, struct vnode *vp, struct timespec atime, struct timespec mtime);
void    mac_vnode_notify_truncate(vfs_context_t ctx, kauth_cred_t file_cred, struct vnode *vp);
int     mac_vnode_find_sigs(struct proc *p, struct vnode *vp, off_t offsetInMacho) __result_use_check;
int     vnode_label(struct mount *mp, struct vnode *dvp, struct vnode *vp,
    struct componentname *cnp, int flags, vfs_context_t ctx) __result_use_check;
void    vnode_relabel(struct vnode *vp);
void    mac_pty_notify_grant(proc_t p, struct tty *tp, dev_t dev, struct label *label);
void    mac_pty_notify_close(proc_t p, struct tty *tp, dev_t dev, struct label *label);
int     mac_kext_check_load(kauth_cred_t cred, const char *identifier) __result_use_check;
int     mac_kext_check_unload(kauth_cred_t cred, const char *identifier) __result_use_check;
int     mac_kext_check_query(kauth_cred_t cred) __result_use_check;
int     mac_skywalk_flow_check_connect(proc_t p, void *flow, const struct sockaddr *addr, int type, int protocol) __result_use_check;
int     mac_skywalk_flow_check_listen(proc_t p, void *flow, const struct sockaddr *addr, int type, int protocol) __result_use_check;
void    mac_vnode_notify_reclaim(vnode_t vp);
void    mac_vnode_notify_unlink(vfs_context_t ctx, struct vnode *dvp,
    struct vnode *vp, struct componentname *cnp);
void psem_label_associate(struct fileproc *fp, struct vnode *vp, struct vfs_context *ctx);
void pshm_label_associate(struct fileproc *fp, struct vnode *vp, struct vfs_context *ctx);
SYSCTL_DECL(_security);
SYSCTL_DECL(_security_mac);
SLIST_HEAD(mac_label_listeners_t, mac_label_listener);
SLIST_HEAD(mac_label_element_list_t, mac_label_element);
static bool mac_proc_check_enforce(proc_t p);
static __inline__ bool
mac_proc_check_enforce(proc_t p)
{
	
	return p != kernproc;
}

static bool mac_cred_check_enforce(kauth_cred_t cred);
static __inline__ bool
mac_cred_check_enforce(kauth_cred_t cred)
{
	return cred != proc_ucred_unsafe(kernproc);
}



int mac_error_select(int error1, int error2);
void  mac_policy_list_busy(void);
int   mac_policy_list_conditional_busy(void);
void  mac_policy_list_unbusy(void);
int   mac_check_structmac_consistent(struct user_mac *mac);
int mac_cred_label_externalize(struct label *, char *e, char *out, size_t olen, int flags);
int mac_vnode_label_externalize(struct label *, char *e, char *out, size_t olen, int flags);
int mac_cred_label_internalize(struct label *label, char *string);
int mac_vnode_label_internalize(struct label *label, char *string);
int mac_do_get(struct proc *p, user_addr_t mac_p, mac_getter_t getter);
int mac_do_set(struct proc *p, user_addr_t mac_p, mac_setter_t setter);
void mac_policy_addto_labellist(const mac_policy_handle_t, int);
void mac_policy_removefrom_labellist(const mac_policy_handle_t);
int mac_externalize(size_t mpo_externalize_off, struct label *label,
    const char *elementlist, char *outbuf, size_t outbuflen);
int mac_internalize(size_t mpo_internalize_off, struct label *label,
    char *elementlist);
int	mac_do_machexc(int64_t code, int64_t subcode, uint32_t flags __unused);
int	mac_schedule_userret(void);
int mac_schedule_telemetry(void);
void mac_policy_init(void);
void mac_policy_initmach(void);
int	mac_task_check_expose_task(struct task *t, mach_task_flavor_t flavor);
int	mac_task_check_task_id_token_get_task(struct task *t, mach_task_flavor_t flavor);
int	mac_task_check_set_host_special_port(struct task *task,
	    int id, struct ipc_port *port);
int	mac_task_check_set_host_exception_port(struct task *task,
	    unsigned int exception);
int	mac_task_check_set_host_exception_ports(struct task *task,
	    unsigned int exception_mask);
int	mac_task_check_get_task_special_port(struct task *task,
	    struct task *target, int which);
int	mac_task_check_set_task_special_port(struct task *task,
	    struct task *target, int which, struct ipc_port *port);
int	mac_task_check_set_task_exception_ports(struct task *task,
	    struct task *target, unsigned int exception_mask, int new_behavior);
int	mac_task_check_set_thread_exception_ports(struct task *task,
	    struct task *target, unsigned int exception_mask, int new_behavior);
int mac_task_check_get_movable_control_port(void);
int mac_task_check_dyld_process_info_notify_register(void);
#   define bitstr_test(name, bit) ((name)[((bit) >> 3)] & (1 << ((bit) & 0x7)))

typedef int (*mac_task_mach_filter_cbfunc_t)(struct proc *bsdinfo, int num);
uint8_t *mac_task_get_mach_filter_mask(struct task *task);
uint8_t *mac_task_get_kobj_filter_mask(struct task *task);
void mac_task_set_mach_filter_mask(struct task *task, uint8_t *maskptr);
void mac_task_set_kobj_filter_mask(struct task *task, uint8_t *maskptr);
int  mac_task_register_filter_callbacks(
		const mac_task_mach_filter_cbfunc_t mach_cbfunc,
		const mac_task_kobj_filter_cbfunc_t kobj_cbfunc);
void	act_set_astmacf(struct thread *);
void	mac_thread_userret(struct thread *);
void	mac_thread_telemetry(struct thread *, int, void *, size_t);
void mac_exc_set_label(struct exception_action *action, struct label *label);
void mac_exc_free_label(struct label *label);
void mac_exc_associate_action_label(struct exception_action *action, struct label *label);
void mac_exc_free_action_label(struct exception_action *action);
int mac_exc_update_action_label(struct exception_action *action, struct label *newlabel);
int mac_exc_inherit_action_label(struct exception_action *parent, struct exception_action *child);
int mac_exc_update_task_crash_label(struct task *task, struct label *newlabel);
int mac_exc_action_check_exception_send(struct task *victim_task, struct exception_action *action);
void mac_proc_notify_exec_complete(struct proc *proc);
int mac_proc_check_remote_thread_create(struct task *task, int flavor, thread_state_t new_state, mach_msg_type_number_t new_state_count);
void mac_proc_notify_service_port_derive(struct mach_service_port_info *sp_info);
int     mac_policy_register(struct mac_policy_conf *mpc,
    mac_policy_handle_t *handlep, void *xd);
int     mac_policy_unregister(mac_policy_handle_t handle);
int     mac_audit_text(char *text, mac_policy_handle_t handle);
int     mac_vnop_setxattr(struct vnode *, const char *, char *, size_t);
int     mac_vnop_getxattr(struct vnode *, const char *, char *, size_t,
    size_t *);
int     mac_vnop_removexattr(struct vnode *, const char *);
int     mac_file_setxattr(struct fileglob *fg, const char *name, char *buf, size_t len);
int     mac_file_getxattr(struct fileglob *fg, const char *name, char *buf, size_t len,
    size_t *attrlen);
int     mac_file_removexattr(struct fileglob *fg, const char *name);
intptr_t        mac_label_get(struct label *l, int slot);
void            mac_label_set(struct label *l, int slot, intptr_t v);
void            mac_labelzone_free(struct label *l);
void            mac_labelzone_free_owned(struct label **labelp,
    void (^extra_deinit)(struct label *));
intptr_t        mac_vnode_label_get(struct vnode *vp, int slot, intptr_t sentinel);
void            mac_vnode_label_set(struct vnode *vp, int slot, intptr_t v);
FILE *fopenp(const char *fpath, char *file, char *complete, const char *ftype);
const char *get_VPATH(void);
const char      *get_word(FILE *fp);
char    *ns(const char *str);
char    *qu(int num);
char    *path(const char *file);
dev_t   nametodev(char *name, int defunit, char defpartition);
char    *devtoname(dev_t dev);
char *get_rest(FILE *fp);
int yyparse(void);
void yyerror(const char *s);
void mkioconf(void);
void makefile(void);
void headers(void);
int opteq(const char *cp, const char *dp);
void init_dev(struct device *dp);
void newdev(struct device *dp);
void dev_param(struct device *dp, const char *str, long num);
int searchp(const char *spath, char *file, char *fullname, int (*func)(char *));
__BEGIN_DECLS



extern void kasan_zmem_add(
	vm_address_t            addr,
	vm_size_t               size,
	vm_offset_t             esize,
	vm_offset_t             offs,
	vm_offset_t             rzsize);
extern void kasan_zmem_remove(
	vm_address_t            addr,
	vm_size_t               size,
	vm_offset_t             esize,
	vm_offset_t             offs,
	vm_offset_t             rzsize);
extern void kasan_alloc(
	vm_address_t            addr,
	vm_size_t               size,
	vm_size_t               usize,
	vm_size_t               rzsize,
	bool                    percpu,
	void                   *fp);
extern void kasan_free(
	vm_address_t            addr,
	vm_size_t               size,
	vm_size_t               usize,
	vm_size_t               rzsize,
	bool                    percpu,
	void                   *fp);
extern void kasan_alloc_large(
	vm_address_t            addr,
	vm_size_t               req_size);
extern vm_size_t kasan_user_size(
	vm_address_t            addr);
extern void kasan_check_alloc(
	vm_address_t            addr,
	vm_size_t               size,
	vm_size_t               usize);
extern struct kasan_quarantine_result kasan_quarantine(
	vm_address_t            addr,
	vm_size_t               size);
extern vm_size_t kasan_quarantine_resolve(
	vm_address_t            addr,
	struct zone           **zonep);
void kasan_tbi_retag_unused_space(caddr_t, vm_size_t, vm_size_t);
void kasan_tbi_mark_free_space(caddr_t, vm_size_t);
uint8_t *kasan_tbi_get_tag_address(vm_offset_t);
const char *kasan_handle_brk_failure(void *, uint16_t);
void kasan_impl_report_internal(uptr, uptr, access_t, violation_t, bool);
void kasan_impl_poison_range(vm_offset_t, vm_size_t, uint8_t);
void kasan_impl_kdp_disable(void);
void kasan_impl_init(void);
void kasan_impl_late_init(void);
void kasan_impl_fill_valid_range(uintptr_t, size_t);
void kasan_poison(vm_offset_t, vm_size_t, vm_size_t, vm_size_t, uint8_t);
bool kasan_check_enabled(access_t);
bool kasan_impl_check_enabled(access_t);
void kasan_check_range(const void *, size_t, access_t);
void kasan_init_dybl(void);
bool kasan_is_blacklisted(access_t);
void kasan_dybl_load_kext(uintptr_t, const char *);
void kasan_dybl_unload_kext(uintptr_t);
void kasan_arch_init(void);
bool kasan_is_shadow_mapped(uintptr_t);
void kasan_lock_init(void);
void kasan_lock(boolean_t *);
void kasan_unlock(boolean_t);
bool kasan_lock_held(thread_t);
void kasan_init_fakestack(void);
void kasan_init_globals(vm_offset_t, vm_size_t);
void kasan_violation(uintptr_t, size_t, access_t, violation_t);
size_t kasan_impl_decode_issue(char *, size_t, uptr, uptr, access_t, violation_t);
void NOINLINE OS_NORETURN kasan_crash_report(uptr, uptr, access_t, violation_t);
void kasan_handle_test(void);
SYSCTL_DECL(kasan);
SYSCTL_DECL(_kern_kasan);
OS_ENUM(ubsan_violation_type, uint8_t,
    UBSAN_OVERFLOW_add = 1,
    UBSAN_OVERFLOW_sub,
    UBSAN_OVERFLOW_mul,
    UBSAN_OVERFLOW_divrem,
    UBSAN_OVERFLOW_negate,
    UBSAN_UNREACHABLE,
    UBSAN_SHIFT,
    UBSAN_ALIGN,
    UBSAN_POINTER_OVERFLOW,
    UBSAN_OOB,
    UBSAN_TYPE_MISMATCH,
    UBSAN_LOAD_INVALID_VALUE,
    UBSAN_NULLABILITY_ARG,
    UBSAN_NULLABILITY_RETURN,
    UBSAN_MISSING_RETURN,
    UBSAN_FLOAT_CAST_OVERFLOW,
    UBSAN_IMPLICIT_CONVERSION,
    UBSAN_FUNCTION_TYPE_MISMATCH,
    UBSAN_VLA_BOUND_NOT_POSITIVE,
    UBSAN_INVALID_BUILTIN,
    UBSAN_VIOLATION_MAX
    );
void    ubsan_json_init(ubsan_buf_t *, char *, size_t);
void    ubsan_json_begin(ubsan_buf_t *, size_t);
size_t  ubsan_json_finish(ubsan_buf_t *);
bool    ubsan_json_format(ubsan_violation_t *, ubsan_buf_t *);
void __ubsan_handle_add_overflow(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_add_overflow_abort(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_builtin_unreachable(struct ubsan_unreachable_desc *);
void __ubsan_handle_divrem_overflow(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_divrem_overflow_abort(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_float_cast_overflow(struct ubsan_float_desc *, uint64_t);
void __ubsan_handle_float_cast_overflow_abort(struct ubsan_float_desc *, uint64_t);
void __ubsan_handle_function_type_mismatch(struct ubsan_func_type_mismatch_desc*, uint64_t);
void __ubsan_handle_function_type_mismatch_abort(struct ubsan_func_type_mismatch_desc *, uint64_t);
void __ubsan_handle_implicit_conversion(struct ubsan_implicit_conv_desc *, uint64_t, uint64_t);
void __ubsan_handle_implicit_conversion_abort(struct ubsan_implicit_conv_desc *, uint64_t, uint64_t);
void __ubsan_handle_invalid_builtin(struct ubsan_invalid_builtin *);
void __ubsan_handle_invalid_builtin_abort(struct ubsan_invalid_builtin *);
void __ubsan_handle_load_invalid_value(struct ubsan_load_invalid_desc *, uint64_t);
void __ubsan_handle_load_invalid_value_abort(struct ubsan_load_invalid_desc *, uint64_t);
void __ubsan_handle_missing_return(struct ubsan_missing_ret_desc *);
void __ubsan_handle_missing_return_abort(struct ubsan_missing_ret_desc *);
void __ubsan_handle_mul_overflow(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_mul_overflow_abort(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_negate_overflow(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_negate_overflow_abort(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_nonnull_arg(struct ubsan_nullability_arg_desc *);
void __ubsan_handle_nonnull_arg_abort(struct ubsan_nullability_arg_desc *);
void __ubsan_handle_nonnull_return_v1(struct ubsan_nullability_ret_desc *, uint64_t);
void __ubsan_handle_nonnull_return_v1_abort(struct ubsan_nullability_ret_desc *, uint64_t);
void __ubsan_handle_nullability_arg(struct ubsan_nullability_arg_desc *);
void __ubsan_handle_nullability_arg_abort(struct ubsan_nullability_arg_desc *);
void __ubsan_handle_nullability_return_v1(struct ubsan_nullability_ret_desc *, uint64_t);
void __ubsan_handle_nullability_return_v1_abort(struct ubsan_nullability_ret_desc *, uint64_t);
void __ubsan_handle_out_of_bounds(struct ubsan_oob_desc *, uint64_t idx);
void __ubsan_handle_out_of_bounds_abort(struct ubsan_oob_desc *, uint64_t idx);
void __ubsan_handle_pointer_overflow(struct ubsan_ptroverflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_pointer_overflow_abort(struct ubsan_ptroverflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_shift_out_of_bounds(struct ubsan_shift_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_shift_out_of_bounds_abort(struct ubsan_shift_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_sub_overflow(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_sub_overflow_abort(struct ubsan_overflow_desc *, uint64_t lhs, uint64_t rhs);
void __ubsan_handle_type_mismatch_v1(struct ubsan_align_desc *, uint64_t val);
void __ubsan_handle_type_mismatch_v1_abort(struct ubsan_align_desc *, uint64_t val);
void __ubsan_handle_vla_bound_not_positive(struct ubsan_vla_bound_desc *, uint64_t);
void __ubsan_handle_vla_bound_not_positive_abort(struct ubsan_vla_bound_desc *, uint64_t);
void PE_update_panic_crc(
	unsigned char *,
	unsigned int *);
extern void lpss_uart_enable(boolean_t on_off);
void PE_enter_debugger(
	const char *cause);
void PE_init_platform(
	boolean_t vm_initialized,
	void *args);
uint32_t PE_get_random_seed(
	unsigned char * dst_random_seed,
	uint32_t request_size);
uint32_t PE_i_can_has_debugger(
	uint32_t *);
int PE_stub_poll_input(unsigned int options, char *c);
boolean_t PE_panic_debugging_enabled(void);
void PE_mark_hwaccess(uint64_t thread);
void PE_mark_hwaccess_data(uint8_t type, uint8_t size, uint64_t paddr);
boolean_t PE_arm_debug_and_trace_initialized(void);
uint32_t PE_get_offset_into_panic_region(
	char *location);
void PE_init_panicheader(
	void);
void PE_update_panicheader_nestedpanic(
	void);
bool PE_handle_platform_error(
	vm_offset_t far);
extern uint32_t PE_i_can_has_kernel_configuration(void);
unsigned int PE_init_taproot(vm_offset_t *taddr);
void PE_init_printf(
	boolean_t vm_initialized);
void PE_init_iokit(
	void);
void PE_lockdown_iokit(
	void);
void PE_register_timebase_callback(timebase_callback_func callback);
void PE_call_timebase_callback(void);
void PE_install_interrupt_handler(
	void *nub, int source,
	void *target, IOInterruptHandler handler, void *refCon);
void kprintf(const char *fmt, ...) __printflike(1, 2);
void _consume_kprintf_args(int, ...);
void init_display_putc(unsigned char *baseaddr, int rowbytes, int height);
void display_putc(char c);
extern void initialize_screen(PE_Video *, unsigned int);
extern void dim_screen(void);
extern int PE_current_console(
	PE_Video *info);
extern void PE_create_console(
	void);
extern int PE_initialize_console(
	PE_Video *newInfo,
	int op);
extern void PE_display_icon( unsigned int flags,
    const char * name );
extern char * PE_boot_args(
	void);
extern boolean_t PE_parse_boot_argn(
	const char      *arg_string,
	void            *arg_ptr,
	int                     max_arg);
extern boolean_t PE_boot_arg_uint64_eq(const char *arg_string, uint64_t value);
extern boolean_t PE_parse_boot_arg_str(
	const char *arg_string,
	char *      arg_ptr,
	int         size);
extern boolean_t PE_get_default(
	const char      *property_name,
	void            *property_ptr,
	unsigned int max_property);
extern boolean_t PE_get_hotkey(
	unsigned char   key);
extern kern_return_t __abortlike
PE_cpu_start_from_kext(
	cpu_id_t target,
	vm_offset_t start_paddr,
	vm_offset_t arg_paddr);
extern void PE_cpu_start_internal(
	cpu_id_t target,
	vm_offset_t start_paddr,
	vm_offset_t arg_paddr);
extern void PE_cpu_halt(
	cpu_id_t target);
extern bool PE_cpu_down(
	cpu_id_t target);
extern void PE_cpu_signal(
	cpu_id_t source,
	cpu_id_t target);
extern void PE_cpu_signal_deferred(
	cpu_id_t source,
	cpu_id_t target);
extern void PE_cpu_signal_cancel(
	cpu_id_t source,
	cpu_id_t target);
extern void PE_cpu_machine_init(
	cpu_id_t target,
	boolean_t bootb);
extern void PE_cpu_machine_quiesce(
	cpu_id_t target);
extern void pe_init_debug(void);
extern boolean_t PE_imgsrc_mount_supported(void);
extern void PE_panic_hook(const char *str);
extern void PE_init_cpu(void);
extern void PE_handle_ext_interrupt(void);
extern void PE_cpu_power_enable(int cpu_id);
extern void PE_cpu_power_disable(int cpu_id);
extern bool PE_cpu_power_check_kdp(int cpu_id);
extern void PE_singlestep_hook(void);
extern kern_return_t PE_cpu_perfmon_interrupt_install_handler(perfmon_interrupt_handler_func handler);
extern void PE_cpu_perfmon_interrupt_enable(cpu_id_t target, boolean_t enable);
extern void PE_set_kc_header(kc_kind_t type, kernel_mach_header_t *header, uintptr_t slide);
void PE_reset_kc_header(kc_kind_t type);
extern void PE_set_kc_header_and_base(kc_kind_t type, kernel_mach_header_t *header, void *base, uintptr_t slide);
extern void *PE_get_kc_header(kc_kind_t type);
extern void *PE_get_kc_baseaddress(kc_kind_t type);
extern const void * const*PE_get_kc_base_pointers(void);
extern uintptr_t PE_get_kc_slide(kc_kind_t type);
extern bool PE_get_primary_kc_format(kc_format_t *type);
extern bool PE_get_kc_format(kc_kind_t type, kc_format_t *format);
extern void PE_set_kc_vp(kc_kind_t type, void *vp);
void * PE_get_kc_vp(kc_kind_t type);
void PE_reset_all_kc_vp(void);
boolean_t PE_reboot_on_panic(void);
void PE_sync_panic_buffers(void);
extern void PE_read_socd_client_buffer(vm_offset_t offset, void *out_buff, vm_size_t size);
extern void PE_write_socd_client_buffer(vm_offset_t offset, const void *in_buff, vm_size_t size);
int PE_consistent_debug_register(uint64_t record_id, uint64_t physaddr, uint64_t length);
boolean_t PE_consistent_debug_lookup_entry(uint64_t record_id, uint64_t *phys_addr, uint64_t *length);
int PE_consistent_debug_enabled(void);
extern vm_offset_t pe_arm_get_soc_base_phys(void);
extern uint32_t pe_arm_init_interrupts(void *args);
extern void pe_arm_debug_init_early(void *args);
extern void pe_arm_debug_init_late(void);
int serial_init(void);
void
serial_set_on_demand(bool);
int serial_getc(void);
void serial_putc(char);
void uart_putc(char);
void serial_putc_options(char, bool);
void uart_putc_options(char, bool);
int uart_getc(void);
void pe_init_fiq(void);
kern_return_t serial_irq_enable(serial_device_t device);
kern_return_t serial_irq_action(serial_device_t device);
bool serial_irq_filter(serial_device_t device);
void serial_go_to_sleep(void);
int switch_to_serial_console(void);
void switch_to_old_console(int);
void apt_msg_init(void);
uint8_t apt_msg_policy(void);
void apt_msg_init_cpu(void);
void apt_msg_emit(int ns, int type, int num_payloads, uint64_t *payloads);
__IN(b, char)
__IN(w, short)
__IN(l, long)

__OUT(b, char)
__OUT(w, short)
__OUT(l, long)

extern void cninit(void);
extern int  sprintf(char * str, const char * format, ...);
int switch_to_serial_console(void);
void switch_to_old_console(int);
boolean_t console_is_serial(void);
int serial_init(void);
void serial_putc(char);
int serial_getc(void);
void serial_putc_options(char, bool);
void console_write_unbuffered(char);
extern void flush_dcache_syscall( vm_offset_t addr, unsigned length);
extern void enable_dc_mva_ops(void);
extern void disable_dc_mva_ops(void);
extern void flush_dcache(vm_offset_t addr, unsigned count, int phys);
extern void flush_dcache64(addr64_t addr, unsigned count, int phys);
extern void invalidate_icache(vm_offset_t addr, unsigned cnt, int phys);
extern void invalidate_icache64(addr64_t addr, unsigned cnt, int phys);
extern void clean_dcache(vm_offset_t addr, unsigned count, int phys);
extern void clean_dcache64(addr64_t addr, unsigned count, int phys);
extern void CleanPoC_Dcache(void);
extern void CleanPoU_Dcache(void);
extern void CleanPoC_DcacheRegion(vm_offset_t va, size_t length);
extern void CleanPoC_DcacheRegion_Force(vm_offset_t va, size_t length);
extern void CleanPoU_DcacheRegion(vm_offset_t va, size_t length);
extern void FlushPoC_Dcache(void);
extern void FlushPoU_Dcache(void);
extern void FlushPoC_DcacheRegion(vm_offset_t va, size_t length);
extern void InvalidatePoU_Icache(void);
extern void InvalidatePoU_IcacheRegion(vm_offset_t va, size_t length);
extern void cache_sync_page(ppnum_t pp);
extern void platform_cache_init(void);
extern void platform_cache_idle_enter(void);
extern void platform_cache_flush(void);
extern boolean_t platform_cache_batch_wimg(unsigned int new_wimg, unsigned int size);
extern void platform_cache_flush_wimg(unsigned int new_wimg);
extern void platform_cache_clean(void);
extern void platform_cache_shutdown(void);
extern void platform_cache_disable(void);
extern arm_cpu_info_t *cpuid_info(void);
extern int cpuid_get_cpufamily(void);
extern int cpuid_get_cpusubfamily(void);
extern void do_debugid(void);
extern arm_debug_info_t *arm_debug_info(void);
extern void do_cacheid(void);
extern cache_info_t *cache_info(void);
extern cache_info_t *cache_info_type(cluster_type_t cluster_type);
extern void do_mvfpid(void);
extern arm_mvfp_info_t *arm_mvfp_info(void);
void machine_do_debugid(void);
arm_debug_info_t *machine_arm_debug_info(void);
void machine_do_mvfpid(void);
arm_mvfp_info_t *machine_arm_mvfp_info(void);
uint32_t machine_read_midr(void);
uint32_t machine_read_clidr(void);
uint32_t machine_read_ccsidr(void);
void machine_write_csselr(csselr_cache_level level, csselr_cache_type type);
__BEGIN_DECLS
extern uint64_t _get_cpu_capabilities( void );
__END_DECLS

__inline static
int
_NumCPUs( void )
{
	return (_get_cpu_capabilities() & kNumCPUs) >> kNumCPUsShift;
}


typedef 

__BEGIN_DECLS
extern vm_address_t                             _get_commpage_priv_address(void);
extern vm_address_t                             _get_commpage_ro_address(void);
extern vm_address_t                             _get_commpage_text_priv_address(void);
__END_DECLS











_Static_assert((_COMM_PAGE64_BASE_ADDRESS >= _COMM_PAGE64_NESTING_START) &&
    (_COMM_PAGE64_BASE_ADDRESS < (_COMM_PAGE64_NESTING_START + _COMM_PAGE64_NESTING_SIZE)),
    "_COMM_PAGE64_BASE_ADDRESS is not within the nesting region. Commpage nesting "
    "region probably needs to be updated.");
__ASSUME_PTR_ABI_SINGLE_BEGIN

static inline __attribute__((const)) thread_t
current_thread_fast(void)
{
	
	unsigned long result;
	__asm__ ("mrs %0, TPIDR_EL1" : "=r" (result));
	return __unsafe_forge_single(thread_t, result);
}


static inline thread_t
current_thread_volatile(void)
{
	
	unsigned long result;
	__asm__ volatile ("mrs %0, TPIDR_EL1" : "=r" (result));
	return __unsafe_forge_single(thread_t, result);
}


static inline vm_offset_t
exception_stack_pointer(void)
{
	vm_offset_t result = 0;
	__asm__ volatile (
                 "msr		SPSel, #1  \n"
                 "mov		%0, sp     \n"
                 "msr		SPSel, #0  \n"
                 : "=r" (result));

	return result;
}



extern int                       get_preemption_level(void);
extern unsigned int              get_preemption_level_for_thread(thread_t);
static_assert((CPUWINDOWS_BASE >= VM_MIN_KERNEL_ADDRESS) && ((CPUWINDOWS_TOP - 1) <= VM_MAX_KERNEL_ADDRESS),
    "CPU copy windows too large for CPUWINDOWS_BASE_MASK value");
__options_closed_decl(cpu_signal_t, unsigned int, {
	SIGPnop         = 0x00000000U,     
	
	
	SIGPxcall       = 0x00000004U,     
	SIGPast         = 0x00000008U,     
	SIGPdebug       = 0x00000010U,     
	SIGPLWFlush     = 0x00000020U,     
	SIGPLWClean     = 0x00000040U,     
	
	SIGPkppet       = 0x00000100U,     
	SIGPxcallImm    = 0x00000200U,     
	SIGPTimerLocal  = 0x00000400U,     

	SIGPdisabled    = 0x80000000U,     
});
PERCPU_DECL(cpu_data_t, cpu_data);
extern cpu_data_t      *cpu_datap(int cpu);
extern cpu_data_t      *cpu_data_alloc(boolean_t is_boot);
extern void             cpu_stack_alloc(cpu_data_t*);
extern void             cpu_data_init(cpu_data_t *cpu_data_ptr);
extern void             cpu_data_register(cpu_data_t *cpu_data_ptr);
extern cpu_data_t      *processor_to_cpu_datap( processor_t processor);
extern void                                             cpu_bootstrap(
	void);
extern void                                             cpu_init(
	void);
extern void                                             cpu_timebase_init(boolean_t from_boot);
extern kern_return_t                    cpu_signal(
	cpu_data_t              *target,
	cpu_signal_t    signal,
	void                    *p0,
	void                    *p1);
extern kern_return_t                    cpu_signal_deferred(
	cpu_data_t              *target);
extern void                     cpu_signal_cancel(
	cpu_data_t              *target);
extern bool cpu_has_SIGPdebug_pending(void);
__BEGIN_DECLS

extern int      cpu_number(void);
extern int      cpu_cluster_id(void);
__BEGIN_DECLS


typedef 


typedef 






typedef 


const ml_topology_info_t *ml_get_topology_info(void);
extern uint64_t _get_x86_64_cpu_capabilities(void);
#pragma once


__BEGIN_DECLS

typedef arm_thread_state64_t __attribute__((aligned(16))) dbgwrap_thread_state_t;
static inline const char*
ml_dbgwrap_strerror(dbgwrap_status_t status)
{
	switch (status) {
	case DBGWRAP_ERR_SELF_HALT:             return "CPU attempted to halt itself";
	case DBGWRAP_ERR_UNSUPPORTED:           return "halt not supported for this configuration";
	case DBGWRAP_ERR_INPROGRESS:            return "halt in progress on another CPU";
	case DBGWRAP_ERR_INSTR_ERROR:           return "instruction-stuffing failure";
	case DBGWRAP_ERR_INSTR_TIMEOUT:         return "instruction-stuffing timeout";
	case DBGWRAP_ERR_HALT_TIMEOUT:          return "halt ack timeout, CPU likely wedged";
	case DBGWRAP_SUCCESS:                   return "halt succeeded";
	case DBGWRAP_WARN_ALREADY_HALTED:       return "CPU already halted";
	case DBGWRAP_WARN_CPU_OFFLINE:          return "CPU offline";
	default:                                return "unrecognized status";
	}
}

boolean_t ml_dbgwrap_cpu_is_halted(int cpu_index);
dbgwrap_status_t ml_dbgwrap_wait_cpu_halted(int cpu_index, uint64_t timeout_ns);
dbgwrap_status_t ml_dbgwrap_halt_cpu(int cpu_index, uint64_t timeout_ns);
dbgwrap_status_t ml_dbgwrap_halt_cpu_with_state(int cpu_index, uint64_t timeout_ns, dbgwrap_thread_state_t *state);
extern vm_address_t             thread_get_cthread_self(void);
extern kern_return_t            thread_set_cthread_self(vm_address_t);
uintptr_t                       get_tpidrro(void);
void                            set_tpidrro(uintptr_t);
extern void cpu_machine_init(void);
extern kern_return_t cpu_register(int *slot_nump);
extern void cpu_signal_handler(void);
extern void cpu_signal_handler_internal(boolean_t disable_signal);
extern void cpu_doshutdown(void (*doshutdown)(processor_t), processor_t processor);
extern void cpu_idle(void);
extern void cpu_idle_exit(boolean_t from_reset) __attribute__((noreturn));
extern void cpu_idle_tickle(void);
extern void cpu_machine_idle_init(boolean_t from_boot);
extern void arm_init_cpu(cpu_data_t *args, uint64_t hib_header_phys);
extern void arm_init_idle_cpu(cpu_data_t *args);
extern void init_cpu_timebase(boolean_t enable_fiq);
bool
wfe_to_deadline_or_interrupt(uint32_t cid, uint64_t wfe_deadline, cpu_data_t *cdp, bool unmask, bool check_cluster_recommendation);
arm_isa_feat1_reg machine_read_isa_feat1(void);
void ml_cpu_signal(unsigned int cpu_id);
void ml_cpu_signal_deferred_adjust_timer(uint64_t nanosecs);
uint64_t ml_cpu_signal_deferred_get_timer(void);
void ml_cpu_signal_deferred(unsigned int cpu_id);
void ml_cpu_signal_retract(unsigned int cpu_id);
extern void ml_wait_for_cpu_signal_to_enable(void);
extern void assert_ml_cpu_signal_is_enabled(bool enabled);
void    ml_init_interrupt(void);
boolean_t ml_get_interrupts_enabled(void);
boolean_t ml_set_interrupts_enabled_with_debug(boolean_t enable, boolean_t debug);
boolean_t ml_set_interrupts_enabled(boolean_t enable);
boolean_t ml_early_set_interrupts_enabled(boolean_t enable);
boolean_t sched_perfcontrol_ml_set_interrupts_without_measurement(boolean_t enable);
void sched_perfcontrol_abandon_preemption_disable_measurement(void);
boolean_t ml_at_interrupt_context(void);
void ml_cause_interrupt(void);
void siq_init(void);
void siq_cpu_init(void);
bool ml_did_interrupt_userspace(void);
extern bool ml_snoop_thread_is_on_core(thread_t thread);
extern boolean_t ml_is_quiescing(void);
extern void ml_set_is_quiescing(boolean_t);
extern uint64_t ml_get_booter_memory_size(void);
kern_return_t ex_cb_register(
	ex_cb_class_t   cb_class,
	ex_cb_t                 cb,
	void                    *refcon );
ex_cb_action_t ex_cb_invoke(
	ex_cb_class_t   cb_class,
	vm_offset_t         far);
void ml_parse_cpu_topology(void);
unsigned int ml_get_cpu_count(void);
unsigned int ml_get_cpu_number_type(cluster_type_t cluster_type, bool logical, bool available);
unsigned int ml_get_cluster_number_type(cluster_type_t cluster_type);
unsigned int ml_cpu_cache_sharing(unsigned int level, cluster_type_t cluster_type, bool include_all_cpu_types);
unsigned int ml_get_cpu_types(void);
int ml_get_boot_cpu_number(void);
int ml_get_cpu_number(uint32_t phys_id);
unsigned int ml_get_cpu_number_local(void);
int ml_get_cluster_number(uint32_t phys_id);
int ml_get_max_cpu_number(void);
int ml_get_max_cluster_number(void);
unsigned int ml_get_first_cpu_id(unsigned int cluster_id);
unsigned int ml_get_die_id(unsigned int cluster_id);
unsigned int ml_get_die_cluster_id(unsigned int cluster_id);
unsigned int ml_get_max_die_id(void);
int ml_get_cluster_number_local(void);
cluster_type_t ml_get_boot_cluster_type(void);
void ml_map_cpu_pio(void);
kern_return_t ml_processor_register(ml_processor_info_t *ml_processor_info,
    processor_t *processor, ipi_handler_t *ipi_handler,
    perfmon_interrupt_handler_func *pmi_handler);
kern_return_t ml_lockdown_handler_register(lockdown_handler_t, void *);
kern_return_t ml_mcache_flush_callback_register(mcache_flush_function func, void *service);
kern_return_t ml_mcache_flush(void);
void ml_lockdown_init(void);
__printflike(1, 0)
void ml_panic_trap_to_debugger(const char *panic_format_str,
    va_list *panic_args,
    unsigned int reason,
    void *ctx,
    uint64_t panic_options_mask,
    unsigned long panic_caller,
    const char *panic_initiator);
void ml_install_interrupt_handler(
	void *nub,
	int source,
	void *target,
	IOInterruptHandler handler,
	void *refCon);
vm_offset_t
    ml_static_vtop(
	vm_offset_t);
kern_return_t
ml_static_verify_page_protections(
	uint64_t base, uint64_t size, vm_prot_t prot);
vm_offset_t
    ml_static_ptovirt(
	vm_offset_t);
uint64_t ml_get_abstime_offset(void);
uint64_t ml_get_conttime_offset(void);
ml_page_protection_t ml_page_protection_type(void);
void ml_hibernate_active_pre(void);
void ml_hibernate_active_post(void);
void ml_report_minor_badness(uint32_t badness_id);
uint64_t ml_get_backtrace_pc(struct arm_saved_state *state);
bool ml_is_secure_hib_supported(void);
bool ml_task_uses_1ghz_timebase(const task_t task);
bool ml_paddr_is_exclaves_owned(vm_offset_t paddr);
extern void machine_startup(__unused boot_args *args) __attribute__((noinline));
extern void arm_auxkc_init(void *mh, void *base);
extern void arm_vm_init(uint64_t memory_size, boot_args *args);
extern void arm_vm_prot_init(boot_args *args);
extern void arm_vm_prot_finalize(boot_args *args);
extern void arm_set_kernel_tbi(void);
void __attribute__((__noreturn__)) _was_in_userspace(void);
extern kern_return_t DebuggerXCallEnter(boolean_t, bool);
extern void DebuggerXCallReturn(void);
extern void Load_context(thread_t);
extern void Idle_load_context(void) __attribute__((noreturn));
extern thread_t Switch_context(thread_t, thread_continue_t, thread_t);
extern thread_t Shutdown_context(void (*doshutdown)(processor_t), processor_t  processor);
extern void __dead2 Call_continuation(thread_continue_t, void *, wait_result_t, boolean_t enable_interrupts);
extern arm_sme_version_t arm_sme_version(void) __pure2;
extern void arm_get_matrix_cpu_state(struct arm_matrix_cpu_state *cpu_state);
extern void DebuggerCall(unsigned int reason, void *ctx);
extern void DebuggerXCall(void *ctx);
extern int copyout_kern(const char *kernel_addr, user_addr_t user_addr, vm_size_t nbytes);
extern int copyin_kern(const user_addr_t user_addr, char *kernel_addr, vm_size_t nbytes);
extern void bcopy_phys(addr64_t from, addr64_t to, vm_size_t nbytes);
extern void bcopy_phys_with_options(addr64_t from, addr64_t to, vm_size_t nbytes, int options);
extern void dcache_incoherent_io_flush64(addr64_t pa, unsigned int count, unsigned int remaining, unsigned int *res);
extern void dcache_incoherent_io_store64(addr64_t pa, unsigned int count, unsigned int remaining, unsigned int *res);
extern void copy_legacy_debug_state(arm_legacy_debug_state_t * src, arm_legacy_debug_state_t *target, __unused boolean_t all);
extern void copy_debug_state32(arm_debug_state32_t * src, arm_debug_state32_t *target, __unused boolean_t all);
extern void copy_debug_state64(arm_debug_state64_t * src, arm_debug_state64_t *target, __unused boolean_t all);
extern boolean_t debug_legacy_state_is_valid(arm_legacy_debug_state_t *ds);
extern boolean_t debug_state_is_valid32(arm_debug_state32_t *ds);
extern boolean_t debug_state_is_valid64(arm_debug_state64_t *ds);
extern int copyio_check_user_addr(user_addr_t user_addr, vm_size_t nbytes);
extern int apply_func_phys(addr64_t src64, vm_size_t bytes, int (*func)(void * buffer, vm_size_t bytes, void * arg), void * arg);
extern int  pal_serial_init(void);
extern void pal_serial_putc(char a);
extern void pal_serial_putc_nocr(char a);
extern int  pal_serial_getc(void);
#pragma once





PERCPU_DECL(struct _preemption_disable_pcpu, _preemption_disable_pcpu_data);
extern void rtclock_intr(unsigned int);
extern boolean_t SetIdlePop(void);
extern void ClearIdlePop(boolean_t);
extern void rtclock_early_init(void);
extern void     arm_usimple_lock_init(simple_lock_t, __unused unsigned short);
static inline long
ml_make_pcpu_base_and_cpu_number(long base, uint16_t cpu)
{
	return (base << 16) | cpu;
}

extern struct arm_saved_state *    get_user_regs(thread_t);
extern struct arm_saved_state *    find_user_regs(thread_t);
extern struct arm_saved_state *    find_kern_regs(thread_t);
extern struct arm_vfpsaved_state * find_user_vfp(thread_t);
extern arm_debug_state32_t *       find_debug_state32(thread_t);
extern arm_debug_state32_t *       find_or_allocate_debug_state32(thread_t);
extern arm_debug_state64_t *       find_debug_state64(thread_t);
extern arm_debug_state64_t *       find_or_allocate_debug_state64(thread_t);
extern void                        set_user_neon_reg(thread_t, unsigned int, uint128_t);
extern void arm_debug_set(arm_debug_state_t *debug_state);
extern void arm_debug_set32(arm_debug_state_t *debug_state);
extern void arm_debug_set64(arm_debug_state_t *debug_state);
extern void *act_thread_csave(void);
extern void act_thread_catt(void *ctx);
extern void act_thread_cfree(void *ctx);
extern boolean_t arm_force_fast_fault(ppnum_t, vm_prot_t, int, void *);
extern kern_return_t arm_fast_fault(pmap_t, vm_map_address_t, vm_prot_t, bool, bool);
#pragma once


__enum_closed_decl(cpc_event_policy_t, unsigned int, {
	CPC_EVPOL_DENY_ALL = 0,
	CPC_EVPOL_ALLOW_ALL,
	CPC_EVPOL_RESTRICT_TO_KNOWN,
});
cpc_event_policy_t cpc_get_event_policy(void);
void cpc_set_event_policy(cpc_event_policy_t new_policy);
void secure_hmac_init(void);
vm_address_t secure_hmac_get_reg_base(void);
vm_address_t secure_hmac_get_aes_reg_base(void);
vm_address_t secure_hmac_get_aes_offset(void);
void secure_hmac_hibernate_begin(
	secure_hmac_hib_state_t state,
	uint64_t *io_buffer_pages,
	uint32_t num_io_buffer_pages);
void secure_hmac_hibernate_end(void);
void secure_hmac_reset(secure_hmac_hib_state_t state, bool wired_pages);
int secure_hmac_update_and_compress_page(
	secure_hmac_hib_state_t state,
	ppnum_t page_number,
	const void **uncompressed,
	const void **encrypted,
	void *compressed);
void secure_hmac_final(secure_hmac_hib_state_t state, uint8_t *output, size_t output_len);
uint64_t secure_hmac_fetch_hibseg_and_info(
	 void *buffer,
	 uint64_t buffer_len,
	 IOHibernateHibSegInfo *info);
void secure_hmac_compute_rorgn_hmac(void);
void secure_hmac_fetch_rorgn_sha(uint8_t *output, size_t output_len);
void secure_hmac_fetch_rorgn_hmac(uint8_t *output, size_t output_len);
void secure_hmac_finalize_image(
	const void *image_hash,
	size_t image_hash_len,
	uint8_t *hmac,
	size_t hmac_len);
void secure_hmac_get_io_ranges(const hib_phys_range_t **io_ranges, size_t *num_io_ranges);
#pragma pack(8) 
typedef 
#pragma pack()

extern lowglo lowGlo;
void patch_low_glo(void);
void patch_low_glo_static_region(uint64_t address, uint64_t size);
void patch_low_glo_vm_page_info(void *, void *, uint32_t);
__BEGIN_DECLS
void mach_bridge_recv_timestamps(uint64_t bridgeTimestamp, uint64_t localTimestamp);
void mach_bridge_init_timestamp(void);
void mach_bridge_set_params(uint64_t local_timestamp, uint64_t remote_timestamp, double rate);
void pal_hib_get_stack_pages(vm_offset_t *first_page, vm_offset_t *page_count);
void pal_hib_resume_tramp(uint32_t headerPpnum);
void atm_init(void);
void atm_reset(void);
kern_return_t atm_set_diagnostic_config(uint32_t);
uint32_t atm_get_diagnostic_config(void);
os_refgrp_decl(static, bank_elem_refgrp, "bank element", NULL);
extern void bank_task_destroy(task_t);
extern void bank_task_initialize(task_t task);
extern void bank_billed_balance_safe(task_t task, uint64_t *cpu_time, uint64_t *energy);
extern void bank_billed_balance(bank_task_t bank_task, uint64_t *cpu_time, uint64_t *energy);
extern void bank_serviced_balance_safe(task_t task, uint64_t *cpu_time, uint64_t *energy);
extern void bank_serviced_balance(bank_task_t bank_task, uint64_t *cpu_time, uint64_t *energy);
extern kern_return_t bank_get_bank_ledger_thread_group_and_persona(ipc_voucher_t voucher,
    ledger_t *bankledger, struct thread_group **banktg, uint32_t *persona_id);
extern uint64_t bank_get_bank_ledger_resource_coalition_id(ipc_voucher_t voucher);
extern void bank_swap_thread_bank_ledger(thread_t thread, ledger_t ledger);
void serial_keyboard_init(void);
void serial_keyboard_start(void) __dead2;
void serial_keyboard_poll(void) __dead2;
void console_init(void);
int _serial_getc(bool wait);
int _vcgetc(bool wait);
boolean_t console_is_serial(void);
int switch_to_serial_console(void);
int switch_to_video_console(void);
void switch_to_old_console(int old_console);
void console_printbuf_state_init(struct console_printbuf_state * data, int write_on_newline, int can_block);
void console_printbuf_putc(int ch, void *arg);
void console_printbuf_clear(struct console_printbuf_state * info);
int console_write_try(char * str, int size);
void vcputc(char);
void vcputc_options(char, bool);
void video_scroll_up(   void    *start,
    void    *end,
    void    *dest );
void video_scroll_down( void    *start,  
    void    *end,                        
    void    *dest );
void vc_progress_initialize( vc_progress_element * desc,
    const unsigned char * data1x,
    const unsigned char * data2x,
    const unsigned char * data3x,
    const unsigned char * clut );
void vc_progress_set(boolean_t enable, uint32_t vc_delay);
void vc_display_icon( vc_progress_element * desc, const unsigned char * data );
int vc_display_lzss_icon(uint32_t dst_x, uint32_t dst_y,
    uint32_t image_width, uint32_t image_height,
    const uint8_t *compressed_image,
    uint32_t       compressed_size,
    const uint8_t *clut);
extern void vc_enable_progressmeter(int new_value);
extern void vc_set_progressmeter(int new_value);
extern void vc_progress_setdiskspeed(uint32_t speed);
void ccdigest_final_64be(const struct ccdigest_info *di, ccdigest_ctx_t,
    unsigned char *digest);
void ccdigest_final_64le(const struct ccdigest_info *di, ccdigest_ctx_t,
    unsigned char *digest);
void ccdrbg_factory_trng(struct ccdrbg_info *info);
int ccmode_gcm_init(const struct ccmode_gcm *gcm, ccgcm_ctx *ctx,
    size_t rawkey_len, const void *rawkey);
int ccmode_gcm_set_iv(ccgcm_ctx *ctx, size_t iv_nbytes, const void *iv);
int ccmode_gcm_aad(ccgcm_ctx *ctx, size_t nbytes, const void *in);
int ccmode_gcm_decrypt(ccgcm_ctx *ctx, size_t nbytes, const void *in,
    void *out);
int ccmode_gcm_encrypt(ccgcm_ctx *ctx, size_t nbytes, const void *in,
    void *out);
int ccmode_gcm_finalize(ccgcm_ctx *key, size_t tag_nbytes, void *tag);
int ccmode_gcm_reset(ccgcm_ctx *key);
void ccmode_gcm_gf_mult(const unsigned char *a, const unsigned char *b, unsigned char *c);
void ccmode_gcm_gf_mult_32(const unsigned char *a, const unsigned char *b, unsigned char *c);
void ccmode_gcm_gf_mult_64(const unsigned char *a, const unsigned char *b, unsigned char *c);
void ccmode_gcm_mult_h(ccgcm_ctx *key, unsigned char *I);
CC_NONNULL_ALL
void inc_uint(uint8_t *buf, size_t nbytes);
CC_NONNULL_ALL
void ccmode_gcm_update_pad(ccgcm_ctx *key);
CC_NONNULL_ALL
void ccmode_gcm_aad_finalize(ccgcm_ctx *key);
void ccmode_xts_mult_alpha(cc_unit *tweak);
int ccmode_cbc_init(const struct ccmode_cbc *cbc, cccbc_ctx *ctx,
    size_t rawkey_len, const void *rawkey);
int ccmode_cbc_decrypt(const cccbc_ctx *ctx, cccbc_iv *iv, size_t nblocks,
    const void *in, void *out);
int ccmode_cbc_encrypt(const cccbc_ctx *ctx, cccbc_iv *iv, size_t nblocks,
    const void *in, void *out);
CC_INLINE
const struct ccmode_ecb *
ccmode_cbc_key_ecb(const cccbc_ctx *K)
{
	return ((const struct _ccmode_cbc_key *)K)->ecb;
}

CC_INLINE
const ccecb_ctx *
ccmode_cbc_key_ecb_key(const cccbc_ctx *K)
{
	return (const ccecb_ctx *)&((const struct _ccmode_cbc_key *)K)->u[0];
}

int ccmode_cfb_init(const struct ccmode_cfb *cfb, cccfb_ctx *ctx,
    size_t rawkey_len, const void *rawkey,
    const void *iv);
int ccmode_cfb_decrypt(cccfb_ctx *ctx, size_t nbytes,
    const void *in, void *out);
int ccmode_cfb_encrypt(cccfb_ctx *ctx, size_t nbytes,
    const void *in, void *out);
int ccmode_cfb8_init(const struct ccmode_cfb8 *cfb8, cccfb8_ctx *ctx,
    size_t rawkey_len, const void *rawkey, const void *iv);
int ccmode_cfb8_decrypt(cccfb8_ctx *ctx, size_t nbytes,
    const void *in, void *out);
int ccmode_cfb8_encrypt(cccfb8_ctx *ctx, size_t nbytes,
    const void *in, void *out);
int ccmode_ctr_init(const struct ccmode_ctr *ctr, ccctr_ctx *ctx,
    size_t rawkey_len, const void *rawkey, const void *iv);
int ccmode_ctr_setctr(const struct ccmode_ctr *mode, ccctr_ctx *ctx, const void *ctr);
int ccmode_ctr_crypt(ccctr_ctx *ctx, size_t nbytes,
    const void *in, void *out);
CC_INLINE int
ccctr_setctr(const struct ccmode_ctr *mode, ccctr_ctx *ctx, const void *ctr)
{
	return mode->setctr(mode, ctx, ctr);
}

int ccmode_ofb_init(const struct ccmode_ofb *ofb, ccofb_ctx *ctx,
    size_t rawkey_len, const void *rawkey,
    const void *iv);
int ccmode_ofb_crypt(ccofb_ctx *ctx, size_t nbytes,
    const void *in, void *out);
int ccmode_xts_init(const struct ccmode_xts *xts, ccxts_ctx *ctx,
    size_t key_nbytes, const void *data_key,
    const void *tweak_key);
void ccmode_xts_key_sched(const struct ccmode_xts *xts, ccxts_ctx *ctx,
    size_t key_nbytes, const void *data_key,
    const void *tweak_key);
void *ccmode_xts_crypt(const ccxts_ctx *ctx, ccxts_tweak *tweak,
    size_t nblocks, const void *in, void *out);
int ccmode_xts_set_tweak(const ccxts_ctx *ctx, ccxts_tweak *tweak,
    const void *iv);
CC_INLINE
const struct ccmode_ecb *
ccmode_xts_key_ecb(const ccxts_ctx *K)
{
	return ((const struct _ccmode_xts_key *)K)->ecb;
}

CC_INLINE
const struct ccmode_ecb *
ccmode_xts_key_ecb_encrypt(const ccxts_ctx *K)
{
	return ((const struct _ccmode_xts_key *)K)->ecb_encrypt;
}

CC_INLINE
const ccecb_ctx *
ccmode_xts_key_data_key(const ccxts_ctx *K)
{
	return (const ccecb_ctx *)&((const struct _ccmode_xts_key *)K)->u[0];
}

CC_INLINE
const ccecb_ctx *
ccmode_xts_key_tweak_key(const ccxts_ctx *K)
{
	return (const ccecb_ctx *)&((const struct _ccmode_xts_key *)K)->u[ccn_nof_size(ccmode_xts_key_ecb(K)->size)];
}








int ccmode_ccm_init(const struct ccmode_ccm *ccm, ccccm_ctx *ctx,
    size_t rawkey_len, const void *rawkey);
int ccmode_ccm_set_iv(ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nonce_len, const void *nonce,
    size_t mac_size, size_t auth_len, size_t data_len);
void ccmode_ccm_macdata(ccccm_ctx *key, ccccm_nonce *nonce_ctx, unsigned new_block, size_t nbytes, const void *in);
int ccmode_ccm_cbcmac(ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const void *in);
void ccmode_ccm_crypt(ccccm_ctx *key, ccccm_nonce *nonce_ctx, size_t nbytes, const void *in, void *out);
int ccmode_ccm_decrypt(ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const void *in,
    void *out);
int ccmode_ccm_encrypt(ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const void *in,
    void *out);
int ccmode_ccm_finalize(ccccm_ctx *key, ccccm_nonce *nonce_ctx, void *mac);
int ccmode_ccm_finalize_and_verify(ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, void *mac);
int ccmode_ccm_reset(ccccm_ctx *key, ccccm_nonce *nonce_ctx);
int ccmode_omac_decrypt(ccomac_ctx *ctx, size_t nblocks,
    const void *tweak, const void *in, void *out);
int ccmode_omac_encrypt(ccomac_ctx *ctx, size_t nblocks,
    const void *tweak, const void *in, void *out);
int ccmode_omac_init(const struct ccmode_omac *omac, ccomac_ctx *ctx,
    size_t tweak_len, size_t rawkey_len,
    const void *rawkey);
CC_PTRCHECK_CAPABLE_HEADER()



CC_NONNULL((2, 3))
void ccn_set(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s);
CC_INLINE
CC_NONNULL((2, 4))
void
ccn_setn(cc_size n, cc_unit *cc_counted_by (n)r, const cc_size s_size, const cc_unit *cc_counted_by (s_size)s)
{
	cc_assert(n > 0 && s_size <= n);

	if (s_size > 0) {
		ccn_set(s_size, r, s);
	}

	ccn_zero(n - s_size, r + s_size);
}

CC_INLINE
CC_NONNULL((2))
void
ccn_clear(cc_size n, cc_unit *cc_sized_by (n)r)
{
	cc_clear(ccn_sizeof_n(n), r);
}


CC_INLINE cc_unit
ccn_bit(const cc_unit *cc_indexable x, size_t k)
{
	return 1 & (x[k >> CCN_LOG2_BITS_PER_UNIT] >> (k & (CCN_UNIT_BITS - 1)));
}


CC_WARN_RESULT
cc_unit ccn_abs(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit *cc_counted_by(n) t);
CC_WARN_RESULT
CC_NONNULL((2))
size_t ccn_trailing_zeros(cc_size n, const cc_unit *s);
CC_NONNULL_ALL
void ccn_shift_right(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, size_t k) __asm__("_ccn_shift_right");
CC_NONNULL_ALL
void ccn_shift_right_multi(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, size_t k);
CC_NONNULL_ALL
void ccn_shift_left(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, size_t k) __asm__("_ccn_shift_left");
CC_NONNULL_ALL
void ccn_shift_left_multi(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, size_t k);
CC_NONNULL_ALL
void ccn_cond_swap(cc_size n, cc_unit ki, cc_unit *cc_counted_by(n) r0, cc_unit *cc_counted_by(n) r1);
CC_NONNULL_ALL
void ccn_cond_shift_right(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) a, size_t k);
void ccn_cond_neg(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) x);
CC_NONNULL_ALL
void ccn_cond_shift_right_carry(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) a, size_t k, cc_unit c);
CC_WARN_RESULT CC_NONNULL_ALL
cc_unit ccn_cond_add(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) x, const cc_unit *cc_counted_by(n) y);
CC_WARN_RESULT CC_NONNULL_ALL
cc_unit ccn_cond_rsub(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) x, const cc_unit *cc_counted_by(n) y);
CC_WARN_RESULT CC_NONNULL_ALL
cc_unit ccn_cond_sub(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) x, const cc_unit *cc_counted_by(n) y);
CC_NONNULL_ALL
void ccn_cond_clear(cc_size n, cc_unit s, cc_unit *r);
CC_NONNULL_ALL
void ccn_mux(cc_size n, cc_unit s, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) a, const cc_unit *cc_counted_by(n) b);
CC_WARN_RESULT
CC_NONNULL_ALL
size_t ccn_gcd_ws(cc_ws_t ws, cc_size rn, cc_unit *cc_counted_by(rn) r, cc_size sn, const cc_unit *cc_counted_by(sn) s, cc_size tn, const cc_unit *cc_counted_by(tn) t);
void ccn_lcm_ws(cc_ws_t ws, cc_size n, cc_unit *cc_unsafe_indexable r2n, const cc_unit *cc_counted_by(n)s, const cc_unit *cc_counted_by(n)t);
CC_NONNULL((2, 3, 4))
void ccn_mul(cc_size n, cc_unit *cc_unsafe_indexable r_2n, const cc_unit *cc_counted_by(n)s, const cc_unit *cc_counted_by(n)t) __asm__("_ccn_mul");
CC_WARN_RESULT
CC_NONNULL((2, 3))
cc_unit ccn_mul1(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit v);
CC_NONNULL_ALL
void ccn_muln(cc_size n, cc_unit *cc_counted_by(n + nv) r, const cc_unit *cc_counted_by(n) s, cc_size nv, const cc_unit *cc_counted_by(n) v);
CC_WARN_RESULT
CC_NONNULL((2, 3))
cc_unit ccn_addmul1(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit v);
CC_NONNULL_ALL
void ccn_mul_ws(cc_ws_t ws, cc_size count, cc_unit *cc_unsafe_indexable r, const cc_unit *cc_counted_by(count)s, const cc_unit *cc_counted_by(count)t);
CC_NONNULL_ALL
void ccn_sqr_ws(cc_ws_t ws, cc_size n, cc_unit *cc_unsafe_indexable r, const cc_unit *cc_counted_by(n)s);
CC_NONNULL_ALL
void ccn_neg(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) x);
CC_WARN_RESULT
CC_CONST CC_NONNULL_ALL
CC_INLINE cc_unit
ccn_invert(cc_unit x)
{
	cc_assert(x & 1);

	
	cc_unit y = (3 * x) ^ 2;

	
	
	y *= 2 - y * x;
	y *= 2 - y * x;
	y *= 2 - y * x;

	cc_assert(y * x == 1);
	return y;
}


CC_NONNULL_ALL
void ccn_div_exact_ws(cc_ws_t ws, cc_size n, cc_unit *cc_counted_by(n) q, const cc_unit *cc_counted_by(n) a, const cc_unit *cc_counted_by(n) d);
CC_WARN_RESULT
CC_NONNULL_ALL
bool ccn_divides1(cc_size n, const cc_unit *cc_counted_by(n)x, cc_unit q);
CC_WARN_RESULT
CC_INLINE cc_unit
ccn_select(cc_size start, cc_size end, const cc_unit *cc_counted_by(end)r, cc_size i)
{
	cc_unit ri = 0;

	for (cc_size j = start; j < end; j++) {
		cc_size i_neq_j; 
		CC_HEAVISIDE_STEP(i_neq_j, i ^ j);
		ri |= r[j] & ((cc_unit)i_neq_j - 1);
	}

	return ri;
}


CC_WARN_RESULT
int ccn_invmod_ws(cc_ws_t ws, cc_size n, cc_unit *cc_counted_by(n) r, cc_size xn, const cc_unit *cc_counted_by(xn) x, const cc_unit *cc_counted_by(n) m);
void ccn_mux_seed_mask(cc_unit seed);
CC_NONNULL((2, 7)) CC_WARN_RESULT
int ccn_divmod(cc_size na, const cc_unit *cc_counted_by(na) a, cc_size nq, cc_unit *cc_counted_by(nq) q, cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) d);
CC_NONNULL((1, 3, 8))
void ccn_divmod_ws(cc_ws_t ws, cc_size na, const cc_unit *cc_counted_by(na) a, cc_size nq, cc_unit *cc_counted_by(nq) q, cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) d);
CC_NONNULL((2)) CC_SENTINEL
void ccn_zero_multi(cc_size n, cc_unit *r, ...);
CC_NONNULL((3, 4, 5))
cc_unit ccn_add_ws(cc_ws_t ws, cc_size count, cc_unit *r, const cc_unit *s, const cc_unit *t);
CC_NONNULL((3, 4, 5))
cc_unit ccn_sub_ws(cc_ws_t ws, cc_size count, cc_unit *r, const cc_unit *s, const cc_unit *t);
CC_NONNULL((3, 4))
cc_unit ccn_add1_ws(cc_ws_t ws, cc_size n, cc_unit *r, const cc_unit *s, cc_unit v);
CC_NONNULL_ALL
cc_unit ccn_addn(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, cc_size nt, const cc_unit *cc_counted_by(nt) t);
CC_NONNULL_ALL
cc_unit ccn_sub1(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, cc_unit v);
CC_NONNULL_ALL
cc_unit ccn_subn(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, cc_size nt, const cc_unit *cc_counted_by(nt) t);
CC_NONNULL_ALL
cc_size ccn_n(cc_size n, const cc_unit *cc_counted_by(n)s);
CC_NONNULL_ALL
int ccn_random_bits(cc_size nbits, cc_unit *cc_unsafe_indexable r, struct ccrng_state *rng);
CC_NONNULL_ALL
int ccn_random_bits_fips(cc_size nbits, cc_unit *cc_unsafe_indexable r, struct ccrng_state *rng);
CC_NONNULL_ALL
void ccn_recode_jsf_init(struct ccn_rjsf_state *r, size_t nbits, const cc_unit *s, const cc_unit *t);
CC_NONNULL_ALL
void ccn_recode_jsf_column(struct ccn_rjsf_state *r, size_t k, int c[2]);
CC_NONNULL_ALL CC_WARN_RESULT
size_t ccn_recode_jsf_index(int c[2]);
CC_NONNULL_ALL CC_WARN_RESULT
int ccn_recode_jsf_direction(int c[2]);
CC_NONNULL_ALL
CC_INLINE void
ccn_read_le_bytes(cc_size n, const uint8_t *in, cc_unit *out)
{
	for (cc_size i = 0; i < n; i++) {
		out[i] = cc_load_le(&in[i * CCN_UNIT_SIZE]);
	}
}


CC_NONNULL_ALL
CC_INLINE void
ccn_write_le_bytes(cc_size n, const cc_unit *in, uint8_t *out)
{
	for (cc_size i = 0; i < n; i++) {
		cc_store_le(in[i], &out[i * CCN_UNIT_SIZE]);
	}
}


CC_NONNULL_ALL
void ccn_recode_ssw(cc_size n, const cc_unit *s, int w, int8_t *r);
void ccsha256_ltc_compress(ccdigest_state_t state, size_t nblocks, const void *buf);
void ccsha512_ltc_compress(ccdigest_state_t state, size_t nblocks, const void *in);
void ccsha512_final(const struct ccdigest_info *di, ccdigest_ctx_t ctx, unsigned char *digest);
void cc_print(const char *label, size_t count, const uint8_t *s);
extern bool cc_rdrand(uint64_t *rand);
extern bool cc_is_vmm_present(void);
extern const char *cc_current_arch(void);
CC_PTRCHECK_CAPABLE_HEADER()



struct cc_ws;
void *cc_malloc_clear(size_t s);
void cc_free(void *p, size_t size);
cc_unit *cc_counted_by(n) cc_ws_alloc(cc_ws_t ws, cc_size n);
void cc_ws_free(cc_ws_t ws);
void cc_ws_free_stack(cc_ws_t ws);
void cc_ws_free_null(cc_ws_t ws);
CC_PURE size_t sizeof_cc_unit(void);
CC_PURE size_t sizeof_struct_ccbfv_cipher_plain_ctx(void);
CC_PURE size_t sizeof_struct_ccbfv_ciphertext(void);
CC_PURE size_t sizeof_struct_ccbfv_dcrt_plaintext(void);
CC_PURE size_t sizeof_struct_ccbfv_decrypt_ctx(void);
CC_PURE size_t sizeof_struct_ccbfv_encrypt_params(void);
CC_PURE size_t sizeof_struct_ccbfv_galois_key(void);
CC_PURE size_t sizeof_struct_ccbfv_param_ctx(void);
CC_PURE size_t sizeof_struct_ccbfv_plaintext(void);
CC_PURE size_t sizeof_struct_ccbfv_relin_key(void);
CC_PURE size_t sizeof_struct_ccdh_full_ctx(void);
CC_PURE size_t sizeof_struct_ccdh_pub_ctx(void);
CC_PURE size_t sizeof_struct_ccec_full_ctx(void);
CC_PURE size_t sizeof_struct_ccec_pub_ctx(void);
CC_PURE size_t sizeof_struct_ccpolyzp_po2cyc(void);
CC_PURE size_t sizeof_struct_ccpolyzp_po2cyc_base_convert(void);
CC_PURE size_t sizeof_struct_ccpolyzp_po2cyc_block_rng_state(void);
CC_PURE size_t sizeof_struct_ccpolyzp_po2cyc_ctx(void);
CC_PURE size_t sizeof_struct_ccpolyzp_po2cyc_ctx_chain(void);
CC_PURE size_t sizeof_struct_ccrns_mul_modulus(void);
CC_PURE size_t sizeof_struct_ccrsa_full_ctx(void);
CC_PURE size_t sizeof_struct_ccrsa_pub_ctx(void);
CC_PURE size_t sizeof_struct_cczp(void);
CC_PURE cc_size CCBFV_CIPHERTEXT_APPLY_GALOIS_WORKSPACE_N(cc_size degree, cc_size num_ctext_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_GALOIS_KEY_SWITCH_WORKSPACE_N(cc_size degree, cc_size num_galois_key_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_PLAINTEXT_ADD_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCBFV_CIPHERTEXT_COEFF_PLAINTEXT_MUL_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_EVAL_PLAINTEXT_MUL_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_ROTATE_ROWS_LEFT_WORKSPACE_N(cc_size degree, cc_size num_ctext_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_ROTATE_ROWS_RIGHT_WORKSPACE_N(cc_size degree, cc_size num_ctext_moduli);
CC_PURE cc_size CCBFV_CIPHERTEXT_SWAP_COLUMNS_WORKSPACE_N(cc_size degree, cc_size num_ctext_moduli);
CC_PURE cc_size CCBFV_CIPHER_PLAIN_CTX_INIT_WORKSPACE_N(cc_size num_moduli);
CC_PURE cc_size CCBFV_DECODE_SIMD_INT64_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_DECODE_SIMD_UINT64_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_DECRYPT_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_DESERIALIZE_SEEDED_CIPHERTEXT_EVAL_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCBFV_ENCRYPT_SYMMETRIC_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_ENCRYPT_ZERO_SYMMETRIC_COEFF_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_ENCRYPT_ZERO_SYMMETRIC_EVAL_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_ENCRYPT_ZERO_SYMMETRIC_HELPER_WORKSPACE_N(cc_size degree, cc_size nmoduli);
CC_PURE cc_size CCBFV_GALOIS_KEY_GENERATE_SINGLE_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_GALOIS_KEY_GENERATE_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCBFV_RELIN_KEY_GENERATE_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCDH_POWER_BLINDED_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCEC_AFFINIFY_POINTS_WORKSPACE_N(cc_size n, cc_size npoints);
CC_PURE cc_size CCN_P224_INV_ASM_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCN_P256_INV_ASM_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCN_P384_INV_ASM_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCN_SQR_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_DIVIDE_AND_ROUND_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_INIT_PUNC_PROD_WORKSPACE_N(cc_size num_moduli);
CC_PURE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_INIT_WORKSPACE_N(cc_size num_moduli);
CC_PURE cc_size CCPOLYZP_PO2CYC_CTX_Q_PROD_WORKSPACE_N(cc_size num_moduli);
CC_PURE cc_size CCPOLYZP_PO2CYC_CTX_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_CTX_INIT_WORKSPACE_N(cc_size n);
CC_PURE cc_size CCPOLYZP_PO2CYC_DESERIALIZE_POLY_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_RANDOM_TERNARY_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_RANDOM_UNIFORM_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_RANDOM_CBD_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_SERIALIZE_POLY_WORKSPACE_N(cc_size degree);
CC_PURE cc_size CCPOLYZP_PO2CYC_WORKSPACE_N(cc_size degree, cc_size num_moduli);
CC_PURE cc_size CCRSA_CRT_POWER_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_CIPHERTEXT_MOD_SWITCH_DOWN_TO_SINGLE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_CIPHERTEXT_MOD_SWITCH_DOWN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_DCRT_PLAINTEXT_ENCODE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_DECRYPT_CTX_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_DESERIALIZE_CIPHERTEXT_COEFF_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_DESERIALIZE_CIPHERTEXT_EVAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_DESERIALIZE_SEEDED_CIPHERTEXT_COEFF_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_GALOIS_KEY_LOAD_SINGLE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_GALOIS_KEY_LOAD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_GALOIS_KEY_SAVE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_PARAM_CTX_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_RELIN_KEY_LOAD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_RELIN_KEY_SAVE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SECRET_KEY_GENERATE_FROM_SEED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SECRET_KEY_GENERATE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SERIALIZE_CIPHERTEXT_COEFF_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SERIALIZE_CIPHERTEXT_EVAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SERIALIZE_SEEDED_CIPHERTEXT_COEFF_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCBFV_SERIALIZE_SEEDED_CIPHERTEXT_EVAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCCKG_CONTRIBUTOR_FINISH_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCCKG_OWNER_GENERATE_SHARE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCCURVE25519_INTERNAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCCURVE448_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDER_DECODE_RSA_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_CHECK_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_COMPUTE_SHARED_SECRET_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_GENERATE_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_GENERATE_PRIVATE_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_IMPORT_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_INIT_GP_FROM_BYTES_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_IS_SAFE_PRIME_GROUP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCDH_PAIRWISE_CONSISTENCY_CHECK_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_ADD_OPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_INV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_MUL121666_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_MUL_OPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_SQR_OPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_SUB_OPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC25519_SUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_INV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC448_SUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_COMPUTE_SHARED_SECRET_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_FAST_COMPUTE_PUB_FROM_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_FAST_COMPUTE_SHARED_SECRET_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_FAST_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_GENERATE_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_PAIRWISE_CONSISTENCY_CHECK_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_PCC_COMPUTE_PUB_FROM_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECDH_PCC_COMPUTE_SHARED_SECRET_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECIES_DECRYPT_GCM_COMPOSITE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECIES_ENCRYPT_GCM_COMPOSITE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCECIES_IMPORT_EPH_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_AFFINE_POINT_FROM_X_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_AFFINIFY_HOMOGENEOUS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_AFFINIFY_JACOBIAN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_AFFINIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_AFFINIFY_X_ONLY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_BLINDING_OP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_GENERATE_KEY_CHECKSIGN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_GENERATE_KEY_CHECKVERIFY_AND_EXTRACT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_GENERATE_KEY_STEP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_GENERATE_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_IMPORT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPACT_TRANSFORM_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_COMPRESSED_X962_IMPORT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DER_EXPORT_DIVERSIFIED_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DER_EXPORT_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DIVERSIFY_PRIV_TWIN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DIVERSIFY_PUB_TWIN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DIVERSIFY_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DIVERSIFY_TWIN_SCALARS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_DOUBLE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_EXTRACT_RS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_FULL_ADD_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_FULL_ADD_NORMALIZED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_FULL_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_FULL_SUB_NORMALIZED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_FULL_SUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_BLINDING_KEYS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_DIVERSIFIED_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_KEY_DETERMINISTIC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_KEY_FIPS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_KEY_INTERNAL_FIPS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_KEY_INTERNAL_LEGACY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_SCALAR_FIPS_EXTRABITS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_SCALAR_FIPS_RETRY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_GENERATE_SCALAR_LEGACY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_IMPORT_AFFINE_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_IMPORT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_IS_COMPACTABLE_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_IS_POINT_PROJECTIVE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_IS_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MAKE_PUB_FROM_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_INNER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_XYCZADDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_XYCZADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_XYCZDBLJAC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_MULT_XYCZRECOVERCOEFFJAC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_PAIRWISE_CONSISTENCY_CHECK_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_PRINT_PROJECTIVE_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_PROJECTIFY_HOMOGENEOUS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_PROJECTIFY_JACOBIAN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_PROJECTIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_RFC6637_UNWRAP_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_RFC6637_WRAP_CORE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_RFC6637_WRAP_KEY_DIVERSIFIED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_RFC6637_WRAP_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_SIGN_COMPOSITE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_SIGN_INTERNAL_INNER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_SIGN_INTERNAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_SIGN_MSG_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_SIGN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_TWIN_MULT_NORMALIZE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_TWIN_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VALIDATE_POINT_AND_PROJECTIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VALIDATE_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_COMPOSITE_DIGEST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_COMPUTEMULTS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_DIGEST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_INTERNAL_WITH_BASE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_INTERNAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_MSG_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_SINGLEMULTS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_STRICT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_VERIFY_TWINMULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_X963_IMPORT_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCEC_X963_IMPORT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_ADD_POINTS_UNIFIED_CACHED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_ADD_POINTS_UNIFIED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_ADD_PRECOMPUTED_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_DBL_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_DECODE_PUBLIC_AND_NEGATE_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_DOUBLE_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_FROM_ED25519_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_FULL_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_HASH_TO_SCALAR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_POINT_TO_CACHED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_TO_ED25519_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED25519_VERIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_ADD_POINTS_UNIFIED_CACHED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_ADD_POINTS_UNIFIED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_ADD_POINTS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_ADD_PRECOMPUTED_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_DBL_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_DECODE_PUBLIC_KEY_AND_NEGATE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_DOUBLE_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_FROM_ED448_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_FULL_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_MAKE_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_POINT_TO_CACHED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_SCALAR_MULT_BASE_MASKED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_SHAKE_TO_SCALAR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_SIGN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_TO_ED448_POINT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCED448_VERIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_CCZP_INV0_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_ENCODE_TO_CURVE_RO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_ENCODE_TO_CURVE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_HASH_TO_BASE_RFC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_HASH_TO_BASE_SAE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_HASH_TO_BASE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_MAP_TO_CURVE_SSWU_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_MAP_TO_CURVE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCH2C_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_ADD1_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_DIVMOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_DIV_EXACT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_GCD_UPDATE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_GCD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_INVMOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_LCM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_MOD_192_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_MOD_521_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_SQRT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P224_TO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P256_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P256_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P256_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P256_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P256_TO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P384_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P384_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P384_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P384_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_P384_TO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCN_SUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_INIT_INV_PUNC_PROD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_POLY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_BASE_CONVERT_Q_INV_MOD_T_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_COEFF_SCALAR_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_CTX_CHAIN_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_CTX_INIT_NTT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_DIVIDE_AND_ROUND_Q_LAST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_EVAL_SCALAR_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_GEN_PRIMITIVE_ROOT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_IS_PRIMITIVE_ROOT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_MIN_PRIMITIVE_ROOT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_MODULUS_TO_CCZP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPOLYZP_PO2CYC_SCALAR_DIVMOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPRIME_PICK_RANDOM_BASE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPRIME_RABIN_MILLER_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPRIME_RABIN_MILLER_ITERATION_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCPRIME_RABIN_MILLER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MODULUS_COMPUTE_MOD_FACTOR_VAR_TIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MODULUS_COMPUTE_MOD_FACTOR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MODULUS_INIT_HELPER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MODULUS_INIT_VAR_TIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MODULUS_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MUL_MODULUS_INIT_VAR_TIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRNS_MUL_MODULUS_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSABSSA_BLIND_MESSAGE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSABSSA_SIGN_BLINDED_MESSAGE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSABSSA_UNBLIND_SIGNATURE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSABSSA_VERIFY_SIGNATURE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_CHECK_DELTA_100BITS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_COMPUTE_R_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_CRT_MAKEKEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_CRT_MAKE_FIPS186_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_DECRYPT_EME_PKCS1V15_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_DECRYPT_EME_PKCS1V15_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_DECRYPT_OAEP_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_DECRYPT_OAEP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_EMSA_PSS_DECODE_CANARY_OUT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_EMSA_PSS_DECODE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_ENCRYPT_EME_PKCS1V15_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_ENCRYPT_OAEP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_ENSURE_2P_GT_Q_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_FIND_NEXT_PRIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_FIND_PRIME_MULTIPLE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_AUXILIARY_PRIMES_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_FIPS186_KEY_TRACE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_FIPS186_PRIME_FACTORS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_KEY_INTERNAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_PRIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_PROBABLE_PRIME_FROM_AUX_PRIMES_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_GENERATE_PROBABLE_PRIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_IMPORT_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_INIT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_IS_VALID_PRIME_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_MAKE_FIPS186_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_MAKE_PRIV_PARSE_INPUT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_MAKE_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_OAEP_DECODE_PARAMETER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_OAEP_ENCODE_PARAMETER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_PAIRWISE_CONSISTENCY_CHECK_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_PRIV_CRYPT_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_PRIV_CRYPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_PUB_CRYPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_RECOVER_PQ_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_RECOVER_PRIV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_SEED_X_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_SIGN_PKCS1V15_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_SIGN_PSS_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_VERIFY_PKCS1V15_DIGEST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_VERIFY_PKCS1V15_INTERNAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_VERIFY_PSS_DIGEST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCRSA_VERIFY_PSS_MSG_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_COMMITMENT_FINALIZE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_COMMITMENT_PARTIAL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_COMMITMENT_SHARED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_CONFIRMATION_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_H2C_COMMIT_FINALIZE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_H2C_COMMIT_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_H2C_PT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GENERATE_KEYSEED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GEN_KEYS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GEN_PASSWORD_VALUE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_GET_KEYS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_VERIFY_COMMITMENT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_VERIFY_CONFIRMATION_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSAE_Y2_FROM_X_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_GENERATE_L_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_IMPORT_PUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_KEX_PROCESS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_LAZY_GEN_XY_XY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_REDUCE_W_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_VERIFIER_INITIALIZE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSPAKE_VERIFIER_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_CLIENT_PROCESS_CHALLENGE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_CLIENT_START_AUTHENTICATION_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_DIGEST_CCN_CCN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_DIGEST_CCN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_DIGEST_UPDATE_CCN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_CLIENT_PUBKEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_CLIENT_S_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_H_AMK_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_K_FROM_S_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_K_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_M_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_SERVER_PUBKEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_SERVER_S_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_U_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_VERIFIER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_GENERATE_V_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_MGF_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_SERVER_COMPUTE_SESSION_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_SERVER_GENERATE_PUBLIC_KEY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSRP_SHA_INTERLEAVE_RFC2945_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_EVALUATE_POLY_TO_BUFFER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_GENERATE_RANDOM_POLY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_GENERATE_SHARE_POLY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_LAGRANGE_PRODUCT_FROM_BAG_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_SHARE_BAG_RECOVER_SECRET_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCSS_SHAMIR_SHARE_GENERATOR_INIT_BACKER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCX25519_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCX448_SCALAR_MULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_ADD_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_ADD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_DIV2_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_FROM_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_GENERATE_NON_ZERO_ELEMENT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_GENERATE_RANDOM_ELEMENT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INIT_COMPUTE_R2_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INV_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INV_FIELD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INV_UPDATE_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INV_UPDATE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_INV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_IS_QUADRATIC_RESIDUE_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_FROM_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_INIT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_INV_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_MOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_POWER_FAST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_POWER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_REDC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_SQRT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MM_TO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MODN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MOD_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MUL_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_MUL_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_POWER_BLINDED_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_POWER_FAST_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_POWER_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQRT_3MOD4_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQRT_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQRT_TONELLI_SHANKS_PRECOMP_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQRT_TONELLI_SHANKS_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQRT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQR_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SQR_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SUB_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_SUB_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_TO_DEFAULT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZP_TO_WORKSPACE_N(cc_size n);
CC_INLINE cc_size CCZ_EXPMOD_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDH_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDSA_KAT_SIGN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDSA_KAT_VERIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDSA_SIGN_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDSA_VERIFY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_ECDSA_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_FFDH_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_RSA_ENC_DEC_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_RSA_OAEP_CONSISTENCY_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_RSA_OAEP_DECRYPT_WORKSPACE_N(cc_size n);
CC_INLINE cc_size FIPSPOST_POST_RSA_OAEP_KAT_WORKSPACE_N(cc_size n);
extern kern_return_t task_mark_corpse(task_t task);
extern kern_return_t task_deliver_crash_notification(task_t, thread_t, exception_type_t, mach_exception_subcode_t);
__options_closed_decl(corpse_flags_t, uint16_t, {
	CORPSE_CRASHINFO_HAS_REF    = 0x1,
	CORPSE_CRASHINFO_USER_FAULT = 0x2
});
extern kcdata_descriptor_t task_get_corpseinfo(task_t task);
extern kcdata_descriptor_t  task_crashinfo_alloc_init(
	mach_vm_address_t crash_data_p,
	unsigned size, corpse_flags_t kc_u_flags, unsigned kc_flags);
extern kcdata_descriptor_t task_btinfo_alloc_init(
	mach_vm_address_t addr, unsigned size);
extern kern_return_t task_crashinfo_destroy(kcdata_descriptor_t data);
extern unsigned long total_corpses_count(void) __attribute__((pure));
extern boolean_t corpses_enabled(void);
extern kern_return_t task_generate_corpse_internal(
	task_t task,
	task_t *corpse_task,
	thread_t *thread,
	exception_type_t etype,
	mach_exception_data_type_t code,
	mach_exception_data_type_t subcode,
	void *reason);
extern void task_clear_corpse(task_t task);
extern kern_return_t task_duplicate_map_and_threads(
	task_t task,
	void *p,
	task_t new_task,
	thread_t *thread,
	uint64_t **udata_buffer,
	int *size,
	int *num_udata,
	bool for_exception);
extern kern_return_t task_enqueue_exception_with_corpse(
	task_t task,
	exception_type_t etype,
	mach_exception_data_t code,
	mach_msg_type_number_t codeCnt,
	void *reason,
	boolean_t lightweight);
extern kern_return_t current_thread_collect_backtrace_info(
	kcdata_descriptor_t *new_desc,
	exception_type_t etype,
	mach_exception_data_t code,
	mach_msg_type_number_t codeCnt,
	void *reason);
extern void task_add_to_corpse_task_list(task_t corpse_task);
void task_remove_from_corpse_task_list(task_t corpse_task);
void task_purge_all_corpses(void);
kern_return_t find_corpse_task_by_uniqueid_grp(uint64_t uid, task_t *target, task_grp_t grp);
extern uint64_t task_corpse_get_crashed_thread_id(task_t corpse_task);
_Static_assert(((DEVICE_PAGER_CONTIGUOUS | DEVICE_PAGER_NOPHYSCACHE) & VM_WIMG_MASK) == 0,
    "device pager flags overlap WIMG mask");
_Static_assert(DEVICE_PAGER_GUARDED == VM_MEM_GUARDED, "DEVICE_PAGER_GUARDED != VM_MEM_GUARDED");
_Static_assert(DEVICE_PAGER_COHERENT == VM_MEM_COHERENT, "DEVICE_PAGER_COHERENT != VM_MEM_COHERENT");
_Static_assert(DEVICE_PAGER_CACHE_INHIB == VM_MEM_NOT_CACHEABLE, "DEVICE_PAGER_CACHE_INHIB != VM_MEM_NOT_CACHEABLE");
_Static_assert(DEVICE_PAGER_WRITE_THROUGH == VM_MEM_WRITE_THROUGH, "DEVICE_PAGER_WRITE_THROUGH != VM_MEM_WRITE_THROUGH");
extern vm_offset_t acpi_install_wake_handler(void);
extern void        acpi_sleep_kernel(acpi_sleep_callback func, void * refcon);
extern void        acpi_idle_kernel(acpi_sleep_callback func, void * refcon);
void install_real_mode_bootstrap(void *prot_entry);
extern uint32_t    acpi_count_enabled_logical_processors(void);
extern int              ml_get_max_affinity_sets(void);
extern processor_set_t  ml_affinity_to_pset(uint32_t affinity_num);
static inline thread_t
get_active_thread_volatile(void)
{
	return CPU_DATA()->cpu_active_thread;
}

static inline __attribute__((const)) thread_t
get_active_thread(void)
{
	return CPU_DATA()->cpu_active_thread;
}



static inline int
get_preemption_level(void)
{
	return CPU_DATA()->cpu_preemption_level;
}
static inline int
get_interrupt_level(void)
{
	return CPU_DATA()->cpu_interrupt_level;
}
static inline int
get_cpu_number(void)
{
	return CPU_DATA()->cpu_number;
}
static inline vm_offset_t
get_current_percpu_base(void)
{
	return CPU_DATA()->cpu_pcpu_base;
}
static inline int
get_cpu_phys_number(void)
{
	return CPU_DATA()->cpu_phys_number;
}

static inline cpu_data_t *
current_cpu_datap(void)
{
	return CPU_DATA()->cpu_this;
}



__header_always_inline void
pltrace(boolean_t plenable)
{
}

static inline void
disable_preemption_internal(void)
{
	assert(get_preemption_level() >= 0);

	os_compiler_barrier();
	CPU_DATA()->cpu_preemption_level++;
	os_compiler_barrier();
	pltrace(FALSE);
}

static inline void
enable_preemption_internal(void)
{
	assert(get_preemption_level() > 0);
	pltrace(TRUE);
	os_compiler_barrier();
	if (0 == --CPU_DATA()->cpu_preemption_level) {
		kernel_preempt_check();
	}
	os_compiler_barrier();
}

static inline void
enable_preemption_no_check(void)
{
	assert(get_preemption_level() > 0);

	pltrace(TRUE);
	os_compiler_barrier();
	CPU_DATA()->cpu_preemption_level--;
	os_compiler_barrier();
}

static inline void
_enable_preemption_no_check(void)
{
	enable_preemption_no_check();
}

static inline void
mp_disable_preemption(void)
{
	disable_preemption_internal();
}

static inline void
_mp_disable_preemption(void)
{
	disable_preemption_internal();
}

static inline void
mp_enable_preemption(void)
{
	enable_preemption_internal();
}

static inline void
_mp_enable_preemption(void)
{
	enable_preemption_internal();
}

static inline void
mp_enable_preemption_no_check(void)
{
	enable_preemption_no_check();
}

static inline void
_mp_enable_preemption_no_check(void)
{
	enable_preemption_no_check();
}


static inline cpu_data_t *
cpu_datap(int cpu)
{
	return cpu_data_ptr[cpu];
}

static inline int
cpu_is_running(int cpu)
{
	return (cpu_datap(cpu) != NULL) && (cpu_datap(cpu)->cpu_running);
}

static inline cpu_data_t *
cpu_shadowp(int cpu)
{
	return cpu_data_ptr[cpu]->cd_shadow;
}

extern cpu_data_t *cpu_data_alloc(boolean_t is_boot_cpu);
extern void cpu_data_realloc(void);
extern int      cpu_number(void);
boolean_t       mp_safe_spin_lock(usimple_lock_t lock);
extern decl_simple_lock_data(, x86_topo_lock);
extern void *cpu_thread_alloc(int);
extern void cpu_thread_init(void);
extern void cpu_thread_halt(void);
extern void x86_set_logical_topology(x86_lcpu_t *lcpu, int pnum, int lnum);
extern void x86_validate_topology(void);
extern kern_return_t    cpu_topology_start_cpu(int cpunum);
int diagCall64(x86_saved_state_t *regs);
static inline uint64_t
xgetbv(uint32_t c)
{
	uint32_t        mask_hi, mask_lo;
	__asm__ __volatile__ ("xgetbv" : "=a"(mask_lo), "=d"(mask_hi) : "c" (c));
	return ((uint64_t) mask_hi << 32) + (uint64_t) mask_lo;
}

static inline void
xsetbv(uint32_t mask_hi, uint32_t mask_lo)
{
	__asm__ __volatile__ ("xsetbv" :: "a"(mask_lo), "d"(mask_hi), "c" (XCR0));
}

extern void             init_fpu(void);
extern void             fpu_module_init(void);
extern void             fpu_free(
	thread_t        thr_act,
	void            *fps);
extern kern_return_t    fpu_set_fxstate(
	thread_t        thr_act,
	thread_state_t  state,
	thread_flavor_t f);
extern kern_return_t    fpu_get_fxstate(
	thread_t        thr_act,
	thread_state_t  state,
	thread_flavor_t f);
extern void             fpu_dup_fxstate(
	thread_t        parent,
	thread_t        child);
extern void             fpnoextflt(void);
extern void             fpextovrflt(void);
extern void             fpexterrflt(void);
extern void             fpSSEexterrflt(void);
extern void             fpflush(thread_t);
extern void             fp_setvalid(boolean_t);
extern void             clear_fpu(void);
extern void             fpu_switch_context(
	thread_t        old,
	thread_t        new);
extern void             fpu_switch_addrmode(
	thread_t        thread,
	boolean_t       is_64bit);
extern xstate_t         current_xstate(void);
extern int              fpUDflt(user_addr_t rip);
extern void vzeroall(void);
extern void xmmzeroall(void);
extern void avx512_zero(void);
extern void map_rcbaAread(void);
extern void hpet_init(void);
extern void hpet_save(void);
extern void hpet_restore(void);
extern int HPETInterrupt(void);
extern int hpet_register_callback(int (*hpet_reqst)(uint32_t apicid, void *arg, hpetRequest_t *hpet), void *arg);
extern int hpet_request(uint32_t cpu);
extern uint64_t rdHPET(void);
extern void hpet_get_info(hpetInfo_t *info);
extern uint32_t         lapic_safe_apicid(void);
extern void             lapic_init_slave(void);
extern void             lapic_init(void);
extern void             lapic_configure(bool for_wake);
extern void             lapic_shutdown(bool for_sleep);
extern void             lapic_smm_restore(void);
extern boolean_t        lapic_probe(void);
extern void             lapic_dump(void);
extern void             lapic_cpu_map_dump(void);
extern int              lapic_interrupt(
	int interrupt, x86_saved_state_t *state);
extern void             lapic_end_of_interrupt(void);
extern void             lapic_unmask_perfcnt_interrupt(void);
extern void             lapic_set_perfcnt_interrupt_mask(boolean_t);
extern void             lapic_send_ipi(int cpu, int interupt);
extern void             lapic_send_nmi(int cpu);
extern void             lapic_cpu_map_init(void);
extern void             lapic_cpu_map(int lapic, int cpu_num);
extern uint32_t         ml_get_apicid(uint32_t cpu);
extern uint32_t         ml_get_cpuid(uint32_t lapic_index);
extern void             lapic_config_timer(
	boolean_t               interrupt,
	lapic_timer_mode_t      mode,
	lapic_timer_divide_t    divisor);
extern void             lapic_set_timer_fast(
	lapic_timer_count_t     initial_count);
extern void             lapic_set_timer(
	boolean_t               interrupt,
	lapic_timer_mode_t      mode,
	lapic_timer_divide_t    divisor,
	lapic_timer_count_t     initial_count);
extern void             lapic_get_timer(
	lapic_timer_mode_t      *mode,
	lapic_timer_divide_t    *divisor,
	lapic_timer_count_t     *initial_count,
	lapic_timer_count_t     *current_count);
extern void             lapic_config_tsc_deadline_timer(void);
extern void             lapic_set_tsc_deadline_timer(uint64_t deadline);
extern uint64_t         lapic_get_tsc_deadline_timer(void);
extern void             lapic_set_intr_func(int intr, i386_intr_func_t func);
extern void             lapic_set_pmi_func(i386_intr_func_t);
static inline void
lapic_set_timer_func(i386_intr_func_t func)
{
	lapic_set_intr_func(LAPIC_VECTOR(TIMER), func);
}

static inline void
lapic_timer_swi(void)
{
	__asm__ __volatile__ ("int %0" :: "i"(LAPIC_DEFAULT_INTERRUPT_BASE + LAPIC_TIMER_INTERRUPT):"memory");
}

static inline void
lapic_set_thermal_func(i386_intr_func_t func)
{
	lapic_set_intr_func(LAPIC_VECTOR(THERMAL), func);
}
static inline void
lapic_set_cmci_func(i386_intr_func_t func)
{
	lapic_set_intr_func(LAPIC_VECTOR(CMCI), func);
}
static inline void
lapic_set_pm_func(i386_intr_func_t func)
{
	lapic_set_intr_func(LAPIC_VECTOR(PM), func);
}

extern boolean_t        lapic_is_interrupt_pending(void);
extern boolean_t        lapic_is_interrupting(uint8_t vector);
extern void             lapic_interrupt_counts(uint64_t intrs[256]);
extern void             lapic_disable_timer(void);
extern uint8_t          lapic_get_cmci_vector(void);
void lbr_for_kmode_init(uint32_t lbr_count);
extern void             hw_lock_byte_init(volatile uint8_t *lock_byte);
extern void             hw_lock_byte_lock(volatile uint8_t *lock_byte);
extern void             hw_lock_byte_unlock(volatile uint8_t *lock_byte);
extern void             kernel_preempt_check(void);
extern kern_return_t            thread_fast_set_cthread_self(uint32_t);
extern kern_return_t            thread_fast_set_cthread_self64(uint64_t);
extern kern_return_t            thread_set_user_ldt(uint32_t, uint32_t, uint32_t);
extern int              i386_set_ldt(uint32_t *, uint32_t, uint32_t, uint32_t);
extern int              i386_get_ldt(uint32_t *, uint32_t, uint32_t, uint32_t);
extern int              i386_set_ldt64(uint32_t *, uint64_t, uint64_t, uint64_t);
extern int              i386_get_ldt64(uint32_t *, uint64_t, uint64_t, uint64_t);
extern void                     machdep_syscall(x86_saved_state_t *);
extern void                     machdep_syscall64(x86_saved_state_t *);
extern void             mca_cpu_alloc(cpu_data_t *cdp);
extern void             mca_cpu_init(void);
extern void             mca_dump(void);
extern void             mca_check_save(void);
extern boolean_t        mca_is_cmci_present(void);
__BEGIN_DECLS
void    cpu_machine_init(
	void);
void    handle_pending_TLB_flushes(
	void);
int cpu_signal_handler(x86_saved_state_t *regs);
kern_return_t cpu_register(
	int *slot_nump);
__BEGIN_DECLS




boolean_t ml_is64bit(void);
boolean_t ml_thread_is64bit(thread_t);
boolean_t ml_state_is64bit(void *);
void    ml_fp_setvalid(boolean_t);
void    ml_cpu_set_ldt(int);
void    ml_init_interrupt(void);
void ml_cause_interrupt(void);
void ml_install_interrupt_handler(
	void *nub,
	int source,
	void *target,
	IOInterruptHandler handler,
	void *refCon);
uint64_t ml_get_timebase(void);
uint64_t ml_get_timebase_entropy(void);
static inline void
ml_memory_to_timebase_fence(void)
{
	
}


static inline void
ml_timebase_to_memory_fence(void)
{
}

void ml_init_delay_spin_threshold(int);
boolean_t ml_delay_should_spin(uint64_t interval);
extern void ml_delay_on_yield(void);
vm_offset_t
    ml_static_ptovirt(
	vm_offset_t);
void ml_static_mfree(
	vm_offset_t,
	vm_size_t);
kern_return_t
ml_static_protect(
	vm_offset_t start,
	vm_size_t size,
	vm_prot_t new_prot);
kern_return_t
ml_static_verify_page_protections(
	uint64_t base, uint64_t size, vm_prot_t prot);
vm_offset_t ml_vtophys(
	vm_offset_t vaddr);
vm_size_t ml_nofault_copy(
	vm_offset_t virtsrc, vm_offset_t virtdst, vm_size_t size);
boolean_t ml_validate_nofault(
	vm_offset_t virtsrc, vm_size_t size);
uint64_t ml_cpu_cache_size(unsigned int level);
void ml_set_max_cpus(
	unsigned int max_cpus);
extern void     ml_cpu_init_completed(void);
extern void     ml_cpu_up(void);
extern void     ml_cpu_down(void);
extern void     ml_cpu_up_update_counts(int cpu_id);
extern void     ml_cpu_down_update_counts(int cpu_id);
void bzero_phys_nc(
	addr64_t phys_address,
	uint32_t length);
extern vm_offset_t      io_map(
	vm_map_offset_t         phys_addr,
	vm_size_t               size,
	unsigned int            flags,
	vm_prot_t               prot,
	bool                    unmappable);
vm_offset_t ml_io_map(
	vm_offset_t phys_addr,
	vm_size_t size);
vm_offset_t ml_io_map_wcomb(
	vm_offset_t phys_addr,
	vm_size_t size);
vm_offset_t ml_io_map_unmappable(
	vm_offset_t phys_addr,
	vm_size_t size,
	unsigned int flags);
void    ml_get_bouncepool_info(
	vm_offset_t *phys_addr,
	vm_size_t   *size);
boolean_t machine_timeout_suspended(void);
void plctrace_disable(void);
kern_return_t ml_interrupt_prewarm(uint64_t deadline);
__printflike(1, 0)
void ml_panic_trap_to_debugger(const char *panic_format_str,
    va_list *panic_args,
    unsigned int reason,
    void *ctx,
    uint64_t panic_options_mask,
    unsigned long panic_caller,
    const char *panic_initiator);
kern_return_t
ml_processor_register(
	cpu_id_t        cpu_id,
	uint32_t        lapic_id,
	processor_t     *processor_out,
	boolean_t       boot_cpu,
	boolean_t       start );
boolean_t ml_probe_read(
	vm_offset_t paddr,
	unsigned int *val);
boolean_t ml_probe_read_64(
	addr64_t paddr,
	unsigned int *val);
unsigned int ml_phys_read_byte(
	vm_offset_t paddr);
unsigned int ml_phys_read_byte_64(
	addr64_t paddr);
unsigned int ml_phys_read_half(
	vm_offset_t paddr);
unsigned int ml_phys_read_half_64(
	addr64_t paddr);
unsigned int ml_phys_read(
	vm_offset_t paddr);
unsigned int ml_phys_read_64(
	addr64_t paddr);
unsigned int ml_phys_read_word(
	vm_offset_t paddr);
unsigned int ml_phys_read_word_64(
	addr64_t paddr);
unsigned long long ml_phys_read_double(
	vm_offset_t paddr);
unsigned long long ml_phys_read_double_64(
	addr64_t paddr);
extern uint32_t ml_port_io_read(uint16_t ioport, int size);
extern uint8_t ml_port_io_read8(uint16_t ioport);
extern uint16_t ml_port_io_read16(uint16_t ioport);
extern uint32_t ml_port_io_read32(uint16_t ioport);
extern void ml_port_io_write(uint16_t ioport, uint32_t val, int size);
extern void ml_port_io_write8(uint16_t ioport, uint8_t val);
extern void ml_port_io_write16(uint16_t ioport, uint16_t val);
extern void ml_port_io_write32(uint16_t ioport, uint32_t val);
void ml_phys_write_byte(
	vm_offset_t paddr, unsigned int data);
void ml_phys_write_byte_64(
	addr64_t paddr, unsigned int data);
void ml_phys_write_half(
	vm_offset_t paddr, unsigned int data);
void ml_phys_write_half_64(
	addr64_t paddr, unsigned int data);
void ml_phys_write(
	vm_offset_t paddr, unsigned int data);
void ml_phys_write_64(
	addr64_t paddr, unsigned int data);
void ml_phys_write_word(
	vm_offset_t paddr, unsigned int data);
void ml_phys_write_word_64(
	addr64_t paddr, unsigned int data);
void ml_phys_write_double(
	vm_offset_t paddr, unsigned long long data);
void ml_phys_write_double_64(
	addr64_t paddr, unsigned long long data);
void ml_cpu_get_info(ml_cpu_info_t *ml_cpu_info);
void ml_thread_policy(
	thread_t thread,
	unsigned policy_id,
	unsigned policy_info);
unsigned int ml_wait_max_cpus(
	void);
extern void ml_set_maxsnoop(uint32_t maxdelay);
extern unsigned ml_get_maxsnoop(void);
extern void ml_set_maxbusdelay(uint32_t mdelay);
extern uint32_t ml_get_maxbusdelay(void);
extern void ml_set_maxintdelay(uint64_t mdelay);
extern uint64_t ml_get_maxintdelay(void);
extern boolean_t ml_get_interrupt_prewake_applicable(void);
extern uint64_t tmrCvt(uint64_t time, uint64_t conversion);
extern uint64_t ml_cpu_int_event_time(void);
boolean_t ml_get_interrupts_enabled(void);
boolean_t ml_set_interrupts_enabled(boolean_t enable);
boolean_t ml_early_set_interrupts_enabled(boolean_t enable);
boolean_t ml_at_interrupt_context(void);
bool ml_did_interrupt_userspace(void);
extern boolean_t ml_is_quiescing(void);
extern void ml_set_is_quiescing(boolean_t);
extern uint64_t ml_get_booter_memory_size(void);
unsigned int ml_cpu_cache_sharing(unsigned int level, cluster_type_t cluster_type, bool include_all_cpu_types);
void ml_cpu_get_info_type(ml_cpu_info_t * ml_cpu_info, cluster_type_t cluster_type);
unsigned int ml_get_cpu_number_type(cluster_type_t cluster_type, bool logical, bool available);
unsigned int ml_get_cluster_number_type(cluster_type_t cluster_type);
unsigned int ml_get_cpu_types(void);
void bzero_phys(
	addr64_t phys_address,
	uint32_t length);
vm_offset_t ml_stack_remaining(void);
__private_extern__ uint64_t ml_phys_read_data(uint64_t paddr, int psz);
__private_extern__ void ml_phys_write_data(uint64_t paddr,
    unsigned long long data, int size);
__private_extern__ uintptr_t
pmap_verify_noncacheable(uintptr_t vaddr);
void machine_lockdown(void);
boolean_t ml_fpu_avx_enabled(void);
boolean_t ml_fpu_avx512_enabled(void);
void interrupt_latency_tracker_setup(void);
void interrupt_reset_latency_stats(void);
void interrupt_populate_latency_stats(char *, unsigned);
void ml_get_power_state(boolean_t *, boolean_t *);
void timer_queue_expire_rescan(void*);
void ml_timer_evaluate(void);
boolean_t ml_timer_forced_evaluation(void);
void ml_gpu_stat_update(uint64_t);
uint64_t ml_gpu_stat(thread_t);
boolean_t ml_recent_wake(void);
void i386_lbr_init(struct i386_cpu_info *info_p, bool is_master);
int i386_filtered_lbr_state_to_mach_thread_state(thread_t thr_act, last_branch_state_t *machlbrp, boolean_t from_userspace);
void i386_lbr_synch(thread_t thr);
void i386_lbr_enable(void);
void i386_lbr_disable(void);
void ml_hibernate_active_pre(void);
void ml_hibernate_active_post(void);
int ml_page_protection_type(void);
extern void             vstart(vm_offset_t);
extern void             i386_init(void);
extern void             x86_init_wrapper(uintptr_t, uintptr_t) __attribute__((noreturn));
extern void             i386_vm_init(
	uint64_t,
	boolean_t,
	struct boot_args *);
extern void             machine_startup(void);
extern void             get_root_device(void);
extern void             picinit(void);
extern void             interrupt_processor(
	int             cpu);
extern void             mp_probe_cpus(void);
extern void             remote_kdb(void);
extern void             clear_kdb_intr(void);
extern void             cpu_init(void);
extern void             fix_desc(
	void            * desc,
	int             num_desc);
extern void             fix_desc64(
	void            * desc,
	int             num_desc);
extern void             cnpollc(
	boolean_t       on);
extern void             form_pic_mask(void);
extern void             intnull(
	int             unit);
extern char *           i386_boot_info(
	char            *buf,
	vm_size_t       buf_len);
extern void             blkclr(
	const char       *from,
	int              nbytes);
extern void             memset_word(
	int              *dst,
	int              pattern,
	int              nwords);
extern void bcopy_phys(addr64_t from, addr64_t to, vm_size_t nbytes);
extern int apply_func_phys(addr64_t src64, vm_size_t bytes, int (*func)(void * buffer, vm_size_t bytes, void * arg), void * arg);
extern int ml_copy_phys(addr64_t, addr64_t, vm_size_t);
extern void cache_flush_page_phys(ppnum_t pa);
extern void dcache_incoherent_io_flush64(addr64_t pa, unsigned int count);
extern void dcache_incoherent_io_store64(addr64_t pa, unsigned int count);
extern void             sysclk_gettime_interrupts_disabled(
	mach_timespec_t *cur_time);
extern void rtc_nanotime_init_commpage(void);
extern void     rtc_sleep_wakeup(uint64_t base);
extern void     rtc_timer_start(void);
extern void     rtc_clock_napped(uint64_t, uint64_t);
extern void     rtc_clock_adjust(uint64_t);
extern void     pmap_lowmem_finalize(void);
thread_t Switch_context(thread_t, thread_continue_t, thread_t);
__not_tail_called thread_t
Shutdown_context(thread_t thread, void (*doshutdown)(processor_t), processor_t  processor);
boolean_t
debug_state_is_valid32(x86_debug_state32_t *ds);
boolean_t
debug_state_is_valid64(x86_debug_state64_t *ds);
void
copy_debug_state32(x86_debug_state32_t *src, x86_debug_state32_t *target, boolean_t all);
void
copy_debug_state64(x86_debug_state64_t *src, x86_debug_state64_t *target, boolean_t all);
extern void act_machine_switch_pcb(thread_t old, thread_t new);
extern void Idle_PTs_release(vm_offset_t start, vm_offset_t end);
void cpu_pmc_control(void *);
extern void pstate_trace(void);
extern void mp_interrupt_watchdog(void);
extern kern_return_t i386_slide_individual_kext(kernel_mach_header_t *mh, uintptr_t slide);
extern kern_return_t i386_slide_kext_collection_mh_addrs(kernel_mach_header_t *mh, uintptr_t slide, bool adjust_mach_headers);
__BEGIN_DECLS

extern kern_return_t intel_startCPU(int slot_num);
extern kern_return_t intel_startCPU_fast(int slot_num);
extern void i386_init_slave(void) __dead2;
extern void i386_init_slave_fast(void) __dead2;
extern void smp_init(void);
extern void cpu_interrupt(int cpu);
decl_simple_lock_data(extern, kdb_lock);
__BEGIN_DECLS

extern  void    console_init(void);
extern  void    *console_cpu_alloc(boolean_t boot_cpu);
extern  void    console_cpu_free(void *console_buf);
extern  void      mp_kdp_enter(boolean_t proceed_on_failure, bool is_stackshot);
extern  void      mp_kdp_exit(void);
extern  boolean_t mp_kdp_all_cpus_halted(void);
extern  boolean_t       mp_recent_debugger_activity(void);
extern  void    kernel_spin(uint64_t spin_ns);
extern void mp_rendezvous(
	void (*setup_func)(void *),
	void (*action_func)(void *),
	void (*teardown_func)(void *),
	void *arg);
extern void mp_rendezvous_no_intrs(
	void (*action_func)(void *),
	void *arg);
extern void mp_rendezvous_break_lock(void);
extern void mp_rendezvous_lock(void);
extern void mp_rendezvous_unlock(void);
extern void mp_broadcast(
	void (*action_func)(void *),
	void *arg);
static_assert(sizeof(cpumask_t) * CHAR_BIT >= MAX_CPUS, "cpumask_t bitvector is too small for current MAX_CPUS value");
static inline cpumask_t
cpu_to_cpumask(cpu_t cpu)
{
	return (cpu < MAX_CPUS) ? (1ULL << cpu) : 0;
}


extern void mp_cpus_call_cpu_init(int cpu);
extern cpu_t mp_cpus_call(
	cpumask_t       cpus,
	mp_sync_t       mode,
	void            (*action_func)(void *),
	void            *arg);
extern cpu_t mp_cpus_call1(
	cpumask_t       cpus,
	mp_sync_t       mode,
	void            (*action_func)(void *, void*),
	void            *arg0,
	void            *arg1,
	cpumask_t       *cpus_calledp);
extern void NMIPI_panic(cpumask_t cpus, NMI_reason_t reason);
extern long NMI_pte_corruption_callback(void *arg0, void *arg1, uint16_t lcpu);
extern void mp_cpus_kick(cpumask_t cpus);
extern void PM_interrupt_register(void (*fn)(void));
extern void cpu_PM_interrupt(int cpu);
__BEGIN_DECLS





typedef 

typedef 




extern void     cpu_syscall_init(cpu_data_t *cdp);
extern void     cpu_desc_init(cpu_data_t *cdp);
extern void     cpu_desc_load(cpu_data_t *cdp);
extern boolean_t
valid_user_data_selector(uint16_t selector);
extern boolean_t
valid_user_code_selector(uint16_t selector);
extern boolean_t
valid_user_stack_selector(uint16_t selector);
extern boolean_t
valid_user_segment_selectors(uint16_t cs,
    uint16_t ss,
    uint16_t ds,
    uint16_t es,
    uint16_t fs,
    uint16_t gs);
__BEGIN_DECLS

extern void     i386_signal_cpu(int cpu, mp_event_t event, mp_sync_t mode);
extern void     i386_activate_cpu(void);
extern void     i386_deactivate_cpu(void);
extern void     cpu_NMI_interrupt(int );
kern_return_t
pal_efi_call_in_64bit_mode(uint64_t func,
    struct pal_efi_registers *efi_reg,
    void *stack_contents,
    size_t stack_contents_size,                        
    uint64_t *efi_status);
kern_return_t
pal_efi_call_in_32bit_mode(uint32_t func,
    struct pal_efi_registers *efi_reg,
    void *stack_contents,
    size_t stack_contents_size,                        
    uint32_t *efi_status);
boolean_t pal_machine_sleep(uint8_t type_a,
    uint8_t type_b,
    uint32_t bit_position,
    uint32_t disable_mask,
    uint32_t enable_mask);
extern int  pal_serial_init(void);
extern void pal_serial_putc(char);
extern void pal_serial_putc_nocr(char);
extern int  pal_serial_getc(void);
extern void pal_i386_init(void);
extern void pal_set_signal_delivery(thread_t);
extern void pal_get_control_registers( pal_cr_t *cr0, pal_cr_t *cr2,
    pal_cr_t *cr3, pal_cr_t *cr4 );
extern void pal_dbg_page_fault( thread_t thread, user_addr_t vadddr,
    kern_return_t kr );
extern void pal_dbg_set_task_name( task_t task );
void pal_syscall_restart(thread_t thread, x86_saved_state_t *state);
void pal_execve_return(thread_t thread);
void pal_thread_terminate_self(thread_t thread);
void pal_ast_check(thread_t thread);
extern void pal_get_kern_regs( x86_saved_state_t *state ) __dead2;
extern void pal_hlt(void);
extern void pal_sti(void);
extern void pal_cli(void);
void pal_register_cache_state(thread_t thread, pal_cache_state_t state);
void pal_preemption_assert(void);
static inline void
pal_get_resource_property(const char **property_name, int *property_value)
{
	*property_name = PAL_AICPM_PROPERTY_NAME;
	if (virtualized) {
		*property_value = PAL_VIRTUALIZED_PROPERTY_VALUE;
	} else {
		*property_value = PAL_XCPM_PROPERTY_VALUE;
	}
}


extern void _pal_rtc_nanotime_store(
	uint64_t                tsc,
	uint64_t                nsec,
	uint32_t                scale,
	uint32_t                shift,
	struct pal_rtc_nanotime *dst);
void panic_hooks_init(void);
void panic_check_hook(void);
void panic_hook(panic_hook_t *hook, panic_hook_fn_t hook_fn);
void panic_unhook(panic_hook_t *hook);
void panic_dump_mem(const void *addr, int len);
extern void panic_notify_init(void);
extern void panic_notify(void);
static inline pt_entry_t
pte_remove_ex(pt_entry_t pte, boolean_t is_ept)
{
	if (__probable(!is_ept)) {
		return pte | INTEL_PTE_NX;
	}

	return pte & (~INTEL_EPT_EX);
}

static inline pt_entry_t
pte_set_ex(pt_entry_t pte, boolean_t is_ept)
{
	if (__probable(!is_ept)) {
		return pte & (~INTEL_PTE_NX);
	}

	return pte | INTEL_EPT_EX;
}

static inline pt_entry_t
pte_set_uex(pt_entry_t pte)
{
	return pte | INTEL_EPT_UEX;
}

static inline pt_entry_t
physmap_refmod_to_ept(pt_entry_t physmap_pte)
{
	pt_entry_t ept_pte = 0;

	if (physmap_pte & INTEL_PTE_MOD) {
		ept_pte |= INTEL_EPT_MOD;
	}

	if (physmap_pte & INTEL_PTE_REF) {
		ept_pte |= INTEL_EPT_REF;
	}

	return ept_pte;
}

static inline pt_entry_t
ept_refmod_to_physmap(pt_entry_t ept_pte)
{
	pt_entry_t physmap_pte = 0;

	assert((ept_pte & ~(INTEL_EPT_REF | INTEL_EPT_MOD)) == 0);

	if (ept_pte & INTEL_EPT_REF) {
		physmap_pte |= INTEL_PTE_REF;
	}

	if (ept_pte & INTEL_EPT_MOD) {
		physmap_pte |= INTEL_PTE_MOD;
	}

	return physmap_pte;
}


extern boolean_t pmap_ept_support_ad;
static inline void
pmap_corrupted_pte_detected(pt_entry_t *ptep, uint64_t clear_bits, uint64_t set_bits)
{
	if (__c11_atomic_compare_exchange_strong((_Atomic(pt_entry_t *)*) & PTE_corrupted_ptr, &PTE_corrupted_ptr, ptep,
	    memory_order_acq_rel_smp, memory_order_relaxed)) {
		force_immediate_debugger_NMI = TRUE;
		NMIPI_panic(CPUMASK_REAL_OTHERS, PTE_CORRUPTION);
		if (clear_bits == 0 && set_bits == 0) {
			panic("PTE Corruption detected: ptep 0x%llx pte value 0x%llx", (unsigned long long)(uintptr_t)ptep, *(uint64_t *)ptep);
		} else {
			panic("PTE Corruption detected: ptep 0x%llx pte value 0x%llx clear 0x%llx set 0x%llx",
			    (unsigned long long)(uintptr_t)ptep, *(uint64_t *)ptep, clear_bits, set_bits);
		}
	}
}


static inline void
pmap_store_pte(boolean_t is_ept, pt_entry_t *entryp, pt_entry_t value)
{
	
	*entryp = value;


	if (__improbable((is_ept == FALSE) && (value & PTE_COMPRESSED) && (value & INTEL_PTE_NX))) {
		pmap_corrupted_pte_detected(entryp, 0, 0);
	}
}

static inline boolean_t
physmap_enclosed(addr64_t a)
{
	return a < (NPHYSMAP * GB);
}

static  inline void *
PHYSMAP_PTOV_check(void *paddr)
{
	uint64_t pvaddr = (uint64_t)paddr + physmap_base;

	if (__improbable(pvaddr >= physmap_max)) {
		panic("PHYSMAP_PTOV bounds exceeded, 0x%qx, 0x%qx, 0x%qx",
		    pvaddr, physmap_base, physmap_max);
	}

	return (void *)pvaddr;
}

extern uint64_t dblmap_base, dblmap_max, dblmap_dist;
static inline uint64_t
DBLMAP_CHECK(uintptr_t x)
{
	uint64_t dbladdr = (uint64_t)x + dblmap_dist;
	if (__improbable((dbladdr >= dblmap_max) || (dbladdr < dblmap_base))) {
		panic("DBLMAP bounds exceeded, 0x%qx, 0x%qx 0x%qx, 0x%qx",
		    (uint64_t)x, dbladdr, dblmap_base, dblmap_max);
	}
	return dbladdr;
}
extern uint64_t ldt_alias_offset;
static inline uint64_t
LDTALIAS_CHECK(uintptr_t x)
{
	uint64_t dbladdr = (uint64_t)x + ldt_alias_offset;
	if (__improbable((dbladdr >= dblmap_max) || (dbladdr < dblmap_base))) {
		panic("LDTALIAS: bounds exceeded, 0x%qx, 0x%qx 0x%qx, 0x%qx",
		    (uint64_t)x, dbladdr, dblmap_base, dblmap_max);
	}
	return dbladdr;
}





extern void
pmap_tlbi_range(uint64_t startv, uint64_t endv, bool global, uint16_t pcid);
static inline boolean_t
is_ept_pmap(pmap_t p)
{
	if (__probable(p->pm_cr3 != 0)) {
		assert(p->pm_eptp == 0);
		return FALSE;
	}

	assert(p->pm_eptp != 0);

	return TRUE;
}

void hv_ept_pmap_create(void **ept_pmap, void **eptp);
static inline void
set_dirbase(pmap_t tpmap, thread_t thread, int my_cpu)
{
	int ccpu = my_cpu;
	uint64_t pcr3 = tpmap->pm_cr3, ucr3 = tpmap->pm_ucr3;
	cpu_datap(ccpu)->cpu_task_cr3 = pcr3;
	cpu_shadowp(ccpu)->cpu_shadowtask_cr3 = pcr3;

	cpu_datap(ccpu)->cpu_ucr3 = ucr3;
	cpu_shadowp(ccpu)->cpu_ucr3 = ucr3;

	cpu_datap(ccpu)->cpu_task_map = cpu_shadowp(ccpu)->cpu_task_map =
	    tpmap->pm_task_map;

	assert((get_preemption_level() > 0) || (ml_get_interrupts_enabled() == FALSE));
	assert(ccpu == cpu_number());
	
	boolean_t nopagezero = tpmap->pagezero_accessible;
	boolean_t priorpagezero = cpu_datap(ccpu)->cpu_pagezero_mapped;
	cpu_datap(ccpu)->cpu_pagezero_mapped = nopagezero;

	if (__probable(!no_shared_cr3)) {
		if (__improbable(nopagezero)) {
			boolean_t copyio_active = ((thread->machine.specFlags & CopyIOActive) != 0);
			if (pmap_pcid_ncpus) {
				pmap_pcid_activate(tpmap, ccpu, TRUE, copyio_active);
			} else {
				if (copyio_active) {
					if (get_cr3_base() != tpmap->pm_cr3) {
						set_cr3_raw(tpmap->pm_cr3);
					}
				} else if (get_cr3_base() != cpu_datap(ccpu)->cpu_kernel_cr3) {
					set_cr3_raw(cpu_datap(ccpu)->cpu_kernel_cr3);
				}
			}
		} else if ((get_cr3_base() != tpmap->pm_cr3) || priorpagezero) {
			if (pmap_pcid_ncpus) {
				pmap_pcid_activate(tpmap, ccpu, FALSE, FALSE);
			} else {
				set_cr3_raw(tpmap->pm_cr3);
			}
		}
	} else {
		if (get_cr3_base() != cpu_datap(ccpu)->cpu_kernel_cr3) {
			set_cr3_raw(cpu_datap(ccpu)->cpu_kernel_cr3);
		}
	}
}



extern void             pmap_update_interrupt(void);
extern addr64_t(kvtophys)(
	vm_offset_t     addr);
extern kern_return_t    pmap_expand(
	pmap_t          pmap,
	vm_map_offset_t addr,
	unsigned int options);
extern vm_offset_t      pmap_map(
	vm_offset_t     virt,
	vm_map_offset_t start,
	vm_map_offset_t end,
	vm_prot_t       prot,
	unsigned int    flags);
extern vm_offset_t      pmap_map_bd(
	vm_offset_t     virt,
	vm_map_offset_t start,
	vm_map_offset_t end,
	vm_prot_t       prot,
	unsigned int    flags);
extern void             pmap_bootstrap(
	vm_offset_t     load_start,
	boolean_t       IA32e);
extern boolean_t        pmap_valid_page(
	ppnum_t pn);
extern int              pmap_list_resident_pages(
	struct pmap     *pmap,
	vm_offset_t     *listp,
	int             space);
extern void             x86_filter_TLB_coherency_interrupts(boolean_t);
extern void
pmap_mark_range(pmap_t npmap, uint64_t sv, uint64_t nxrosz, boolean_t NX,
    boolean_t ro);
extern  unsigned        pmap_get_cache_attributes(ppnum_t, boolean_t is_ept);
extern kern_return_t    pmap_map_block_addr(
	pmap_t pmap,
	addr64_t va,
	pmap_paddr_t pa,
	uint32_t size,
	vm_prot_t prot,
	int attr,
	unsigned int flags);
extern kern_return_t    pmap_map_block(
	pmap_t pmap,
	addr64_t va,
	ppnum_t pa,
	uint32_t size,
	vm_prot_t prot,
	int attr,
	unsigned int flags);
extern void invalidate_icache(vm_offset_t addr, unsigned cnt, int phys);
extern void flush_dcache(vm_offset_t addr, unsigned count, int phys);
extern pmap_paddr_t pmap_find_pa(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys_nofault(pmap_t pmap, addr64_t va);
extern kern_return_t pmap_get_prot(pmap_t pmap, addr64_t va, vm_prot_t *protp);
extern void pmap_cpu_init(void);
extern void pmap_disable_NX(pmap_t pmap);
extern void pmap_pagetable_corruption_msg_log(int (*)(const char * fmt, ...)__printflike(1, 2));
extern void x86_64_protect_data_const(void);
extern uint64_t pmap_commpage_size_min(pmap_t pmap);
static inline vm_offset_t
pmap_ro_zone_align(vm_offset_t value)
{
	return value;
}

extern void pmap_ro_zone_memcpy(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    vm_offset_t new_data, vm_size_t new_data_size);
extern uint64_t pmap_ro_zone_atomic_op(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    uint32_t op, uint64_t value);
extern void pmap_ro_zone_bzero(zone_id_t zid, vm_offset_t va, vm_offset_t offset, vm_size_t size);
extern boolean_t pmap_is_empty(pmap_t           pmap,
    vm_map_offset_t  start,
    vm_map_offset_t  end);
kern_return_t
    pmap_permissions_verify(pmap_t, vm_map_t, vm_offset_t, vm_offset_t);
static inline void
PMAP_LOCK_EXCLUSIVE(pmap_t p)
{
	mp_disable_preemption();
	lck_rw_lock_exclusive(&p->pmap_rwl);
}

static inline void
PMAP_LOCK_SHARED(pmap_t p)
{
	mp_disable_preemption();
	lck_rw_lock_shared(&p->pmap_rwl);
}

static inline void
PMAP_LOCK_SHARED_TO_EXCLUSIVE(pmap_t p)
{
	lck_rw_lock_shared_to_exclusive(&p->pmap_rwl);
}

static inline void
PMAP_LOCK_EXCLUSIVE_TO_SHARED(pmap_t p)
{
	lck_rw_lock_exclusive_to_shared(&p->pmap_rwl);
}

static inline void
PMAP_UNLOCK_EXCLUSIVE(pmap_t p)
{
	lck_rw_unlock_exclusive(&p->pmap_rwl);
	mp_enable_preemption();
}

static inline void
PMAP_UNLOCK_SHARED(pmap_t p)
{
	lck_rw_unlock_shared(&p->pmap_rwl);
	mp_enable_preemption();
}




kern_return_t   pmap_expand_pml4(
	pmap_t          map,
	vm_map_offset_t v,
	unsigned int options);
kern_return_t   pmap_expand_pdpt(
	pmap_t          map,
	vm_map_offset_t v,
	unsigned int options);
void            phys_attribute_set(
	ppnum_t         phys,
	int             bits);
void            pmap_set_reference(
	ppnum_t pn);
boolean_t       phys_page_exists(
	ppnum_t pn);
void
    pmap_flush_tlbs(pmap_t, vm_map_offset_t, vm_map_offset_t, int, pmap_flush_context *);
void
    pmap_update_cache_attributes_locked(ppnum_t, unsigned);
static inline void
PMAP_UPDATE_TLBS(pmap_t fp, addr64_t s, addr64_t e)
{
	pmap_flush_tlbs(fp, s, e, 0, NULL);
}


static inline void
PMAP_UPDATE_TLBS_DELAYED(pmap_t fp, addr64_t s, addr64_t e, pmap_flush_context *pfc)
{
	pmap_flush_tlbs(fp, s, e, PMAP_DELAY_TLB_FLUSH, pfc);
}







typedef 


typedef 






extern volatile uint32_t        mappingrecurse;
decl_simple_lock_data(extern, pv_hashed_free_list_lock);
decl_simple_lock_data(extern, pv_hashed_kern_free_list_lock);
decl_simple_lock_data(extern, pv_hash_table_lock);
decl_simple_lock_data(extern, phys_backup_lock);
static inline void
PV_HASHED_ALLOC(pv_hashed_entry_t *pvh_ep)
{
	pmap_assert(*pvh_ep == PV_HASHED_ENTRY_NULL);
	simple_lock(&pv_hashed_free_list_lock, LCK_GRP_NULL);
	
	if ((pv_hashed_kern_free_count > pv_hashed_kern_low_water_mark) && ((*pvh_ep = pv_hashed_free_list) != 0)) {
		pv_hashed_free_list = (pv_hashed_entry_t)(*pvh_ep)->qlink.next;
		pv_hashed_free_count--;
	}

	simple_unlock(&pv_hashed_free_list_lock);

	if (pv_hashed_free_count <= pv_hashed_low_water_mark) {
		if (!mappingrecurse && os_atomic_cmpxchg(&mappingrecurse, 0, 1, acq_rel)) {
			thread_wakeup(&mapping_replenish_event);
		}
	}
}

static inline void
PV_HASHED_FREE_LIST(pv_hashed_entry_t pvh_eh, pv_hashed_entry_t pvh_et, int pv_cnt)
{
	simple_lock(&pv_hashed_free_list_lock, LCK_GRP_NULL);
	pvh_et->qlink.next = (queue_entry_t)pv_hashed_free_list;
	pv_hashed_free_list = pvh_eh;
	pv_hashed_free_count += (uint32_t)pv_cnt;
	simple_unlock(&pv_hashed_free_list_lock);
}

extern unsigned pmap_kern_reserve_alloc_stat;
static inline void
PV_HASHED_KERN_ALLOC(pv_hashed_entry_t *pvh_e)
{
	pmap_assert(*pvh_e == PV_HASHED_ENTRY_NULL);
	simple_lock(&pv_hashed_kern_free_list_lock, LCK_GRP_NULL);

	if ((*pvh_e = pv_hashed_kern_free_list) != 0) {
		pv_hashed_kern_free_list = (pv_hashed_entry_t)(*pvh_e)->qlink.next;
		pv_hashed_kern_free_count--;
		pmap_kern_reserve_alloc_stat++;
	}

	simple_unlock(&pv_hashed_kern_free_list_lock);

	if (pv_hashed_kern_free_count < pv_hashed_kern_low_water_mark) {
		if (!mappingrecurse && os_atomic_cmpxchg(&mappingrecurse, 0, 1, acq_rel)) {
			thread_wakeup(&mapping_replenish_event);
		}
	}
}

static inline void
PV_HASHED_KERN_FREE_LIST(pv_hashed_entry_t pvh_eh, pv_hashed_entry_t pvh_et, int pv_cnt)
{
	simple_lock(&pv_hashed_kern_free_list_lock, LCK_GRP_NULL);
	pvh_et->qlink.next = (queue_entry_t)pv_hashed_kern_free_list;
	pv_hashed_kern_free_list = pvh_eh;
	pv_hashed_kern_free_count += (uint32_t)pv_cnt;
	simple_unlock(&pv_hashed_kern_free_list_lock);
}

extern uint64_t pmap_pv_throttle_stat, pmap_pv_throttled_waiters;
static inline void
pmap_pv_throttle(__unused pmap_t p)
{
	pmap_assert(p != kernel_pmap);
	
	if (pv_hashed_kern_free_count < (pv_hashed_kern_low_water_mark / 2)) {
		pmap_pv_throttle_stat++;
		
		pmap_pv_throttled_waiters++;
		assert_wait_timeout(&pmap_user_pv_throttle_event, THREAD_UNINT, 1, 1000 * NSEC_PER_USEC);
		thread_block(THREAD_CONTINUE_NULL);
	}
}



















extern uint64_t pde_mapped_size;
static inline uint32_t
pvhashidx(pmap_t pmap, vm_map_offset_t va)
{
	uint32_t hashidx = ((uint32_t)(uintptr_t)pmap ^
	    ((uint32_t)(va >> PAGE_SHIFT) & 0xFFFFFFFF)) &
	    npvhashmask;
	return hashidx;
}


static inline void
pmap_pvh_unlink(pv_hashed_entry_t pvh)
{
	pv_hashed_entry_t       curh;
	pv_hashed_entry_t       *pprevh;
	uint32_t                pvhash_idx;

	CHK_NPVHASH();
	pvhash_idx = pvhashidx(pvh->pmap, PVE_VA(pvh));

	pprevh = pvhash(pvhash_idx);

	curh = *pprevh;

	while (PV_HASHED_ENTRY_NULL != curh) {
		if (pvh == curh) {
			break;
		}
		pprevh = &curh->nexth;
		curh = curh->nexth;
	}
	if (PV_HASHED_ENTRY_NULL == curh) {
		panic("pmap_pvh_unlink no pvh");
	}
	*pprevh = pvh->nexth;
	return;
}

static inline void
pv_hash_add(pv_hashed_entry_t   pvh_e,
    pv_rooted_entry_t   pv_h)
{
	pv_hashed_entry_t       *hashp;
	uint32_t                pvhash_idx;

	CHK_NPVHASH();
	pvhash_idx = pvhashidx(pvh_e->pmap, PVE_VA(pvh_e));
	LOCK_PV_HASH(pvhash_idx);
	insque(&pvh_e->qlink, &pv_h->qlink);
	hashp = pvhash(pvhash_idx);
	pvh_e->nexth = *hashp;
	*hashp = pvh_e;
	UNLOCK_PV_HASH(pvhash_idx);
}

static inline void
pv_hash_remove(pv_hashed_entry_t pvh_e)
{
	uint32_t                pvhash_idx;

	CHK_NPVHASH();
	pvhash_idx = pvhashidx(pvh_e->pmap, PVE_VA(pvh_e));
	LOCK_PV_HASH(pvhash_idx);
	remque(&pvh_e->qlink);
	pmap_pvh_unlink(pvh_e);
	UNLOCK_PV_HASH(pvhash_idx);
}

static inline boolean_t
popcnt1(uint64_t distance)
{
	return (distance & (distance - 1)) == 0;
}



typedef enum {
	PTE_VALID                = 0x0,
	PTE_INVALID              = 0x1,
	PTE_RSVD                 = 0x2,
	PTE_SUPERVISOR           = 0x4,
	PTE_BITFLIP              = 0x8,
	PV_BITFLIP               = 0x10,
	PTE_INVALID_CACHEABILITY = 0x20,
	PTE_NXBITFLIP            = 0x40
} pmap_pagetable_corruption_t;
static inline pmap_pagetable_corruption_action_t
pmap_pagetable_corruption_log(pmap_pv_assertion_t incident, pmap_pagetable_corruption_t suppress_reason,
    pmap_pagetable_corruption_action_t action, pmap_t pmap, vm_map_offset_t vaddr, pt_entry_t *ptep,
    ppnum_t ppn, pmap_t pvpmap, vm_map_offset_t pvva, int adj_pteps_cnt, uint64_t **adj_pteps)
{
	uint32_t pmap_pagetable_corruption_log_index;
	uint64_t curtime = mach_absolute_time();

	if ((curtime - pmap_pagetable_corruption_last_abstime) < pmap_pagetable_corruption_interval_abstime) {
		pmap_pagetable_corruption_timeout = TRUE;
		action = PMAP_ACTION_ASSERT;
	} else {
		pmap_pagetable_corruption_last_abstime = curtime;
	}

	pmap_pagetable_corruption_log_index = pmap_pagetable_corruption_incidents++ % PMAP_PAGETABLE_CORRUPTION_MAX_LOG;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].incident = incident;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].reason = suppress_reason;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].action = action;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].pmap = pmap;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].vaddr = vaddr;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].pte = *ptep;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].ppn = ppn;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].pvpmap = pvpmap;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].pvva = pvva;
	pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].abstime = curtime;
	if (adj_pteps_cnt > 0 && adj_pteps != NULL) {
		pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].adj_ptes_count = MIN(adj_pteps_cnt, PMPTCR_MAX_ADJ_PTES);
		for (int i = 0; i < pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].adj_ptes_count; i++) {
			pmap_pagetable_corruption_records[pmap_pagetable_corruption_log_index].adj_ptes[i] = *adj_pteps[i];
		}
	}
	
	thread_call_enter(pmap_pagetable_corruption_log_call);

	return action;
}

static inline pmap_pagetable_corruption_action_t
pmap_classify_pagetable_corruption(pmap_t pmap, vm_map_offset_t vaddr, ppnum_t *ppnp, pt_entry_t *ptep, pmap_pv_assertion_t incident)
{
	pmap_pagetable_corruption_action_t      action = PMAP_ACTION_ASSERT;
	pmap_pagetable_corruption_t     suppress_reason = PTE_VALID;
	ppnum_t                 suppress_ppn = 0;
	pt_entry_t cpte = *ptep;
	ppnum_t cpn = pa_index(pte_to_pa(cpte));
	ppnum_t ppn = *ppnp;
	pv_rooted_entry_t       pv_h = pai_to_pvh(ppn_to_pai(ppn));
	pv_rooted_entry_t       pv_e = pv_h;
	uint32_t        bitdex;
	pmap_t pvpmap = pv_h->pmap;
	vm_map_offset_t pvva = PVE_VA(pv_h);
	vm_map_offset_t pve_flags;
	boolean_t ppcd = FALSE;
	boolean_t is_ept;

	

	
	pmap_phys_attributes[ppn_to_pai(ppn)] |= (PHYS_MODIFIED | PHYS_REFERENCED);
	is_ept = is_ept_pmap(pmap);

	
	do {
		if ((popcnt1((uintptr_t)pv_e->pmap ^ (uintptr_t)pmap) && PVE_VA(pv_e) == vaddr) ||
		    (pv_e->pmap == pmap && popcnt1(PVE_VA(pv_e) ^ vaddr))) {
			pve_flags = PVE_FLAGS(pv_e);
			pv_e->pmap = pmap;
			pv_h->va_and_flags = vaddr | pve_flags;
			suppress_reason = PV_BITFLIP;
			action = PMAP_ACTION_RETRY;
			goto pmap_cpc_exit;
		}
	} while (((pv_e = (pv_rooted_entry_t) queue_next(&pv_e->qlink))) && (pv_e != pv_h));

	
	for (bitdex = 0; bitdex < (sizeof(ppnum_t) << 3); bitdex++) {
		ppnum_t npn = cpn ^ (ppnum_t) (1ULL << bitdex);
		if (IS_MANAGED_PAGE(npn)) {
			pv_rooted_entry_t npv_h = pai_to_pvh(ppn_to_pai(npn));
			if (PVE_VA(npv_h) == vaddr && npv_h->pmap == pmap) {
				suppress_reason = PTE_BITFLIP;
				suppress_ppn = npn;
				action = PMAP_ACTION_RETRY_RELOCK;
				UNLOCK_PVH(ppn_to_pai(ppn));
				*ppnp = npn;
				goto pmap_cpc_exit;
			}
		}
	}

	if (pmap == kernel_pmap) {
		action = PMAP_ACTION_ASSERT;
		goto pmap_cpc_exit;
	}

	
	if (!is_ept && ((cpte & (INTEL_PTE_NCACHE | INTEL_PTE_WTHRU | INTEL_PTE_PAT)) == (INTEL_PTE_NCACHE | INTEL_PTE_WTHRU))) {
		action = PMAP_ACTION_IGNORE;
		suppress_reason = PTE_INVALID_CACHEABILITY;
	} else if (cpte & INTEL_PTE_RSVD) {
		action = PMAP_ACTION_IGNORE;
		suppress_reason = PTE_RSVD;
	} else if ((pmap != kernel_pmap) && (!is_ept) && ((cpte & INTEL_PTE_USER) == 0)) {
		action = PMAP_ACTION_IGNORE;
		suppress_reason = PTE_SUPERVISOR;
	}
pmap_cpc_exit:
	PE_parse_boot_argn("-pmap_pagetable_corruption_deassert", &ppcd, sizeof(ppcd));

	if (debug_boot_arg && !ppcd) {
		action = PMAP_ACTION_ASSERT;
	}

	return pmap_pagetable_corruption_log(incident, suppress_reason, action, pmap, vaddr, &cpte, *ppnp, pvpmap, pvva, 0, 0);
}

static inline boolean_t
pmap_compressed_pte_corruption_repair(uint64_t pte, uint64_t *pte_addr, uint64_t *ptep, pmap_t pmap,
    vm_map_offset_t vaddr)
{
	uint64_t *adj_pteps[2];
	int pteidx = ((uintptr_t)ptep & INTEL_OFFMASK) / sizeof(pt_entry_t);
	pmap_pagetable_corruption_action_t action = PMAP_ACTION_IGNORE;

	
	if (pteidx == 0) {
		adj_pteps[0] = ptep + 1;
		adj_pteps[1] = ptep + 2;
	} else if (pteidx == (NPTPG - 1)) {
		adj_pteps[0] = ptep - 2;
		adj_pteps[1] = ptep - 1;
	} else {
		adj_pteps[0] = ptep - 1;
		adj_pteps[1] = ptep + 1;
	}

	
	if (pmap_pagetable_corruption_log(ROOT_ABSENT, (pte & INTEL_PTE_NX) ? PTE_NXBITFLIP : PTE_BITFLIP,
	    action, pmap, vaddr, ptep, (ppnum_t)~0UL, 0, 0, sizeof(adj_pteps) / sizeof(adj_pteps[0]),
	    adj_pteps) != PMAP_ACTION_ASSERT) {
		
		pmap_store_pte(is_ept_pmap(pmap), ptep, pte & INTEL_PTE_COMPRESSED_MASK);
		pmap->corrected_compressed_ptes_count++;
		return TRUE; 
	}

	panic("compressed PTE %p 0x%llx has extra bits 0x%llx: corrupted? Adjacent PTEs: 0x%llx@%p, 0x%llx@%p",
	    pte_addr, pte, pte & ~INTEL_PTE_COMPRESSED_MASK, *adj_pteps[0], adj_pteps[0], *adj_pteps[1], adj_pteps[1]);
	
}


static inline __attribute__((always_inline)) pv_hashed_entry_t
pmap_pv_remove(pmap_t           pmap,
    vm_map_offset_t  vaddr,
    ppnum_t          *ppnp,
    pt_entry_t       *pte,
    boolean_t        *was_altacct)
{
	pv_hashed_entry_t       pvh_e;
	pv_rooted_entry_t       pv_h;
	pv_hashed_entry_t       *pprevh;
	uint32_t                pvhash_idx;
	uint32_t                pv_cnt;
	ppnum_t                 ppn;

	*was_altacct = FALSE;
pmap_pv_remove_retry:
	ppn = *ppnp;
	pvh_e = PV_HASHED_ENTRY_NULL;
	pv_h = pai_to_pvh(ppn_to_pai(ppn));

	if (__improbable(pv_h->pmap == PMAP_NULL)) {
		pmap_pagetable_corruption_action_t pac = pmap_classify_pagetable_corruption(pmap, vaddr, ppnp, pte, ROOT_ABSENT);
		if (pac == PMAP_ACTION_IGNORE) {
			goto pmap_pv_remove_exit;
		} else if (pac == PMAP_ACTION_ASSERT) {
			panic("Possible memory corruption: pmap_pv_remove(%p,0x%llx,0x%x, 0x%llx, %p, %p): null pv_list, priors: %d", pmap, vaddr, ppn, *pte, ppnp, pte, pmap_pagetable_corruption_incidents);
		} else if (pac == PMAP_ACTION_RETRY_RELOCK) {
			LOCK_PVH(ppn_to_pai(*ppnp));
			pmap_phys_attributes[ppn_to_pai(*ppnp)] |= (PHYS_MODIFIED | PHYS_REFERENCED);
			goto pmap_pv_remove_retry;
		} else if (pac == PMAP_ACTION_RETRY) {
			goto pmap_pv_remove_retry;
		}
	}

	if (PVE_VA(pv_h) == vaddr && pv_h->pmap == pmap) {
		*was_altacct = IS_ALTACCT_PAGE(ppn_to_pai(*ppnp), pv_h);
		
		pvh_e = (pv_hashed_entry_t) queue_next(&pv_h->qlink);
		if (pv_h != (pv_rooted_entry_t) pvh_e) {
			
			CHK_NPVHASH();
			pvhash_idx = pvhashidx(pvh_e->pmap, PVE_VA(pvh_e));
			LOCK_PV_HASH(pvhash_idx);
			remque(&pvh_e->qlink);
			pprevh = pvhash(pvhash_idx);
			if (PV_HASHED_ENTRY_NULL == *pprevh) {
				panic("Possible memory corruption: pmap_pv_remove(%p,0x%llx,0x%x): "
				    "empty hash, removing rooted, priors: %d",
				    pmap, vaddr, ppn, pmap_pagetable_corruption_incidents);
			}
			pmap_pvh_unlink(pvh_e);
			UNLOCK_PV_HASH(pvhash_idx);
			pv_h->pmap = pvh_e->pmap;
			pv_h->va_and_flags = pvh_e->va_and_flags;
			
		} else {
			
			pv_h->pmap = PMAP_NULL;
			pvh_e = PV_HASHED_ENTRY_NULL;
		}
	} else {
		
		CHK_NPVHASH();
		pvhash_idx = pvhashidx(pmap, vaddr);
		LOCK_PV_HASH(pvhash_idx);
		pprevh = pvhash(pvhash_idx);
		if (PV_HASHED_ENTRY_NULL == *pprevh) {
			panic("Possible memory corruption: pmap_pv_remove(%p,0x%llx,0x%x, 0x%llx, %p): empty hash, priors: %d",
			    pmap, vaddr, ppn, *pte, pte, pmap_pagetable_corruption_incidents);
		}
		pvh_e = *pprevh;
		pmap_pv_hashlist_walks++;
		pv_cnt = 0;
		while (PV_HASHED_ENTRY_NULL != pvh_e) {
			pv_cnt++;
			if (pvh_e->pmap == pmap &&
			    PVE_VA(pvh_e) == vaddr &&
			    pvh_e->ppn == ppn) {
				break;
			}
			pprevh = &pvh_e->nexth;
			pvh_e = pvh_e->nexth;
		}

		if (PV_HASHED_ENTRY_NULL == pvh_e) {
			pmap_pagetable_corruption_action_t pac = pmap_classify_pagetable_corruption(pmap, vaddr, ppnp, pte, ROOT_PRESENT);

			if (pac == PMAP_ACTION_ASSERT) {
				panic("Possible memory corruption: pmap_pv_remove(%p, 0x%llx, 0x%x, 0x%llx, %p, %p): pv not on hash, head: %p, 0x%llx, priors: %d", pmap, vaddr, ppn, *pte, ppnp, pte, pv_h->pmap, PVE_VA(pv_h), pmap_pagetable_corruption_incidents);
			} else {
				UNLOCK_PV_HASH(pvhash_idx);
				if (pac == PMAP_ACTION_RETRY_RELOCK) {
					LOCK_PVH(ppn_to_pai(*ppnp));
					pmap_phys_attributes[ppn_to_pai(*ppnp)] |= (PHYS_MODIFIED | PHYS_REFERENCED);
					goto pmap_pv_remove_retry;
				} else if (pac == PMAP_ACTION_RETRY) {
					goto pmap_pv_remove_retry;
				} else if (pac == PMAP_ACTION_IGNORE) {
					goto pmap_pv_remove_exit;
				}
			}
		}

		*was_altacct = IS_ALTACCT_PAGE(ppn_to_pai(*ppnp), pvh_e);

		pmap_pv_hashlist_cnts += pv_cnt;
		if (pmap_pv_hashlist_max < pv_cnt) {
			pmap_pv_hashlist_max = pv_cnt;
		}
		*pprevh = pvh_e->nexth;
		remque(&pvh_e->qlink);
		UNLOCK_PV_HASH(pvhash_idx);
	}
pmap_pv_remove_exit:
	return pvh_e;
}

static inline __attribute__((always_inline)) boolean_t
pmap_pv_is_altacct(
	pmap_t          pmap,
	vm_map_offset_t vaddr,
	ppnum_t         ppn)
{
	pv_hashed_entry_t       pvh_e;
	pv_rooted_entry_t       pv_h;
	uint32_t                pvhash_idx;
	boolean_t               is_altacct;

	pvh_e = PV_HASHED_ENTRY_NULL;
	pv_h = pai_to_pvh(ppn_to_pai(ppn));

	if (__improbable(pv_h->pmap == PMAP_NULL)) {
		return FALSE;
	}

	if (PVE_VA(pv_h) == vaddr && pv_h->pmap == pmap) {
		
		return IS_ALTACCT_PAGE(ppn, pv_h);
	}

	CHK_NPVHASH();
	pvhash_idx = pvhashidx(pmap, vaddr);
	LOCK_PV_HASH(pvhash_idx);
	pvh_e = *(pvhash(pvhash_idx));
	while (PV_HASHED_ENTRY_NULL != pvh_e) {
		if (pvh_e->pmap == pmap &&
		    PVE_VA(pvh_e) == vaddr &&
		    pvh_e->ppn == ppn) {
			break;
		}
		pvh_e = pvh_e->nexth;
	}
	if (PV_HASHED_ENTRY_NULL == pvh_e) {
		is_altacct = FALSE;
	} else {
		is_altacct = IS_ALTACCT_PAGE(ppn, pvh_e);
	}
	UNLOCK_PV_HASH(pvhash_idx);

	return is_altacct;
}

static inline void
PMAP_ZINFO_PALLOC(pmap_t pmap, vm_size_t bytes)
{
	pmap_ledger_credit(pmap, task_ledgers.tkm_private, (ledger_amount_t)bytes);
}

static inline void
PMAP_ZINFO_PFREE(pmap_t pmap, vm_size_t bytes)
{
	pmap_ledger_debit(pmap, task_ledgers.tkm_private, (ledger_amount_t)bytes);
}

extern boolean_t        pmap_initialized;
int             phys_attribute_test(
	ppnum_t         phys,
	int             bits);
void            phys_attribute_clear(
	ppnum_t         phys,
	int             bits,
	unsigned int    options,
	void            *arg);
void    pmap_pcid_configure(void);
static inline boolean_t
pmap_cmpx_pte(pt_entry_t *entryp, pt_entry_t old, pt_entry_t new)
{
	return __c11_atomic_compare_exchange_strong((_Atomic pt_entry_t *)entryp, &old, new,
	           memory_order_acq_rel_smp, memory_order_relaxed);
}



static inline void
pmap_update_pte(boolean_t is_ept, pt_entry_t *mptep, uint64_t pclear_bits, uint64_t pset_bits, bool oldpte_invalid_ok)
{
	pt_entry_t npte, opte;
	do {
		opte = *mptep;
		if (__improbable(opte == 0)) {
			return;
		} else if (__improbable(!oldpte_invalid_ok && (opte & PTE_VALID_MASK(is_ept)) == 0)) {
			return;
		}
		npte = opte & ~(pclear_bits);
		npte |= pset_bits;
	}       while (!pmap_cmpx_pte(mptep, opte, npte));

	if (__improbable((is_ept == FALSE) && (npte & PTE_COMPRESSED) && (npte & INTEL_PTE_NX))) {
		pmap_corrupted_pte_detected(mptep, pclear_bits, pset_bits);
	}
}


static inline
pml4_entry_t *
pmap64_pml4(pmap_t pmap, vm_map_offset_t vaddr)
{
	if (__improbable((vaddr > 0x00007FFFFFFFFFFFULL) &&
	    (vaddr < 0xFFFF800000000000ULL))) {
		return NULL;
	}

}

static inline pml4_entry_t *
pmap64_user_pml4(pmap_t pmap, vm_map_offset_t vaddr)
{
	if (__improbable((vaddr > 0x00007FFFFFFFFFFFULL) &&
	    (vaddr < 0xFFFF800000000000ULL))) {
		return NULL;
	}

}


static inline pdpt_entry_t *
pmap64_pdpt(pmap_t pmap, vm_map_offset_t vaddr)
{
	pml4_entry_t    newpf;
	pml4_entry_t    *pml4;
	boolean_t       is_ept;

	pml4 = pmap64_pml4(pmap, vaddr);
	is_ept = is_ept_pmap(pmap);

	if (pml4 && (*pml4 & PTE_VALID_MASK(is_ept))) {
		newpf = *pml4 & PG_FRAME;
		return &((pdpt_entry_t *) PHYSMAP_PTOV(newpf))
		       [(vaddr >> PDPTSHIFT) & (NPDPTPG - 1)];
	}
	return NULL;
}

static inline pd_entry_t *
pmap_pde_internal1(vm_map_offset_t vaddr, boolean_t is_ept, pdpt_entry_t *pdpte)
{
	if (*pdpte & PTE_VALID_MASK(is_ept)) {
		pdpt_entry_t    newpf = *pdpte & PG_FRAME;
		return &((pd_entry_t *) PHYSMAP_PTOV(newpf))
		       [(vaddr >> PDSHIFT) & (NPDPG - 1)];
	} else {
		return NULL;
	}
}

static inline pd_entry_t *
pmap_pde_internal0(pmap_t pmap, vm_map_offset_t vaddr, boolean_t is_ept)
{
	pdpt_entry_t    *pdpt;

	pdpt = pmap64_pdpt(pmap, vaddr);
	if (pdpt) {
		return pmap_pde_internal1(vaddr, is_ept, pdpt);
	} else {
		return NULL;
	}
}


static inline pd_entry_t *
pmap_pde(pmap_t pmap, vm_map_offset_t vaddr)
{
	pdpt_entry_t    *pdpt;
	boolean_t       is_ept;

	pdpt = pmap64_pdpt(pmap, vaddr);
	is_ept = is_ept_pmap(pmap);

	if (pdpt) {
		return pmap_pde_internal1(vaddr, is_ept, pdpt);
	} else {
		return NULL;
	}
}





static inline pt_entry_t *
pmap_pte_internal(vm_map_offset_t vaddr, boolean_t is_ept, pd_entry_t *pde)
{
	if (*pde & PTE_VALID_MASK(is_ept)) {
		if (__improbable(*pde & PTE_PS)) {
			return pde;
		}
		pd_entry_t      newpf = *pde & PG_FRAME;

		return &((pt_entry_t *)PHYSMAP_PTOV(newpf))
		       [i386_btop(vaddr) & (ppnum_t)(NPTEPG - 1)];
	} else {
		return NULL;
	}
}

static inline pt_entry_t *
pmap_pte(pmap_t pmap, vm_map_offset_t vaddr)
{
	pd_entry_t      *pde;

	boolean_t       is_ept;

	is_ept = is_ept_pmap(pmap);

	pde = pmap_pde_internal0(pmap, vaddr, is_ept);

	if (pde) {
		return pmap_pte_internal(vaddr, is_ept, pde);
	} else {
		return NULL;
	}
}

extern void     pmap_alias(
	vm_offset_t     ava,
	vm_map_offset_t start,
	vm_map_offset_t end,
	vm_prot_t       prot,
	unsigned int options);
void power_management_init(void);
void pmKextRegister(uint32_t version, pmDispatch_t *cpuFuncs,
    pmCallBacks_t *callbacks);
void pmUnRegister(pmDispatch_t *cpuFuncs);
void pmCPUStateInit(void);
uint64_t pmCPUGetDeadline(struct cpu_data *cpu);
uint64_t pmCPUSetDeadline(struct cpu_data *cpu, uint64_t deadline);
void pmCPUDeadline(struct cpu_data *cpu);
boolean_t pmCPUExitIdle(struct cpu_data *cpu);
void pmCPUMarkRunning(struct cpu_data *cpu);
void pmMarkAllCPUsOff(void);
int pmCPUControl(uint32_t cmd, void *datap);
void pmCPUHalt(uint32_t reason);
void pmTimerSave(void);
void pmTimerRestore(void);
kern_return_t pmCPUExitHalt(int cpu);
kern_return_t pmCPUExitHaltToOff(int cpu);
uint32_t pmTimerQueueMigrate(int);
void pmSafeMode(x86_lcpu_t *lcpu, uint32_t flags);
x86_lcpu_t *
pmGetLogicalCPU(int cpu);
x86_lcpu_t *
pmGetMyLogicalCPU(void);
processor_t
pmLCPUtoProcessor(int lcpu);
x86_pkg_t *
pmGetPkgRoot(void);
__BEGIN_DECLS


static inline uint16_t
get_es(void)
{
	uint16_t es;
	__asm__ volatile ("mov %%es, %0" : "=r" (es));
	return es;
}

static inline void
set_es(uint16_t es)
{
	__asm__ volatile ("mov %0, %%es" : : "r" (es));
}

static inline uint16_t
get_ds(void)
{
	uint16_t ds;
	__asm__ volatile ("mov %%ds, %0" : "=r" (ds));
	return ds;
}

static inline void
set_ds(uint16_t ds)
{
	__asm__ volatile ("mov %0, %%ds" : : "r" (ds));
}

static inline uint16_t
get_fs(void)
{
	uint16_t fs;
	__asm__ volatile ("mov %%fs, %0" : "=r" (fs));
	return fs;
}

static inline void
set_fs(uint16_t fs)
{
	__asm__ volatile ("mov %0, %%fs" : : "r" (fs));
}

static inline uint16_t
get_gs(void)
{
	uint16_t gs;
	__asm__ volatile ("mov %%gs, %0" : "=r" (gs));
	return gs;
}

static inline void
set_gs(uint16_t gs)
{
	__asm__ volatile ("mov %0, %%gs" : : "r" (gs));
}

static inline uint16_t
get_ss(void)
{
	uint16_t ss;
	__asm__ volatile ("mov %%ss, %0" : "=r" (ss));
	return ss;
}

static inline void
set_ss(uint16_t ss)
{
	__asm__ volatile ("mov %0, %%ss" : : "r" (ss));
}

static inline uintptr_t
get_cr0(void)
{
	uintptr_t cr0;
	__asm__ volatile ("mov %%cr0, %0" : "=r" (cr0));
	return cr0;
}

static inline void
set_cr0(uintptr_t value)
{
	__asm__ volatile ("mov %0, %%cr0" : : "r" (value));
}

static inline uintptr_t
get_cr2(void)
{
	uintptr_t cr2;
	__asm__ volatile ("mov %%cr2, %0" : "=r" (cr2));
	return cr2;
}

static inline uintptr_t
get_cr3_raw(void)
{
	uintptr_t cr3;
	__asm__ volatile ("mov %%cr3, %0" : "=r" (cr3));
	return cr3;
}

static inline void
set_cr3_raw(uintptr_t value)
{
	__asm__ volatile ("mov %0, %%cr3" : : "r" (value));
}

static inline uintptr_t
get_cr3_base(void)
{
	uintptr_t cr3;
	__asm__ volatile ("mov %%cr3, %0" : "=r" (cr3));
	return cr3 & ~(0xFFFULL);
}

static inline void
set_cr3_composed(uintptr_t base, uint16_t pcid, uint64_t preserve)
{
	__asm__ volatile ("mov %0, %%cr3" : : "r" (base | pcid | ( (preserve) << 63) ));
}

static inline uintptr_t
get_cr4(void)
{
	uintptr_t cr4;
	__asm__ volatile ("mov %%cr4, %0" : "=r" (cr4));
	return cr4;
}

static inline void
set_cr4(uintptr_t value)
{
	__asm__ volatile ("mov %0, %%cr4" : : "r" (value));
}

static inline uintptr_t
x86_get_flags(void)
{
	uintptr_t erflags;
	__asm__ volatile ("pushf; pop	%0"  :  "=r" (erflags));
	return erflags;
}

static inline void
clear_ts(void)
{
	__asm__ volatile ("clts");
}

static inline unsigned short
get_tr(void)
{
	unsigned short seg;
	__asm__ volatile ("str %0" : "=rm" (seg));
	return seg;
}

static inline void
set_tr(unsigned int seg)
{
	__asm__ volatile ("ltr %0" : : "rm" ((unsigned short)(seg)));
}

static inline unsigned short
sldt(void)
{
	unsigned short seg;
	__asm__ volatile ("sldt %0" : "=rm" (seg));
	return seg;
}

static inline void
lldt(unsigned int seg)
{
	__asm__ volatile ("lldt %0" : : "rm" ((unsigned short)(seg)));
}

static inline void
lgdt(uintptr_t *desc)
{
	__asm__ volatile ("lgdt %0" : : "m" (*desc));
}

static inline void
lidt(uintptr_t *desc)
{
	__asm__ volatile ("lidt %0" : : "m" (*desc));
}

static inline void
swapgs(void)
{
	__asm__ volatile ("swapgs");
}

static inline void
hlt(void)
{
	__asm__ volatile ("hlt");
}


extern int rdmsr64_carefully(uint32_t msr, uint64_t *val);
extern int wrmsr64_carefully(uint32_t msr, uint64_t val);
static inline void
wbinvd(void)
{
	__asm__ volatile ("wbinvd");
}

static inline void
invlpg(uintptr_t addr)
{
	__asm__  volatile ("invlpg (%0)" :: "r" (addr) : "memory");
}

static inline void
clac(void)
{
	__asm__  volatile ("clac");
}

static inline void
stac(void)
{
	__asm__  volatile ("stac");
}












typedef 


extern void do_mfence(void);
static inline uint64_t
rdpmc64(uint32_t pmc)
{
	uint32_t lo = 0, hi = 0;
	rdpmc(pmc, lo, hi);
	return (((uint64_t)hi) << 32) | ((uint64_t)lo);
}

static inline uint64_t
rdmsr64(uint32_t msr)
{
	uint32_t lo = 0, hi = 0;
	rdmsr(msr, lo, hi);
	return (((uint64_t)hi) << 32) | ((uint64_t)lo);
}

static inline void
wrmsr64(uint32_t msr, uint64_t val)
{
	wrmsr(msr, (val & 0xFFFFFFFFUL), ((val >> 32) & 0xFFFFFFFFUL));
}

static inline uint64_t
rdtsc64(void)
{
	uint64_t lo, hi;
	rdtsc(lo, hi);
	return ((hi) << 32) | (lo);
}

static inline uint64_t
rdtsc64_nofence(void)
{
	uint64_t lo, hi;
	rdtsc_nofence(lo, hi);
	return ((hi) << 32) | (lo);
}

static inline uint64_t
rdtscp64(uint32_t *aux)
{
	uint64_t lo, hi;
	__asm__ volatile ("rdtscp; mov %%ecx, %1"
                                          : "=a" (lo), "=d" (hi), "=m" (*aux)
                                          :
                                          : "ecx");
	return ((hi) << 32) | (lo);
}


extern int rdmsr_carefully(uint32_t msr, uint32_t *lo, uint32_t *hi);
extern void     _rtc_nanotime_adjust(
	uint64_t                tsc_base_delta,
	pal_rtc_nanotime_t      *dst);
extern uint64_t _rtc_nanotime_read(
	pal_rtc_nanotime_t      *rntp);
extern uint64_t _rtc_tsc_to_nanoseconds(
	uint64_t    value,
	pal_rtc_nanotime_t      *rntp);
extern int     rtclock_intr(x86_saved_state_t *regs);
extern void             rtc_timer_init(void);
extern void             rtclock_early_init(void);
extern void             rtc_nanotime_init(uint64_t);
extern void             rtc_decrementer_configure(void);
static inline uint16_t
sel_to_selector(sel_t   sel)
{
	union {
		sel_t           sel;
		uint16_t        selector;
	} tconv;

	tconv.sel = sel;

	return tconv.selector;
}

static inline sel_t
selector_to_sel(uint16_t selector)
{
	union {
		uint16_t        selector;
		sel_t           sel;
	} tconv;

	tconv.selector = selector;

	return tconv.sel;
}


















typedef struct __attribute__((packed)) {
	uint16_t        size;
	void            *ptr;
} x86_64_desc_register_t;
extern void                     df_task_start(void);
extern void                     mc_task_start(void);
int  serial_init(void);
void serial_putc(char);
int  serial_getc(void);
extern void *get_user_regs(thread_t);
extern void *act_thread_csave(void);
extern void act_thread_catt(void *ctx);
extern void act_thread_cfree(void *ctx);
extern void             i386_exception(
	int                     exc,
	mach_exception_code_t   code,
	mach_exception_subcode_t subcode);
extern void             sync_iss_to_iks(x86_saved_state_t *regs);
extern void             sync_iss_to_iks_unconditionally(
	x86_saved_state_t       *regs);
extern void             kernel_trap(x86_saved_state_t *regs, uintptr_t *lo_spp);
extern void             user_trap(x86_saved_state_t *regs);
extern void             interrupt(x86_saved_state_t *regs);
extern void             panic_double_fault64(x86_saved_state_t *regs) __abortlike;
extern void             panic_machine_check64(x86_saved_state_t *regs) __abortlike;
extern void             panic_i386_backtrace(void *, int, const char *, boolean_t, x86_saved_state_t *);
extern void     print_one_backtrace(pmap_t pmap, vm_offset_t topfp, const char *cur_marker, boolean_t is_64_bit);
extern void     print_thread_num_that_crashed(task_t task);
extern void     print_tasks_user_threads(task_t task);
extern void     print_threads_registers(thread_t thread);
extern void     print_uuid_info(task_t task);
extern void     print_launchd_info(void);
extern void tsc_get_info(tscInfo_t *info);
extern void tsc_init(void);
extern int ucode_interface(uint64_t addr);
extern void ucode_update_wake_and_apply_cpu_was(void);
int host_vmxon(boolean_t exclusive);
void host_vmxoff(void);
KALLOC_ARRAY_TYPE_DECL(ipc_entry_table, struct ipc_entry);
static inline ipc_entry_bits_t
ipc_entry_new_rollpoint(
	ipc_entry_bits_t rollbits)
{
	rollbits = (rollbits << IE_BITS_ROLL_POS) & IE_BITS_ROLL_MASK;
	ipc_entry_bits_t newgen = IE_BITS_GEN_MASK + IE_BITS_GEN_ONE;
	return newgen | rollbits;
}


static inline ipc_entry_bits_t
ipc_entry_new_gen(
	ipc_entry_bits_t oldgen)
{
	ipc_entry_bits_t sum  = (oldgen + IE_BITS_GEN_ONE) & IE_BITS_GEN_MASK;
	ipc_entry_bits_t roll = oldgen & IE_BITS_ROLL_MASK;
	ipc_entry_bits_t newgen = (sum % IE_BITS_ROLL(oldgen)) | roll;
	return newgen;
}


static inline boolean_t
ipc_entry_gen_rolled(
	ipc_entry_bits_t oldgen,
	ipc_entry_bits_t newgen)
{
	return (oldgen & IE_BITS_GEN_MASK) > (newgen & IE_BITS_GEN_MASK);
}




extern unsigned int ipc_entry_table_count_max(void) __pure2;
extern ipc_entry_t ipc_entry_lookup(
	ipc_space_t             space,
	mach_port_name_t        name);
extern kern_return_t ipc_entries_hold(
	ipc_space_t             space,
	natural_t               count);
extern kern_return_t ipc_entry_claim(
	ipc_space_t             space,
	ipc_object_t            object,
	mach_port_name_t        *namep,
	ipc_entry_t             *entryp);
extern kern_return_t ipc_entry_alloc(
	ipc_space_t             space,
	ipc_object_t            object,
	mach_port_name_t        *namep,
	ipc_entry_t             *entryp);
extern kern_return_t ipc_entry_alloc_name(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             *entryp);
extern void ipc_entry_dealloc(
	ipc_space_t             space,
	ipc_object_t            object,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern void ipc_entry_modified(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern kern_return_t ipc_entry_grow_table(
	ipc_space_t             space,
	ipc_table_elems_t       target_size);
extern mach_port_name_t ipc_entry_name_mask(
	mach_port_name_t name);
__options_decl(ipc_eventlink_option_t, uint64_t, {
	IPC_EVENTLINK_NONE          = 0,
	IPC_EVENTLINK_NO_WAIT       = 0x1,
	IPC_EVENTLINK_HANDOFF       = 0x2,
	IPC_EVENTLINK_FORCE_WAKEUP  = 0x4,
});
__options_decl(ipc_eventlink_type_t, uint8_t, {
	IPC_EVENTLINK_TYPE_NO_COPYIN         = 0x1,
	IPC_EVENTLINK_TYPE_WITH_COPYIN       = 0x2,
});
void ipc_eventlink_init(void);
void
ipc_eventlink_init(void);
void
ipc_eventlink_reference(
	struct ipc_eventlink *ipc_eventlink);
void
ipc_eventlink_deallocate(
	struct ipc_eventlink *ipc_eventlink);
uint64_t
    mach_eventlink_signal_trap(
	mach_port_name_t port,
	uint64_t         signal_count __unused);
uint64_t
mach_eventlink_wait_until_trap(
	mach_port_name_t                    eventlink_port,
	uint64_t                            wait_count,
	mach_eventlink_signal_wait_option_t option,
	kern_clock_id_t                     clock_id,
	uint64_t                            deadline);
uint64_t
    mach_eventlink_signal_wait_until_trap(
	mach_port_name_t                    eventlink_port,
	uint64_t                            wait_count,
	uint64_t                            signal_count __unused,
	mach_eventlink_signal_wait_option_t option,
	kern_clock_id_t                     clock_id,
	uint64_t                            deadline);
extern boolean_t ipc_hash_lookup(
	ipc_space_t             space,
	ipc_object_t            obj,
	mach_port_name_t        *namep,
	ipc_entry_t             *entryp);
extern void ipc_hash_insert(
	ipc_space_t             space,
	ipc_object_t            obj,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern void ipc_hash_delete(
	ipc_space_t             space,
	ipc_object_t            obj,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern boolean_t ipc_hash_table_lookup(
	ipc_entry_table_t       table,
	ipc_object_t            obj,
	mach_port_name_t        *namep,
	ipc_entry_t             *entryp);
extern void ipc_hash_table_insert(
	ipc_entry_table_t       table,
	ipc_object_t            obj,
	mach_port_index_t       index,
	ipc_entry_t             entry);
extern void ipc_hash_table_delete(
	ipc_entry_table_t       table,
	ipc_object_t            obj,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern natural_t ipc_hash_info(
	hash_info_bucket_t      *info,
	natural_t count);
os_refgrp_decl(static, iie_refgrp, "IIERefGroup", NULL);
__BEGIN_DECLS


extern void ipc_importance_reference(ipc_importance_elem_t elem);
extern void ipc_importance_release(ipc_importance_elem_t elem);
extern void ipc_importance_task_reference(ipc_importance_task_t task_elem);
extern void ipc_importance_task_release(ipc_importance_task_t task_imp);
extern void ipc_importance_reset(ipc_importance_task_t task_imp, boolean_t donor);
extern ipc_importance_task_t ipc_importance_for_task(task_t task, boolean_t made);
extern void ipc_importance_disconnect_task(task_t task);
extern ipc_importance_inherit_t ipc_importance_exec_switch_task(task_t old_task, task_t new_task);
extern boolean_t ipc_importance_task_is_donor(ipc_importance_task_t task_imp);
extern boolean_t ipc_importance_task_is_never_donor(ipc_importance_task_t task_imp);
extern boolean_t ipc_importance_task_is_marked_donor(ipc_importance_task_t task_imp);
extern boolean_t ipc_importance_task_is_marked_live_donor(ipc_importance_task_t task_imp);
extern void ipc_importance_task_mark_donor(ipc_importance_task_t task_imp, boolean_t donating);
extern void ipc_importance_task_mark_live_donor(ipc_importance_task_t task_imp, boolean_t live_donating);
extern void ipc_importance_task_update_live_donor(ipc_importance_task_t task_imp);
extern boolean_t ipc_importance_task_is_marked_receiver(ipc_importance_task_t task_imp);
extern void ipc_importance_task_mark_receiver(ipc_importance_task_t task_imp, boolean_t receiving);
extern boolean_t ipc_importance_task_is_denap_receiver(ipc_importance_task_t task_imp);
extern boolean_t ipc_importance_task_is_marked_denap_receiver(ipc_importance_task_t task_imp);
extern void ipc_importance_task_mark_denap_receiver(ipc_importance_task_t task_imp, boolean_t receiving);
extern boolean_t ipc_importance_task_is_any_receiver_type(ipc_importance_task_t task_imp);
extern kern_return_t ipc_importance_task_hold_internal_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern kern_return_t ipc_importance_task_drop_internal_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern kern_return_t ipc_importance_task_hold_file_lock_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern kern_return_t ipc_importance_task_drop_file_lock_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern kern_return_t ipc_importance_task_hold_legacy_external_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern kern_return_t ipc_importance_task_drop_legacy_external_assertion(ipc_importance_task_t task_imp, uint32_t count);
extern boolean_t ipc_importance_check_circularity(ipc_port_t port, ipc_port_t dest);
extern boolean_t ipc_importance_send(
	ipc_kmsg_t              kmsg,
	mach_msg_option64_t     option);
extern void ipc_importance_receive(
	ipc_kmsg_t              kmsg,
	mach_msg_option64_t     option);
extern void ipc_importance_unreceive(
	ipc_kmsg_t              kmsg,
	mach_msg_option64_t     option);
extern void ipc_importance_clean(ipc_kmsg_t kmsg);
extern void ipc_importance_assert_clean(ipc_kmsg_t kmsg);
extern int task_importance_list_pids(task_t task, int flags, char *pid_list, unsigned int max_count);
__options_decl(ipc_control_port_options_t, uint32_t, {
	ICP_OPTIONS_NONE         = 0x00,

	
	ICP_OPTIONS_PINNED_1P_SOFT       = 0x01,
	ICP_OPTIONS_PINNED_1P_HARD       = 0x02,
	ICP_OPTIONS_IMMOVABLE_1P_SOFT    = 0x04,
	ICP_OPTIONS_IMMOVABLE_1P_HARD    = 0x08,

	ICP_OPTIONS_PINNED_3P_SOFT       = 0x10,
	ICP_OPTIONS_PINNED_3P_HARD       = 0x20,
	ICP_OPTIONS_IMMOVABLE_3P_SOFT    = 0x40,
	ICP_OPTIONS_IMMOVABLE_3P_HARD    = 0x80,

	ICP_OPTIONS_PINNED_ALL_HARD      = ICP_OPTIONS_PINNED_1P_HARD | ICP_OPTIONS_PINNED_3P_HARD,
	ICP_OPTIONS_PINNED_ALL_SOFT      = ICP_OPTIONS_PINNED_1P_SOFT | ICP_OPTIONS_PINNED_3P_SOFT,

	ICP_OPTIONS_IMMOVABLE_ALL_HARD   = ICP_OPTIONS_IMMOVABLE_1P_HARD | ICP_OPTIONS_IMMOVABLE_3P_HARD,
	ICP_OPTIONS_IMMOVABLE_ALL_SOFT   = ICP_OPTIONS_IMMOVABLE_1P_SOFT | ICP_OPTIONS_IMMOVABLE_3P_SOFT,
});
__enum_decl(ipc_kmsg_type_t, uint8_t, {
	
	IKM_TYPE_ALL_INLINED    = 0,
	
	IKM_TYPE_UDATA_OOL      = 1,
	
	IKM_TYPE_KDATA_OOL      = 2,
	
	IKM_TYPE_ALL_OOL        = 3
});
__options_closed_decl(ipc_kmsg_keep_alive_t, uint8_t, {
	IKM_KEEP_ALIVE_NONE   = 0x0, 
	IKM_KEEP_ALIVE_OWNED  = 0x1, 
	IKM_KEEP_ALIVE_IN_USE = 0x2, 
});
static_assert(sizeof(struct ipc_kmsg) == IKM_ALLOC_SIZE);
static_assert(offsetof(struct ipc_kmsg, ikm_big_data) +
    IKM_BIG_MSG_SIZE == IKM_ALLOC_SIZE);
static_assert(offsetof(struct ipc_kmsg, ikm_small_data) + IKM_SMALL_MSG_SIZE +
    2 * sizeof(void *) + 2 * sizeof(mach_msg_size_t) == IKM_ALLOC_SIZE);
KALLOC_TYPE_VAR_DECLARE(KT_IPC_KMSG_KDATA_OOL);
extern bool ipc_kmsg_enqueue_qos(
	ipc_kmsg_queue_t        queue,
	ipc_kmsg_t              kmsg);
extern bool ipc_kmsg_override_qos(
	ipc_kmsg_queue_t        queue,
	ipc_kmsg_t              kmsg,
	mach_msg_qos_t          qos_ovr);
extern void ipc_kmsg_rmqueue_first(
	ipc_kmsg_queue_t        queue,
	ipc_kmsg_t              kmsg);
__options_decl(ipc_kmsg_alloc_flags_t, uint32_t, {
	
	IPC_KMSG_ALLOC_USER             = 0x0000,
	IPC_KMSG_ALLOC_KERNEL           = 0x0001,

	IPC_KMSG_ALLOC_ZERO             = 0x0002,
	IPC_KMSG_ALLOC_ALL_INLINE       = 0x0004,
	IPC_KMSG_ALLOC_NOFAIL           = 0x0008,
	IPC_KMSG_ALLOC_LINEAR           = 0x0010,
	IPC_KMSG_ALLOC_USE_KEEP_ALIVE   = 0x0020, 
});
extern ipc_kmsg_t ipc_kmsg_alloc(
	mach_msg_size_t         msg_size,
	mach_msg_size_t         aux_size,
	mach_msg_size_t         desc_count,
	ipc_kmsg_alloc_flags_t  flags);
extern void ipc_kmsg_free(
	ipc_kmsg_t              kmsg);
extern void ipc_kmsg_clean_descriptors(
	mach_msg_kdescriptor_t * kdesc __counted_by(number),
	mach_msg_type_number_t  number);
extern void ipc_kmsg_sign_descriptors(
	mach_msg_kdescriptor_t *kdesc,
	mach_msg_size_t         dsc_count);
__options_decl(ipc_kmsg_destroy_flags_t, uint32_t, {
	IPC_KMSG_DESTROY_ALL           = 0x0000,
	IPC_KMSG_DESTROY_SKIP_REMOTE   = 0x0001,
	IPC_KMSG_DESTROY_SKIP_LOCAL    = 0x0002,
	IPC_KMSG_DESTROY_NOT_SIGNED    = 0x0004,
});
extern void ipc_kmsg_destroy(
	ipc_kmsg_t                kmsg,
	ipc_kmsg_destroy_flags_t  flags);
extern bool ipc_kmsg_delayed_destroy(
	ipc_kmsg_t              kmsg);
extern bool ipc_kmsg_delayed_destroy_queue(
	ipc_kmsg_queue_t        queue);
extern void ipc_kmsg_reap_delayed(void);
extern bool ipc_kmsg_keep_alive_try_reusing(
	ipc_kmsg_t              kmsg);
extern void ipc_kmsg_keep_alive_abandon(
	ipc_kmsg_t              kmsg);
extern mach_msg_header_t *ikm_header(
	ipc_kmsg_t              kmsg);
extern void *ikm_udata(
	ipc_kmsg_t              kmsg,
	mach_msg_size_t         desc_count,
	bool                    complex);
extern void * ikm_udata_from_header(
	ipc_kmsg_t              kmsg);
extern mach_msg_return_t ipc_kmsg_get_from_kernel(
	mach_msg_header_t      *msg,
	mach_msg_size_t         size,
	mach_msg_option64_t     options,
	ipc_kmsg_t             *kmsgp);
extern mach_msg_return_t ipc_kmsg_send(
	ipc_kmsg_t              kmsg,
	mach_msg_option64_t     options,
	mach_msg_timeout_t      timeout_val);
extern mach_msg_return_t ipc_kmsg_put_to_user(
	ipc_kmsg_t              kmsg,     
	mach_msg_recv_bufs_t   *recv_bufs,
	mach_msg_recv_result_t *msgr,
	mach_msg_option64_t     option,
	vm_map_t                map,
	mach_msg_return_t       mr);
extern void ipc_kmsg_put_to_kernel(
	mach_msg_header_t      *msg,
	mach_msg_option64_t     options,
	ipc_kmsg_t              kmsg,
	mach_msg_size_t         size);
extern mach_msg_return_t ipc_kmsg_copyin_from_user(
	ipc_kmsg_t              kmsg,
	mach_msg_send_uctx_t   *send_uctx,
	ipc_space_t             space,
	vm_map_t                map,
	mach_msg_priority_t     priority,
	mach_msg_option64_t    *optionp);
extern mach_msg_return_t ipc_kmsg_copyin_from_kernel(
	ipc_kmsg_t              kmsg);
extern mach_msg_return_t ipc_kmsg_copyout(
	ipc_kmsg_t              kmsg,
	ipc_space_t             space,
	vm_map_t                map,
	mach_msg_option64_t     option);
extern mach_msg_return_t ipc_kmsg_copyout_pseudo(
	ipc_kmsg_t              kmsg,
	ipc_space_t             space,
	vm_map_t                map);
extern mach_msg_size_t ipc_kmsg_copyout_size(
	ipc_kmsg_t              kmsg,
	vm_map_t                map);
extern void ipc_kmsg_copyout_dest_to_user(
	ipc_kmsg_t              kmsg,
	ipc_space_t             space);
extern void ipc_kmsg_copyout_dest_to_kernel(
	ipc_kmsg_t              kmsg,
	ipc_space_t             space);
extern struct thread_group *ipc_kmsg_get_thread_group(
	ipc_kmsg_t              kmsg);
extern mach_msg_trailer_size_t ipc_kmsg_trailer_size(
	mach_msg_option64_t     option,
	vm_map_t                map);
extern mach_msg_max_trailer_t *ipc_kmsg_get_trailer(
	ipc_kmsg_t              kmsg);
extern void ipc_kmsg_set_voucher_port(
	ipc_kmsg_t              kmsg,
	ipc_port_t              voucher,
	mach_msg_type_name_t    type);
extern ipc_port_t ipc_kmsg_get_voucher_port(
	ipc_kmsg_t              kmsg);
extern void ipc_kmsg_clear_voucher_port(
	ipc_kmsg_t              kmsg);
extern mach_msg_size_t ipc_kmsg_validate_signature(
	ipc_kmsg_t              kmsg) __result_use_check;
extern void send_prp_telemetry(int msgh_id);
extern void ipc_mqueue_init(
	ipc_mqueue_t            mqueue);
extern boolean_t ipc_mqueue_destroy_locked(
	ipc_mqueue_t            mqueue,
	waitq_link_list_t      *free_l);
extern void ipc_mqueue_changed(
	ipc_space_t             space,
	waitq_t                 waitq);
extern kern_return_t ipc_mqueue_add_locked(
	ipc_mqueue_t            mqueue,
	ipc_pset_t              pset,
	waitq_link_t           *linkp);
extern mach_msg_return_t ipc_mqueue_send_locked(
	ipc_mqueue_t            mqueue,
	ipc_kmsg_t              kmsg,
	mach_msg_option64_t     option,
	mach_msg_timeout_t      timeout_val);
extern void ipc_mqueue_override_send_locked(
	ipc_mqueue_t            mqueue,
	mach_msg_qos_t          qos_ovr);
extern void ipc_mqueue_receive(
	waitq_t                 waitq,
	mach_msg_timeout_t      timeout_val,
	int                     interruptible,
	thread_t                thread,
	bool                    has_continuation);
extern wait_result_t ipc_mqueue_receive_on_thread_and_unlock(
	waitq_t                 waitq,
	mach_msg_timeout_t      rcv_timeout,
	int                     interruptible,
	thread_t                thread);
extern void ipc_mqueue_receive_continue(
	void                    *param,
	wait_result_t           wresult);
extern void ipc_mqueue_select_on_thread_locked(
	ipc_mqueue_t            port_mq,
	mach_msg_option64_t     option64,
	thread_t                thread);
extern unsigned ipc_mqueue_peek(
	ipc_mqueue_t            mqueue,
	mach_port_seqno_t       *msg_seqnop,
	mach_msg_size_t         *msg_sizep,
	mach_msg_id_t           *msg_idp,
	mach_msg_max_trailer_t  *msg_trailerp,
	ipc_kmsg_t              *kmsgp);
extern unsigned ipc_mqueue_peek_locked(
	ipc_mqueue_t            mqueue,
	mach_port_seqno_t       *msg_seqnop,
	mach_msg_size_t         *msg_sizep,
	mach_msg_id_t           *msg_idp,
	mach_msg_max_trailer_t  *msg_trailerp,
	ipc_kmsg_t              *kmsgp);
extern void ipc_mqueue_release_msgcount(
	ipc_mqueue_t            port_mq);
extern void ipc_mqueue_set_qlimit_locked(
	ipc_mqueue_t            mqueue,
	mach_port_msgcount_t    qlimit);
extern void ipc_mqueue_set_seqno_locked(
	ipc_mqueue_t            mqueue,
	mach_port_seqno_t       seqno);
extern mach_msg_return_t ipc_mqueue_copyin(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_object_t            *objectp);
extern bool
ipc_port_has_klist(
	ipc_port_t              port);
#pragma GCC visibility push(hidden)

typedef 




extern void ipc_notify_port_deleted(
	ipc_port_t              port,
	mach_port_name_t        name);
extern void ipc_notify_send_possible(
	ipc_port_t              port,
	mach_port_name_t        name);
extern void ipc_notify_port_destroyed(
	ipc_port_t              port,
	ipc_port_t              right);
extern void ipc_notify_no_senders(
	ipc_port_t              notify,
	mach_port_mscount_t     mscount,
	boolean_t               kobject);
extern ipc_notify_nsenders_t ipc_notify_no_senders_prepare(
	ipc_port_t              port);
static inline void
ipc_notify_no_senders_emit(ipc_notify_nsenders_t nsrequest)
{
	if (nsrequest.ns_notify) {
		ipc_notify_no_senders(nsrequest.ns_notify,
		    nsrequest.ns_mscount, nsrequest.ns_is_kobject);
	}
}

extern void ipc_notify_no_senders_consume(
	ipc_notify_nsenders_t   nsrequest);
extern void ipc_notify_send_once_and_unlock(
	ipc_port_t              port);
extern void ipc_notify_dead_name(
	ipc_port_t              port,
	mach_port_name_t        name);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN
#pragma GCC visibility push(hidden)

typedef natural_t ipc_object_refs_t;
__options_closed_decl(ipc_object_copyout_flags_t, uint32_t, {
	IPC_OBJECT_COPYOUT_FLAGS_NONE                 = 0x0,
	IPC_OBJECT_COPYOUT_FLAGS_PINNED               = 0x1,
	IPC_OBJECT_COPYOUT_FLAGS_NO_LABEL_CHECK       = 0x2,
});
__options_closed_decl(ipc_object_copyin_flags_t, uint16_t, {
	IPC_OBJECT_COPYIN_FLAGS_NONE                          = 0x0,
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_IMMOVABLE_SEND          = 0x1, 
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_DEAD_SEND_ONCE          = 0x2,
	IPC_OBJECT_COPYIN_FLAGS_DEADOK                        = 0x4,
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_REPLY_MAKE_SEND_ONCE    = 0x8,  
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_REPLY_MOVE_SEND_ONCE    = 0x10, 
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_IMMOVABLE_RECEIVE       = 0x20,
	IPC_OBJECT_COPYIN_FLAGS_ALLOW_CONN_IMMOVABLE_RECEIVE  = 0x40, 
	IPC_OBJECT_COPYIN_FLAGS_DEST_EXTRA_COPY               = 0x80,
	IPC_OBJECT_COPYIN_FLAGS_DEST_EXTRA_MOVE               = 0x100,
});
static inline void
io_bits_or(ipc_object_t io, ipc_object_bits_t bits)
{
	
	os_atomic_store(&io->io_bits, io_bits(io) | bits, relaxed);
}

static inline void
io_bits_andnot(ipc_object_t io, ipc_object_bits_t bits)
{
	
	os_atomic_store(&io->io_bits, io_bits(io) & ~bits, relaxed);
}





extern zone_t __single ipc_object_zones[IOT_NUMBER];
extern bool ipc_object_lock_allow_invalid(
	ipc_object_t            object) __result_use_check;
extern void ipc_object_deallocate_register_queue(void);
extern void ipc_object_reference(
	ipc_object_t    object);
extern void ipc_object_release(
	ipc_object_t    object);
extern void ipc_object_release_safe(
	ipc_object_t    object);
extern void ipc_object_release_live(
	ipc_object_t    object);
extern kern_return_t ipc_object_translate(
	ipc_space_t             space,
	mach_port_name_t        name,
	mach_port_right_t       right,
	ipc_object_t           *objectp);
extern kern_return_t ipc_object_translate_port_pset(
	ipc_space_t             space,
	mach_port_name_t        port_name,
	ipc_port_t             *port,
	mach_port_name_t        pset_name,
	ipc_pset_t             *pset);
extern void ipc_object_validate(
	ipc_object_t            object,
	ipc_object_type_t       type);
extern kern_return_t ipc_object_alloc_dead(
	ipc_space_t             space,
	mach_port_name_t        *namep);
extern kern_return_t ipc_object_alloc(
	ipc_space_t             space,
	ipc_object_type_t       otype,
	mach_port_type_t        type,
	mach_port_urefs_t       urefs,
	mach_port_name_t        *namep,
	ipc_object_t            *objectp);
extern kern_return_t ipc_object_alloc_name(
	ipc_space_t             space,
	ipc_object_type_t       otype,
	mach_port_type_t        type,
	mach_port_urefs_t       urefs,
	mach_port_name_t        name,
	ipc_object_t            *objectp,
	void                    (^finish_init)(ipc_object_t object));
extern mach_msg_type_name_t ipc_object_copyin_type(
	mach_msg_type_name_t    msgt_name);
extern kern_return_t ipc_object_copyin(
	ipc_space_t             space,
	mach_port_name_t        name,
	mach_msg_type_name_t    msgt_name,
	ipc_object_copyin_flags_t copyin_flags,
	mach_msg_guarded_port_descriptor_t *gdesc,
	ipc_port_t             *portp);
extern void ipc_object_copyin_from_kernel(
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name);
extern void ipc_object_destroy(
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name);
extern void ipc_object_destroy_dest(
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name);
extern kern_return_t ipc_object_insert_send_right(
	ipc_space_t             space,
	mach_port_name_t        name,
	mach_msg_type_name_t    msgt_name);
extern kern_return_t ipc_object_copyout(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name,
	ipc_object_copyout_flags_t flags,
	mach_msg_guarded_port_descriptor_t *gdesc,
	mach_port_name_t        *namep);
extern kern_return_t ipc_object_copyout_name(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name,
	mach_port_name_t        name);
extern void ipc_object_copyout_dest(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name,
	mach_port_name_t        *namep);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN
#pragma GCC visibility push(hidden)




#pragma mark compile time globals and configurations













#pragma mark policy utils






#pragma mark policy options


extern mach_msg_option64_t ipc_current_user_policy(
	task_t                  task,
	mach_msg_option64_t     user_flags);
extern mach_msg_return_t ipc_preflight_msg_option64(
	mach_msg_option64_t     opts);
#pragma mark legacy trap policies


extern mach_msg_return_t ipc_policy_allow_legacy_send_trap(
	mach_msg_id_t           msgid,
	mach_msg_option64_t     opts);
#pragma mark ipc policy telemetry [temporary]


__enum_closed_decl(ipc_policy_violation_id_t, uint8_t, {
	
	IPCPV_REPLY_PORT_SEMANTICS, 
	IPCPV_RIGID_REPLY_PORT_HARDENED_RUNTIME,
	IPCPV_MOVE_REPLY_PORT_HARDENED_RUNTIME,
	IPCPV_MOVE_REPLY_PORT_3P,
	IPCPV_RIGID_REPLY_PORT_3P,
	
	IPCPV_REPLY_PORT_SEMANTICS_OPTOUT,
	

	_IPCPV_VIOLATION_COUNT,
});
extern bool ip_violates_rigid_reply_port_semantics(
	ipc_port_t                 dest_port,
	ipc_port_t                 reply_port,
	ipc_policy_violation_id_t *reply_port_semantics_violation);
extern bool ip_violates_reply_port_semantics(
	ipc_port_t                 dest_port,
	ipc_port_t                 reply_port,
	ipc_policy_violation_id_t *reply_port_semantics_violation);
extern void ipc_stash_policy_violations_telemetry(
	ipc_policy_violation_id_t   violation_id,
	mach_service_port_info_t    sp_info,
	int                         aux_data);
#pragma mark MACH_SEND_MSG policies


extern mach_msg_return_t ipc_validate_kmsg_header_schema_from_user(
	mach_msg_user_header_t *hdr,
	mach_msg_size_t         dsc_count,
	mach_msg_option64_t     opts);
extern mach_msg_return_t ipc_validate_kmsg_schema_from_user(
	mach_msg_header_t      *kdata,
	mach_msg_send_uctx_t   *send_uctx,
	mach_msg_option64_t     opts);
extern mach_msg_return_t ipc_validate_kmsg_header_from_user(
	mach_msg_header_t      *hdr,
	mach_msg_send_uctx_t   *send_uctx,
	mach_msg_option64_t     opts);
#pragma mark policy guard violations


__cold
extern void mach_port_guard_exception(
	uint32_t                target,
	uint64_t                payload,
	unsigned                reason);
__cold
extern void mach_port_guard_exception_immovable(
	ipc_space_t             space,
	mach_port_name_t        name,
	mach_port_t             port);
__cold
extern void mach_port_guard_exception_pinned(
	ipc_space_t             space,
	mach_port_name_t        name,
	__unused mach_port_t    port,
	uint64_t                payload);
extern int proc_isinitproc(struct proc *p);
KALLOC_ARRAY_TYPE_DECL(ipc_port_request_table, struct ipc_port_request);
static inline bool
ip_in_pset(ipc_port_t port)
{
	return !circle_queue_empty(&port->ip_waitq.waitq_links);
}






extern void __ipc_right_delta_overflow_panic(
	ipc_port_t          port,
	natural_t          *field,
	int                 delta) __abortlike;
__enum_decl(ipc_port_request_opts_t, uintptr_t, {
	IPR_SOR_SPARM_MASK = 0x01,              
	IPR_SOR_SPREQ_MASK = 0x02,              
});
extern boolean_t ipc_port_destination_chain_lock(
	ipc_port_t port,
	ipc_port_t *base);
extern ipc_port_timestamp_t ipc_port_timestamp(void);
extern void __abortlike __ipc_port_inactive_panic(ipc_port_t port);
static inline void
require_ip_active(ipc_port_t port)
{
	if (!ip_active(port)) {
		__ipc_port_inactive_panic(port);
	}
}



static inline bool
ip_in_a_space(ipc_port_t port)
{
	
	return ip_active(port) && port->ip_receiver_name != MACH_PORT_NULL;
}

static inline bool
ip_in_space(ipc_port_t port, ipc_space_t space)
{
	ip_mq_lock_held(port); 
	return ip_in_a_space(port) && port->ip_receiver == space;
}


static inline bool
ip_in_space_noauth(ipc_port_t port, void* space)
{
	void *__single raw_ptr = ptrauth_strip(*(void **)&port->ip_receiver, ptrauth_key_process_independent_data);
	return raw_ptr == space;
}

static inline bool
ip_in_transit(ipc_port_t port)
{
	
	ip_mq_lock_held(port); 
	return ip_active(port) && !ip_in_a_space(port) && port->ip_destination != IP_NULL;
}

static inline bool
ip_in_limbo(ipc_port_t port)
{
	
	ip_mq_lock_held(port); 
	return ip_active(port) && !ip_in_a_space(port) && port->ip_destination == IP_NULL;
}

static inline ipc_space_t
ip_get_receiver(ipc_port_t port)
{
	ip_mq_lock_held(port); 
	return ip_in_a_space(port) ? port->ip_receiver : NULL;
}

static inline void*
ip_get_receiver_ptr_noauth(ipc_port_t port)
{
	void *__single raw_ptr = ptrauth_strip(*(void **)&port->ip_receiver, ptrauth_key_process_independent_data);
	return raw_ptr;
}

static inline mach_port_name_t
ip_get_receiver_name(ipc_port_t port)
{
	return ip_in_a_space(port) ? port->ip_receiver_name : MACH_PORT_NULL;
}

static inline ipc_port_t
ip_get_destination(ipc_port_t port)
{
	ip_mq_lock_held(port); 
	return ip_active(port) && !ip_in_a_space(port) ? port->ip_destination : IP_NULL;
}

static inline ipc_port_timestamp_t
ip_get_death_time(ipc_port_t port)
{
	assert(!ip_active(port));
	return port->ip_timestamp;
}

static inline ipc_importance_task_t
ip_get_imp_task(ipc_port_t port)
{
	return (!ip_is_kobject(port) && !port->ip_specialreply && port->ip_tempowner) ? port->ip_imp_task : IIT_NULL;
}

extern kern_return_t ipc_port_translate_send(
	ipc_space_t                     space,
	mach_port_name_t                name,
	ipc_port_t                     *portp);
extern kern_return_t ipc_port_translate_receive(
	ipc_space_t                     space,
	mach_port_name_t                name,
	ipc_port_t                     *portp);
extern kern_return_t ipc_port_request_alloc(
	ipc_port_t                      port,
	mach_port_name_t                name,
	ipc_port_t                      soright,
	ipc_port_request_opts_t         options,
	ipc_port_request_index_t       *indexp);
extern kern_return_t ipc_port_request_hnotify_alloc(
	ipc_port_t                      port,
	struct host_notify_entry       *hnotify,
	ipc_port_request_index_t       *indexp);
extern kern_return_t ipc_port_request_grow(
	ipc_port_t                      port);
extern mach_port_type_t ipc_port_request_type(
	ipc_port_t                      port,
	mach_port_name_t                name,
	ipc_port_request_index_t        index);
extern ipc_port_t ipc_port_request_cancel(
	ipc_port_t                      port,
	mach_port_name_t                name,
	ipc_port_request_index_t        index);
extern bool ipc_port_request_sparm(
	ipc_port_t                port,
	mach_port_name_t          name,
	ipc_port_request_index_t  index,
	mach_msg_option64_t       option,
	mach_msg_priority_t       priority);
extern void ipc_port_nsrequest(
	ipc_port_t              port,
	mach_port_mscount_t     sync,
	ipc_port_t              notify,
	ipc_port_t              *previousp);
extern boolean_t ipc_port_clear_receiver(
	ipc_port_t              port,
	boolean_t               should_destroy,
	waitq_link_list_t      *free_l);
__options_decl(ipc_port_init_flags_t, uint32_t, {
	IPC_PORT_INIT_NONE                              = 0x00000000,
	IPC_PORT_INIT_MAKE_SEND_RIGHT                   = 0x00000001,
	IPC_PORT_INIT_MESSAGE_QUEUE                     = 0x00000002,
	IPC_PORT_INIT_SPECIAL_REPLY                     = 0x00000004,
	IPC_PORT_INIT_FILTER_MESSAGE                    = 0x00000008,
	IPC_PORT_INIT_TG_BLOCK_TRACKING                 = 0x00000010,
	IPC_PORT_INIT_LOCKED                            = 0x00000020,
	IPC_PORT_INIT_REPLY                             = 0x00000040,
	IPC_PORT_ENFORCE_REPLY_PORT_SEMANTICS           = 0x00000080,
	IPC_PORT_INIT_PROVISIONAL_REPLY                 = 0x00000100,
	IPC_PORT_ENFORCE_RIGID_REPLY_PORT_SEMANTICS     = 0x00000400,
	IPC_PORT_INIT_EXCEPTION_PORT                    = 0x00000800,
});
extern void ipc_port_init(
	ipc_port_t              port,
	ipc_space_t             space,
	ipc_port_init_flags_t   flags,
	mach_port_name_t        name);
extern void ipc_port_lock(
	ipc_port_t              port);
extern void ipc_port_lock_check_aligned(
	ipc_port_t              port);
extern bool ipc_port_lock_try(
	ipc_port_t              port);
extern kern_return_t ipc_port_alloc(
	ipc_space_t             space,
	ipc_port_init_flags_t   flags,
	mach_port_name_t        *namep,
	ipc_port_t              *portp);
extern kern_return_t ipc_port_alloc_name(
	ipc_space_t             space,
	ipc_port_init_flags_t   flags,
	mach_port_name_t        name,
	ipc_port_t              *portp);
extern void ipc_port_set_label(
	ipc_port_t              port,
	ipc_label_t             label);
extern void ipc_port_dnnotify(
	ipc_port_t              port);
extern void ipc_port_spnotify(
	ipc_port_t              port);
extern void ipc_port_destroy(
	ipc_port_t      port);
extern boolean_t
ipc_port_check_circularity(
	ipc_port_t      port,
	ipc_port_t      dest);
extern ipc_port_t ipc_port_make_send_any_locked(
	ipc_port_t      port);
extern ipc_port_t ipc_port_make_send_any(
	ipc_port_t      port) __result_use_check;
extern ipc_port_t ipc_port_make_send_mqueue(
	ipc_port_t      port) __result_use_check;
extern void ipc_port_copy_send_any_locked(
	ipc_port_t      port);
extern ipc_port_t ipc_port_copy_send_any(
	ipc_port_t      port) __result_use_check;
extern ipc_port_t ipc_port_copy_send_mqueue(
	ipc_port_t      port) __result_use_check;
extern mach_port_name_t ipc_port_copyout_send(
	ipc_port_t      sright,
	ipc_space_t     space);
extern mach_port_name_t ipc_port_copyout_send_pinned(
	ipc_port_t      sright,
	ipc_space_t     space);
extern void ipc_port_thread_group_blocked(
	ipc_port_t      port);
extern void ipc_port_thread_group_unblocked(void);
extern void ipc_port_release_send_and_unlock(
	ipc_port_t      port);
extern kern_return_t mach_port_deallocate_kernel(
	ipc_space_t             space,
	mach_port_name_t        name,
	natural_t               kotype);
extern void ipc_port_release_send(
	ipc_port_t      port);
extern void ipc_port_reference(
	ipc_port_t port);
extern void ipc_port_release(
	ipc_port_t port);
extern kern_return_t
ipc_port_propagate_thread_attr(
	ipc_port_t port,
	struct thread_attr_for_ipc_propagation attr);
extern kern_return_t
ipc_port_reset_thread_attr(ipc_port_t port);
extern ipc_port_t ipc_port_make_sonce_locked(
	ipc_port_t      port);
extern ipc_port_t ipc_port_make_sonce(
	ipc_port_t      port);
extern void ipc_port_release_sonce(
	ipc_port_t      port);
extern void ipc_port_release_sonce_and_unlock(
	ipc_port_t      port);
extern void ipc_port_release_receive(
	ipc_port_t      port);
extern void ipc_port_finalize(
	ipc_port_t      port);
extern pid_t ipc_port_get_receiver_task_locked(ipc_port_t port, uintptr_t *task);
extern pid_t ipc_port_get_receiver_task(ipc_port_t port, uintptr_t *task);
extern ipc_port_t ipc_port_alloc_special(
	ipc_space_t             space,
	ipc_port_init_flags_t   flags);
extern void ipc_port_dealloc_special_and_unlock(
	ipc_port_t      port,
	ipc_space_t     space);
extern void ipc_port_dealloc_special(
	ipc_port_t      port,
	ipc_space_t     space);
extern void ipc_port_recv_update_inheritor(ipc_port_t port,
    struct turnstile *turnstile,
    turnstile_update_flags_t flags);
extern void ipc_port_send_update_inheritor(ipc_port_t port,
    struct turnstile *turnstile,
    turnstile_update_flags_t flags);
extern int
ipc_special_reply_get_pid_locked(ipc_port_t port);
static inline ipc_pset_t
ips_from_waitq(waitq_t wq)
{
	return __container_of(wq.wqs_set, struct ipc_pset, ips_wqset);
}


extern void ipc_pset_lock(
	ipc_pset_t              pset);
extern kern_return_t ipc_pset_alloc(
	ipc_space_t             space,
	mach_port_name_t        *namep,
	ipc_pset_t              *psetp);
extern kern_return_t ipc_pset_alloc_name(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_pset_t              *psetp);
extern ipc_pset_t ipc_pset_alloc_special(
	ipc_space_t             space);
extern void ipc_pset_destroy(
	ipc_space_t     space,
	ipc_pset_t      pset);
extern void ipc_pset_finalize(
	ipc_pset_t      pset);
extern struct turnstile *filt_ipc_kqueue_turnstile(
	struct knote   *kn);
extern bool filt_machport_kqueue_has_turnstile(
	struct knote   *kn);
extern void filt_machport_turnstile_prepare_lazily(
	struct knote   *kn,
	mach_msg_type_name_t    msgt_name,
	ipc_port_t      port);
extern struct turnstile *filt_machport_stash_port(
	struct knote   *kn,
	ipc_port_t      port,
	int            *link);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN
#pragma GCC visibility push(hidden)


extern bool service_port_defense_enabled;
extern kern_return_t ipc_right_lookup_read(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_bits_t       *bitsp,
	ipc_object_t           *objectp);
extern kern_return_t ipc_right_lookup_write(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t            *entryp);
extern kern_return_t ipc_right_lookup_two_write(
	ipc_space_t             space,
	mach_port_name_t        name1,
	ipc_entry_t            *entryp1,
	mach_port_name_t        name2,
	ipc_entry_t            *entryp2);
extern bool          ipc_right_reverse(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_port_name_t       *namep,
	ipc_entry_t            *entryp);
extern kern_return_t ipc_right_request_alloc(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_port_request_opts_t options,
	ipc_port_t              notify,
	ipc_port_t              *previousp);
extern bool      ipc_right_inuse(
	ipc_entry_t             entry);
extern boolean_t ipc_right_check(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	ipc_object_copyin_flags_t flags);
extern void ipc_right_terminate(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern kern_return_t ipc_right_destroy(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	boolean_t               check_guard,
	uint64_t                guard);
extern kern_return_t ipc_right_dealloc(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry);
extern kern_return_t ipc_right_delta(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	mach_port_right_t       right,
	mach_port_delta_t       delta);
extern kern_return_t ipc_right_destruct(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	mach_port_delta_t       srdelta,
	uint64_t                guard);
extern kern_return_t ipc_right_info(
	ipc_space_t             space,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	mach_port_type_t       *typep,
	mach_port_urefs_t      *urefsp);
extern boolean_t ipc_right_copyin_check_reply(
	ipc_space_t             space,
	mach_port_name_t        reply_name,
	ipc_entry_t             reply_entry,
	mach_msg_type_name_t    reply_type,
	ipc_entry_t             dest_entry,
	uint8_t                *reply_port_semantics_violation);
extern void          ipc_right_copyin_rcleanup_init(
	ipc_copyin_rcleanup_t  *icrc,
	mach_msg_guarded_port_descriptor_t *gdesc);
extern void          ipc_right_copyin_rcleanup_destroy(
	ipc_copyin_rcleanup_t  *icrc);
extern kern_return_t ipc_right_copyin(
	ipc_space_t             space,
	mach_port_name_t        name,
	mach_msg_type_name_t    msgt_name,
	ipc_object_copyin_flags_t  flags,
	ipc_entry_t             entry,
	ipc_port_t             *portp,
	ipc_copyin_cleanup_t   *icc,
	ipc_copyin_rcleanup_t  *icrc);
extern kern_return_t ipc_right_copyout(
	ipc_space_t             space,
	ipc_port_t              port,
	mach_msg_type_name_t    msgt_name,
	ipc_object_copyout_flags_t flags,
	mach_port_name_t        name,
	ipc_entry_t             entry,
	mach_msg_guarded_port_descriptor_t *gdesc);
__options_decl(ipc_service_port_label_flags_t, uint16_t, {
	ISPL_FLAGS_SPECIAL_PDREQUEST      = 1,
	ISPL_FLAGS_SEND_PD_NOTIFICATION   = (1 << 1),
	ISPL_FLAGS_BOOTSTRAP_PORT         = (1 << 2),
	ISPL_FLAGS_THROTTLED              = (1 << 3),
});
static inline void
ipc_service_port_label_set_flag(ipc_service_port_label_t port_splabel, ipc_service_port_label_flags_t flag)
{
	assert(port_splabel != IPC_SERVICE_PORT_LABEL_NULL);
	port_splabel->ispl_flags |= flag;
}

static inline void
ipc_service_port_label_clear_flag(ipc_service_port_label_t port_splabel, ipc_service_port_label_flags_t flag)
{
	assert(port_splabel != IPC_SERVICE_PORT_LABEL_NULL);
	port_splabel->ispl_flags &= ~flag;
}


kern_return_t
ipc_service_port_label_alloc(mach_service_port_info_t sp_info, void **port_label_ptr);
void
ipc_service_port_label_dealloc(void * ip_splabel, bool service_port);
kern_return_t
ipc_service_port_derive_sblabel(mach_port_name_t service_port_name, void **sblabel_ptr, bool *filter_msgs);
void *
ipc_service_port_get_sblabel(ipc_port_t port);
void
ipc_service_port_label_set_attr(ipc_service_port_label_t port_splabel, mach_port_name_t name, mach_port_context_t context);
void
ipc_service_port_label_get_attr(ipc_service_port_label_t port_splabel, mach_port_name_t *name, mach_port_context_t *context);
extern void ipc_space_reference(
	ipc_space_t             space);
extern void ipc_space_release(
	ipc_space_t             space);
extern void ipc_voucher_receive_postprocessing(ipc_kmsg_t kmsg, mach_msg_option64_t option);
extern void ipc_voucher_send_preprocessing(ipc_kmsg_t kmsg);
extern ipc_voucher_t ipc_voucher_get_default_voucher(void);
extern void mach_init_activity_id(void);
#pragma pack(1)
typedef 
typedef ipc_voucher_attr_recipe_data_t *ipc_voucher_attr_recipe_t;
#pragma pack()



typedef kern_return_t (*ipc_voucher_attr_manager_release_value_t)(ipc_voucher_attr_manager_t,
    mach_voucher_attr_key_t,
    mach_voucher_attr_value_handle_t,
    mach_voucher_attr_value_reference_t);
__BEGIN_DECLS


extern uintptr_t unsafe_convert_port_to_voucher(
	ipc_port_t              port) __pure2;
extern ipc_voucher_t convert_port_to_voucher(
	ipc_port_t              port);
extern ipc_voucher_t convert_port_name_to_voucher(
	mach_port_name_t        name);
extern void ipc_voucher_reference(
	ipc_voucher_t           voucher);
extern void ipc_voucher_release(
	ipc_voucher_t           voucher);
extern ipc_port_t convert_voucher_to_port(
	ipc_voucher_t           voucher);
extern kern_return_t
ipc_create_mach_voucher(
	ipc_voucher_attr_raw_recipe_array_t             recipes,
	ipc_voucher_attr_raw_recipe_array_size_t        recipe_size,
	ipc_voucher_t                                   *new_voucher);
extern kern_return_t
ipc_voucher_attr_control_create_mach_voucher(
	ipc_voucher_attr_control_t                      control,
	ipc_voucher_attr_raw_recipe_array_t             recipes,
	ipc_voucher_attr_raw_recipe_array_size_t        recipe_size,
	ipc_voucher_t                                   *new_voucher);
extern void
ipc_register_well_known_mach_voucher_attr_manager(
	ipc_voucher_attr_manager_t              manager,
	mach_voucher_attr_value_handle_t        default_value,
	mach_voucher_attr_key_t                 key,
	ipc_voucher_attr_control_t              *control);
extern kern_return_t
mach_voucher_attr_control_get_values(
	ipc_voucher_attr_control_t              control,
	ipc_voucher_t                           voucher,
	mach_voucher_attr_value_handle_t        *out_values,
	mach_msg_type_number_t                  *in_out_size);
__BEGIN_DECLS



void
kdp_core_exclude_region(vm_offset_t addr, vm_size_t size);
void
kdp_core_unexclude_region(vm_offset_t addr, vm_size_t size);
void
kdp_raise_exception(
	unsigned int                exception,
	unsigned int                code,
	unsigned int                subcode,
	void                        *saved_state
	);
void
kdp_reset(void);
void
kdp_init(void);
void kdp_machine_init(void);
extern void kdp_register_callout(kdp_callout_fn_t fn, void *arg);
void kdp_memcpy(void *dst, const void *src, size_t len);
size_t kdp_strlcpy(char *dst, const char *src, size_t maxlen);
size_t kdp_vm_map_get_page_size(vm_map_t map, size_t *effective_page_mask);
__options_closed_decl(kdp_fault_result_flags_t, uint32_t, {
	KDP_FAULT_RESULT_PAGED_OUT = 0x1, 
	KDP_FAULT_RESULT_TRIED_FAULT = 0x2, 
	KDP_FAULT_RESULT_FAULTED_IN = 0x3, 
});
__options_closed_decl(kdp_fault_flags_t, uint32_t, {
	KDP_FAULT_FLAGS_NONE = 0x0,
	KDP_FAULT_FLAGS_ENABLE_FAULTING = 0x1, 
	KDP_FAULT_FLAGS_MULTICPU = 0x2, 
});
__options_closed_decl(kdp_traverse_mappings_flags_t, uint32_t, {
	KDP_TRAVERSE_MAPPINGS_FLAGS_NONE = 0x0,
	KDP_TRAVERSE_MAPPINGS_FLAGS_PHYSICAL = 0x1 
});
kern_return_t
kdp_traverse_mappings(
	task_t task,
	kdp_fault_flags_t fault_flags,
	kdp_traverse_mappings_flags_t traverse_mappings_flags,
	kdp_traverse_mappings_callback callback,
	void * context);
kern_return_t
kdp_task_dyld_info(task_t task, kdp_fault_flags_t fault_flags, uint64_t * dyld_load_address, uuid_t dyld_uuid, size_t * task_page_size);
vm_offset_t kdp_find_phys(vm_map_t map, vm_offset_t target_addr, kdp_fault_flags_t fault_flags, struct kdp_fault_result *fault_results);
int kdp_generic_copyin(vm_map_t map, uint64_t uaddr, void *dest, size_t size, kdp_fault_flags_t fault_flags, find_phys_fn_t find_phys_fn, void *context);
int kdp_generic_copyin_word(task_t task, uint64_t addr, uint64_t *result, kdp_fault_flags_t fault_flags, find_phys_fn_t find_phys_fn, void *context);
int kdp_generic_copyin_string(task_t task, uint64_t addr, char *buf, int buf_sz, kdp_fault_flags_t fault_flags, find_phys_fn_t find_phys_fn, void *context);
void panic_spin_shmcon(void);
void shmem_mark_as_busy(void);
void shmem_unmark_as_busy(void);
void kdp_panic_dump(void);
void begin_panic_transfer(void);
void abort_panic_transfer(void);
void kdp_set_dump_info(const uint32_t flags, const char *file, const char *destip,
    const char *routerip, const uint32_t port);
void kdp_get_dump_info(kdp_dumpinfo_reply_t *rp);
int kern_dump(enum kern_dump_type kd_variant);
boolean_t dumped_kernel_core(void);
int     kdp_send_crashdump_pkt(unsigned int request, char *corename,
    uint64_t length, void *panic_data);
int     kdp_send_crashdump_data(unsigned int request, char *corename,
    uint64_t length, void * txstart);
void kern_collectth_state_size(uint64_t * tstate_count, uint64_t * tstate_size);
void kern_collectth_state(thread_t thread, void *buffer, uint64_t size, void **iter);
void kern_collect_userth_state_size(task_t task, uint64_t * tstate_count, uint64_t * tstate_size);
void kern_collect_userth_state(task_t task, thread_t thread, void *buffer, uint64_t size);
boolean_t kdp_has_polled_corefile(void);
kern_return_t kdp_polled_corefile_error(void);
IOPolledCoreFileMode_t kdp_polled_corefile_mode(void);
void kdp_core_init(void);
kern_return_t kdp_core_output(void *kdp_core_out_vars, uint64_t length, void * data);
kern_return_t kdp_reset_output_vars(void *kdp_core_out_vars, uint64_t totalbytes, bool encrypt_core, bool *out_should_skip_coredump);
kern_return_t kern_dump_record_file(void *kdp_core_out_vars, const char *filename, uint64_t file_offset, uint64_t *out_file_length, uint64_t details_flags);
kern_return_t kern_dump_seek_to_next_file(void *kdp_core_out_varss, uint64_t next_file_offset);
extern boolean_t bootloader_valid_page(ppnum_t ppn);
kern_return_t kdp_core_handle_encryption_available(void);
kern_return_t kdp_core_handle_lz4_available(void);
void
kdp_register_send_receive(kdp_send_t send, kdp_receive_t receive);
void
kdp_unregister_send_receive(kdp_send_t send, kdp_receive_t receive);
extern
boolean_t
kdp_packet(
	unsigned char *,
	int *,
	unsigned short *
	);
extern
boolean_t
kdp_remove_all_breakpoints(void);
extern
void
kdp_exception(
	unsigned char *,
	int *,
	unsigned short *,
	unsigned int,
	unsigned int,
	unsigned int
	);
extern
boolean_t
kdp_exception_ack(
	unsigned char *,
	int
	);
extern
void
kdp_panic(
	const char          *fmt,
	...
	) __printflike(1, 2);
extern
void
kdp_machine_reboot(
	void
	);
extern
void
kdp_us_spin(
	int usec
	);
extern
int
kdp_intr_disbl(
	void
	);
extern
void
kdp_intr_enbl(
	int s
	);
extern
kdp_error_t
kdp_machine_read_regs(
	unsigned int cpu,
	unsigned int flavor,
	char *data,
	int *size
	);
extern
kdp_error_t
kdp_machine_write_regs(
	unsigned int cpu,
	unsigned int flavor,
	char *data,
	int *size
	);
extern
void
kdp_machine_hostinfo(
	kdp_hostinfo_t *hostinfo
	);
extern
void
kdp_sync_cache(
	void
	);
void
kdp_machine_get_breakinsn(
	uint8_t *bytes,
	uint32_t *size
	);
extern void
kdp_ml_enter_debugger(
	void
	);
mach_vm_size_t
    kdp_machine_vm_read( mach_vm_address_t, caddr_t, mach_vm_size_t);
mach_vm_size_t
    kdp_machine_vm_write( caddr_t, mach_vm_address_t, mach_vm_size_t);
mach_vm_size_t
    kdp_machine_phys_read(kdp_readphysmem64_req_t * rq, caddr_t ,
    uint16_t );
mach_vm_size_t
    kdp_machine_phys_write(kdp_writephysmem64_req_t * rq, caddr_t ,
    uint16_t );
int
    kdp_machine_ioport_read(kdp_readioport_req_t *, caddr_t , uint16_t );
int
    kdp_machine_ioport_write(kdp_writeioport_req_t *, caddr_t , uint16_t );
int
    kdp_machine_msr64_read(kdp_readmsr64_req_t *, caddr_t , uint16_t );
int
    kdp_machine_msr64_write(kdp_writemsr64_req_t *, caddr_t , uint16_t );
vm_map_offset_t
kdp_core_start_addr(void);
static boolean_t
kdp_unknown(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_connect(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_disconnect(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_reattach(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_hostinfo(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_suspend(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_readregs(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_writeregs(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_version(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_kernelversion(
	kdp_pkt_t             *,
	int                   *,
	unsigned short        *
	);
static boolean_t
kdp_regions(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_maxbytes(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_readmem(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_readmem64(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_readphysmem64(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_writemem(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_writemem64(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_writephysmem64(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_resumecpus(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_breakpoint_set(
	kdp_pkt_t *,
	int *,
	unsigned short *t
	);
static boolean_t
kdp_breakpoint64_set(
	kdp_pkt_t *,
	int  *,
	unsigned short *t
	);
static boolean_t
kdp_breakpoint_remove(
	kdp_pkt_t *,
	int *,
	unsigned short *
	);
static boolean_t
kdp_breakpoint64_remove(
	kdp_pkt_t *,
	int   *,
	unsigned short *
	);
static boolean_t
kdp_reboot(
	kdp_pkt_t *,
	int   *,
	unsigned short *
	);
static boolean_t
kdp_readioport(kdp_pkt_t *, int *, unsigned short *);
static boolean_t
kdp_writeioport(kdp_pkt_t *, int *, unsigned short *);
static boolean_t
kdp_readmsr64(kdp_pkt_t *, int *, unsigned short *);
static boolean_t
kdp_writemsr64(kdp_pkt_t *, int *, unsigned short *);
static boolean_t
kdp_dumpinfo(kdp_pkt_t *, int *, unsigned short *);
void kdp_serialize_packet(unsigned char *, unsigned int, void (*func)(char));
unsigned char *kdp_unserialize_packet(unsigned char, unsigned int *);
extern struct kdp_ether_addr kdp_get_mac_addr(void);
unsigned int  kdp_get_ip_address(void);
void    kdp_register_link(kdp_link_t link, kdp_mode_t mode);
void    kdp_unregister_link(kdp_link_t link, kdp_mode_t mode);
__BEGIN_DECLS



void kern_coredump_log(void *context, const char *string, ...) __printflike(2, 3);
kern_return_t kern_register_xnu_coredump_helper(kern_coredump_callback_config *kc_callbacks);
kern_return_t kern_register_sk_coredump_helper(kern_coredump_callback_config *kc_callbacks, void *refcon);
kern_return_t kern_register_userspace_coredump(task_t task, const char * name);
kern_return_t kern_unregister_userspace_coredump(task_t task);
kern_return_t kern_do_coredump(void *core_outvars, boolean_t kernel_only, uint64_t first_file_offset, uint64_t *last_file_offset, uint64_t details_flags);
kern_return_t user_dump_init(void *refcon, void *context);
kern_return_t user_dump_save_summary(void *refcon, core_save_summary_cb callback, void *context);
kern_return_t user_dump_save_seg_descriptions(void *refcon, core_save_segment_descriptions_cb callback, void *context);
kern_return_t user_dump_save_thread_state(void *refcon, void *buf, core_save_thread_state_cb callback, void *context);
kern_return_t user_dump_save_sw_vers_detail(void *refcon, core_save_sw_vers_detail_cb callback, void *context);
kern_return_t user_dump_save_segment_data(void *refcon, core_save_segment_data_cb callback, void *context);
kern_return_t user_dump_save_note_summary(void *refcon, core_save_note_summary_cb callback, void *context);
kern_return_t user_dump_save_note_descriptions(void *refcon, core_save_note_descriptions_cb callback, void *context);
kern_return_t user_dump_save_note_data(void *refcon, core_save_note_data_cb callback, void *context);
bool sk_core_enabled(void);
void sk_core_init(void);
uint64_t sk_core_size(void);
extern boolean_t        thread_affinity_is_supported(void);
extern void             thread_affinity_dup(thread_t parent, thread_t child);
extern void             thread_affinity_terminate(thread_t thread);
extern void             task_affinity_create(
	task_t,
	task_t);
extern void             task_affinity_deallocate(
	task_t);
extern kern_return_t    task_affinity_info(
	task_t,
	task_info_t,
	mach_msg_type_number_t  *);
extern kern_return_t    thread_affinity_set(thread_t thread, uint32_t tag);
extern uint32_t         thread_affinity_get(thread_t thread);
extern void             thread_affinity_exec(thread_t thread);
extern void arcade_init(void);
extern void arcade_ast(thread_t thread);
extern void arcade_prepare(task_t task, thread_t thread);
extern void arcade_register_notify(mach_msg_header_t *msg);
extern void arcade_register_reference(arcade_register_t arcade_reg);
extern void arcade_register_release(arcade_register_t arcade_reg);
extern mach_port_t convert_arcade_register_to_port(arcade_register_t arcade_reg);
extern arcade_register_t convert_port_to_arcade_register(mach_port_t port);
__BEGIN_DECLS

__abortlike
extern void     Assert(
	const char      *file,
	int             line,
	const char      *expression) __attribute__((noinline));
__END_DECLS


__enum_decl(mach_assert_type_t, unsigned char, {
	MACH_ASSERT_DEFAULT,
	MACH_ASSERT_3P,
	MACH_ASSERT_3S,
	MACH_ASSERT_3U,
});
__options_decl(ast_t, uint32_t, {
	AST_PREEMPT               = 0x01,
	AST_QUANTUM               = 0x02,
	AST_URGENT                = 0x04,
	AST_HANDOFF               = 0x08,
	AST_YIELD                 = 0x10,
	AST_APC                   = 0x20,    
	AST_LEDGER                = 0x40,
	AST_BSD                   = 0x80,
	AST_KPERF                 = 0x100,   
	AST_MACF                  = 0x200,   
	AST_RESET_PCS             = 0x400,   
	AST_ARCADE                = 0x800,   
	AST_MACH_EXCEPTION        = 0x1000,
	AST_TELEMETRY_USER        = 0x2000,  
	AST_TELEMETRY_KERNEL      = 0x4000,  
	AST_TELEMETRY_PMI         = 0x8000,  
	AST_SFI                   = 0x10000, 
	AST_DTRACE                = 0x20000,
	AST_TELEMETRY_IO          = 0x40000, 
	AST_KEVENT                = 0x80000,
	AST_REBALANCE             = 0x100000, 
	
	AST_PROC_RESOURCE         = 0x400000, 
	AST_DEBUG_ASSERT          = 0x800000, 
	AST_TELEMETRY_MACF        = 0x1000000, 
});
extern void ast_taken_kernel(void);
extern void ast_taken_user(void);
extern void ast_check(processor_t processor);
extern ast_t *ast_pending(void);
extern void ast_on(ast_t reasons);
extern void ast_off(ast_t reasons);
extern ast_t ast_consume(ast_t reasons);
extern ast_t ast_peek(ast_t reasons);
extern void ast_context(thread_t thread);
extern void ast_propagate(thread_t thread);
extern void ast_dtrace_on(void);
extern void dtrace_ast(void);
extern void kevent_ast(thread_t thread, uint16_t bits);
extern void act_set_astkevent(thread_t thread, uint16_t bits);
extern uint16_t act_clear_astkevent(thread_t thread, uint16_t bits);
extern bool act_set_ast_reset_pcs(task_t task, thread_t thread);
extern void act_set_debug_assert(void);
extern void thread_debug_return_to_user_ast(thread_t thread);
ipc_port_t audit_session_mksend(struct auditinfo_addr *, ipc_port_t *);
void audit_session_portdestroy(ipc_port_t *);
void audit_session_aiaref(struct auditinfo_addr *);
void audit_session_aiaunref(struct auditinfo_addr *);
__BEGIN_DECLS


__options_decl(backtrace_flags_t, uint32_t, {
	BTF_NONE = 0x0,
	
	BTF_KERN_INTERRUPTED = 0x1,
});
errno_t backtrace_user_copy_error(void *ctx, void *dst, user_addr_t src,
    size_t size);
__options_decl(backtrace_info_t, uint32_t, {
	BTI_NONE = 0x0,
	
	BTI_64_BIT = 0x1,
	
	
	
	BTI_TRUNCATED = 0x2,
});
unsigned int backtrace(uintptr_t *bt, unsigned int btlen,
    struct backtrace_control *ctl, backtrace_info_t *info_out)
__attribute__((noinline));
__enum_decl(backtrace_pack_t, uint32_t, {
	
	BTP_NONE = 0x0,
	
	BTP_KERN_OFFSET_32 = 0x01,
});
size_t backtrace_packed(backtrace_pack_t packing, uint8_t *bt, size_t btsize,
	struct backtrace_control *ctl, backtrace_info_t *info_out)
__attribute__((noinline));
size_t backtrace_pack(backtrace_pack_t packing, uint8_t *dst,
    size_t dst_size, const uintptr_t *src, unsigned int src_len);
unsigned int backtrace_unpack(backtrace_pack_t packing, uintptr_t *dst,
    unsigned int dst_len, const uint8_t *src, size_t src_size);
unsigned int backtrace_user(uintptr_t *bt, unsigned int btlen,
    const struct backtrace_control *ctl, struct backtrace_user_info *info_out);
inline static uint64_t
bit_ror64(uint64_t bitmap, uint n)
{
	uint64_t result;
	uint64_t _n = (uint64_t)n;
	asm volatile ("ror %0, %1, %2" : "=r" (result) : "r" (bitmap), "r" (_n));
	return result;
}

inline static uint64_t
bit_rol64(uint64_t bitmap, uint n)
{
	return bit_ror64(bitmap, 64U - n);
}






inline static int
bit_first(uint64_t bitmap)
{
	int64_t result;
	asm volatile ("clz %0, %1" : "=r" (result) : "r" (bitmap));
	return 63 - (int)result;
}


inline static int
__bit_next(uint64_t bitmap, int previous_bit)
{
	uint64_t mask = previous_bit ? mask(previous_bit) : ~0ULL;

	return bit_first(bitmap & mask);
}


inline static int
bit_next(uint64_t bitmap, int previous_bit)
{
	if (previous_bit == 0) {
		return -1;
	} else {
		return __bit_next(bitmap, previous_bit);
	}
}


inline static int
lsb_first(uint64_t bitmap)
{
	return __builtin_ffsll((long long)bitmap) - 1;
}


inline static int
lsb_next(uint64_t bitmap, int previous_bit)
{
	uint64_t mask = mask(previous_bit + 1);

	return lsb_first(bitmap & ~mask);
}

inline static int
bit_count(uint64_t x)
{
	return __builtin_popcountll(x);
}


inline static int
bit_floor(uint64_t n)
{
	return bit_first(n);
}


inline static int
bit_ceiling(uint64_t n)
{
	if (n == 0) {
		return -1;
	}
	return bit_first(n - 1) + 1;
}



typedef uint64_t                bitmap_t;
_Static_assert(sizeof(block_hint_t) <= sizeof(short),
    "block_hint_t must fit within a short");
extern void kdp_lck_mtx_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_sema_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_mqueue_send_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_v2_t *waitinfo,
    struct ipc_service_port_label **isplp);
extern void kdp_mqueue_recv_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_v2_t *waitinfo,
    struct ipc_service_port_label **isplp);
extern void kdp_ulock_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_rwlck_find_owner(struct waitq * waitq, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_pthread_find_owner(thread_t thread, thread_waitinfo_t *waitinfo);
extern void *kdp_pthread_get_thread_kwq(thread_t thread);
extern void kdp_workloop_sync_wait_find_owner(thread_t thread, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_wait4_find_process(thread_t thread, event64_t event, thread_waitinfo_t *waitinfo);
extern void kdp_sleep_with_inheritor_find_owner(struct waitq * waitq, __unused event64_t event, thread_waitinfo_t * waitinfo);
extern void kdp_turnstile_fill_tsinfo(struct turnstile *ts, thread_turnstileinfo_v2_t *tsinfo, struct ipc_service_port_label **isplp);
extern void kdp_ipc_fill_splabel(struct ipc_service_port_label *ispl, struct portlabel_info *spl, const char **namep);
extern void kdp_ipc_splabel_size(size_t *ispl_size, size_t *maxnamelen);
void kdp_eventlink_find_owner(struct waitq *waitq, event64_t event, thread_waitinfo_t *waitinfo);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma GCC visibility push(hidden)







struct btlog;
__options_decl(btref_get_flags_t, uint32_t, {
	BTREF_GET_PERMANENT = 0x0001,
	BTREF_GET_NOWAIT    = 0x0002,
});
extern btref_t btref_get(
	void                   *fp,
	btref_get_flags_t       flags);
extern btref_t btref_retain(
	btref_t                 ref);
extern void btref_put(
	btref_t                 btref);
extern uint32_t btref_decode_unslide(
	btref_t                 btref,
	mach_vm_address_t       bt[__counted_by(BTLOG_MAX_DEPTH)]);
__enum_decl(btlog_type_t, uint8_t, {
	BTLOG_LOG       = 1,
	BTLOG_HASH      = 2,
});
extern btlog_t btlog_create(
	btlog_type_t            type,
	uint32_t                num_records,
	uint32_t                sample);
extern kern_return_t btlog_enable(
	btlog_t                 log);
extern void btlog_disable(
	btlog_t                 log);
extern void btlog_destroy(
	btlog_t                 btlog);
extern btlog_type_t btlog_get_type(
	btlog_t                 btlog) __pure2;
extern uint32_t btlog_get_count(
	btlog_t                 btlog) __pure2;
extern bool btlog_sample(
	btlog_t                 btlog);
extern void btlog_record(
	btlog_t                 btlog,
	void                   *element,
	uint8_t                 op,
	btref_t                 btref);
extern void btlog_erase(
	btlog_t                 btlog,
	void                   *element);
extern uint32_t btlog_guess_top(
	btlog_t                 btlog,
	vm_address_t            bt[__counted_by(BTLOG_MAX_DEPTH)],
	uint32_t               *len);
__BEGIN_DECLS

bool kern_config_is_development(void) OS_CONST;
extern void             clock_config(void);
extern void             clock_oldconfig(void);
extern void             clock_init(void);
extern void             clock_oldinit(void);
extern void             clock_timebase_init(void);
extern void             clock_service_create(void);
extern void clock_gettimeofday_set_commpage(
	uint64_t                                abstime,
	uint64_t                                sec,
	uint64_t                                frac,
	uint64_t                                scale,
	uint64_t                                tick_per_sec);
extern void                     machine_delay_until(uint64_t interval,
    uint64_t                deadline);
extern void             nanotime_to_absolutetime(
	clock_sec_t             secs,
	clock_nsec_t    nanosecs,
	uint64_t                *result);
__BEGIN_DECLS


extern void                     clock_adjtime(
	long            *secs,
	int                     *microsecs);
extern void                     clock_initialize_calendar(void);
extern void                     clock_wakeup_calendar(void);
extern void                     clock_update_calendar(void);
extern void                     clock_get_calendar_uptime(clock_sec_t           *secs);
extern void clock_gettimeofday_new(clock_sec_t          *secs,
    clock_usec_t    *microsecs);
extern void                     clock_gettimeofday(
	clock_sec_t                     *secs,
	clock_usec_t            *microsecs);
extern void                     clock_gettimeofday_and_absolute_time(
	clock_sec_t                     *secs,
	clock_usec_t            *microsecs,
	uint64_t                        *absolute_time);
extern void                     clock_set_calendar_microtime(
	clock_sec_t                     secs,
	clock_usec_t            microsecs);
extern void                     clock_get_boottime_nanotime(
	clock_sec_t                     *secs,
	clock_nsec_t            *nanosecs);
extern void                     clock_get_boottime_microtime(
	clock_sec_t                     *secs,
	clock_nsec_t            *microsecs);
extern void                     clock_deadline_for_periodic_event(
	uint64_t                        interval,
	uint64_t                        abstime,
	uint64_t                        *deadline);
extern void                     clock_get_calendar_nanotime_nowait(
	clock_sec_t                     *secs,
	clock_nsec_t            *nanosecs);
boolean_t kdp_clock_is_locked(void);
extern void                     clock_get_calendar_microtime(
	clock_sec_t                     *secs,
	clock_usec_t            *microsecs);
extern void                     clock_get_calendar_absolute_and_microtime(
	clock_sec_t                     *secs,
	clock_usec_t            *microsecs,
	uint64_t                *abstime);
extern void                     clock_get_calendar_nanotime(
	clock_sec_t                     *secs,
	clock_nsec_t            *nanosecs);
extern void                     clock_get_system_microtime(
	clock_sec_t                     *secs,
	clock_usec_t            *microsecs);
extern void                     clock_get_system_nanotime(
	clock_sec_t                     *secs,
	clock_nsec_t            *nanosecs);
extern void                             clock_timebase_info(
	mach_timebase_info_t    info);
extern void                             clock_get_uptime(
	uint64_t                *result);
extern void                             clock_interval_to_deadline(
	uint32_t                interval,
	uint32_t                scale_factor,
	uint64_t                *result);
extern void                             nanoseconds_to_deadline(
	uint64_t                interval,
	uint64_t                *result);
extern void                             clock_interval_to_absolutetime_interval(
	uint32_t                interval,
	uint32_t                scale_factor,
	uint64_t                *result);
extern void                             clock_absolutetime_interval_to_deadline(
	uint64_t                abstime,
	uint64_t                *result);
extern void                             clock_continuoustime_interval_to_deadline(
	uint64_t                abstime,
	uint64_t                *result);
extern void                             clock_delay_until(
	uint64_t                deadline);
extern void                             absolutetime_to_nanoseconds(
	uint64_t                abstime,
	uint64_t                *result);
extern void                             nanoseconds_to_absolutetime(
	uint64_t                nanoseconds,
	uint64_t                *result);
extern void                             absolutetime_to_microtime(
	uint64_t                abstime,
	clock_sec_t             *secs,
	clock_usec_t            *microsecs);
extern uint64_t                 absolutetime_to_continuoustime(
	uint64_t                abstime);
extern uint64_t                 continuoustime_to_absolutetime(
	uint64_t                conttime);
extern void                             delay_for_interval(
	uint32_t                interval,
	uint32_t                scale_factor);
extern void                             delay_for_interval_with_leeway(
	uint32_t                interval,
	uint32_t                leeway,
	uint32_t                scale_factor);
extern void delay(int usec);
__BEGIN_DECLS


void coalitions_init(void);
kern_return_t coalitions_adopt_task(coalition_t *coaltions, task_t task);
kern_return_t coalitions_adopt_init_task(task_t task);
kern_return_t coalitions_adopt_corpse_task(task_t task);
kern_return_t coalitions_remove_task(task_t task);
void          task_release_coalitions(task_t task);
kern_return_t coalitions_set_roles(coalition_t coalitions[COALITION_NUM_TYPES],
    task_t task, int roles[COALITION_NUM_TYPES]);
uint64_t coalition_id(coalition_t coal);
void     task_coalition_ids(task_t task, uint64_t ids[COALITION_NUM_TYPES]);
void     task_coalition_roles(task_t task, int roles[COALITION_NUM_TYPES]);
int      task_coalition_role_for_type(task_t task, int coalition_type);
int      coalition_type(coalition_t coal);
coalition_t task_get_coalition(task_t task, int type);
void     task_coalition_update_gpu_stats(task_t task, uint64_t gpu_ns_delta);
void     coalition_update_ane_stats(coalition_t coalition, uint64_t ane_mach_time, uint64_t ane_energy_nj);
boolean_t task_coalition_adjust_focal_count(task_t task, int count, uint32_t *new_count);
uint32_t task_coalition_focal_count(task_t task);
uint32_t task_coalition_game_mode_count(task_t task);
bool task_coalition_adjust_game_mode_count(task_t task, int count, uint32_t *new_count);
uint32_t task_coalition_carplay_mode_count(task_t task);
bool task_coalition_adjust_carplay_mode_count(task_t task, int count, uint32_t *new_count);
boolean_t task_coalition_adjust_nonfocal_count(task_t task, int count, uint32_t *new_count);
uint32_t task_coalition_nonfocal_count(task_t task);
void coalition_for_each_task(coalition_t coal,
    coalition_for_each_task_block_t block);
void init_coalition_ledgers(void);
int coalition_ledger_set_logical_writes_limit(coalition_t coal, int64_t limit);
void coalition_io_monitor_ctl(struct coalition *coalition, uint32_t flags, int64_t limit);
ledger_t coalition_ledger_get_from_task(task_t task);
void coalition_io_rate_exceeded(int warning, const void *param0, __unused const void *param1);
void coalition_io_ledger_update(task_t task, int32_t flavor, boolean_t is_credit, uint32_t io_size);
void coalition_mark_swappable(coalition_t coal);
bool coalition_is_swappable(coalition_t coal);
kern_return_t coalition_iterate_stackshot(coalition_iterate_fn_t callout, void *arg, uint32_t coalition_type);
coalition_t coalition_find_by_id(uint64_t coal_id);
coalition_t coalition_find_and_activate_by_id(uint64_t coal_id);
void coalition_remove_active(coalition_t coal);
void coalition_retain(coalition_t coal);
void coalition_release(coalition_t coal);
kern_return_t coalition_reap_internal(coalition_t coal);
kern_return_t coalition_request_terminate_internal(coalition_t coal);
kern_return_t coalition_create_internal(int type, int role, boolean_t privileged, boolean_t efficient, coalition_t *out, uint64_t *cid);
boolean_t coalition_term_requested(coalition_t coal);
boolean_t coalition_is_terminated(coalition_t coal);
boolean_t coalition_is_reaped(coalition_t coal);
boolean_t coalition_is_privileged(coalition_t coal);
boolean_t task_is_in_privileged_coalition(task_t task, int type);
kern_return_t coalition_resource_usage_internal(coalition_t coal, struct coalition_resource_usage *cru_out);
kern_return_t coalition_debug_info_internal(coalition_t coal,
    struct coalinfo_debuginfo *c_debuginfo);
task_t kdp_coalition_get_leader(coalition_t coal);
kern_return_t jetsam_coalition_set_policy(coalition_t coal, int flavor, int value);
kern_return_t jetsam_coalition_get_policy(coalition_t coal, int flavor, int *value);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)


typedef uint32_t                  compact_id_t;
extern void compact_id_table_init(
	compact_id_table_t      table);
extern void **compact_id_resolve(
	compact_id_table_t      table,
	compact_id_t            compact_id) __pure2;
extern bool compact_id_slab_valid(
	compact_id_table_t      table,
	compact_id_t            compact_id) __stateful_pure;
extern compact_id_t compact_id_get_locked(
	compact_id_table_t      table,
	compact_id_t            limit,
	void                   *value);
extern compact_id_t compact_id_get(
	compact_id_table_t      table,
	compact_id_t            limit,
	void                   *value);
extern void *compact_id_put(
	compact_id_table_t      table,
	compact_id_t            compact_id);
extern void compact_id_for_each(
	compact_id_table_t      table,
	uint32_t                stride,
	bool                  (^cb)(void *v));
extern void compact_id_table_lock(
	compact_id_table_t      table);
extern void compact_id_table_unlock(
	compact_id_table_t      table);
kern_return_t
register_copyout_shim(copyout_shim_fn_t copyout_shim_fn, unsigned co_src_flags);
void *
cos_kernel_unslide(const void *);
void *
cos_kernel_reslide(const void *);
OS_OVERLOADABLE
extern void counter_alloc(struct generic_counter_t *);
OS_OVERLOADABLE
extern void counter_free(struct generic_counter_t *);
OS_OVERLOADABLE
extern void counter_add(struct generic_counter_t *, uint64_t amount);
OS_OVERLOADABLE
extern void counter_inc(struct generic_counter_t *);
OS_OVERLOADABLE
extern void counter_dec(struct generic_counter_t *);
OS_OVERLOADABLE
extern void counter_add_preemption_disabled(struct generic_counter_t *, uint64_t amount);
OS_OVERLOADABLE
extern void counter_inc_preemption_disabled(struct generic_counter_t *);
OS_OVERLOADABLE
extern void counter_dec_preemption_disabled(struct generic_counter_t *);
OS_OVERLOADABLE
extern uint64_t counter_load(struct generic_counter_t *);
#pragma mark implementation details


__startup_func void scalable_counter_static_boot_mangle(scalable_counter_t *counter);
__startup_func void scalable_counter_static_init(scalable_counter_t *counter);
COUNTER_MAKE_PROTOTYPES(scalable_counter_t);
COUNTER_MAKE_PROTOTYPES(atomic_counter_t);
#pragma once





__enum_decl(cpc_hw_t, unsigned int, {
	CPC_HW_CPMU,
	CPC_HW_UPMU,
	CPC_HW_COUNT,
});
__result_use_check bool cpc_hw_acquire(cpc_hw_t hw, const char *owner_name);
bool cpc_hw_in_use(cpc_hw_t hw);
void cpc_hw_release(cpc_hw_t hw, const char *owner_name);
bool cpc_event_allowed(cpc_hw_t hw, uint16_t event_selector);
bool cpc_is_secure(void);
__BEGIN_DECLS

extern void             _disable_preemption(void);
extern void             _disable_preemption_without_measurements(void);
extern void             _enable_preemption(void);
void Debugger(const char * message);
void populate_model_name(char *);
boolean_t panic_validate_ptr(void *ptr, vm_size_t size, const char *what);
boolean_t panic_get_thread_proc_task(struct thread *thread, struct task **task, struct proc **proc);
void register_additional_panic_data_buffer(const char *producer_name, void *buf, int len);
unsigned panic_active(void);
boolean_t oslog_is_safe(void);
boolean_t debug_mode_active(void);
boolean_t stackshot_active(void);
void panic_stackshot_reset_state(void);
kern_return_t
stack_snapshot_from_kernel(int pid, void *buf, uint32_t size, uint64_t flags,
    uint64_t delta_since_timestamp, uint32_t pagetable_mask, unsigned *bytes_traced);
boolean_t on_device_corefile_enabled(void);
boolean_t panic_stackshot_to_disk_enabled(void);
extern unsigned char    *__counted_by(sizeof(uuid_t)) kernel_uuid;
extern void debug_log_init(void);
extern void debug_putc(char);
extern boolean_t debug_is_current_cpu_in_panic_state(void);
extern void phys_carveout_init(void);
extern boolean_t debug_is_in_phys_carveout(vm_map_offset_t va);
extern boolean_t debug_can_coredump_phys_carveout(void);
extern boolean_t kernel_debugging_restricted(void);
void    SavePanicInfo(const char *message, void *panic_data, uint64_t panic_options, const char* panic_initiator);
void    paniclog_flush(void);
void    panic_display_zalloc(void);
void    panic_display_kernel_aslr(void);
void    panic_display_hibb(void);
void    panic_display_model_name(void);
void    panic_display_kernel_uuid(void);
void    panic_display_process_name(void);
void    panic_print_symbol_name(vm_address_t search);
void    panic_display_compressor_stats(void);
void    panic_assert_format(char *buf, size_t len, struct mach_assert_hdr *hdr, long a, long b);
__printflike(3, 0)
kern_return_t DebuggerTrapWithState(debugger_op db_op, const char *db_message, const char *db_panic_str, va_list *db_panic_args,
    uint64_t db_panic_options, void *db_panic_data_ptr, boolean_t db_proceed_on_sync_failure, unsigned long db_panic_caller, const char *db_panic_initiator);
void handle_debugger_trap(unsigned int exception, unsigned int code, unsigned int subcode, void *state);
void DebuggerWithContext(unsigned int reason, void *ctx, const char *message, uint64_t debugger_options_mask, unsigned long debugger_caller);
const char *sysctl_debug_get_preoslog(size_t *size);
void sysctl_debug_free_preoslog(void);
extern kern_return_t
IOProvideCoreFileAccess(IOCoreFileAccessRecipient recipient, void *recipient_context);
kern_return_t kdp_core_handle_new_encryption_key(IOCoreFileAccessCallback access_data, void *access_context, void *recipient_context);
extern kern_return_t    ecc_log_record_event(const struct ecc_event *ev);
extern kern_return_t    ecc_log_get_next_event(struct ecc_event *ev);
extern uint32_t         ecc_log_get_correction_count(void);
__options_decl(ecc_flags_t, uint32_t, {
	ECC_NONE                        = 0x00000000,
	
	ECC_IS_CORRECTABLE              = 0x00000001,
	
	ECC_DB_CORRUPTED                = 0x00000002,
	
	ECC_IS_TEST_ERROR               = 0x00000004,
	
	ECC_DB_ONLY                     = 0x00000008,
	
	ECC_REMOVE_ADDR                     = 0x00000010
});
__options_decl(ecc_version_t, uint32_t, {
	ECC_V1,

	
	ECC_NUM_VERSIONS
});
kern_return_t kern_ecc_poll_register(platform_error_handler_ecc_poll_t poll_func, uint32_t max_errors);
__options_decl(mcc_flags_t, uint32_t, {
	MCC_NONE                        = 0x00000000,
	MCC_IS_SINGLE_BIT               = 0x00000001,
	MCC_IS_MULTI_BIT                = 0x00000002,
});
extern kern_return_t ecc_log_memory_error(uint64_t physical_address, ecc_flags_t ecc_flags);
extern kern_return_t ecc_log_memory_error_internal(uint64_t physical_address, ecc_flags_t ecc_flags);
extern kern_return_t ecc_log_memory_error_delayed(uint64_t physical_address, ecc_flags_t ecc_flags);
kern_return_t ecc_log_memory_error_ce(uint64_t physical_address, ecc_flags_t ecc_flags, uint32_t ce_count);
kern_return_t
mcc_log_memory_error(mcc_ecc_event_t event);
void gpu_describe(gpu_descriptor_t);
uint64_t gpu_accumulate_time(uint32_t scope, uint32_t gpu_id, uint32_t gpu_domain, uint64_t gpu_accumulated_ns, uint64_t gpu_tstamp_ns);
extern kern_return_t
current_energy_id(energy_id_t *energy_id);
extern kern_return_t
task_id_token_to_energy_id(mach_port_name_t name, energy_id_t *energy_id);
__enum_decl(energy_id_source_t, uint32_t, {
	ENERGY_ID_SOURCE_GPU = 1,
});
extern kern_return_t
energy_id_report_energy(energy_id_source_t energy_source, energy_id_t self_id,
    energy_id_t on_behalf_of_id, uint64_t energy);
uint64_t io_rate_update(
	uint64_t io_rate_flags, 
	uint64_t read_ops_delta,
	uint64_t write_ops_delta,
	uint64_t read_bytes_delta,
	uint64_t write_bytes_delta);
void io_rate_update_register(io_rate_update_callback_t);
void gpu_submission_telemetry(
	uint64_t gpu_ncmds_total,
	uint64_t gpu_noutstanding,
	uint64_t gpu_busy_ns_total,
	uint64_t gpu_cycles,
	uint64_t gpu_telemetry_valid_flags,
	uint64_t gpu_telemetry_misc);
void gpu_fceiling_cb_register(gpu_set_fceiling_t);
static_assert(ESYNC_SPACE_MAX < (1 << 8));
extern wait_result_t esync_wait(esync_space_t space, uint64_t id,
    uint64_t epoch, os_atomic(uint64_t) * counter, ctid_t owner_ctid,
    esync_policy_t policy, wait_interrupt_t interruptible);
extern kern_return_t esync_wake(esync_space_t space, uint64_t id,
    uint64_t epoch, os_atomic(uint64_t) * counter, esync_wake_mode_t mode,
    ctid_t ctid);
extern void exception_init(void);
extern ipc_port_t exception_port_copy_send(ipc_port_t port);
extern kern_return_t exception_triage(
	exception_type_t        exception,
	mach_exception_data_t   code,
	mach_msg_type_number_t  codeCnt);
extern kern_return_t exception_triage_thread(
	exception_type_t        exception,
	mach_exception_data_t   code,
	mach_msg_type_number_t  codeCnt,
	thread_t                thread);
extern void exception_deliver_backtrace(
	kcdata_object_t bt_object,
	ipc_port_t      exc_ports[static BT_EXC_PORTS_COUNT],
	exception_type_t exception);
extern kern_return_t sys_perf_notify(thread_t thread, int pid);
extern kern_return_t task_exception_notify(exception_type_t exception,
    mach_exception_data_type_t code, mach_exception_data_type_t subcode, const bool fatal);
#pragma once



__BEGIN_DECLS

extern kern_return_t
exclaves_call_upcall_handler(exclaves_id_t upcall_id);
extern kern_return_t
exclaves_upcall_init(void);
extern bool
exclaves_upcall_in_range(uintptr_t, bool);
extern void extmod_statistics_incr_task_for_pid(task_t target);
extern void extmod_statistics_incr_thread_set_state(thread_t target);
extern void extmod_statistics_incr_thread_create(task_t target);
OS_CLOSED_OPTIONS(ext_paniclog_flags, uint32_t,
    EXT_PANICLOG_FLAGS_NONE = 0x0,
    EXT_PANICLOG_FLAGS_ADD_SEPARATE_KEY = 0x1);
OS_CLOSED_OPTIONS(ext_paniclog_create_options, uint32_t,
    EXT_PANICLOG_OPTIONS_NONE = 0x0,
    EXT_PANICLOG_OPTIONS_WITH_BUFFER = 0x1,
    
    EXT_PANICLOG_OPTIONS_ADD_SEPARATE_KEY = 0x2);
int ext_paniclog_handle_set_active(ext_paniclog_handle_t *handle);
int ext_paniclog_handle_set_inactive(ext_paniclog_handle_t *handle);
ext_paniclog_handle_t *ext_paniclog_handle_alloc_with_uuid(uuid_t uuid, const char *data_id, uint32_t max_len, ext_paniclog_create_options_t options);
ext_paniclog_handle_t *ext_paniclog_handle_alloc_with_buffer(uuid_t uuid, const char *data_id, uint32_t max_len, void * buff, ext_paniclog_create_options_t options);
void ext_paniclog_handle_free(ext_paniclog_handle_t *handle);
int ext_paniclog_insert_data(ext_paniclog_handle_t *handle, void *addr, uint32_t len);
int ext_paniclog_append_data(ext_paniclog_handle_t *handle, void *addr, uint32_t len);
void *ext_paniclog_get_buffer(ext_paniclog_handle_t *handle);
uint32_t ext_paniclog_write_panicdata(void);
void *ext_paniclog_claim_buffer(ext_paniclog_handle_t *handle);
int ext_paniclog_yield_buffer(ext_paniclog_handle_t *handle, uint32_t used_len);
int ext_paniclog_set_used_len(ext_paniclog_handle_t *handle, uint32_t used_len);
bool is_debug_ptr_in_ext_paniclog(void);
__abortlike __printflike(5, 6)
void panic_with_data(uuid_t uuid, void *addr, uint32_t len, uint64_t debugger_options_mask, const char *format, ...);
int ext_paniclog_test_hook(uint32_t option);
void ext_paniclog_panic_with_data(uuid_t uuid, void *addr, uint32_t len);
extern kern_return_t host_set_special_port(host_priv_t host_priv, int id, ipc_port_t port);
extern kern_return_t host_get_special_port(host_priv_t host_priv,
    __unused int node, int id, ipc_port_t * portp);
__BEGIN_DECLS

extern host_t                   host_self(void);
extern host_priv_t              host_priv_self(void);
void    host_notify_calendar_change(void);
void    host_notify_calendar_set(void);
void    host_notify_cancel(host_notify_t entry);
SCALABLE_COUNTER_DECLARE(vm_statistics_zero_fill_count);
SCALABLE_COUNTER_DECLARE(vm_statistics_reactivations);
SCALABLE_COUNTER_DECLARE(vm_statistics_pageins);
SCALABLE_COUNTER_DECLARE(vm_statistics_pageouts);
SCALABLE_COUNTER_DECLARE(vm_statistics_faults);
SCALABLE_COUNTER_DECLARE(vm_statistics_cow_faults);
SCALABLE_COUNTER_DECLARE(vm_statistics_lookups);
SCALABLE_COUNTER_DECLARE(vm_statistics_hits);
SCALABLE_COUNTER_DECLARE(vm_statistics_purges);
SCALABLE_COUNTER_DECLARE(vm_statistics_decompressions);
SCALABLE_COUNTER_DECLARE(vm_statistics_compressions);
SCALABLE_COUNTER_DECLARE(vm_statistics_swapins);
SCALABLE_COUNTER_DECLARE(vm_statistics_swapouts);
SCALABLE_COUNTER_DECLARE(vm_statistics_total_uncompressed_pages_in_compressor);
SCALABLE_COUNTER_DECLARE(vm_page_grab_count);
OS_CLOSED_ENUM(hvg_hcall_code, uint32_t,
    HVG_HCALL_TRIGGER_DUMP        = 0x0001,       
    HVG_HCALL_SET_COREDUMP_DATA   = 0x0002,       
    HVG_HCALL_GET_MABS_OFFSET     = 0x0003,       
    HVG_HCALL_GET_BOOTSESSIONUUID = 0x0004,       
    HVG_HCALL_VCPU_WFK            = 0x0005,       
    HVG_HCALL_VCPU_KICK           = 0x0006,       
    HVG_HCALL_COUNT
    );
OS_CLOSED_OPTIONS(hvg_hcall_dump_option, uint32_t,
    HVG_HCALL_DUMP_OPTION_REGULAR   =  0x0001     
    );
extern void hvg_hcall_set_coredump_data(void);
extern hvg_hcall_return_t hvg_hcall_trigger_dump(hvg_hcall_vmcore_file_t *vmcore,
    const hvg_hcall_dump_option_t dump_option);
extern hvg_hcall_return_t hvg_hcall_get_bootsessionuuid(uuid_string_t uuid);
extern hvg_hcall_return_t hvg_hcall_get_mabs_offset(uint64_t *mabs_offset);
extern void hvg_hc_kick_cpu(unsigned int cpu_id);
extern void hvg_hc_wait_for_kick(unsigned int ien);
#pragma once



enum {
	kHV_ION_NONE             = (0u << 0),
	kHV_ION_ANY_VALUE        = (1u << 1),
	kHV_ION_ANY_SIZE         = (1u << 2),
	kHV_ION_EXIT_FULL        = (1u << 3),
};
extern kern_return_t hv_io_notifier_grp_add(hv_ion_grp_t *grp, const hv_ion_t *);
extern kern_return_t hv_io_notifier_grp_remove(hv_ion_grp_t *, const hv_ion_t *);
extern kern_return_t hv_io_notifier_grp_fire(hv_ion_grp_t *, uint64_t, size_t, uint64_t);
extern kern_return_t hv_io_notifier_grp_alloc(hv_ion_grp_t **);
extern void hv_io_notifier_grp_free(hv_ion_grp_t **);
extern void hv_support_init(void);
extern int hv_get_support(void);
extern void hv_set_task_target(void *target);
extern void hv_set_thread_target(void *target);
extern void *hv_get_task_target(void);
extern void *hv_get_thread_target(void);
extern int hv_get_volatile_state(hv_volatile_state_t state);
extern kern_return_t hv_set_traps(hv_trap_type_t trap_type,
    const hv_trap_t *traps, unsigned trap_count);
extern void hv_release_traps(hv_trap_type_t trap_type);
extern kern_return_t hv_set_callbacks(hv_callbacks_t callbacks);
extern void hv_release_callbacks(void);
extern void hv_suspend(void);
extern void hv_resume(void);
extern kern_return_t hv_task_trap(uint64_t index, uint64_t arg);
extern kern_return_t hv_thread_trap(uint64_t index, uint64_t arg);
extern boolean_t hv_ast_pending(void);
extern void hv_trace_guest_enter(uint32_t vcpu_id, uint64_t *vcpu_regs);
extern void hv_trace_guest_exit(uint32_t vcpu_id, uint64_t *vcpu_regs,
    uint32_t reason);
extern void hv_trace_guest_error(uint32_t vcpu_id, uint64_t *vcpu_regs,
    uint32_t failure, uint32_t error);
extern void ipc_host_init(void);
extern void ipc_processor_init(
	processor_t     processor);
extern void ipc_pset_init(
	processor_set_t         pset);
extern void ipc_clock_init(
	clock_t         clock);
extern clock_t convert_port_to_clock(
	ipc_port_t      port);
extern ipc_port_t convert_clock_to_port(
	clock_t         clock);
extern clock_t port_name_to_clock(
	mach_port_name_t clock_name);
extern ipc_port_t host_port_copy_send(
	ipc_port_t      port);
extern host_t convert_port_to_host(
	ipc_port_t      port);
extern host_t convert_port_to_host_priv(
	ipc_port_t      port);
extern ipc_port_t convert_host_to_port(
	host_t          host);
extern processor_t convert_port_to_processor(
	ipc_port_t      port);
extern ipc_port_t convert_processor_to_port(
	processor_t     processor);
extern processor_set_t convert_port_to_pset(
	ipc_port_t      port);
extern processor_set_t convert_port_to_pset_name(
	ipc_port_t      port);
extern ipc_port_t convert_pset_to_port(
	processor_set_t         processor);
extern ipc_port_t convert_pset_name_to_port(
	processor_set_t         processor);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)

__enum_decl(ipc_kotype_t, natural_t, {
	IKOT_NONE                     = 0,
	IKOT_THREAD_CONTROL           = 1,
	IKOT_TASK_CONTROL             = 2,
	IKOT_HOST                     = 3,
	IKOT_HOST_PRIV                = 4,
	IKOT_PROCESSOR                = 5,
	IKOT_PSET                     = 6,
	IKOT_PSET_NAME                = 7,
	IKOT_TIMER                    = 8,
	IKOT_PORT_SUBST_ONCE          = 9,
	
	IKOT_MEMORY_OBJECT            = 11,
	
	
	
	IKOT_UND_REPLY                = 15,
	
	
	
	IKOT_MAIN_DEVICE              = 19,
	IKOT_TASK_NAME                = 20,
	
	
	IKOT_SEMAPHORE                = 23,
	
	IKOT_CLOCK                    = 25,
	
	IKOT_IOKIT_IDENT              = 27,
	IKOT_NAMED_ENTRY              = 28,
	IKOT_IOKIT_CONNECT            = 29,
	IKOT_IOKIT_OBJECT             = 30,
	
	
	IKOT_AU_SESSIONPORT           = 33,
	IKOT_FILEPORT                 = 34,
	
	IKOT_TASK_RESUME              = 36,
	IKOT_VOUCHER                  = 37,
	
	IKOT_WORK_INTERVAL            = 39,
	IKOT_UX_HANDLER               = 40,
	IKOT_UEXT_OBJECT              = 41,
	IKOT_ARCADE_REG               = 42,
	IKOT_EVENTLINK                = 43,
	IKOT_TASK_INSPECT             = 44,
	IKOT_TASK_READ                = 45,
	IKOT_THREAD_INSPECT           = 46,
	IKOT_THREAD_READ              = 47,
	
	IKOT_TASK_ID_TOKEN            = 50,
	IKOT_KCDATA                   = 52,
	
	IKOT_UNKNOWN,
});
extern kern_return_t ipc_kobject_set_kobjidx(
	int                         msgid,
	int                         index);
extern ipc_port_t ipc_kobject_alloc_port(
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          type,
	ipc_kobject_alloc_options_t options);
extern ipc_port_t ipc_kobject_alloc_labeled_port(
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          type,
	ipc_label_t                 label,
	ipc_kobject_alloc_options_t options);
extern ipc_port_t ipc_kobject_alloc_subst_once(
	ipc_port_t                  target);
extern bool ipc_kobject_make_send_lazy_alloc_port(
	ipc_port_t                 *port_store,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          type,
	ipc_kobject_alloc_options_t alloc_opts);
extern boolean_t ipc_kobject_make_send_lazy_alloc_labeled_port(
	ipc_port_t                 *port_store,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          type,
	ipc_label_t                 label) __result_use_check;
extern kern_return_t ipc_kobject_nsrequest(
	ipc_port_t                  port,
	mach_port_mscount_t         sync,
	mach_port_mscount_t        *mscount) __result_use_check;
extern ipc_port_t ipc_kobject_copy_send(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          kotype) __result_use_check;
extern ipc_port_t ipc_kobject_make_send(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          kotype) __result_use_check;
extern kern_return_t ipc_kobject_make_send_nsrequest(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          kotype) __result_use_check;
extern kern_return_t ipc_kobject_make_send_nsrequest_locked(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          kotype) __result_use_check;
extern ipc_kobject_t ipc_kobject_dealloc_port_and_unlock(
	ipc_port_t                  port,
	mach_port_mscount_t         mscount,
	ipc_kobject_type_t          type);
extern ipc_kobject_t ipc_kobject_dealloc_port(
	ipc_port_t                  port,
	mach_port_mscount_t         mscount,
	ipc_kobject_type_t          type);
extern void         ipc_kobject_enable(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          type);
extern void         ipc_kobject_require(
	ipc_port_t                  port,
	ipc_kobject_t               kobject,
	ipc_kobject_type_t          kotype);
extern ipc_kobject_t ipc_kobject_get_raw(
	ipc_port_t                  port,
	ipc_kobject_type_t          type);
extern ipc_kobject_t ipc_kobject_get_locked(
	ipc_port_t                  port,
	ipc_kobject_type_t          type);
extern ipc_kobject_t ipc_kobject_get_stable(
	ipc_port_t                  port,
	ipc_kobject_type_t          type);
extern ipc_kobject_t ipc_kobject_disable_locked(
	ipc_port_t                  port,
	ipc_kobject_type_t          type);
extern ipc_kobject_t ipc_kobject_disable(
	ipc_port_t                  port,
	ipc_kobject_type_t          type);
extern void         ipc_kobject_upgrade_mktimer_locked(
	ipc_port_t                  port,
	ipc_kobject_t               kobject);
extern bool     ipc_kobject_label_check(
	ipc_space_t                 space,
	ipc_port_t                  port,
	mach_msg_type_name_t        msgt_name,
	ipc_object_copyout_flags_t *flags,
	ipc_port_t                 *subst_portp) __result_use_check;
__result_use_check
static inline bool
ip_label_check(
	ipc_space_t                 space,
	ipc_port_t                  port,
	mach_msg_type_name_t        msgt_name,
	ipc_object_copyout_flags_t *flags,
	ipc_port_t                 *subst_portp)
{
	if (!ip_is_kolabeled(port)) {
		*subst_portp = IP_NULL;
		return true;
	}
	return ipc_kobject_label_check(space, port, msgt_name, flags, subst_portp);
}



__startup_func
extern void ipc_kobject_register_startup(
	ipc_kobject_ops_t           ops);
extern void ipc_kobject_init(void);
extern ipc_kmsg_t ipc_kobject_server(
	ipc_port_t                  receiver,
	ipc_kmsg_t                  request,
	mach_msg_option64_t         option);
extern void ipc_kobject_destroy(
	ipc_port_t                  port);
extern void ipc_kobject_notify_no_senders(
	ipc_port_t                  port,
	mach_port_mscount_t         mscount);
extern void ipc_kobject_notify_send_once_and_unlock(
	ipc_port_t                  port);
extern kern_return_t uext_server(
	ipc_port_t                  receiver,
	ipc_kmsg_t                  request,
	ipc_kmsg_t                  *reply);
extern kern_return_t ipc_typed_port_copyin_send(
	ipc_space_t                 space,
	mach_port_name_t            name,
	ipc_kobject_type_t          kotype,
	ipc_port_t                 *port);
extern void       ipc_typed_port_release_send(
	ipc_port_t                  port,
	ipc_kobject_type_t          kotype);
__BEGIN_DECLS



extern mach_msg_return_t mach_msg_send_from_kernel(
	mach_msg_header_t       *msg,
	mach_msg_size_t         send_size);
extern mach_msg_return_t mach_msg_rpc_from_kernel(
	mach_msg_header_t      *msg,
	mach_msg_size_t         send_size,
	mach_msg_size_t         rcv_size);
extern void mach_msg_destroy_from_kernel(
	mach_msg_header_t      *msg);
mach_msg_return_t kernel_mach_msg_rpc(
	mach_msg_header_t      *msg,
	mach_msg_size_t         send_size,
	mach_msg_size_t         rcv_size,
	boolean_t               interruptible,
	boolean_t              *message_moved);
extern mach_msg_return_t kernel_mach_msg_send(
	mach_msg_header_t      *msg,
	mach_msg_size_t         send_size,
	mach_msg_option64_t     option,
	mach_msg_timeout_t      timeout_val,
	boolean_t              *message_moved);
extern mach_msg_return_t kernel_mach_msg_send_kmsg(
	struct ipc_kmsg        *kmsg);
extern mach_msg_return_t kernel_mach_msg_send_with_builder_internal(
	mach_msg_size_t         desc_count,
	mach_msg_size_t         payload_size, 
	mach_msg_option64_t     option,
	mach_msg_timeout_t      timeout_val,
	boolean_t              *message_moved,
	void                  (^builder)(mach_msg_header_t *,
	mach_msg_descriptor_t *, void *));
extern mach_msg_return_t mach_msg_send_from_kernel_with_options(
	mach_msg_header_t      *msg,
	mach_msg_size_t         send_size,
	mach_msg_option64_t     option,
	mach_msg_timeout_t      timeout_val);
extern void mach_msg_receive_continue(void);
ipc_port_t fileport_alloc(struct fileglob *);
void fileport_notify(mach_msg_header_t *);
kern_return_t fileport_invoke(task_t, mach_port_name_t,
    int (*)(mach_port_name_t, struct fileglob *, void *), void *, int *);
kern_return_t fileport_walk(task_t, size_t *count,
    bool (^cb)(size_t i, mach_port_name_t, struct fileglob *));
extern void ipc_task_init(
	task_t          task,
	task_t          parent);
extern void ipc_task_enable(
	task_t          task);
extern void ipc_task_disable(
	task_t          task);
extern void ipc_task_reset(
	task_t          task);
extern void ipc_task_terminate(
	task_t          task);
extern void ipc_task_set_immovable_pinned(
	task_t          task);
extern void ipc_main_thread_set_immovable_pinned(
	thread_t          thread);
__options_decl(ipc_thread_init_options_t, uint32_t, {
	IPC_THREAD_INIT_NONE       = 0x00,
	IPC_THREAD_INIT_MAINTHREAD = 0x01,
});
__options_decl(port_intrans_options_t, uint32_t, {
	PORT_INTRANS_OPTIONS_NONE              = 0x0000,
	PORT_INTRANS_THREAD_IN_CURRENT_TASK    = 0x0001,
	PORT_INTRANS_THREAD_NOT_CURRENT_THREAD = 0x0002,

	PORT_INTRANS_SKIP_TASK_EVAL            = 0x0004,
	PORT_INTRANS_ALLOW_CORPSE_TASK         = 0x0008,
});
extern void ipc_thread_init(
	task_t          task,
	thread_t        thread,
	thread_ro_t     tro,
	ipc_thread_init_options_t options);
extern void ipc_thread_disable(
	thread_t        thread);
extern void ipc_thread_terminate(
	thread_t        thread);
extern void ipc_thread_reset(
	thread_t        thread);
extern ipc_port_t retrieve_thread_self_fast(
	thread_t        thread);
extern task_name_t convert_port_to_task_name(
	ipc_port_t      port);
extern task_name_t convert_port_to_task_name_mig(
	ipc_port_t      port);
extern task_policy_set_t convert_port_to_task_policy_set_mig(
	ipc_port_t      port);
extern task_policy_get_t convert_port_to_task_policy_get_mig(
	ipc_port_t      port);
extern task_inspect_t convert_port_to_task_inspect(
	ipc_port_t      port);
extern task_inspect_t convert_port_to_task_inspect_no_eval(
	ipc_port_t      port);
extern task_inspect_t convert_port_to_task_inspect_mig(
	ipc_port_t      port);
extern task_read_t convert_port_to_task_read(
	ipc_port_t      port);
extern task_read_t convert_port_to_task_read_no_eval(
	ipc_port_t      port);
extern task_read_t convert_port_to_task_read_mig(
	ipc_port_t      port);
extern task_t convert_port_to_task(
	ipc_port_t      port);
extern task_t convert_port_to_task_pinned(
	ipc_port_t      port);
extern task_t convert_port_to_task_mig(
	ipc_port_t      port);
extern task_t port_name_to_current_task_noref(
	mach_port_name_t name);
extern task_read_t port_name_to_current_task_read_noref(
	mach_port_name_t name);
extern task_t port_name_to_task(
	mach_port_name_t name);
extern task_t port_name_to_task_kernel(
	mach_port_name_t name);
extern task_t port_name_to_task_external(
	mach_port_name_t name);
extern task_read_t port_name_to_task_read(
	mach_port_name_t name);
extern task_read_t port_name_to_task_read_and_send_right(
	mach_port_name_t name,
	ipc_port_t *kportp);
extern task_read_t port_name_to_task_read_no_eval(
	mach_port_name_t name);
extern task_t port_name_to_task_name(
	mach_port_name_t name);
extern task_id_token_t port_name_to_task_id_token(
	mach_port_name_t name);
extern host_t port_name_to_host(
	mach_port_name_t name);
extern boolean_t ref_task_port_locked(
	ipc_port_t port, task_t *ptask);
extern ipc_space_t convert_port_to_space(
	ipc_port_t      port);
extern ipc_space_read_t convert_port_to_space_read(
	ipc_port_t      port);
extern ipc_space_read_t convert_port_to_space_read_no_eval(
	ipc_port_t      port);
extern ipc_space_inspect_t convert_port_to_space_inspect(
	ipc_port_t      port);
extern boolean_t ref_space_port_locked(
	ipc_port_t port, ipc_space_t *pspace);
extern vm_map_t convert_port_to_map(
	ipc_port_t      port);
extern vm_map_read_t convert_port_to_map_read(
	ipc_port_t      port);
extern vm_map_inspect_t convert_port_to_map_inspect(
	ipc_port_t      port);
extern thread_t convert_port_to_thread(
	ipc_port_t              port);
extern thread_inspect_t convert_port_to_thread_inspect(
	ipc_port_t              port);
extern thread_read_t convert_port_to_thread_read(
	ipc_port_t              port);
extern thread_t port_name_to_thread(
	mach_port_name_t            port_name,
	port_intrans_options_t    options);
extern void space_deallocate(
	ipc_space_t             space);
extern void space_read_deallocate(
	ipc_space_read_t             space);
extern void space_inspect_deallocate(
	ipc_space_inspect_t     space);
extern kern_return_t thread_get_kernel_special_reply_port(void);
extern void thread_dealloc_kernel_special_reply_port(thread_t thread);
extern kern_return_t
set_exception_ports_validation(
	task_t                  task,
	exception_mask_t        exception_mask,
	ipc_port_t              new_port,
	exception_behavior_t    new_behavior,
	thread_state_flavor_t   new_flavor,
	bool hardened_exception
	);
extern kern_return_t
thread_set_exception_ports_internal(
	thread_t  thread,
	exception_mask_t        exception_mask,
	ipc_port_t              new_port,
	exception_behavior_t    new_behavior,
	thread_state_flavor_t   new_flavor,
	boolean_t                               hardened);
extern void ipc_thread_port_unpin(
	ipc_port_t port);
extern ipc_port_t
convert_task_suspension_token_to_port_external(
	task_suspension_token_t         task);
extern ipc_port_t
convert_task_suspension_token_to_port_mig(
	task_suspension_token_t         task);
extern task_suspension_token_t
convert_port_to_task_suspension_token_external(
	ipc_port_t                      port);
extern task_suspension_token_t
convert_port_to_task_suspension_token_mig(
	ipc_port_t                      port);
extern task_suspension_token_t
convert_port_to_task_suspension_token_kernel(
	ipc_port_t                      port);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN




typedef 




KALLOC_HEAP_DECLARE(KHEAP_DATA_BUFFERS);
KALLOC_HEAP_DECLARE(KHEAP_DATA_SHARED);
KALLOC_HEAP_DECLARE(KHEAP_DEFAULT);
KALLOC_HEAP_DECLARE(KHEAP_KT_VAR);
__options_decl(kalloc_type_flags_t, uint32_t, {
	KT_DEFAULT        = 0x0001,
	KT_PRIV_ACCT      = 0x0002,
	KT_SHARED_ACCT    = 0x0004,
	KT_DATA_ONLY      = 0x0008,
	KT_VM             = 0x0010,
	KT_CHANGED        = 0x0020,
	KT_CHANGED2       = 0x0040,
	KT_PTR_ARRAY      = 0x0080,
	KT_NOEARLY       = 0x2000,
	KT_SLID           = 0x4000,
	KT_PROCESSED      = 0x8000,
	KT_HASH           = 0xffff0000,
});
__options_decl(kalloc_type_version_t, uint16_t, {
	KT_V1             = 0x0001,
});
extern void kheap_free_bounded(
	kalloc_heap_t heap,
	void         *addr __unsafe_indexable,
	vm_size_t     min_sz,
	vm_size_t     max_sz);
extern void kalloc_data_require(
	void         *data __unsafe_indexable,
	vm_size_t     size);
extern void kalloc_non_data_require(
	void         *data __unsafe_indexable,
	vm_size_t     size);
__options_decl(kt_granule_t, uint32_t, {
	KT_GRANULE_PADDING = 0,
	KT_GRANULE_POINTER = 1,
	KT_GRANULE_DATA    = 2,
	KT_GRANULE_DUAL    = 4,
	KT_GRANULE_PAC     = 8
});
extern vm_size_t kalloc_next_good_size(
	vm_size_t               size,
	uint32_t                period);
#pragma mark kalloc_array implementation details




extern struct kalloc_result __kalloc_array_decode(
	vm_address_t            array) __pure2;
__pure2
static inline uint32_t
__kalloc_array_size(vm_address_t array)
{
	vm_address_t size = __kalloc_array_decode(array).size;

	__builtin_assume(size <= KALLOC_ARRAY_SIZE_MAX);
	return (uint32_t)size;
}

__pure2
static inline vm_address_t
__kalloc_array_base(vm_address_t array)
{
	return (vm_address_t)__kalloc_array_decode(array).addr;
}

__pure2
static inline vm_address_t
__kalloc_array_begin(vm_address_t array, vm_size_t hdr_size)
{
	return (vm_address_t)__kalloc_array_decode(array).addr + hdr_size;
}

__pure2
static inline vm_address_t
__kalloc_array_end(vm_address_t array)
{
	struct kalloc_result kr = __kalloc_array_decode(array);

	return (vm_address_t)kr.addr + kr.size;
}

#pragma mark implementation details


static inline void *__unsafe_indexable
kt_mangle_var_view(kalloc_type_var_view_t kt_view)
{
	return (void *__unsafe_indexable)((uintptr_t)kt_view | 1ul);
}

static inline kalloc_type_var_view_t __unsafe_indexable
kt_demangle_var_view(void *ptr)
{
	return (kalloc_type_var_view_t __unsafe_indexable)((uintptr_t)ptr & ~1ul);
}



static inline vm_size_t
kt_size(vm_size_t s1, vm_size_t s2, vm_size_t c2)
{
	
	const vm_size_t limit = 1ull << (8 * sizeof(vm_size_t) - 1);

	if (os_mul_and_add_overflow(s2, c2, s1, &s1) || (s1 & limit)) {
		return limit;
	}
	return s1;
}


















































extern struct kalloc_result kalloc_ext(
	void                   *kheap_or_kt_view __unsafe_indexable,
	vm_size_t               size,
	zalloc_flags_t          flags,
	void                   *site);
static inline struct kalloc_result
__kalloc_ext(
	void                   *kheap_or_kt_view __unsafe_indexable,
	vm_size_t               size,
	zalloc_flags_t          flags,
	void                   *site)
{
	struct kalloc_result kr;

	kr    = (kalloc_ext)(kheap_or_kt_view, size, flags, site);
	if (flags & Z_NOFAIL) {
		__builtin_assume(kr.addr != NULL);
	}
	return kr;
}


extern void kfree_ext(
	void                   *kheap_or_kt_view __unsafe_indexable,
	void                   *addr __unsafe_indexable,
	vm_size_t               size);
static inline void *__unsafe_indexable
kalloc_type_var_impl(
	kalloc_type_var_view_t    kt_view,
	vm_size_t                 size,
	zalloc_flags_t            flags,
	void                      *site)
{
	struct kalloc_result kr;

	kr = kalloc_ext(kt_mangle_var_view(kt_view), size, flags, site);
	return kr.addr;
}

static inline void
kfree_type_var_impl(
	kalloc_type_var_view_t      kt_view,
	void                       *ptr __unsafe_indexable,
	vm_size_t                   size)
{
	kfree_ext(kt_mangle_var_view(kt_view), ptr, size);
}


__attribute__((malloc, alloc_size(2)))
static inline void *
__sized_by(size)
__kalloc_type_var_impl(
	kalloc_type_var_view_t  kt_view,
	vm_size_t               size,
	zalloc_flags_t          flags,
	void                   *site)
{
	void *__unsafe_indexable addr;

	addr = (kalloc_type_var_impl)(kt_view, size, flags, site);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return __unsafe_forge_bidi_indexable(void *, addr, size);
}


extern void *kalloc_type_impl_external(
	kalloc_type_view_t  kt_view,
	zalloc_flags_t      flags);
extern void kfree_type_impl_external(
	kalloc_type_view_t  kt_view,
	void               *ptr __unsafe_indexable);
extern void *OSObject_typed_operator_new(
	kalloc_type_view_t  ktv,
	vm_size_t           size);
extern void OSObject_typed_operator_delete(
	kalloc_type_view_t  ktv,
	void               *mem __unsafe_indexable,
	vm_size_t           size);
#pragma GCC visibility push(hidden)


static inline uint32_t
kalloc_type_get_size(uint32_t kt_size)
{
	return kt_size & KALLOC_TYPE_SIZE_MASK;
}

extern bool IOMallocType_from_vm(
	kalloc_type_view_t ktv);
KALLOC_HEAP_DECLARE(KERN_OS_MALLOC);
extern void kheap_startup_init(kalloc_heap_t heap);
extern void kheap_var_startup_init(kalloc_heap_t heap);
__attribute__((malloc, alloc_size(2)))
static inline void *
__sized_by(size)
__kheap_alloc(
	kalloc_heap_t           kheap,
	vm_size_t               size,
	zalloc_flags_t          flags,
	void                   *site)
{
	struct kalloc_result kr;
	__builtin_assume(!kt_is_var_view(kheap));
	kr = kalloc_ext(kheap, size, flags, site);
	return __unsafe_forge_bidi_indexable(void *, kr.addr, size);
}

extern struct kalloc_result krealloc_ext(
	void                   *kheap_or_kt_view __unsafe_indexable,
	void                   *addr __unsafe_indexable,
	vm_size_t               old_size,
	vm_size_t               new_size,
	zalloc_flags_t          flags,
	void                   *site);
static inline struct kalloc_result
__krealloc_ext(
	void                   *kheap_or_kt_view __unsafe_indexable,
	void                   *addr __sized_by(old_size),
	vm_size_t               old_size,
	vm_size_t               new_size,
	zalloc_flags_t          flags,
	void                   *site)
{
	struct kalloc_result kr = (krealloc_ext)(kheap_or_kt_view, addr, old_size,
	    new_size, flags, site);
	if (flags & Z_NOFAIL) {
		__builtin_assume(kr.addr != NULL);
	}
	return kr;
}


__attribute__((malloc, alloc_size(4)))
static inline void *
__sized_by(new_size)
__kheap_realloc(
	kalloc_heap_t           kheap,
	void                   *addr __sized_by(old_size),
	vm_size_t               old_size,
	vm_size_t               new_size,
	zalloc_flags_t          flags,
	void                   *site)
{
	struct kalloc_result kr;
	__builtin_assume(!kt_is_var_view(kheap));
	kr = krealloc_ext(kheap, addr, old_size, new_size, flags, site);
	return __unsafe_forge_bidi_indexable(void *, kr.addr, new_size);
}

__attribute__((malloc, alloc_size(4)))
static inline void *
__sized_by(new_size)
__krealloc_type(
	kalloc_type_var_view_t  kt_view,
	void                   *addr __sized_by(old_size),
	vm_size_t               old_size,
	vm_size_t               new_size,
	zalloc_flags_t          flags,
	void                   *site)
{
	struct kalloc_result kr;
	kr = krealloc_ext(kt_mangle_var_view(kt_view), addr,
	    old_size, new_size, flags, site);
	return __unsafe_forge_bidi_indexable(void *, kr.addr, new_size);
}

extern void kfree_addr_ext(
	kalloc_heap_t           kheap,
	void                   *addr __unsafe_indexable);
extern zone_t kalloc_zone_for_size(
	zone_id_t             zid,
	vm_size_t             size);
SCALABLE_COUNTER_DECLARE(kalloc_large_count);
SCALABLE_COUNTER_DECLARE(kalloc_large_total);
extern void kern_os_typed_free(
	kalloc_type_view_t    ktv,
	void                 *addr __unsafe_indexable,
	vm_size_t             esize);
#pragma GCC visibility pop

extern void kern_os_zfree(
	zone_t        zone,
	void         *addr __unsafe_indexable,
	vm_size_t     size);
static inline uint32_t
kcs_get_elem_size(kcdata_subtype_descriptor_t d)
{
	if (d->kcs_flags & KCS_SUBTYPE_FLAGS_ARRAY) {
		
		return (uint32_t)((d->kcs_elem_size & 0xffff) * ((d->kcs_elem_size & 0xffff0000) >> 16));
	}
	return d->kcs_elem_size;
}

static inline uint32_t
kcs_get_elem_count(kcdata_subtype_descriptor_t d)
{
	if (d->kcs_flags & KCS_SUBTYPE_FLAGS_ARRAY) {
		return (d->kcs_elem_size >> 16) & 0xffff;
	}
	return 1;
}

static inline int
kcs_set_elem_size(kcdata_subtype_descriptor_t d, uint32_t size, uint32_t count)
{
	if (count > 1) {
		
		if (size > 0xffff || count > 0xffff) {
			return -1; 
		}
		d->kcs_elem_size = ((count & 0xffff) << 16 | (size & 0xffff));
	} else {
		d->kcs_elem_size = size;
	}
	return 0;
}
















                                                             
                                                             
                                                             
                                                             
                                                             
                                                             
                                                             























enum dyld_shared_cache_flags {
	kSharedCacheSystemPrimary = 0x1, 
	kSharedCacheDriverkit = 0x2, 
	kSharedCacheAOT = 0x4,    
};
__options_decl(kern_apfs_reflock_out_flags_t, uint32_t, {
	KERN_APFS_REFLOCK_OUT_DEFAULT =      0x0,
	KERN_APFS_REFLOCK_OUT_LOCKED =       0x1,
});
__BEGIN_DECLS




void kern_apfs_reflock_init(kern_apfs_reflock_t reflock);
void kern_apfs_reflock_destroy(kern_apfs_reflock_t reflock);
kern_apfs_reflock_t kern_apfs_reflock_alloc_init(void);
void kern_apfs_reflock_free(kern_apfs_reflock_t reflock);
bool kern_apfs_reflock_try_get_ref(kern_apfs_reflock_t reflock, kern_apfs_reflock_in_flags_t in_flags, kern_apfs_reflock_out_flags_t *out_flags);
bool kern_apfs_reflock_try_put_ref(kern_apfs_reflock_t reflock, kern_apfs_reflock_in_flags_t in_flags, kern_apfs_reflock_out_flags_t *out_flags);
bool kern_apfs_reflock_try_lock(kern_apfs_reflock_t reflock, kern_apfs_reflock_in_flags_t in_flags, uint32_t *refcount_when_lock);
wait_result_t kern_apfs_reflock_wait_for_unlock(kern_apfs_reflock_t reflock, wait_interrupt_t interruptible, uint64_t deadline);
void kern_apfs_reflock_unlock(kern_apfs_reflock_t reflock);
uint64_t kern_apfs_reflock_read_ref(kern_apfs_reflock_t reflock);
__options_decl(kcd_compression_type_t, uint64_t, {
	KCDCT_NONE = 0x00,
	KCDCT_ZLIB = 0x01,
});
__options_decl(kcd_cd_flag_t, uint64_t, {
	KCD_CD_FLAG_IN_MARK  = 0x01,
	KCD_CD_FLAG_FINALIZE = 0x02,
});
__options_decl(kcdata_obj_flags_t, uint32_t, {
	KCDATA_OBJECT_TYPE_LW_CORPSE  = 0x1, 
});
kcdata_descriptor_t kcdata_memory_alloc_init(mach_vm_address_t crash_data_p, unsigned data_type, unsigned size, unsigned flags);
kern_return_t kcdata_memory_static_init(
	kcdata_descriptor_t data, mach_vm_address_t buffer_addr_p, unsigned data_type, unsigned size, unsigned flags);
kern_return_t kcdata_memory_destroy(kcdata_descriptor_t data);
kern_return_t
kcdata_add_container_marker(kcdata_descriptor_t data, uint32_t header_type, uint32_t container_type, uint64_t identifier);
kern_return_t kcdata_add_type_definition(kcdata_descriptor_t data,
    uint32_t type_id,
    char * type_name,
    struct kcdata_subtype_descriptor * elements_array_addr,
    uint32_t elements_count);
kern_return_t kcdata_add_uint64_with_description(kcdata_descriptor_t crashinfo, uint64_t data, const char * description);
kern_return_t kcdata_add_uint32_with_description(kcdata_descriptor_t crashinfo, uint32_t data, const char * description);
kern_return_t kcdata_undo_add_container_begin(kcdata_descriptor_t data);
kern_return_t kcdata_write_buffer_end(kcdata_descriptor_t data);
void *kcdata_memory_get_begin_addr(kcdata_descriptor_t data);
kern_return_t kcdata_init_compress(kcdata_descriptor_t, int hdr_tag, void (*memcpy_f)(void *, const void *, size_t), uint64_t type);
kern_return_t kcdata_push_data(kcdata_descriptor_t data, uint32_t type, uint32_t size, const void *input_data);
kern_return_t kcdata_push_array(kcdata_descriptor_t data, uint32_t type_of_element, uint32_t size_of_element, uint32_t count, const void *input_data);
kern_return_t kcdata_compress_memory_addr(kcdata_descriptor_t data, void *ptr);
void *kcdata_endalloc(kcdata_descriptor_t data, size_t length);
kern_return_t kcdata_finish(kcdata_descriptor_t data);
void kcdata_compression_window_open(kcdata_descriptor_t data);
kern_return_t kcdata_compression_window_close(kcdata_descriptor_t data);
void kcd_finalize_compression(kcdata_descriptor_t data);
kern_return_t kcdata_object_throttle_get(kcdata_obj_flags_t flags);
void kcdata_object_throttle_release(kcdata_obj_flags_t flags);
kern_return_t kcdata_create_object(kcdata_descriptor_t data, kcdata_obj_flags_t flags, uint32_t size, kcdata_object_t *objp);
void kcdata_object_release(kcdata_object_t obj);
void kcdata_object_reference(kcdata_object_t obj);
kcdata_object_t convert_port_to_kcdata_object(ipc_port_t port);
ipc_port_t convert_kcdata_object_to_port(kcdata_object_t obj);
uint32_t kcdata_estimate_required_buffer_size(uint32_t num_items, uint32_t payload_size);
uint64_t kcdata_memory_get_used_bytes(kcdata_descriptor_t kcd);
uint64_t kcdata_memory_get_uncompressed_bytes(kcdata_descriptor_t kcd);
kern_return_t kcdata_memcpy(kcdata_descriptor_t data, mach_vm_address_t dst_addr, const void * src_addr, uint32_t size);
kern_return_t kcdata_bzero(kcdata_descriptor_t data, mach_vm_address_t dst_addr, uint32_t size);
kern_return_t kcdata_get_memory_addr(kcdata_descriptor_t data, uint32_t type, uint32_t size, mach_vm_address_t * user_addr);
kern_return_t kcdata_get_memory_addr_for_array(
	kcdata_descriptor_t data, uint32_t type_of_element, uint32_t size_of_element, uint32_t count, mach_vm_address_t * user_addr);
__BEGIN_DECLS


extern void                   kdp_snapshot_preflight(int pid, void * tracebuf, uint32_t tracebuf_size,
    uint64_t flags, kcdata_descriptor_t data_p, uint64_t since_timestamp, uint32_t pagetable_mask);
extern uint32_t               kdp_stack_snapshot_bytes_traced(void);
extern uint32_t               kdp_stack_snapshot_bytes_uncompressed(void);
extern boolean_t              stackshot_thread_is_idle_worker_unsafe(thread_t thread);
extern void                   stackshot_cpu_preflight(void);
extern void                   stackshot_aux_cpu_entry(void);
extern void                   stackshot_cpu_signal_panic(void);
extern kern_return_t          kern_stack_snapshot_internal(int stackshot_config_version, void *stackshot_config,
    size_t stackshot_config_size, boolean_t stackshot_from_user);
extern kern_return_t          do_stackshot(void* context);
extern boolean_t              stackshot_active(void);
extern boolean_t              panic_stackshot_active(void);
extern kern_return_t do_panic_stackshot(void *context);
extern void *                 stackshot_alloc_with_size(size_t size, kern_return_t *err);
__options_decl(cluster_shared_rsrc_type_t, uint32_t, {
	CLUSTER_SHARED_RSRC_TYPE_RR                     = 0,
	CLUSTER_SHARED_RSRC_TYPE_NATIVE_FIRST           = 1,
	CLUSTER_SHARED_RSRC_TYPE_COUNT                  = 2,
	CLUSTER_SHARED_RSRC_TYPE_MIN                    = CLUSTER_SHARED_RSRC_TYPE_RR,
	CLUSTER_SHARED_RSRC_TYPE_NONE                   = CLUSTER_SHARED_RSRC_TYPE_COUNT,
});
__BEGIN_DECLS

vm_offset_t get_address_from_kext_map(vm_size_t fsize);
void kext_alloc_init(void);
kern_return_t kext_alloc(vm_offset_t *addr, vm_size_t size, boolean_t fixed);
void kext_free(vm_offset_t addr, vm_size_t size);
kern_return_t kext_receipt(void **addrp, size_t *sizep);
kern_return_t kext_receipt_set_queried(void);
extern void kpc_register_cpu(struct cpu_data *cpu_data);
extern void kpc_unregister_cpu(struct cpu_data *cpu_data);
extern void kpc_init(void);
extern void kpc_arch_init(void);
extern uint32_t kpc_get_classes(void);
extern uint32_t kpc_get_running(void);
extern int kpc_get_pmu_version(void);
extern int kpc_set_running(uint32_t classes);
extern int kpc_get_cpu_counters(boolean_t all_cpus, uint32_t classes,
    int *curcpu, uint64_t *buf);
extern int kpc_get_shadow_counters( boolean_t all_cpus, uint32_t classes,
    int *curcpu, uint64_t *buf );
extern int kpc_get_curthread_counters(uint32_t *inoutcount, uint64_t *buf);
extern uint32_t kpc_get_counter_count(uint32_t classes);
extern uint32_t kpc_get_config_count(uint32_t classes);
extern uint32_t kpc_get_thread_counting(void);
extern int      kpc_set_thread_counting(uint32_t classes);
extern int kpc_get_config(uint32_t classes, kpc_config_t *current_config);
extern int kpc_set_config(uint32_t classes, kpc_config_t *new_config);
extern int kpc_get_period(uint32_t classes, uint64_t *period);
extern int kpc_set_period(uint32_t classes, uint64_t *period);
extern int kpc_get_actionid(uint32_t classes, uint32_t *actionid);
extern int kpc_set_actionid(uint32_t classes, uint32_t *actionid);
extern void kpc_thread_create(thread_t thread);
extern void kpc_thread_destroy(thread_t thread);
extern uint64_t *kpc_counterbuf_alloc(void);
extern void      kpc_counterbuf_free(uint64_t*);
extern uint32_t  kpc_get_counterbuf_size(void);
extern void kpc_thread_ast_handler( thread_t thread );
int kpc_set_config_kernel(uint32_t classes, kpc_config_t *new_config);
extern void kpc_off_cpu_internal(thread_t thread);
extern void kpc_off_cpu_update(void);
static inline void
kpc_off_cpu(thread_t thread)
{
	if (__improbable(kpc_off_cpu_active)) {
		kpc_off_cpu_internal(thread);
	}
}



extern int kpc_force_all_ctrs( task_t task, int val );
extern int kpc_get_force_all_ctrs( void );
extern int kpc_force_all_ctrs_arch( task_t task, int val );
extern int kpc_set_sw_inc( uint32_t mask );
extern boolean_t kpc_register_pm_handler(void (*handler)(boolean_t));
extern boolean_t kpc_reserve_pm_counters(uint64_t pmc_mask, kpc_pm_handler_t handler,
    boolean_t custom_config);
extern void kpc_release_pm_counters(void);
extern void kpc_pm_acknowledge(boolean_t available_to_pm);
extern boolean_t kpc_multiple_clients(void);
extern boolean_t kpc_controls_fixed_counters(void);
extern boolean_t kpc_controls_counter(uint32_t ctr);
extern void kpc_idle(void);
extern void kpc_idle_exit(void);
int kpc_get_all_cpus_counters(uint32_t classes, int *curcpu, uint64_t *buf);
int kpc_get_curcpu_counters(uint32_t classes, int *curcpu, uint64_t *buf);
int kpc_get_fixed_counters(uint64_t *counterv);
int kpc_get_configurable_counters(uint64_t *counterv, uint64_t pmc_mask);
boolean_t kpc_is_running_fixed(void);
boolean_t kpc_is_running_configurable(uint64_t pmc_mask);
uint32_t kpc_fixed_count(void);
uint32_t kpc_configurable_count(void);
uint32_t kpc_fixed_config_count(void);
uint32_t kpc_configurable_config_count(uint64_t pmc_mask);
uint32_t kpc_rawpmu_config_count(void);
int kpc_get_fixed_config(kpc_config_t *configv);
int kpc_get_configurable_config(kpc_config_t *configv, uint64_t pmc_mask);
int kpc_get_rawpmu_config(kpc_config_t *configv);
uint64_t kpc_fixed_max(void);
uint64_t kpc_configurable_max(void);
int kpc_set_config_arch(struct kpc_config_remote *mp_config);
int kpc_set_period_arch(struct kpc_config_remote *mp_config);
__options_decl(kperf_kpc_flags_t, uint16_t, {
	KPC_KERNEL_PC = 0x01, 
	KPC_KERNEL_COUNTING = 0x02, 
	KPC_USER_COUNTING = 0x04, 
	KPC_CAPTURED_PC = 0x08, 
});
void kpc_sample_kperf(uint32_t actionid, uint32_t counter, uint64_t config,
    uint64_t count, uintptr_t pc, kperf_kpc_flags_t flags);
int kpc_set_running_arch(struct kpc_running_remote *mp_config);
extern uint8_t kpc_popcount(uint64_t value);
extern uint64_t kpc_get_configurable_pmc_mask(uint32_t classes);
extern ledger_template_t ledger_template_create(const char *name);
extern ledger_template_t ledger_template_copy(ledger_template_t template, const char *name);
extern void ledger_template_dereference(ledger_template_t template);
extern int ledger_entry_add(ledger_template_t template, const char *key,
    const char *group, const char *units);
extern kern_return_t ledger_set_callback(ledger_template_t template, int entry,
    ledger_callback_t callback, const void *param0, const void *param1);
extern kern_return_t ledger_track_maximum(ledger_template_t template, int entry,
    int period_in_secs);
extern kern_return_t ledger_panic_on_negative(ledger_template_t template,
    int entry);
extern kern_return_t ledger_track_credit_only(ledger_template_t template,
    int entry);
extern int ledger_key_lookup(ledger_template_t template, const char *key);
__options_decl(ledger_entry_flags, uint64_t, {
	LEDGER_ENTRY_ALLOW_CALLBACK = 0x1,
	LEDGER_ENTRY_ALLOW_MAXIMUM = 0x2,
	LEDGER_ENTRY_ALLOW_PANIC_ON_NEGATIVE = 0x4,
	LEDGER_ENTRY_ALLOW_DEBIT = 0x8,
	LEDGER_ENTRY_ALLOW_LIMIT = 0x10,
	LEDGER_ENTRY_ALLOW_ACTION = 0x20,
	LEDGER_ENTRY_ALLOW_INACTIVE = 0x40,
});
extern int ledger_entry_add_with_flags(ledger_template_t template, const char *key,
    const char *group, const char *units, ledger_entry_flags flags);
extern ledger_t ledger_instantiate(ledger_template_t template, int entry_type);
extern void ledger_template_complete(ledger_template_t template);
extern void ledger_template_complete_secure_alloc(ledger_template_t template);
extern kern_return_t ledger_disable_callback(ledger_t ledger, int entry);
extern kern_return_t ledger_enable_callback(ledger_t ledger, int entry);
extern kern_return_t ledger_get_limit(ledger_t ledger, int entry,
    ledger_amount_t *limit);
extern kern_return_t ledger_set_limit(ledger_t ledger, int entry,
    ledger_amount_t limit, uint8_t warn_level_percentage);
extern kern_return_t ledger_get_lifetime_max(ledger_t ledger, int entry,
    ledger_amount_t *max_lifetime_balance);
extern kern_return_t ledger_get_actions(ledger_t ledger, int entry, int *actions);
extern kern_return_t ledger_set_action(ledger_t ledger, int entry, int action);
extern kern_return_t ledger_get_period(ledger_t ledger, int entry,
    uint64_t *period);
extern kern_return_t ledger_set_period(ledger_t ledger, int entry,
    uint64_t period);
extern kern_return_t ledger_disable_refill(ledger_t l, int entry);
extern kern_return_t ledger_entry_setactive(ledger_t ledger, int entry);
extern void ledger_check_new_balance(thread_t thread, ledger_t ledger, int entry);
extern kern_return_t ledger_credit(ledger_t ledger, int entry,
    ledger_amount_t amount);
extern kern_return_t ledger_credit_nocheck(ledger_t ledger, int entry,
    ledger_amount_t amount);
extern kern_return_t ledger_debit(ledger_t ledger, int entry,
    ledger_amount_t amount);
extern kern_return_t ledger_debit_nocheck(ledger_t ledger, int entry,
    ledger_amount_t amount);
extern kern_return_t ledger_credit_thread(thread_t thread, ledger_t ledger,
    int entry, ledger_amount_t amount);
extern kern_return_t ledger_debit_thread(thread_t thread, ledger_t ledger,
    int entry, ledger_amount_t amount);
extern kern_return_t ledger_zero_balance(ledger_t ledger, int entry);
extern kern_return_t ledger_get_entries(ledger_t ledger, int entry,
    ledger_amount_t *credit, ledger_amount_t *debit);
extern kern_return_t ledger_get_balance(ledger_t ledger, int entry,
    ledger_amount_t *balance);
extern kern_return_t ledger_reset_callback_state(ledger_t ledger, int entry);
extern kern_return_t ledger_disable_panic_on_negative(ledger_t ledger, int entry);
extern kern_return_t ledger_get_panic_on_negative(ledger_t ledger, int entry, int *panic_on_negative);
extern kern_return_t ledger_rollup(ledger_t to_ledger, ledger_t from_ledger);
extern kern_return_t ledger_rollup_entry(ledger_t to_ledger, ledger_t from_ledger, int entry);
extern void ledger_ast(thread_t thread);
extern void ledger_reference(ledger_t ledger);
extern void ledger_dereference(ledger_t ledger);
extern ledger_amount_t ledger_get_remaining(ledger_t ledger, int entry);
extern void ledger_restart(ledger_t ledger, int entry, uint64_t now);
extern uint64_t ledger_get_interval_remaining(ledger_t ledger, int entry, uint64_t now);
extern int ledger_info(task_t task, struct ledger_info *info);
extern int
ledger_get_task_entry_info_multiple(task_t task, void **buf, int *len);
extern void
ledger_get_entry_info(ledger_t ledger, int entry,
    struct ledger_entry_info *lei);
extern int ledger_template_info(void **buf, int *len);
__BEGIN_DECLS


extern lck_spin_t      *lck_spin_alloc_init(
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_spin_init(
	lck_spin_t              *lck,
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_spin_lock(
	lck_spin_t              *lck);
extern void             lck_spin_lock_grp(
	lck_spin_t              *lck,
	lck_grp_t               *grp);
extern void             lck_spin_unlock(
	lck_spin_t              *lck);
extern void             lck_spin_destroy(
	lck_spin_t              *lck,
	lck_grp_t               *grp);
extern void             lck_spin_free(
	lck_spin_t              *lck,
	lck_grp_t               *grp);
extern wait_result_t    lck_spin_sleep(
	lck_spin_t              *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible);
extern wait_result_t    lck_spin_sleep_grp(
	lck_spin_t              *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible,
	lck_grp_t               *grp);
extern wait_result_t    lck_spin_sleep_deadline(
	lck_spin_t              *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern void             lck_spin_lock_nopreempt(
	lck_spin_t              *lck);
extern void             lck_spin_lock_nopreempt_grp(
	lck_spin_t              *lck, lck_grp_t *grp);
extern void             lck_spin_unlock_nopreempt(
	lck_spin_t              *lck);
extern boolean_t        lck_spin_try_lock_grp(
	lck_spin_t              *lck,
	lck_grp_t               *grp);
extern boolean_t        lck_spin_try_lock(
	lck_spin_t              *lck);
extern boolean_t        lck_spin_try_lock_nopreempt(
	lck_spin_t              *lck);
extern boolean_t        lck_spin_try_lock_nopreempt_grp(
	lck_spin_t              *lck,
	lck_grp_t               *grp);
extern boolean_t        kdp_lck_spin_is_acquired(
	lck_spin_t              *lck);
extern wait_result_t lck_spin_sleep_with_inheritor(
	lck_spin_t              *lock,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	thread_t                inheritor,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t hw_lck_ticket_sleep_with_inheritor(
	hw_lck_ticket_t         *lock,
	lck_grp_t               *grp,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	thread_t                inheritor,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t lck_ticket_sleep_with_inheritor(
	lck_ticket_t            *lock,
	lck_grp_t               *grp,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	thread_t                inheritor,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t lck_mtx_sleep_with_inheritor(
	lck_mtx_t               *lock,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	thread_t                inheritor,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t lck_rw_sleep_with_inheritor(
	lck_rw_t                *lock,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	thread_t                inheritor,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern kern_return_t wakeup_one_with_inheritor(
	event_t                 event,
	wait_result_t           result,
	lck_wake_action_t       action,
	thread_t                *thread_wokenup);
extern kern_return_t wakeup_thread_with_inheritor(
	event_t                 event,
	wait_result_t           result,
	lck_wake_action_t       action,
	thread_t                thread_towake);
extern kern_return_t wakeup_all_with_inheritor(
	event_t                 event,
	wait_result_t           result);
extern kern_return_t change_sleep_inheritor(
	event_t                 event,
	thread_t                inheritor);
extern wait_result_t cond_sleep_with_inheritor32(
	cond_swi_var_t          cond,
	cond_swi_var32_s        expected_cond,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t cond_sleep_with_inheritor64(
	cond_swi_var_t          cond,
	cond_swi_var64_s        expected_cond,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t cond_sleep_with_inheritor64_mask(
	cond_swi_var_t          cond,
	cond_swi_var64_s        expected_cond,
	uint64_t                check_mask,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern kern_return_t cond_wakeup_one_with_inheritor(
	cond_swi_var_t          cond,
	wait_result_t           result,
	lck_wake_action_t       action,
	thread_t                *thread_wokenup);
extern kern_return_t cond_wakeup_all_with_inheritor(
	cond_swi_var_t          cond,
	wait_result_t           result);
__options_decl(gate_assert_flags_t, unsigned int, {
	GATE_ASSERT_CLOSED = 0x00,         
	GATE_ASSERT_OPEN   = 0x01,         
	GATE_ASSERT_HELD   = 0x02,         
});
__options_decl(gate_handoff_flags_t, unsigned int, {
	GATE_HANDOFF_DEFAULT            = 0x00,         
	GATE_HANDOFF_OPEN_IF_NO_WAITERS = 0x1,         
});
extern void lck_rw_gate_init(lck_rw_t *lock, gate_t *gate);
extern void lck_rw_gate_destroy(lck_rw_t *lock, gate_t *gate);
extern gate_t* lck_rw_gate_alloc_init(lck_rw_t *lock);
extern void lck_rw_gate_free(lck_rw_t *lock, gate_t *gate);
extern kern_return_t lck_rw_gate_try_close(lck_rw_t *lock, gate_t *gate);
extern void lck_rw_gate_close(lck_rw_t *lock, gate_t *gate);
extern void lck_rw_gate_open(lck_rw_t *lock, gate_t *gate);
extern kern_return_t lck_rw_gate_handoff(lck_rw_t *lock, gate_t *gate, gate_handoff_flags_t flags);
extern void lck_rw_gate_steal(lck_rw_t *lock, gate_t *gate);
extern gate_wait_result_t lck_rw_gate_wait(
	lck_rw_t               *lock,
	gate_t                 *gate,
	lck_sleep_action_t      lck_sleep_action,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern void lck_rw_gate_assert(lck_rw_t *lock, gate_t *gate, gate_assert_flags_t flags);
extern void lck_mtx_gate_init(lck_mtx_t *lock, gate_t *gate);
extern void lck_mtx_gate_destroy(lck_mtx_t *lock, gate_t *gate);
extern gate_t* lck_mtx_gate_alloc_init(lck_mtx_t *lock);
extern void lck_mtx_gate_free(lck_mtx_t *lock, gate_t *gate);
extern kern_return_t lck_mtx_gate_try_close(lck_mtx_t *lock, gate_t *gate);
extern void lck_mtx_gate_close(lck_mtx_t *lock, gate_t *gate);
extern void lck_mtx_gate_open(lck_mtx_t *lock, gate_t *gate);
extern kern_return_t lck_mtx_gate_handoff(lck_mtx_t *lock, gate_t *gate, gate_handoff_flags_t flags);
extern void lck_mtx_gate_steal(lck_mtx_t *lock, gate_t *gate);
extern gate_wait_result_t lck_mtx_gate_wait(
	lck_mtx_t              *lock,
	gate_t                 *gate,
	lck_sleep_action_t      lck_sleep_action,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern void lck_mtx_gate_assert(lck_mtx_t *lock, gate_t *gate, gate_assert_flags_t flags);
extern void             lck_spin_assert(
	const lck_spin_t              *lck,
	unsigned                int    type);
PERCPU_DECL(struct lck_spinlock_to_info, lck_spinlock_to_info);
extern void             lck_spinlock_timeout_set_orig_owner(
	uintptr_t               owner);
extern void             lck_spinlock_timeout_set_orig_ctid(
	uint32_t                ctid);
extern lck_spinlock_to_info_t lck_spinlock_timeout_hit(
	void                   *lck,
	uintptr_t               owner);
uintptr_t unslide_for_kdebug(const void* object) __pure2;
extern void             lck_attr_startup_init(
	struct lck_attr_startup_spec *spec);
extern void             lck_spin_startup_init(
	struct lck_spin_startup_spec *spec);
extern void             lck_ticket_startup_init(
	struct lck_ticket_startup_spec *spec);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma GCC visibility push(hidden)














__enum_decl(lck_type_t, uint8_t, {
	LCK_TYPE_NONE           = 0x00,
	LCK_TYPE_MUTEX          = 0x22,
	LCK_TYPE_RW             = 0x33,
	LCK_TYPE_TICKET         = 0x44,
	LCK_TYPE_GATE           = 0x55,
});
extern  lck_attr_t      *lck_attr_alloc_init(void);
extern  void            lck_attr_setdefault(
	lck_attr_t              *attr);
extern  void            lck_attr_setdebug(
	lck_attr_t              *attr);
extern  void            lck_attr_cleardebug(
	lck_attr_t              *attr);
extern  void            lck_attr_free(
	lck_attr_t              *attr);
extern  void            lck_attr_rw_shared_priority(
	lck_attr_t              *attr);
extern lck_grp_attr_t  *lck_grp_attr_alloc_init(
	void);
extern void             lck_grp_attr_setdefault(
	lck_grp_attr_t         *attr);
extern void             lck_grp_attr_setstat(
	lck_grp_attr_t         *attr);
extern void             lck_grp_attr_free(
	lck_grp_attr_t         *attr);
extern lck_grp_t       *lck_grp_alloc_init(
	const char             *grp_name,
	lck_grp_attr_t         *attr);
extern void             lck_grp_free(
	lck_grp_t              *grp);
#pragma GCC visibility push(hidden)



__enum_decl(lck_debug_feature_t, uint32_t, {
	LCK_DEBUG_LOCKSTAT,
	LCK_DEBUG_LOCKPROF,

	LCK_DEBUG_MAX,
});
extern 

__options_decl(lck_grp_options_t, uint32_t, {
	LCK_GRP_ATTR_NONE       = 0x00000000,

	LCK_GRP_ATTR_ID_MASK    = 0x0000ffff,
	LCK_GRP_ATTR_STAT       = 0x00010000, 
	LCK_GRP_ATTR_TIME_STAT  = 0x00020000, 
	LCK_GRP_ATTR_DEBUG      = 0x00040000, 
	LCK_GRP_ATTR_ALLOCATED  = 0x80000000,
});
extern void             lck_grp_startup_init(
	struct lck_grp_spec    *spec);
extern void             lck_grp_init(
	lck_grp_t              *grp,
	const char*             grp_name,
	lck_grp_attr_t         *attr);
extern lck_grp_t       *lck_grp_init_flags(
	lck_grp_t              *grp,
	const char*             grp_name,
	lck_grp_options_t       grp_flags);
extern lck_grp_t       *lck_grp_resolve(
	uint32_t                grp_attr_id) __pure2;
extern void             lck_grp_assert_id(
	lck_grp_t              *grp,
	uint32_t                grp_attr_id);
extern void             lck_grp_reference(
	lck_grp_t              *grp,
	uint32_t               *cnt);
extern void             lck_grp_deallocate(
	lck_grp_t              *grp,
	uint32_t               *cnt);
extern void             lck_grp_foreach(
	bool                  (^block)(lck_grp_t *));
extern void             lck_grp_enable_feature(
	lck_debug_feature_t     feat);
extern void             lck_grp_disable_feature(
	lck_debug_feature_t     feat);
__BEGIN_DECLS



extern lck_mtx_t        *lck_mtx_alloc_init(
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_mtx_init(
	lck_mtx_t               *lck,
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_mtx_lock(
	lck_mtx_t               *lck);
extern void             lck_mtx_unlock(
	lck_mtx_t               *lck);
extern void             lck_mtx_destroy(
	lck_mtx_t               *lck,
	lck_grp_t               *grp);
extern void             lck_mtx_free(
	lck_mtx_t               *lck,
	lck_grp_t               *grp);
extern wait_result_t    lck_mtx_sleep(
	lck_mtx_t               *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible);
extern wait_result_t    lck_mtx_sleep_deadline(
	lck_mtx_t               *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern void             lck_mtx_assert(
	lck_mtx_t               *lck,
	unsigned                int    type);
extern boolean_t        lck_mtx_try_lock(
	lck_mtx_t               *lck);
extern void             mutex_pause(uint32_t);
extern boolean_t        lck_mtx_yield(
	lck_mtx_t               *lck);
extern boolean_t        lck_mtx_try_lock_spin(
	lck_mtx_t               *lck);
extern void             lck_mtx_lock_spin(
	lck_mtx_t               *lck);
extern boolean_t        kdp_lck_mtx_lock_spin_is_acquired(
	lck_mtx_t               *lck);
extern void             lck_mtx_convert_spin(
	lck_mtx_t               *lck);
extern void             lck_mtx_lock_spin_always(
	lck_mtx_t               *lck);
extern boolean_t        lck_mtx_try_lock_spin_always(
	lck_mtx_t               *lck);
extern void             lck_mtx_startup_init(
	struct lck_mtx_startup_spec *spec);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)



typedef union hw_lck_ptr {
	struct {
		intptr_t        lck_ptr_bits    : HW_LCK_PTR_BITS;
		uintptr_t       lck_ptr_locked  : 1;
		uintptr_t       lck_ptr_stats   : 1;
		uint16_t        lck_ptr_mcs_tail __kernel_ptr_semantics;
	};
	uintptr_t               lck_ptr_value;
} hw_lck_ptr_t;
extern void hw_lck_ptr_init(
	hw_lck_ptr_t           *lck,
	void                   *val,
	lck_grp_t              *grp);
extern void hw_lck_ptr_destroy(
	hw_lck_ptr_t           *lck,
	lck_grp_t              *grp);
extern bool hw_lck_ptr_held(
	hw_lck_ptr_t           *lck) __result_use_check;
extern void *hw_lck_ptr_lock(
	hw_lck_ptr_t           *lck,
	lck_grp_t              *grp) __result_use_check;
extern void *hw_lck_ptr_lock_nopreempt(
	hw_lck_ptr_t           *lck,
	lck_grp_t              *grp) __result_use_check;
extern void hw_lck_ptr_unlock(
	hw_lck_ptr_t           *lck,
	void                   *val,
	lck_grp_t              *grp);
extern void hw_lck_ptr_unlock_nopreempt(
	hw_lck_ptr_t           *lck,
	void                   *val,
	lck_grp_t              *grp);
static inline void *
__hw_lck_ptr_value(hw_lck_ptr_t val)
{
	vm_offset_t ptr = val.lck_ptr_bits;

	return (void *)ptr;
}



static inline void *
hw_lck_ptr_value(hw_lck_ptr_t *lck)
{
	hw_lck_ptr_t tmp;

	tmp = atomic_load_explicit((hw_lck_ptr_t _Atomic *)lck,
	    memory_order_relaxed);

	return __hw_lck_ptr_value(tmp);
}


extern void hw_lck_ptr_wait_for_value(
	hw_lck_ptr_t           *lck,
	void                   *val,
	lck_grp_t              *grp);
extern void             lck_rw_startup_init(
	struct lck_rw_startup_spec *spec);
extern lck_rw_t         *lck_rw_alloc_init(
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_rw_init(
	lck_rw_t                *lck,
	lck_grp_t               *grp,
	lck_attr_t              *attr);
extern void             lck_rw_free(
	lck_rw_t                *lck,
	lck_grp_t               *grp);
extern void             lck_rw_destroy(
	lck_rw_t                *lck,
	lck_grp_t               *grp);
extern void             lck_rw_lock(
	lck_rw_t                *lck,
	lck_rw_type_t           lck_rw_type);
extern boolean_t        lck_rw_try_lock(
	lck_rw_t                *lck,
	lck_rw_type_t           lck_rw_type);
extern void             lck_rw_unlock(
	lck_rw_t                *lck,
	lck_rw_type_t           lck_rw_type);
extern void             lck_rw_lock_shared(
	lck_rw_t                *lck);
extern boolean_t
    lck_rw_lock_shared_b(
	lck_rw_t        * lock,
	bool            (^lock_pause)(void));
extern boolean_t
    lck_rw_lock_exclusive_b(
	lck_rw_t        * lock,
	bool            (^lock_pause)(void));
extern boolean_t        lck_rw_lock_shared_to_exclusive(
	lck_rw_t                *lck);
extern void             lck_rw_unlock_shared(
	lck_rw_t                *lck);
extern void             lck_rw_lock_exclusive(
	lck_rw_t                *lck);
extern void             lck_rw_lock_exclusive_to_shared(
	lck_rw_t                *lck);
extern void             lck_rw_unlock_exclusive(
	lck_rw_t                *lck);
extern wait_result_t    lck_rw_sleep(
	lck_rw_t                *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible);
extern wait_result_t    lck_rw_sleep_deadline(
	lck_rw_t                *lck,
	lck_sleep_action_t      lck_sleep_action,
	event_t                 event,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern boolean_t        kdp_lck_rw_lock_is_acquired_exclusive(
	lck_rw_t                *lck);
extern bool             lck_rw_lock_exclusive_check_contended(
	lck_rw_t                *lck);
extern bool             lck_rw_lock_yield_shared(
	lck_rw_t                *lck,
	boolean_t               force_yield);
extern bool             lck_rw_lock_would_yield_shared(
	lck_rw_t                *lck);
__enum_decl(lck_rw_yield_t, uint32_t, {
	LCK_RW_YIELD_WRITERS_ONLY,
	LCK_RW_YIELD_ANY_WAITER,
	LCK_RW_YIELD_ALWAYS,
});
extern bool             lck_rw_lock_yield_exclusive(
	lck_rw_t                *lck,
	lck_rw_yield_t          mode);
extern bool             lck_rw_lock_would_yield_exclusive(
	lck_rw_t                *lck,
	lck_rw_yield_t          mode);
extern void lck_rw_lock_count_inc(
	thread_t                thread,
	const void             *lock);
extern void lck_rw_lock_count_dec(
	thread_t                thread,
	const void             *lock);
extern void             lck_rw_set_promotion_locked(
	thread_t                thread);
extern boolean_t        lck_rw_try_lock_shared(
	lck_rw_t                *lck);
extern boolean_t        lck_rw_try_lock_exclusive(
	lck_rw_t                *lck);
extern lck_rw_type_t    lck_rw_done(
	lck_rw_t                *lck);
extern void             lck_rw_assert(
	lck_rw_t                *lck,
	unsigned int            type);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)




enum lockstat_probe_id {
	LS_NO_PROBE,

	
	LS_LCK_SPIN_LOCK_ACQUIRE,
	LS_LCK_SPIN_LOCK_SPIN,
	LS_LCK_SPIN_UNLOCK_RELEASE,

	
	LS_LCK_MTX_LOCK_ACQUIRE,
	LS_LCK_MTX_LOCK_SPIN_ACQUIRE,
	LS_LCK_MTX_TRY_LOCK_ACQUIRE,
	LS_LCK_MTX_TRY_LOCK_SPIN_ACQUIRE,
	LS_LCK_MTX_UNLOCK_RELEASE,

	LS_LCK_MTX_LOCK_BLOCK,
	LS_LCK_MTX_LOCK_ADAPTIVE_SPIN,
	LS_LCK_MTX_LOCK_SPIN_SPIN,


	
	LS_LCK_RW_LOCK_SHARED_ACQUIRE,
	LS_LCK_RW_LOCK_SHARED_BLOCK,
	LS_LCK_RW_LOCK_SHARED_SPIN,

	LS_LCK_RW_LOCK_EXCL_ACQUIRE,
	LS_LCK_RW_LOCK_EXCL_BLOCK,
	LS_LCK_RW_LOCK_EXCL_SPIN,

	LS_LCK_RW_DONE_RELEASE,

	LS_LCK_RW_TRY_LOCK_SHARED_ACQUIRE,
	LS_LCK_RW_TRY_LOCK_SHARED_SPIN,

	LS_LCK_RW_TRY_LOCK_EXCL_ACQUIRE,
	LS_LCK_RW_TRY_LOCK_EXCL_ILK_SPIN,

	LS_LCK_RW_LOCK_SHARED_TO_EXCL_UPGRADE,
	LS_LCK_RW_LOCK_SHARED_TO_EXCL_SPIN,
	LS_LCK_RW_LOCK_SHARED_TO_EXCL_BLOCK,

	LS_LCK_RW_LOCK_EXCL_TO_SHARED_DOWNGRADE,
	LS_LCK_RW_LOCK_EXCL_TO_SHARED_ILK_SPIN,

	
	LS_LCK_TICKET_LOCK_ACQUIRE,
	LS_LCK_TICKET_LOCK_RELEASE,
	LS_LCK_TICKET_LOCK_SPIN,

	LS_NPROBES
};
extern void lck_grp_stat_enable(lck_grp_stat_t *stat);
extern void lck_grp_stat_disable(lck_grp_stat_t *stat);
extern bool lck_grp_stat_enabled(lck_grp_stat_t *stat);
extern void lck_grp_stat_inc(lck_grp_t *grp, lck_grp_stat_t *stat, bool always);
extern void dtrace_probe(uint32_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
static inline void
lockprof_probe(lck_grp_t *grp, lck_grp_stat_t *stat, uint64_t val)
{
	dtrace_probe(stat->lgs_probeid, (uintptr_t)grp, val, 0, 0, 0);
}

__attribute__((always_inline))
static inline void
lockstat_probe(
	enum lockstat_probe_id  pid,
	const void             *lock,
	uint64_t                arg0,
	uint64_t                arg1,
	uint64_t                arg2,
	uint64_t                arg3)
{
	uint32_t id = lockstat_probemap[pid];

	if (__improbable(id)) {
		dtrace_probe(id, (uintptr_t)lock, arg0, arg1, arg2, arg3);
	}
}

__pure2
static inline uint32_t
lockstat_enabled(void)
{
	return lck_debug_state.lds_value;
}



__attribute__((always_inline, overloadable))
static inline bool
__lck_time_stat_enabled(enum lockstat_probe_id lspid, uint32_t grp_attr_id)
{
	if (__improbable(grp_attr_id & LCK_GRP_ATTR_TIME_STAT)) {
		return true;
	}
	if (__improbable(lspid && lockstat_probemap[lspid])) {
		return true;
	}
	return false;
}

__attribute__((always_inline, overloadable))
static inline bool
__lck_time_stat_enabled(enum lockstat_probe_id lspid, lck_grp_t *grp)
{
	uint32_t grp_attr_id = grp ? grp->lck_grp_attr_id : 0;

	return __lck_time_stat_enabled(lspid, grp_attr_id);
}


static inline enum lockstat_probe_id
lck_mtx_acquire_probe(bool spin, bool try_lock)
{
	if (spin) {
		if (try_lock) {
			return LS_LCK_MTX_TRY_LOCK_SPIN_ACQUIRE;
		}
		return LS_LCK_MTX_LOCK_SPIN_ACQUIRE;
	} else {
		if (try_lock) {
			return LS_LCK_MTX_TRY_LOCK_ACQUIRE;
		}
		return LS_LCK_MTX_LOCK_ACQUIRE;
	}
}

__attribute__((cold))
__header_always_inline void
lck_mtx_prof_probe(
	enum lockstat_probe_id  id,
	lck_mtx_t              *mtx,
	uint32_t                grp_attr_id,
	bool                    profile)
{
#pragma unused(mtx)
	if (profile) {
		lck_grp_t *grp = LCK_GRP_NULL;

		switch (id) {
		case LS_LCK_MTX_LOCK_ACQUIRE:
		case LS_LCK_MTX_LOCK_SPIN_ACQUIRE:
		case LS_LCK_MTX_TRY_LOCK_ACQUIRE:
		case LS_LCK_MTX_TRY_LOCK_SPIN_ACQUIRE:
			grp = lck_grp_resolve(grp_attr_id);
			__builtin_assume(grp != NULL);
			lck_grp_stat_inc(grp, &grp->lck_grp_stats.lgss_mtx_held, true);
			break;
		default:
			break;
		}
	}
	LOCKSTAT_RECORD(id, mtx, (uintptr_t)lck_grp_resolve(grp_attr_id));
}


extern void lck_mtx_time_stat_record(
	enum lockstat_probe_id  id,
	lck_mtx_t              *mtx,
	uint32_t                grp_attr_id,
	uint64_t                start);
__BEGIN_DECLS



__options_decl(lck_sleep_action_t, unsigned int, {
	LCK_SLEEP_DEFAULT      = 0x00,    
	LCK_SLEEP_UNLOCK       = 0x01,    
	LCK_SLEEP_SHARED       = 0x02,    
	LCK_SLEEP_EXCLUSIVE    = 0x04,    
	LCK_SLEEP_SPIN         = 0x08,    
	LCK_SLEEP_PROMOTED_PRI = 0x10,    
	LCK_SLEEP_SPIN_ALWAYS  = 0x20,    
});
__options_decl(lck_wake_action_t, unsigned int, {
	LCK_WAKE_DEFAULT                = 0x00,  
	LCK_WAKE_DO_NOT_TRANSFER_PUSH   = 0x01,  
});
__options_decl(lck_option_t, unsigned int, {
	LCK_OPTION_ENABLE_DEBUG     = 0x01, 
	LCK_OPTION_ENABLE_STAT      = 0x02, 
	LCK_OPTION_DISABLE_RW_PRIO  = 0x04, 
	LCK_OPTION_ENABLE_TIME_STAT = 0x08, 
	LCK_OPTION_DISABLE_RW_DEBUG = 0x10, 
});
__enum_closed_decl(hw_lock_status_t, int, {
	
	HW_LOCK_INVALID    = -1,

	
	HW_LOCK_CONTENDED  =  0,

	
	HW_LOCK_ACQUIRED   =  1,
});
__enum_closed_decl(hw_spin_timeout_status_t, _Bool, {
	HW_LOCK_TIMEOUT_RETURN,         
	HW_LOCK_TIMEOUT_CONTINUE,       
});
extern void processor_up(
	processor_t processor);
extern void processor_start_thread(void *machine_param,
    wait_result_t result);
extern void init_ast_check(
	processor_t         processor);
extern void cause_ast_check(
	processor_t         processor);
extern kern_return_t cpu_control(
	int                 slot_num,
	processor_info_t    info,
	unsigned int        count);
extern void     cpu_sleep(void);
extern void cpu_start(
	int                 slot_num);
extern void cpu_exit_wait(
	int                 slot_num);
extern kern_return_t cpu_info(
	processor_flavor_t  flavor,
	int                 slot_num,
	processor_info_t    info,
	unsigned int        *count);
extern kern_return_t cpu_info_count(
	processor_flavor_t  flavor,
	unsigned int        *count);
extern thread_t         machine_processor_shutdown(
	thread_t            thread,
	void                (*doshutdown)(processor_t),
	processor_t         processor);
extern void machine_idle(void);
extern void machine_track_platform_idle(boolean_t);
extern void machine_signal_idle(
	processor_t         processor);
extern void machine_signal_idle_deferred(
	processor_t         processor);
extern void machine_signal_idle_cancel(
	processor_t         processor);
extern void halt_cpu(void);
extern void halt_all_cpus(
	boolean_t           reboot);
extern char *machine_boot_info(
	char                *buf,
	vm_size_t           buf_len);
extern void consider_machine_collect(void);
extern void     machine_thread_going_on_core(thread_t   new_thread,
    thread_urgency_t    urgency,
    uint64_t        sched_latency,
    uint64_t        same_pri_latency,
    uint64_t        dispatch_time);
extern void machine_thread_going_off_core(thread_t old_thread, boolean_t thread_terminating,
    uint64_t last_dispatch, boolean_t thread_runnable);
extern void machine_max_runnable_latency(uint64_t bg_max_latency,
    uint64_t default_max_latency,
    uint64_t realtime_max_latency);
extern void machine_work_interval_notify(thread_t thread, struct kern_work_interval_args* kwi_args);
extern void machine_perfcontrol_deadline_passed(uint64_t deadline);
extern void machine_switch_perfcontrol_context(perfcontrol_event event,
    uint64_t timestamp,
    uint32_t flags,
    uint64_t new_thread_same_pri_latency,
    thread_t old,
    thread_t new);
extern void machine_switch_perfcontrol_state_update(perfcontrol_event event,
    uint64_t timestamp,
    uint32_t flags,
    thread_t thread);
__BEGIN_DECLS

int mach_msg_filter_register_callback(const struct mach_msg_filter_callbacks *callbacks);
static inline bool __pure2
mach_msg_filter_at_least(unsigned int version)
{
	if (version == 0) {
		
		return mach_msg_filter_callbacks.fetch_filter_policy != NULL;
	}
	return mach_msg_filter_callbacks.version >= version;
}





extern
boolean_t mach_msg_fetch_filter_policy(void *portlabel, mach_msg_id_t msgh_id, mach_msg_filter_id *fid);
void mnl_msg_free(mnl_msg_t msg, uint32_t flags);
mnl_node_info_t mnl_instantiate(mach_node_id_t  nid,
    uint32_t        flags);
kern_return_t mnl_register(mnl_node_info_t  node,
    uint32_t         flags);
kern_return_t mnl_set_link_state(mnl_node_info_t    node,
    int                link,
    uint32_t           flags);
kern_return_t mnl_terminate(mnl_node_info_t node,
    uint32_t        flags);
void mnl_msg_from_node(mnl_node_info_t  node,
    mnl_msg_t        msg,
    uint32_t         flags);
mnl_msg_t mnl_msg_to_node(mnl_node_info_t   node,
    uint32_t          flags);
void mnl_msg_complete(mnl_node_info_t   node,
    mnl_msg_t         msg,
    uint32_t          flags);
extern void setbit(
	int             which,
	int             *bitmap);
extern void clrbit(
	int             which,
	int             *bitmap);
extern int ffsbit(
	int             *bitmap);
extern int ffs(
	unsigned int    mask);
extern int ffsll(
	unsigned long long mask);
extern int fls(
	unsigned int    mask);
extern int flsll(
	unsigned long long mask);
extern int testbit(
	int             which,
	int             *bitmap);
extern int copyin_atomic32(
	const user_addr_t   user_addr,
	uint32_t            *kernel_addr);
extern int copyin_atomic64(
	const user_addr_t   user_addr,
	uint64_t            *kernel_addr);
extern int dtrace_nofault_copy8(
	const uintptr_t     kernel_addr,
	uint8_t             *value);
extern int dtrace_nofault_copy16(
	const uintptr_t     kernel_addr,
	uint16_t            *value);
extern int dtrace_nofault_copy32(
	const uintptr_t     kernel_addr,
	uint32_t            *value);
extern int dtrace_nofault_copy64(
	const uintptr_t     kernel_addr,
	uint64_t            *value);
extern int copyin_atomic32_wait_if_equals(
	const user_addr_t   user_addr,
	uint32_t            value);
extern int copyout_atomic32(
	uint32_t            u32,
	user_addr_t         user_addr);
extern int copyout_atomic64(
	uint64_t            u64,
	user_addr_t         user_addr);
extern int copyinstr(
	const user_addr_t   user_addr,
	char                *kernel_addr,
	vm_size_t           max,
	vm_size_t           *actual);
extern int copyinmsg(
	const user_addr_t   user_addr,
	void                *kernel_addr,
	mach_msg_size_t     nbytes);
extern int copyoutmsg(
	const void      *kernel_addr,
	user_addr_t     user_addr,
	mach_msg_size_t nbytes);
extern int sscanf(const char *input, const char *fmt, ...) __scanflike(2, 3);
extern integer_t sprintf(char *buf, const char *fmt, ...) __printflike(2, 3) __deprecated;
extern int printf(const char *format, ...) __printflike(1, 2);
extern int vprintf(const char *format, va_list ap) __printflike(1, 0);
int     _consume_printf_args(int, ...);
extern int paniclog_append_noflush(const char *format, ...) __printflike(1, 2);
extern int kdb_printf(const char *format, ...) __printflike(1, 2);
extern int kdb_log(const char *format, ...) __printflike(1, 2);
extern int kdb_printf_unbuffered(const char *format, ...) __printflike(1, 2);
extern int snprintf(char *, size_t, const char *, ...) __printflike(3, 4);
extern int scnprintf(char *, size_t, const char *, ...) __printflike(3, 4);
extern const char *tsnprintf(char *, size_t, const char *, ...) __printflike(3, 4);
extern void log(int level, char *fmt, ...) __printflike(2, 3);
void
_doprnt(
	const char     *fmt,
	va_list        *argp,
	void          (*putc)(char),
	int             radix) __printflike(1, 0);
void
_doprnt_log(
	const char     *fmt,
	va_list        *argp,
	void          (*putc)(char),
	int             radix) __printflike(1, 0);
int
__doprnt(
	const char     *fmt,
	va_list         argp,
	void          (*putc)(int, void *),
	void           *arg,
	int             radix,
	int             is_log) __printflike(1, 0);
extern void console_write_char(char);
extern void conslog_putc(char);
extern void cons_putc_locked(char);
extern void consdebug_putc(char);
extern void consdebug_log(char);
extern void consdebug_putc_unbuffered(char);
extern void console_write_unbuffered(char);
extern void console_write(char *, int);
extern void console_suspend(void);
extern void console_resume(void);
extern int console_read_char(void);
extern int console_try_read_char(void);
extern int _setjmp(
	jmp_buf_t       *jmp_buf);
extern int _longjmp(
	jmp_buf_t       *jmp_buf,
	int             value);
extern void bootstrap_create(void);
extern kern_return_t    kernel_set_special_port(
	host_priv_t     host_priv,
	int             which,
	ipc_port_t      port);
extern kern_return_t    kernel_get_special_port(
	host_priv_t     host_priv,
	int             which,
	ipc_port_t      *portp);
user_addr_t get_useraddr(void);
extern uint64_t early_random(void);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN


typedef 


typedef 





static inline void
mpsc_queue_init(mpsc_queue_head_t q)
{
	os_atomic_init(&q->mpqh_head.mpqc_next, NULL);
	os_atomic_init(&q->mpqh_tail, &q->mpqh_head);
}


typedef enum mpsc_queue_options {
	MPSC_QUEUE_NONE                = 0,
	MPSC_QUEUE_DISABLE_PREEMPTION  = 1 << 0,
} mpsc_queue_options_t;
#pragma mark Advanced Multi Producer calls


static inline mpsc_queue_chain_t
__mpsc_queue_append_update_tail(mpsc_queue_head_t q, mpsc_queue_chain_t elm)
{
	os_atomic_store(&elm->mpqc_next, (struct mpsc_queue_chain *__single)NULL, relaxed);
	return os_atomic_xchg(&q->mpqh_tail, elm, release);
}


static inline bool
__mpsc_queue_append_was_empty(mpsc_queue_head_t q, mpsc_queue_chain_t prev)
{
	return &q->mpqh_head == prev;
}


static inline void
__mpsc_queue_append_update_prev(mpsc_queue_chain_t prev, mpsc_queue_chain_t elm)
{
	os_atomic_store(&prev->mpqc_next, elm, relaxed);
}


#pragma mark Multi Producer calls


static inline bool
mpsc_queue_append_list(mpsc_queue_head_t q, mpsc_queue_chain_t first,
    mpsc_queue_chain_t last)
{
	mpsc_queue_chain_t prev = __mpsc_queue_append_update_tail(q, last);
	__mpsc_queue_append_update_prev(prev, first);
	return __mpsc_queue_append_was_empty(q, prev);
}


static inline bool
mpsc_queue_append(mpsc_queue_head_t q, mpsc_queue_chain_t elm)
{
	return mpsc_queue_append_list(q, elm, elm);
}


#pragma mark Single Consumer calls


mpsc_queue_chain_t
mpsc_queue_dequeue_batch(mpsc_queue_head_t q, mpsc_queue_chain_t *tail,
    os_atomic_dependency_t dependency);
mpsc_queue_chain_t
mpsc_queue_batch_next(mpsc_queue_chain_t cur, mpsc_queue_chain_t tail);
void
mpsc_queue_restore_batch(mpsc_queue_head_t q, mpsc_queue_chain_t first,
    mpsc_queue_chain_t last);
#pragma mark "GCD"-like facilities


__options_decl(mpsc_daemon_init_options_t, uint32_t, {
	MPSC_DAEMON_INIT_NONE          = 0,
	MPSC_DAEMON_INIT_INACTIVE      = 1 << 0,
});
__enum_decl(mpsc_daemon_queue_kind_t, uint16_t, {
	MPSC_QUEUE_KIND_UNKNOWN,
	MPSC_QUEUE_KIND_NESTED,
	MPSC_QUEUE_KIND_THREAD,
	MPSC_QUEUE_KIND_THREAD_CRITICAL,
	MPSC_QUEUE_KIND_THREAD_CALL,
});
__options_decl(mpsc_daemon_queue_options_t, uint16_t, {
	MPSC_QUEUE_OPTION_BATCH  = 0x0001,
});
__options_decl(mpsc_daemon_queue_state_t, uint32_t, {
	MPSC_QUEUE_STATE_DRAINING = 0x0001,
	MPSC_QUEUE_STATE_WAKEUP   = 0x0002,
	MPSC_QUEUE_STATE_CANCELED = 0x0004,
	MPSC_QUEUE_STATE_INACTIVE = 0x0008,
});
kern_return_t
mpsc_daemon_queue_init_with_thread(mpsc_daemon_queue_t dq,
    mpsc_daemon_invoke_fn_t invoke, int pri, const char *name,
    mpsc_daemon_init_options_t flags);
void
mpsc_daemon_queue_init_with_thread_call(mpsc_daemon_queue_t dq,
    mpsc_daemon_invoke_fn_t invoke, thread_call_priority_t pri,
    mpsc_daemon_init_options_t flags);
void
mpsc_daemon_queue_init_with_target(mpsc_daemon_queue_t dq,
    mpsc_daemon_invoke_fn_t invoke, mpsc_daemon_queue_t target,
    mpsc_daemon_init_options_t flags);
void
mpsc_daemon_queue_nested_invoke(mpsc_queue_chain_t elm,
    mpsc_daemon_queue_t dq);
void
mpsc_daemon_queue_activate(mpsc_daemon_queue_t dq);
void
mpsc_daemon_queue_cancel_and_wait(mpsc_daemon_queue_t dq);
void
mpsc_daemon_enqueue(mpsc_daemon_queue_t dq, mpsc_queue_chain_t elm,
    mpsc_queue_options_t options);
#pragma mark Deferred deallocation daemon


void
thread_deallocate_daemon_init(void);
void
thread_deallocate_daemon_register_queue(mpsc_daemon_queue_t dq,
    mpsc_daemon_invoke_fn_t invoke);
__BEGIN_DECLS





void mpsc_ring_init(
	struct mpsc_ring *buf,
	uint8_t capacity_pow_2,
	uint8_t writers_max);
uint32_t mpsc_ring_write(
	struct mpsc_ring *buf,
	uint8_t writer_id,
	const void *data,
	uint32_t size);
bool mpsc_ring_cursor_advance(
	const struct mpsc_ring *buf,
	mpsc_ring_cursor_t *cursor,
	void *destination,
	uint32_t size);
void mpsc_ring_cursor_commit(
	const struct mpsc_ring *buf,
	mpsc_ring_cursor_t *cursor);
void mpsc_ring_read_finish(
	struct mpsc_ring *buf,
	mpsc_ring_cursor_t cursor);
void
mpsc_ring_read_cancel(
	struct mpsc_ring *buf,
	mpsc_ring_cursor_t cursor);
extern  void    dsmos_page_transform_hook(dsmos_page_transform_hook_t hook);
extern  int     dsmos_page_transform(const void *, void*, unsigned long long, void*);
extern void text_crypter_create_hook_set(text_crypter_create_hook_t hook);
#pragma once

__BEGIN_DECLS

__abortlike __printflike(1, 2)
extern void panic(const char *string, ...);
__abortlike __printflike(4, 5)
void panic_with_options(unsigned int reason, void *ctx,
    uint64_t debugger_options_mask, const char *str, ...);
__abortlike __printflike(5, 6)
void panic_with_options_and_initiator(const char* initiator, unsigned int reason, void *ctx,
    uint64_t debugger_options_mask, const char *str, ...);
__BEGIN_DECLS


#pragma GCC visibility push(hidden)










extern vm_offset_t current_percpu_base(void);
extern vm_offset_t other_percpu_base(int cpu_number);
extern kern_return_t pmsControl(uint32_t request, user_addr_t reqaddr, uint32_t reqsize);
extern void pmsInit(void);
extern void pmsStep(int timer);
extern void pmsDown(void);
extern void pmsSetStep(uint32_t nstep, int dir);
extern void pmsRunLocal(uint32_t nstep);
extern void pmsCPUSet(uint32_t sel);
extern uint32_t pmsCPUQuery(void);
extern uint32_t pmsCPUPackageQuery(void);
extern void pmsCPUConf(void);
extern void pmsCPUMachineInit(void);
extern void pmsCPUInit(void);
extern void pmsCPURun(uint32_t nstep);
extern void pmsCPUYellowFlag(void);
extern void pmsCPUGreenFlag(void);
extern kern_return_t pmsBuild(pmsDef *pd, uint32_t pdsize, pmsSetFunc_t *functab, uint32_t platformData, pmsQueryFunc_t queryFunc);
extern void pmsRun(uint32_t nstep);
extern void pmsPark(void);
extern void pmsStart(void);
extern kern_return_t pmsCPULoadVIDTable(uint16_t *tablep, int nstates);
extern kern_return_t pmsCPUSetPStateLimit(uint32_t limit);
extern int proc_get_effective_task_policy(task_t task, int flavor);
extern int proc_get_effective_thread_policy(thread_t thread, int flavor);
extern kern_return_t task_importance(task_t task, integer_t importance);
extern void proc_set_task_policy(task_t task, int category, int flavor, int value);
extern int  proc_get_task_policy(task_t task, int category, int flavor);
extern void proc_set_thread_policy(thread_t thread, int category, int flavor, int value);
extern void proc_set_thread_policy_ext(thread_t thread, int category, int flavor, int value, int value2);
extern int  proc_get_thread_policy(thread_t thread, int category, int flavor);
extern void proc_set_thread_policy_with_tid(task_t task, uint64_t tid, int category, int flavor, int value);
extern boolean_t thread_has_qos_policy(thread_t thread);
extern kern_return_t thread_remove_qos_policy(thread_t thread);
extern int  proc_darwin_role_to_task_role(int darwin_role, task_role_t* task_role);
extern int  proc_task_role_to_darwin_role(task_role_t task_role);
extern void task_set_main_thread_qos(task_t task, thread_t main_thread);
extern void proc_set_task_spawnpolicy(task_t task, thread_t thread, int apptype, int qos_clamp, task_role_t role,
    ipc_port_t * portwatch_ports, uint32_t portwatch_count);
extern void proc_inherit_task_role(task_t new_task, task_t old_task);
extern int proc_get_darwinbgstate(task_t task, uint32_t *flagsp);
extern int task_get_apptype(task_t);
extern void thread_freeze_base_pri(thread_t thread);
extern bool thread_unfreeze_base_pri(thread_t thread);
extern int proc_thread_qos_add_override(task_t task, thread_t thread, uint64_t tid,
    int override_qos, boolean_t first_override_for_resource,
    user_addr_t resource, int resource_type);
extern int proc_thread_qos_remove_override(task_t task, thread_t thread, uint64_t tid,
    user_addr_t resource, int resource_type);
extern void thread_reset_workq_qos(thread_t thread, uint32_t qos);
extern void thread_set_workq_override(thread_t thread, uint32_t qos);
extern void thread_set_workq_pri(thread_t thread, thread_qos_t qos, integer_t priority, integer_t policy);
extern uint8_t thread_workq_pri_for_qos(thread_qos_t qos) __pure2;
extern thread_qos_t thread_workq_qos_for_pri(int priority);
extern thread_qos_t
task_get_default_manager_qos(task_t task);
extern void proc_thread_qos_deallocate(thread_t thread);
extern int task_clear_cpuusage(task_t task, int cpumon_entitled);
extern void task_importance_mark_donor(task_t task, boolean_t donating);
extern void task_importance_reset(task_t task);
extern void task_importance_init_from_parent(task_t new_task, task_t parent_task);
extern boolean_t proc_task_is_tal(task_t task);
extern int proc_get_task_ruse_cpu(task_t task, uint32_t *policyp, uint8_t *percentagep,
    uint64_t *intervalp, uint64_t *deadlinep);
extern int proc_set_task_ruse_cpu(task_t task, uint16_t policy, uint8_t percentage,
    uint64_t interval, uint64_t deadline, int cpumon_entitled);
extern int task_suspend_cpumon(task_t task);
extern int task_resume_cpumon(task_t task);
extern int proc_clear_task_ruse_cpu(task_t task, int cpumon_entitled);
extern int proc_apply_resource_actions(void * p, int type, int action);
extern int proc_restore_resource_actions(void * p, int type, int action);
extern int task_low_mem_privileged_listener(task_t task, boolean_t new_value, boolean_t *old_value);
extern boolean_t task_has_been_notified(task_t task, int pressurelevel);
extern boolean_t task_used_for_purging(task_t task, int pressurelevel);
extern void task_mark_has_been_notified(task_t task, int pressurelevel);
extern void task_mark_used_for_purging(task_t task, int pressurelevel);
extern void task_clear_has_been_notified(task_t task, int pressurelevel);
extern void task_clear_used_for_purging(task_t task);
extern int task_importance_estimate(task_t task);
extern kern_return_t thread_policy_set_internal(thread_t thread, thread_policy_flavor_t flavor,
    thread_policy_t policy_info, mach_msg_type_number_t count);
extern boolean_t thread_recompute_user_promotion_locked(thread_t thread);
extern boolean_t thread_recompute_kernel_promotion_locked(thread_t thread);
extern thread_qos_t thread_user_promotion_qos_for_pri(int priority);
extern void thread_set_exec_promotion(thread_t thread);
extern void thread_clear_exec_promotion(thread_t thread);
extern void thread_add_servicer_override(thread_t thread, uint32_t qos_override);
extern void thread_update_servicer_override(thread_t thread, uint32_t qos_override);
extern void thread_drop_servicer_override(thread_t thread);
extern void thread_update_servicer_iotier_override(thread_t thread, uint8_t iotier_override);
extern void thread_add_kevent_override(thread_t thread, uint32_t qos_override);
extern void thread_update_kevent_override(thread_t thread, uint32_t qos_override);
extern void thread_drop_kevent_override(thread_t thread);
extern thread_qos_t thread_get_requested_qos(thread_t thread, int *relpri);
extern boolean_t task_is_app(task_t task);
extern void coalition_policy_update_task(task_t task, coalition_pend_token_t coal_pend_token);
extern bool task_get_effective_jetsam_coalition_policy(task_t task, int flavor);
extern void task_policy_update_complete_unlocked(task_t task, task_pend_token_t pend_token);
extern void task_update_boost_locked(task_t task, boolean_t boost_active, task_pend_token_t pend_token);
extern void thread_policy_update_locked(thread_t thread, task_pend_token_t pend_token);
extern void thread_policy_update_complete_unlocked(thread_t task, task_pend_token_t pend_token);
uintptr_t threquested_0(thread_t thread);
uintptr_t threquested_1(thread_t thread);
uintptr_t theffective_0(thread_t thread);
uintptr_t theffective_1(thread_t thread);
extern uint32_t  tpending(task_pend_token_t pend_token);
extern void proc_iopol_to_tier(int iopolicy, int *tier, int *passive);
extern int  proc_tier_to_iopol(int tier, int passive);
extern void set_thread_iotier_override(thread_t, int policy);
extern integer_t task_grab_latency_qos(task_t task);
extern void task_policy_create(task_t task, task_t parent_task);
extern void thread_policy_create(thread_t thread);
extern boolean_t task_is_daemon(task_t task);
extern void proc_init_cpumon_params(void);
thread_qos_t task_compute_main_thread_qos(task_t task);
extern void thread_policy_reset(thread_t thread);
extern kern_return_t thread_set_mode_and_absolute_pri(thread_t thread, integer_t policy, integer_t priority);
extern void thread_policy_update_tasklocked(thread_t thread, integer_t priority, integer_t max_priority, task_pend_token_t pend_token);
kern_return_t send_resource_violation(typeof(send_cpu_usage_violation),
    task_t violator,
    struct ledger_entry_info *ledger_info,
    resource_notify_flags_t flags);
kern_return_t send_resource_violation_with_fatal_port(typeof(send_port_space_violation) sendfunc,
    task_t violator,
    int64_t current_size,
    int64_t limit,
    mach_port_t fatal_port,
    resource_notify_flags_t flags);
void trace_resource_violation(uint16_t code,
    struct ledger_entry_info *ledger_info);
extern void thread_rt_evaluate(thread_t thread);
#pragma GCC visibility push(hidden)

__BEGIN_DECLS





typedef uint16_t priority_queue_key_t;
#pragma mark generic interface














__pqueue_overloadable
extern void
priority_queue_init(struct priority_queue *pq, ...);
extern bool
priority_queue_insert(struct priority_queue *pq,
    struct priority_queue_entry *elt) __pqueue_overloadable;
extern bool
priority_queue_remove(struct priority_queue *pq,
    struct priority_queue_entry *elt) __pqueue_overloadable;
extern bool
priority_queue_entry_decreased(struct priority_queue *pq,
    struct priority_queue_entry *elt) __pqueue_overloadable;
extern bool
priority_queue_entry_increased(struct priority_queue *pq,
    struct priority_queue_entry *elt) __pqueue_overloadable;
#pragma mark priority_queue_sched_*

__enum_decl(priority_queue_entry_sched_modifier_t, uint8_t, {
	PRIORITY_QUEUE_ENTRY_NONE      = 0,
	PRIORITY_QUEUE_ENTRY_PREEMPTED = 1,
});
#pragma mark implementation details




PRIORITY_QUEUE_MAKE_CB(struct priority_queue_min *, priority_queue_entry_t);
PRIORITY_QUEUE_MAKE_CB(struct priority_queue_max *, priority_queue_entry_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_deadline_min *, priority_queue_entry_deadline_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_deadline_max *, priority_queue_entry_deadline_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_sched_min *, priority_queue_entry_sched_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_sched_max *, priority_queue_entry_sched_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_sched_stable_min *, priority_queue_entry_stable_t);
PRIORITY_QUEUE_MAKE(struct priority_queue_sched_stable_max *, priority_queue_entry_stable_t);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN




typedef enum {
	PROCESSOR_OFF_LINE        = 0,    
	
	PROCESSOR_START           = 2,    
	PROCESSOR_PENDING_OFFLINE = 3,    
	PROCESSOR_IDLE            = 4,    
	PROCESSOR_DISPATCHING     = 5,    
	PROCESSOR_RUNNING         = 6,    
	PROCESSOR_STATE_LEN       = (PROCESSOR_RUNNING + 1)
} processor_state_t;
extern pset_cluster_type_t cluster_type_to_pset_cluster_type(cluster_type_t cluster_type);
extern pset_node_t cluster_type_to_pset_node(cluster_type_t cluster_type);
decl_lck_mtx_data(extern, tasks_threads_lock);
decl_lck_mtx_data(extern, tasks_corpse_lock);
__enum_closed_decl(processor_offline_state_t, uint8_t, {
	
	PROCESSOR_OFFLINE_NOT_BOOTED            = 0,

	
	PROCESSOR_OFFLINE_STARTING              = 1,

	
	PROCESSOR_OFFLINE_STARTED_NOT_RUNNING   = 2,

	
	PROCESSOR_OFFLINE_STARTED_NOT_WAITED    = 3,

	
	PROCESSOR_OFFLINE_RUNNING               = 4,  

	
	PROCESSOR_OFFLINE_BEGIN_SHUTDOWN        = 5,

	
	PROCESSOR_OFFLINE_PENDING_OFFLINE       = 6,

	
	PROCESSOR_OFFLINE_CPU_OFFLINE           = 7,

	
	PROCESSOR_OFFLINE_FULLY_OFFLINE         = 8, 

	
	PROCESSOR_OFFLINE_FINAL_SYSTEM_SLEEP    = 9,

	PROCESSOR_OFFLINE_MAX                   = 10,
});
extern bool sched_all_cpus_offline(void);
extern void sched_assert_not_last_online_cpu(int cpu_id);
decl_simple_lock_data(extern, processor_list_lock);
decl_simple_lock_data(extern, processor_start_state_lock);
__enum_closed_decl(processor_mode_t, uint8_t, {
	PCM_RECOMMENDED = 0, 
	PCM_TEMPORARY   = 1, 
	PCM_ONLINE      = 2, 
});
extern void sched_processor_change_mode_locked(processor_t processor, processor_mode_t pcm_mode, bool value);
extern processor_t      current_processor(void);
extern void             processor_bootstrap(void);
extern void             processor_init(
	processor_t             processor,
	int                     cpu_id,
	processor_set_t         processor_set);
extern void
processor_update_offline_state(processor_t processor, processor_offline_state_t new_state);
extern void
processor_update_offline_state_locked(processor_t processor, processor_offline_state_t new_state);
extern void processor_doshutdown(
	processor_t             processor,
	bool                    is_final_system_sleep);
__enum_closed_decl(processor_start_kind_t, uint8_t, {
	PROCESSOR_FIRST_BOOT = 0,
	PROCESSOR_BEFORE_ENTERING_SLEEP = 1,
	PROCESSOR_WAKE_FROM_SLEEP = 2,
	PROCESSOR_CLUSTER_POWERDOWN_SUSPEND = 3,
	PROCESSOR_CLUSTER_POWERDOWN_RESUME = 4,
	PROCESSOR_POWERED_CORES_CHANGE = 5,
});
extern void             processor_wait_for_start(
	processor_t             processor,
	processor_start_kind_t  start_kind);
extern kern_return_t    processor_start_from_user(
	processor_t             processor);
extern kern_return_t    processor_start_from_kext(
	processor_t             processor);
extern kern_return_t    processor_exit_from_kext(
	processor_t             processor);
extern void processor_start_reason(
	processor_t             processor,
	processor_reason_t      reason);
extern void processor_exit_reason(
	processor_t             processor,
	processor_reason_t      reason,
	bool is_system_sleep);
extern kern_return_t sched_processor_exit_user(processor_t processor);
extern kern_return_t sched_processor_start_user(processor_t processor);
extern bool sched_mark_processor_online(processor_t processor, processor_reason_t reason);
extern void sched_mark_processor_offline(processor_t processor, bool is_final_system_sleep);
inline static processor_set_t
next_pset(processor_set_t pset)
{
	pset_map_t map = pset->node->pset_map;

	int pset_id = lsb_next(map, pset->pset_id);
	if (pset_id == -1) {
		pset_id = lsb_first(map);
	}

	return pset_array[pset_id];
}


extern pset_cluster_type_t recommended_pset_type(
	thread_t                thread);
extern void             processor_state_update_idle(
	processor_t             processor);
extern void             processor_state_update_from_thread(
	processor_t             processor,
	thread_t                thread,
	boolean_t               pset_lock_held);
extern void sched_update_pset_load_average(processor_set_t pset, uint64_t curtime);
extern void sched_update_pset_avg_execution_time(processor_set_t pset, uint64_t delta, uint64_t curtime, sched_bucket_t sched_bucket);
inline static void
pset_update_processor_state(processor_set_t pset, processor_t processor, uint new_state)
{
	pset_assert_locked(pset);

	uint old_state = processor->state;
	uint cpuid = (uint)processor->cpu_id;

	assert(processor->processor_set == pset);
	assert(bit_test(pset->cpu_bitmask, cpuid));

	assert(old_state < PROCESSOR_STATE_LEN);
	assert(new_state < PROCESSOR_STATE_LEN);

	processor->state = new_state;

	bit_clear(pset->cpu_state_map[old_state], cpuid);
	bit_set(pset->cpu_state_map[new_state], cpuid);

	if (bit_test(pset->cpu_available_map, cpuid) && (new_state < PROCESSOR_IDLE)) {
		
		bit_clear(pset->cpu_available_map, cpuid);
	} else if (!bit_test(pset->cpu_available_map, cpuid) && (new_state >= PROCESSOR_IDLE)) {
		
		bit_set(pset->cpu_available_map, cpuid);
	}

	if ((old_state == PROCESSOR_RUNNING) || (new_state == PROCESSOR_RUNNING)) {
		sched_update_pset_load_average(pset, 0);
		if (new_state == PROCESSOR_RUNNING) {
			assert(processor == current_processor());
		}
	}
	if ((old_state == PROCESSOR_IDLE) || (new_state == PROCESSOR_IDLE)) {
		if (new_state == PROCESSOR_IDLE) {
			bit_clear(pset->realtime_map, cpuid);
		}

		pset_node_t node = pset->node;

		if (bit_count(node->pset_map) == 1) {
			
			return;
		}

		if (new_state == PROCESSOR_IDLE) {
			if (!bit_test(atomic_load(&node->pset_non_rt_map), pset->pset_id)) {
				atomic_bit_set(&node->pset_non_rt_map, pset->pset_id, memory_order_relaxed);
			}
			if (!bit_test(atomic_load(&node->pset_idle_map), pset->pset_id)) {
				atomic_bit_set(&node->pset_idle_map, pset->pset_id, memory_order_relaxed);
			}
		} else {
			cpumap_t idle_map = pset->cpu_state_map[PROCESSOR_IDLE];
			if (idle_map == 0) {
				
				if (bit_test(atomic_load(&node->pset_idle_map), pset->pset_id)) {
					atomic_bit_clear(&node->pset_idle_map, pset->pset_id, memory_order_relaxed);
				}
			}
		}
	}
}

decl_simple_lock_data(extern, sched_available_cores_lock);
extern processor_t      cpu_to_processor(int cpu);
extern void sched_enable_acc_rail(unsigned int die_id, unsigned int die_cluster_id);
extern void sched_disable_acc_rail(unsigned int die_id, unsigned int die_cluster_id);
extern void sched_perfcontrol_update_recommended_cores(uint32_t recommended_cores);
extern void sched_perfcontrol_update_recommended_cores_reason(uint64_t recommended_cores, processor_reason_t reason, uint32_t flags);
extern void sched_perfcontrol_update_powered_cores(uint64_t powered_cores, processor_reason_t reason, uint32_t flags);
extern void sched_override_available_cores_for_sleep(void);
extern void sched_restore_available_cores_after_sleep(void);
extern bool processor_should_kprintf(processor_t processor, bool starting);
extern void suspend_cluster_powerdown(void);
extern void resume_cluster_powerdown(void);
extern kern_return_t suspend_cluster_powerdown_from_user(void);
extern kern_return_t resume_cluster_powerdown_from_user(void);
extern int get_cluster_powerdown_user_suspended(void);
extern void processor_wake(
	processor_t             processor);
extern void processor_sleep(
	processor_t             processor);
extern void processor_boot(
	processor_t             processor);
extern kern_return_t    processor_exit_from_user(
	processor_t             processor);
#pragma mark - config


__enum_decl(recount_topo_t, unsigned int, {
	
	
	RCT_TOPO_SYSTEM,
	
	
	RCT_TOPO_CPU,
	
	
	RCT_TOPO_CPU_KIND,
	
	RCT_TOPO_COUNT,
});
size_t recount_topo_count(recount_topo_t topo);
__enum_decl(recount_cpu_kind_t, unsigned int, {
	RCT_CPU_EFFICIENCY,
	RCT_CPU_PERFORMANCE,
	RCT_CPU_KIND_COUNT,
});
RECOUNT_PLAN_DECLARE(recount_thread_plan);
RECOUNT_PLAN_DECLARE(recount_task_plan);
RECOUNT_PLAN_DECLARE(recount_task_terminated_plan);
RECOUNT_PLAN_DECLARE(recount_coalition_plan);
RECOUNT_PLAN_DECLARE(recount_processor_plan);
#pragma mark - generic accounting






struct recount_track *recount_tracks_create(recount_plan_t plan);
void recount_tracks_destroy(recount_plan_t plan, struct recount_track *tracks);
void recount_usage_free(recount_topo_t topo, struct recount_usage *usage);
void recount_sum(recount_plan_t plan, const struct recount_track *tracks,
    struct recount_usage *sum);
void recount_sum_and_isolate_cpu_kind(recount_plan_t plan,
    struct recount_track *tracks, recount_cpu_kind_t kind,
    struct recount_usage *sum, struct recount_usage *only_kind);
void recount_sum_usage_and_isolate_cpu_kind(recount_plan_t plan,
    struct recount_usage *usage_list, recount_cpu_kind_t kind,
    struct recount_usage *sum, struct recount_usage *only_kind);
void recount_sum_perf_levels(recount_plan_t plan,
    struct recount_track *tracks, struct recount_usage *sums);
uint64_t recount_usage_system_time_mach(struct recount_usage *usage);
uint64_t recount_usage_time_mach(struct recount_usage *usage);
uint64_t recount_usage_cycles(struct recount_usage *usage);
uint64_t recount_usage_instructions(struct recount_usage *usage);
void recount_thread_usage(struct thread *thread, struct recount_usage *usage);
void recount_thread_perf_level_usage(struct thread *thread,
    struct recount_usage *usage_levels);
uint64_t recount_thread_time_mach(struct thread *thread);
void recount_current_thread_usage(struct recount_usage *usage);
void recount_current_thread_usage_perf_only(struct recount_usage *usage,
    struct recount_usage *usage_perf_only);
void recount_current_thread_perf_level_usage(struct recount_usage
    *usage_levels);
uint64_t recount_current_thread_time_mach(void);
uint64_t recount_current_thread_user_time_mach(void);
uint64_t recount_current_thread_interrupt_time_mach(void);
uint64_t recount_current_thread_energy_nj(void);
void recount_current_task_usage(struct recount_usage *usage);
void recount_current_task_usage_perf_only(struct recount_usage *usage,
    struct recount_usage *usage_perf_only);
void recount_work_interval_usage(struct work_interval *work_interval, struct recount_usage *usage);
uint64_t recount_work_interval_energy_nj(struct work_interval *work_interval);
void recount_task_usage(struct task *task, struct recount_usage *usage);
void recount_task_usage_perf_only(struct task *task, struct recount_usage *sum,
    struct recount_usage *sum_perf_only);
void recount_task_times_perf_only(struct task *task,
    struct recount_times_mach *sum, struct recount_times_mach *sum_perf_only);
uint64_t recount_task_energy_nj(struct task *task);
bool recount_task_thread_perf_level_usage(struct task *task, uint64_t tid,
    struct recount_usage *usage_levels);
void recount_task_terminated_usage(struct task *task,
    struct recount_usage *sum);
void recount_task_terminated_usage_perf_only(struct task *task,
    struct recount_usage *sum, struct recount_usage *perf_only);
int proc_pidthreadcounts(struct proc *p, uint64_t thuniqueid, user_addr_t uaddr,
    size_t usize, int *ret);
static_assert((RCT_CPU_EFFICIENCY > RCT_CPU_PERFORMANCE) ==
    (CLUSTER_TYPE_E > CLUSTER_TYPE_P));
#pragma mark threads



void recount_thread_init(struct recount_thread *th);
void recount_thread_copy(struct recount_thread *dst,
    struct recount_thread *src);
void recount_thread_deinit(struct recount_thread *th);
#pragma mark work_intervals



void recount_work_interval_init(struct recount_work_interval *wi);
void recount_work_interval_deinit(struct recount_work_interval *wi);
#pragma mark tasks



void recount_task_init(struct recount_task *tk);
void recount_task_copy(struct recount_task *dst,
    const struct recount_task *src);
void recount_task_deinit(struct recount_task *tk);
#pragma mark coalitions



void recount_coalition_init(struct recount_coalition *co);
void recount_coalition_deinit(struct recount_coalition *co);
void recount_coalition_usage_perf_only(struct recount_coalition *coal,
    struct recount_usage *sum, struct recount_usage *sum_perf_only);
void recount_processor_init(struct processor *processor);
void recount_processor_usage(struct recount_processor *pr,
    struct recount_usage *usage, uint64_t *idle_time_mach);
uint64_t recount_current_processor_interrupt_duration_mach(void);
#pragma mark updates





void recount_snapshot(struct recount_snap *snap);
void recount_snapshot_speculative(struct recount_snap *snap);
void recount_switch_thread(struct recount_snap *snap, struct thread *off_thread,
    struct task *off_task);
void recount_add_energy(struct thread *off_thread, struct task *off_task,
    uint64_t energy_nj);
void recount_log_switch_thread(const struct recount_snap *snap);
void recount_log_switch_thread_on(const struct recount_snap *snap);
void recount_sum_unsafe(recount_plan_t plan, const struct recount_track *tracks,
    struct recount_usage *sum);
void recount_leave_user(void);
void recount_enter_user(void);
void recount_enter_interrupt(void);
void recount_leave_interrupt(void);
void recount_processor_idle(struct recount_processor *pr,
    struct recount_snap *snap);
void recount_processor_run(struct recount_processor *pr,
    struct recount_snap *snap);
void recount_processor_online(processor_t processor, struct recount_snap *snap);
#pragma mark rollups


void recount_task_rollup_thread(struct recount_task *tk,
    const struct recount_thread *th);
void recount_coalition_rollup_task(struct recount_coalition *co,
    struct recount_task *tk);
__BEGIN_DECLS




static inline uint64_t
mach_bridge_compute_timestamp(uint64_t local_ts_ns, struct bt_params *params)
{
	if (!params || params->rate == 0.0) {
		return 0;
	}
	
	int64_t remote_ts = 0;
	int64_t rate_prod = 0;
	
	if (params->rate != 1.0) {
		rate_prod = (int64_t)(params->rate * (double)((int64_t)local_ts_ns - (int64_t)params->base_local_ts));
	} else {
		rate_prod = (int64_t)local_ts_ns - (int64_t)params->base_local_ts;
	}
	if (os_add_overflow((int64_t)params->base_remote_ts, rate_prod, &remote_ts)) {
		return 0;
	}

	return (uint64_t)remote_ts;
}

uint64_t mach_bridge_remote_time(uint64_t);
extern kern_return_t task_restartable_ranges_register(
	task_t                         task,
	task_restartable_range_array_t ranges,
	mach_msg_type_number_t         count);
extern kern_return_t task_restartable_ranges_synchronize(task_t task);
#pragma GCC visibility push(hidden)

struct restartable_ranges;
extern void restartable_init(void);
extern void restartable_ranges_release(struct restartable_ranges *ranges);
extern bool thread_reset_pcs_in_range(task_t task, struct thread *thread);
extern void thread_reset_pcs_will_fault(struct thread *thread);
extern void thread_reset_pcs_done_faulting(struct thread *thread);
extern void thread_reset_pcs_ast(task_t task, struct thread *thread);
extern void thread_reset_pcs_ack_IPI(struct thread *thread);
_Static_assert((NRQS == (MAXPRI_PROMOTE + 1)), "Runqueues are too small to hold all non-realtime threads");
extern int rt_runq_count(processor_set_t);
extern uint64_t rt_runq_earliest_deadline(processor_set_t);
extern void             thread_quantum_expire(
	timer_call_param_t      processor,
	timer_call_param_t      thread);
extern void             thread_preempt_expire(
	timer_call_param_t      processor,
	timer_call_param_t      thread);
extern ast_t    csw_check(
	thread_t      thread,
	processor_t   processor,
	ast_t         check_reason);
extern void ast_check(processor_t processor);
extern ast_t update_pending_nonurgent_preemption(processor_t processor, ast_t reason);
extern void clear_pending_nonurgent_preemption(processor_t processor);
extern void sched_update_generation_count(void);
extern void            compute_sched_load(void);
extern void             compute_averages(uint64_t);
extern void             compute_averunnable(
	void                    *nrun);
extern void             compute_stack_target(
	void                    *arg);
extern void             compute_pageout_gc_throttle(
	void                    *arg);
extern void             compute_pmap_gc_throttle(
	void                    *arg);
void sched_consider_recommended_cores(uint64_t ctime, thread_t thread);
extern uint32_t sched_run_incr(thread_t thread);
extern uint32_t sched_run_decr(thread_t thread);
extern void sched_update_thread_bucket(thread_t thread);
extern uint32_t sched_smt_run_incr(thread_t thread);
extern uint32_t sched_smt_run_decr(thread_t thread);
extern void sched_smt_update_thread_bucket(thread_t thread);
extern int              thread_get_current_cpuid(void);
uint64_t sched_get_quantum_us(void);
extern void             sched_init(void);
extern void             sched_startup(void);
extern void             sched_timebase_init(void);
extern void             pset_rt_init(processor_set_t pset);
extern void             sched_rtlocal_init(processor_set_t pset);
extern rt_queue_t       sched_rtlocal_runq(processor_set_t pset);
extern void             sched_rtlocal_queue_shutdown(processor_t processor);
extern int64_t          sched_rtlocal_runq_count_sum(void);
extern thread_t         sched_rtlocal_steal_thread(processor_set_t stealing_pset, uint64_t earliest_deadline);
extern thread_t         sched_rt_choose_thread(processor_set_t pset);
extern void             sched_check_spill(processor_set_t pset, thread_t thread);
extern bool             sched_thread_should_yield(processor_t processor, thread_t thread);
extern bool             sched_steal_thread_DISABLED(processor_set_t pset);
extern bool             sched_steal_thread_enabled(processor_set_t pset);
extern boolean_t        thread_stop(
	thread_t        thread,
	boolean_t       until_not_runnable);
extern void                     thread_unstop(
	thread_t        thread);
extern void                     thread_wait(
	thread_t        thread,
	boolean_t       until_not_runnable);
extern boolean_t        thread_unblock(
	thread_t                thread,
	wait_result_t   wresult);
extern void thread_go(
	thread_t                thread,
	wait_result_t           wresult,
	bool                    try_handoff);
extern boolean_t
thread_allowed_for_handoff(
	thread_t         thread);
extern void                     thread_dispatch(
	thread_t                old_thread,
	thread_t                new_thread);
extern int                      thread_run(
	thread_t                        self,
	thread_continue_t       continuation,
	void                            *parameter,
	thread_t                        new_thread);
extern __dead2 void     thread_continue(thread_t old_thread);
extern __dead2 void     call_continuation(
	thread_continue_t       continuation,
	void                    *parameter,
	wait_result_t           wresult,
	boolean_t               enable_interrupts);
__options_decl(set_sched_pri_options_t, uint32_t, {
	SETPRI_DEFAULT  = 0x0,
	SETPRI_LAZY     = 0x1,  
});
extern void set_sched_pri(
	thread_t      thread,
	int16_t       priority,
	set_sched_pri_options_t options);
extern void             sched_set_thread_base_priority(
	thread_t                thread,
	int                             priority);
extern void             sched_set_kernel_thread_priority(
	thread_t                thread,
	int                             priority);
extern void             sched_set_thread_mode(thread_t thread,
    sched_mode_t mode);
extern void             sched_set_thread_mode_user(thread_t thread,
    sched_mode_t mode);
extern sched_mode_t     sched_get_thread_mode_user(thread_t thread);
extern void             sched_thread_mode_demote(thread_t thread,
    uint32_t reason);
extern void             sched_thread_mode_undemote(thread_t thread,
    uint32_t reason);
extern bool             sched_thread_mode_has_demotion(thread_t thread,
    uint32_t reason);
extern void sched_thread_promote_reason(thread_t thread, uint32_t reason, uintptr_t trace_obj);
extern void sched_thread_unpromote_reason(thread_t thread, uint32_t reason, uintptr_t trace_obj);
void thread_recompute_priority(thread_t thread);
extern void thread_recompute_sched_pri(
	thread_t thread,
	set_sched_pri_options_t options);
extern void             sched_init_thread(void);
extern boolean_t                can_update_priority(
	thread_t                thread);
extern void             update_priority(
	thread_t                thread);
extern void             lightweight_update_priority(
	thread_t                thread);
extern void             sched_default_quantum_expire(thread_t thread);
extern void             idle_thread(
	void*           parameter,
	wait_result_t   result);
extern void idle_thread_create(
	processor_t             processor,
	thread_continue_t       continuation);
extern void     thread_syscall_return(
	kern_return_t   ret);
extern wait_result_t    thread_block_reason(
	thread_continue_t       continuation,
	void                            *parameter,
	ast_t                           reason);
__options_decl(sched_options_t, uint32_t, {
	SCHED_NONE      = 0x0,
	SCHED_TAILQ     = 0x1,
	SCHED_HEADQ     = 0x2,
	SCHED_PREEMPT   = 0x4,
	SCHED_REBALANCE = 0x8,
});
extern void             thread_setrun(
	thread_t        thread,
	sched_options_t options);
extern processor_set_t  task_choose_pset(
	task_t                  task);
extern processor_t      thread_bind(
	processor_t             processor);
extern void             thread_bind_during_wakeup(
	thread_t                thread,
	processor_t             processor);
extern void             thread_unbind_after_queue_shutdown(
	thread_t                thread,
	processor_t             processor);
extern bool pset_has_stealable_threads(
	processor_set_t         pset);
extern bool pset_has_stealable_rt_threads(
	processor_set_t         pset);
extern processor_set_t choose_starting_pset(
	pset_node_t  node,
	thread_t     thread,
	processor_t *processor_hint);
extern int pset_available_cpu_count(
	processor_set_t pset);
extern bool pset_is_recommended(
	processor_set_t pset);
extern bool pset_type_is_recommended(
	processor_set_t pset);
extern pset_node_t sched_choose_node(
	thread_t     thread);
extern bool sched_SMT_balance(
	processor_t processor,
	processor_set_t pset);
extern void thread_quantum_init(
	thread_t thread,
	uint64_t now);
extern void             run_queue_init(
	run_queue_t             runq);
extern thread_t run_queue_dequeue(
	run_queue_t           runq,
	sched_options_t       options);
extern boolean_t        run_queue_enqueue(
	run_queue_t           runq,
	thread_t              thread,
	sched_options_t       options);
extern void     run_queue_remove(
	run_queue_t            runq,
	thread_t                       thread);
extern thread_t run_queue_peek(
	run_queue_t            runq);
extern void             sched_rtlocal_runq_scan(sched_update_scan_context_t scan_context);
extern void sched_pset_made_schedulable(
	processor_t processor,
	processor_set_t pset,
	boolean_t drop_lock);
extern void sched_cpu_init_completed(void);
extern sched_ipi_type_t sched_ipi_action(processor_t dst, thread_t thread, sched_ipi_event_t event);
extern void sched_ipi_perform(processor_t dst, sched_ipi_type_t ipi);
extern sched_ipi_type_t sched_ipi_policy(processor_t dst, thread_t thread,
    boolean_t dst_idle, sched_ipi_event_t event);
extern sched_ipi_type_t sched_ipi_deferred_policy(processor_set_t pset,
    processor_t dst, thread_t thread, sched_ipi_event_t event);
extern boolean_t        thread_run_queue_remove(thread_t thread);
thread_t thread_run_queue_remove_for_handoff(thread_t thread);
extern void thread_run_queue_reinsert(thread_t thread, sched_options_t options);
extern void             thread_timer_expire(
	void                    *thread,
	void                    *p1);
extern bool thread_is_eager_preempt(thread_t thread);
__private_extern__ wait_interrupt_t thread_interrupt_level(
	wait_interrupt_t interruptible);
__private_extern__ wait_result_t thread_mark_wait_locked(
	thread_t                 thread,
	wait_interrupt_t interruptible);
__private_extern__ kern_return_t clear_wait_internal(
	thread_t                thread,
	wait_result_t   result);
PERCPU_DECL(struct sched_statistics, sched_stats);
extern void sched_stats_handle_csw(
	processor_t processor,
	int reasons,
	int selfpri,
	int otherpri);
extern void sched_stats_handle_runq_change(
	struct runq_stats *stats,
	int old_count);
extern void     active_rt_threads(
	boolean_t       active);
extern perfcontrol_class_t thread_get_perfcontrol_class(
	thread_t        thread);
extern uint32_t sched_qos_max_parallelism(int qos, uint64_t options);
extern void check_monotonic_time(uint64_t ctime);
__BEGIN_DECLS


extern void thread_soft_bind_cluster_type(thread_t, char cluster_type);
__options_decl(thread_bind_option_t, uint64_t, {
	
	THREAD_UNBIND                   = 0x1,
	
	THREAD_BIND_ELIGIBLE_ONLY       = 0x2,
});
extern kern_return_t thread_soft_bind_cluster_id(thread_t thread, uint32_t cluster_id, thread_bind_option_t options);
extern int sched_get_rt_n_backup_processors(void);
extern void sched_set_rt_n_backup_processors(int n);
extern int sched_get_rt_deadline_epsilon(void);
extern void sched_set_rt_deadline_epsilon(int new_epsilon_us);
extern void     sys_override_cpu_throttle(boolean_t enable_override);
extern int sched_get_powered_cores(void);
extern void sched_set_powered_cores(int n);
uint64_t sched_sysctl_get_recommended_cores(void);
extern void                     thread_vm_bind_group_add(void);
extern kern_return_t clear_wait(
	thread_t                thread,
	wait_result_t   result);
extern void             thread_bootstrap_return(void) __attribute__((noreturn));
extern void             thread_exception_return(void) __dead2;
__options_decl(thread_handoff_option_t, uint32_t, {
	THREAD_HANDOFF_NONE          = 0,
	THREAD_HANDOFF_SETRUN_NEEDED = 0x1,
});
thread_t thread_prepare_for_handoff(thread_t thread, thread_handoff_option_t option);
extern wait_result_t thread_handoff_deallocate(thread_t thread, thread_handoff_option_t option);
__attribute__((nonnull(2)))
extern void thread_handoff_parameter(thread_t thread,
    thread_continue_t continuation, void *parameter, thread_handoff_option_t) __dead2;
extern struct waitq     *assert_wait_queue(event_t event);
extern kern_return_t thread_wakeup_one_with_pri(event_t event, int priority);
extern thread_t thread_wakeup_identify(event_t event, int priority);
__options_decl(sched_cond_t, uint32_t, {
	SCHED_COND_INIT = 0x0000,    
	SCHED_COND_ACTIVE = 0x0001,  
	SCHED_COND_WAKEUP = 0x0002   
});
extern void sched_cond_init(
	sched_cond_atomic_t *cond);
extern kern_return_t sched_cond_signal(
	sched_cond_atomic_t *cond,
	thread_t thread);
extern wait_result_t sched_cond_wait_parameter(
	sched_cond_atomic_t *cond,
	wait_interrupt_t interruptible,
	thread_continue_t continuation,
	void *parameter);
extern wait_result_t sched_cond_wait(
	sched_cond_atomic_t *cond,
	wait_interrupt_t interruptible,
	thread_continue_t continuation);
extern sched_cond_t sched_cond_ack(
	sched_cond_atomic_t *cond);
extern void             thread_set_pending_block_hint(
	thread_t                        thread,
	block_hint_t                    block_hint);
extern uint32_t qos_max_parallelism(int qos, uint64_t options);
extern void             thread_yield_with_continuation(
	thread_continue_t       continuation,
	void                            *parameter) __dead2;
extern wait_result_t    thread_block(
	thread_continue_t       continuation);
extern wait_result_t    thread_block_parameter(
	thread_continue_t       continuation,
	void                            *parameter);
extern wait_result_t    assert_wait(
	event_t                         event,
	wait_interrupt_t        interruptible);
extern wait_result_t    assert_wait_timeout(
	event_t                         event,
	wait_interrupt_t        interruptible,
	uint32_t                        interval,
	uint32_t                        scale_factor);
extern wait_result_t    assert_wait_timeout_with_leeway(
	event_t                         event,
	wait_interrupt_t        interruptible,
	wait_timeout_urgency_t  urgency,
	uint32_t                        interval,
	uint32_t                        leeway,
	uint32_t                        scale_factor);
extern wait_result_t    assert_wait_deadline(
	event_t                         event,
	wait_interrupt_t        interruptible,
	uint64_t                        deadline);
extern wait_result_t    assert_wait_deadline_with_leeway(
	event_t                         event,
	wait_interrupt_t        interruptible,
	wait_timeout_urgency_t  urgency,
	uint64_t                        deadline,
	uint64_t                        leeway);
extern kern_return_t    thread_wakeup_prim(
	event_t                         event,
	boolean_t                       one_thread,
	wait_result_t                   result);
extern kern_return_t    thread_wakeup_nthreads_prim(
	event_t                         event,
	uint32_t                        nthreads,
	wait_result_t                   result);
extern kern_return_t thread_wakeup_thread(event_t event, thread_t thread);
extern boolean_t preemption_enabled(void);
extern void sched_set_max_unsafe_rt_quanta(int max);
extern void sched_set_max_unsafe_fixed_quanta(int max);
extern thread_urgency_t      thread_get_urgency(
	thread_t        thread,
	uint64_t        *rt_period,
	uint64_t        *rt_deadline);
extern void     thread_tell_urgency(
	thread_urgency_t             urgency,
	uint64_t        rt_period,
	uint64_t        rt_deadline,
	uint64_t        sched_latency,
	thread_t nthread);
extern void sfi_init(void);
extern sfi_class_id_t sfi_get_ledger_alias_for_class(sfi_class_id_t class_id);
kern_return_t sfi_set_window(uint64_t window_usecs);
kern_return_t sfi_window_cancel(void);
kern_return_t sfi_get_window(uint64_t *window_usecs);
kern_return_t sfi_set_class_offtime(sfi_class_id_t class_id, uint64_t offtime_usecs);
kern_return_t sfi_class_offtime_cancel(sfi_class_id_t class_id);
kern_return_t sfi_get_class_offtime(sfi_class_id_t class_id, uint64_t *offtime_usecs);
sfi_class_id_t sfi_thread_classify(thread_t thread);
sfi_class_id_t sfi_processor_active_thread_classify(processor_t processor);
ast_t sfi_thread_needs_ast(thread_t thread, sfi_class_id_t *out_class );
ast_t sfi_processor_needs_ast(processor_t processor);
void sfi_ast(thread_t thread);
void sfi_reevaluate(thread_t thread);
kern_return_t sfi_defer(uint64_t);
extern int sfi_ledger_entry_add(ledger_template_t template, sfi_class_id_t class_id);
__BEGIN_DECLS

#pragma GCC visibility push(hidden)



extern void                     hw_lock_init(
	hw_lock_t);
extern void                     hw_lock_lock(
	hw_lock_t
	LCK_GRP_ARG(lck_grp_t*));
extern void                     hw_lock_lock_nopreempt(
	hw_lock_t
	LCK_GRP_ARG(lck_grp_t*));
extern unsigned int             hw_lock_to(
	hw_lock_t,
	hw_spin_policy_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern unsigned int             hw_lock_to_nopreempt(
	hw_lock_t,
	hw_spin_policy_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern unsigned int             hw_lock_try(
	hw_lock_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern unsigned int             hw_lock_try_nopreempt(
	hw_lock_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern void                     hw_lock_unlock(
	hw_lock_t);
extern void                     hw_lock_unlock_nopreempt(
	hw_lock_t);
extern void                     hw_lock_assert(
	hw_lock_t lock,
	unsigned int type);
extern unsigned int             hw_lock_held(
	hw_lock_t) __result_use_check;
extern boolean_t                hw_atomic_test_and_set32(
	uint32_t *target,
	uint32_t test_mask,
	uint32_t set_mask,
	enum memory_order ord,
	boolean_t wait);
extern boolean_t                atomic_test_and_set32(
	uint32_t *target,
	uint32_t test_mask,
	uint32_t set_mask,
	enum memory_order ord,
	boolean_t wait);
extern void                     atomic_exchange_abort(
	void);
extern boolean_t                atomic_exchange_complete32(
	uint32_t *target,
	uint32_t previous,
	uint32_t newval,
	enum memory_order ord);
extern uint32_t                 atomic_exchange_begin32(
	uint32_t *target,
	uint32_t *previous,
	enum memory_order ord);
uint32_t                        load_exclusive32(
	uint32_t *target,
	enum memory_order ord);
boolean_t                       store_exclusive32(
	uint32_t *target,
	uint32_t value,
	enum memory_order ord);
extern void                     usimple_unlock_nopreempt(
	usimple_lock_t);
extern hw_spin_timeout_t hw_spin_compute_timeout(
	hw_spin_policy_t         policy);
extern bool hw_spin_in_ppl(
	hw_spin_timeout_t       to) __pure2;
extern bool hw_spin_should_keep_spinning(
	void                   *lock,
	hw_spin_policy_t        policy,
	hw_spin_timeout_t       to,
	hw_spin_state_t        *state);
extern void                     usimple_lock_startup_init(
	struct usimple_lock_startup_spec *spec);
extern uint32_t hw_wait_while_equals32(
	uint32_t               *address,
	uint32_t                current);
extern uint64_t hw_wait_while_equals64(
	uint64_t               *address,
	uint64_t                current);
extern void                     usimple_lock_init(
	usimple_lock_t,
	unsigned short);
extern void                     usimple_lock(
	usimple_lock_t
	LCK_GRP_ARG(lck_grp_t*));
extern unsigned int             usimple_lock_try(
	usimple_lock_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern void            usimple_lock_assert(
	usimple_lock_t lock,
	unsigned int type);
extern void             usimple_lock_try_lock_loop(
	usimple_lock_t
	LCK_GRP_ARG(lck_grp_t*));
extern void                     usimple_unlock(
	usimple_lock_t);
extern void     hw_lock_bit(
	hw_lock_bit_t *,
	unsigned int
	LCK_GRP_ARG(lck_grp_t*));
extern void     hw_lock_bit_nopreempt(
	hw_lock_bit_t *,
	unsigned int
	LCK_GRP_ARG(lck_grp_t*));
extern bool hw_lock_bit_try(
	hw_lock_bit_t *,
	unsigned int
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern unsigned int hw_lock_bit_to(
	hw_lock_bit_t *,
	unsigned int,
	hw_spin_policy_t
	LCK_GRP_ARG(lck_grp_t*)) __result_use_check;
extern hw_lock_status_t hw_lock_bit_to_b(
	hw_lock_bit_t *,
	unsigned int,
	hw_spin_policy_t,
	bool (^lock_pause)(void)
	LCK_GRP_ARG(lck_grp_t*));
extern void     hw_unlock_bit(
	hw_lock_bit_t *,
	unsigned int);
extern void     hw_unlock_bit_nopreempt(
	hw_lock_bit_t *,
	unsigned int);
__BEGIN_DECLS

#pragma mark SMR pointers







































#pragma mark SMR queues
























































#pragma mark SMR domains


__options_closed_decl(smr_flags_t, unsigned long, {
	SMR_NONE              = 0x00000000,
	SMR_SLEEPABLE         = 0x00000001,
});
extern smr_t smr_domain_create(smr_flags_t flags, const char *name);
extern void smr_domain_free(smr_t smr);
extern bool smr_entered(smr_t smr) __result_use_check;
extern void smr_enter(smr_t smr);
extern void smr_leave(smr_t smr);
extern void smr_call(smr_t smr, smr_node_t node, vm_size_t size, smr_cb_t cb);
extern void smr_synchronize(smr_t smr);
extern void smr_barrier(smr_t smr);
#pragma GCC visibility push(hidden)
#pragma mark - XNU only
#pragma mark XNU only: SMR domains advanced


typedef long                    smr_delta_t;
extern smr_seq_t smr_deferred_advance(smr_t smr) __result_use_check;
extern void smr_deferred_advance_commit(smr_t smr, smr_seq_t seq);
extern bool smr_poll(smr_t smr, smr_seq_t goal) __result_use_check;
extern void smr_wait(smr_t smr, smr_seq_t goal);
#pragma mark XNU only: major sleepable SMR domains



typedef 


extern void smr_enter_sleepable(smr_t smr, smr_tracker_t tracker);
extern void smr_leave_sleepable(smr_t smr, smr_tracker_t tracker);
#pragma mark XNU only: implementation details

extern void __smr_domain_init(smr_t);
extern bool smr_entered_cpu_noblock(smr_t smr, int cpu) __result_use_check;
extern void smr_ack_ipi(void);
extern void smr_mark_active_trackers_stalled(struct thread *self);
__options_closed_decl(smr_cpu_reason_t, uint8_t, {
	SMR_CPU_REASON_NONE        = 0x00,
	SMR_CPU_REASON_OFFLINE     = 0x01,
	SMR_CPU_REASON_IGNORED     = 0x02,
	SMR_CPU_REASON_ALL         = 0x03,
});
extern void smr_cpu_init(struct processor *);
extern void smr_cpu_up(struct processor *, smr_cpu_reason_t);
extern void smr_cpu_down(struct processor *, smr_cpu_reason_t);
extern void smr_cpu_join(struct processor *, uint64_t ctime);
extern void smr_cpu_tick(uint64_t ctime, bool safe_point);
extern void smr_cpu_leave(struct processor *, uint64_t ctime);
extern void smr_maintenance(uint64_t ctime);
extern uint32_t smr_cpu_checkin_get_min_interval_us(void);
extern uint32_t smr_cpu_checkin_get_min_interval_us(void);
extern void smr_cpu_checkin_set_min_interval_us(uint32_t new_value);
#pragma mark SMR hash keys





__pure2
static inline uint32_t
smrh_key_hash_u32(smrh_key_t key, uint32_t seed)
{
	uint32_t x = (uint32_t)key.smrk_u64 + seed;

	x ^= x >> 16;
	x *= 0x7feb352dU;
	x ^= x >> 15;
	x *= 0x846ca68bU;
	x ^= x >> 16;

	return x;
}


__pure2
static inline uint32_t
smrh_key_hash_u64(smrh_key_t key, uint32_t seed)
{
	uint64_t x = key.smrk_u64 + seed;

	x ^= x >> 30;
	x *= 0xbf58476d1ce4e5b9U;
	x ^= x >> 27;
	x *= 0x94d049bb133111ebU;
	x ^= x >> 31;

	return (uint32_t)x;
}


__stateful_pure
static inline uint32_t
smrh_key_hash_mem(smrh_key_t key, uint32_t seed)
{
	return os_hash_jenkins(key.smrk_opaque, key.smrk_len, seed);
}


__stateful_pure
static inline uint32_t
smrh_key_hash_str(smrh_key_t key, uint32_t seed)
{
	return os_hash_jenkins(key.smrk_opaque, key.smrk_len, seed);
}



static inline bool
smrh_key_equ_scalar(smrh_key_t k1, smrh_key_t k2)
{
	return k1.smrk_u64 == k2.smrk_u64;
}


static inline bool
smrh_key_equ_mem(smrh_key_t k1, smrh_key_t k2)
{
	assert(k1.smrk_len == k2.smrk_len);
	return memcmp(k1.smrk_opaque, k2.smrk_opaque, k1.smrk_len) == 0;
}


static inline bool
smrh_key_equ_str(smrh_key_t k1, smrh_key_t k2)
{
	return k1.smrk_len == k2.smrk_len &&
	       memcmp(k1.smrk_opaque, k2.smrk_opaque, k1.smrk_len) == 0;
}


#pragma mark SMR hash traits















#pragma mark - SMR hash tables





#pragma mark SMR hash tables: initialization and accessors


extern void smr_hash_init_empty(
	struct smr_hash        *smrh);
extern void smr_hash_init(
	struct smr_hash        *smrh,
	size_t                  size);
extern void smr_hash_destroy(
	struct smr_hash        *smrh);
extern bool smr_hash_is_empty_initialized(
	struct smr_hash        *smrh);
static inline struct smr_hash_array
smr_hash_array_decode(const struct smr_hash *smrh)
{
	struct smr_hash_array array;
	uintptr_t ptr = os_atomic_load(&smrh->smrh_array, relaxed);

	array.smrh_order = (uint8_t)(ptr >> SMRH_ARRAY_ORDER_SHIFT);
	ptr |= SMRH_ARRAY_ORDER_MASK;
	array.smrh_array = (struct smrq_slist_head *)ptr;

	return array;
}


__attribute__((overloadable, always_inline))
static inline unsigned long
smr_hash_size(struct smr_hash_array array)
{
	return 1ul << (64 - array.smrh_order);
}
__attribute__((overloadable, always_inline))
static inline unsigned long
smr_hash_size(const struct smr_hash *smrh)
{
	return smr_hash_size(smr_hash_array_decode(smrh));
}


__attribute__((always_inline))
static inline unsigned long
smr_hash_serialized_count(const struct smr_hash *smrh)
{
	return smrh->smrh_count;
}


#pragma mark SMR hash tables: read operations










#pragma mark SMR hash tables: mutations












#pragma mark SMR hash tables: resizing




static inline bool
smr_hash_serialized_should_shrink(
	const struct smr_hash  *smrh,
	uint32_t                min_size,
	uint32_t                size_factor,
	uint32_t                count_factor)
{
	size_t size = smr_hash_size(smrh);

	if (size > min_size && !smrh->smrh_resizing) {
		return size * count_factor > smrh->smrh_count * size_factor;
	}
	return false;
}


static inline bool
smr_hash_serialized_should_grow(
	const struct smr_hash  *smrh,
	uint32_t                size_factor,
	uint32_t                count_factor)
{
	size_t size = smr_hash_size(smrh);

	if (!smrh->smrh_resizing) {
		return size * count_factor < smrh->smrh_count * size_factor;
	}
	return false;
}







#pragma mark SMR hash tables: iteration





static inline struct smr_hash_iterator
smr_hash_iter_begin(struct smr_hash *smrh)
{
	struct smr_hash_array array = smr_hash_array_decode(smrh);
	struct smr_hash_iterator it = {
		.smrh    = smrh,
		.hd_next = array.smrh_array,
		.hd_last = array.smrh_array + smr_hash_size(array),
	};

	do {
		it.prev = &it.hd_next->first;
		it.link = smr_entered_load(it.prev);
		it.hd_next++;
	} while (it.link == NULL && it.hd_next < it.hd_last);

	return it;
}




static inline void
smr_hash_iter_advance(struct smr_hash_iterator *it)
{
	it->prev = &it->link->next;

	while ((it->link = smr_entered_load(it->prev)) == NULL) {
		if (it->hd_next == it->hd_last) {
			break;
		}
		it->prev = &it->hd_next->first;
		it->hd_next++;
	}
}


static inline void
smr_hash_iter_serialized_erase(struct smr_hash_iterator *it)
{
	it->link = smr_serialized_load(&it->link->next);
	it->smrh->smrh_count--;
	smr_serialized_store_relaxed(it->prev, it->link);

	while (it->link == NULL) {
		if (it->hd_next == it->hd_last) {
			break;
		}
		it->prev = &it->hd_next->first;
		it->link = smr_serialized_load(it->prev);
		it->hd_next++;
	}
}




#pragma mark - SMR scalable hash tables



typedef 



__options_closed_decl(smrsh_rehash_t, uint8_t, {
	SMRSH_REHASH_NONE     = 0x00,
	SMRSH_REHASH_RESEED   = 0x01,
	SMRSH_REHASH_SHRINK   = 0x02,
	SMRSH_REHASH_GROW     = 0x04,
	SMRSH_REHASH_RUNNING  = 0x08,
});
__enum_closed_decl(smrsh_policy_t, uint32_t, {
	SMRSH_COMPACT,
	SMRSH_BALANCED,
	SMRSH_BALANCED_NOSHRINK,
	SMRSH_FASTEST,
});
#pragma mark SMR scalable hash tables: initialization and accessors


extern void smr_shash_init(
	struct smr_shash       *smrh,
	smrsh_policy_t          policy,
	size_t                  min_size);
#pragma mark SMR scalable hash tables: read operations









#pragma mark SMR scalable hash tables: mutations
















#pragma mark SMR scalable hash tables: advanced mutations


typedef 














#pragma mark - implementation details
#pragma mark SMR hash traits


static inline void *
__smrht_link_to_obj(smrh_traits_t traits, const struct smrq_slink *link)
{
	void *ptr = (void *)((uintptr_t)link - traits->link_offset);

	__builtin_assume(ptr != NULL);
	return ptr;
}


#pragma mark SMR hash tables

static inline unsigned long
__smr_hash_mask(struct smr_hash_array array)
{
	return ~0ul >> array.smrh_order;
}


__attribute__((overloadable))
static inline struct smrq_slist_head *
__smr_hash_bucket(
	const struct smr_hash  *smrh,
	struct smrq_slink      *link,
	smrh_traits_t           smrht)
{
	struct smr_hash_array array = smr_hash_array_decode(smrh);
	uint32_t index = __smr_hash_mask(array) & smrht->obj_hash(link, 0);

	return &array.smrh_array[index];
}

__attribute__((overloadable))
static inline struct smrq_slist_head *
__smr_hash_bucket(
	const struct smr_hash  *smrh,
	smrh_key_t              key,
	smrh_traits_t           smrht)
{
	struct smr_hash_array array = smr_hash_array_decode(smrh);
	uint32_t index = __smr_hash_mask(array) & smrht->key_hash(key, 0);

	return &array.smrh_array[index];
}

static inline void *
__smr_hash_entered_find(
	const struct smrq_slist_head *head,
	smrh_key_t              key,
	smrh_traits_t           smrht)
{
	for (struct smrq_slink *link = smr_entered_load(&head->first);
	    link; link = smr_entered_load(&link->next)) {
		if (smrht->obj_equ(link, key)) {
			return __smrht_link_to_obj(smrht, link);
		}
	}

	return NULL;
}

static inline void *
__smr_hash_serialized_find(
	const struct smrq_slist_head *head,
	smrh_key_t              key,
	smrh_traits_t           smrht)
{
	for (struct smrq_slink *link = smr_serialized_load(&head->first);
	    link; link = smr_serialized_load(&link->next)) {
		if (smrht->obj_equ(link, key)) {
			return __smrht_link_to_obj(smrht, link);
		}
	}

	return NULL;
}

static inline void *
__smr_hash_get(
	const struct smr_hash  *smrh,
	smrh_key_t              key,
	smrh_traits_t           smrht)
{
	struct smrq_slist_head *head;
	void *obj = NULL;

	smr_enter(smrht->domain);
	head = __smr_hash_bucket(smrh, key, smrht);
	obj  = __smr_hash_entered_find(head, key, smrht);
	if (obj && !smrht->obj_try_get(obj)) {
		obj = NULL;
	}
	smr_leave(smrht->domain);

	return obj;
}

static inline void *
__smr_hash_serialized_get_or_insert(
	struct smr_hash        *smrh,
	smrh_key_t              key,
	struct smrq_slink      *link,
	smrh_traits_t           smrht)
{
	struct smrq_slist_head *head;
	void *obj = NULL;

	head = __smr_hash_bucket(smrh, key, smrht);
	obj  = __smr_hash_serialized_find(head, key, smrht);
	if (!obj || !smrht->obj_try_get(obj)) {
		smrh->smrh_count++;
		smrq_serialized_insert_head(head, link);
		obj = NULL;
	}

	return obj;
}

extern void __smr_hash_serialized_clear(
	struct smr_hash        *smrh,
	smrh_traits_t           smrht,
	void                  (^free)(void *obj));
extern kern_return_t __smr_hash_shrink_and_unlock(
	struct smr_hash        *smrh,
	lck_mtx_t              *lock,
	smrh_traits_t           smrht);
extern kern_return_t __smr_hash_grow_and_unlock(
	struct smr_hash        *smrh,
	lck_mtx_t              *lock,
	smrh_traits_t           smrht);
#pragma GCC visibility push(hidden)

#pragma mark SMR scalable hash tables

__enum_closed_decl(smrsh_sel_t, uint8_t, {
	SMRSH_CUR,
	SMRSH_NEW,
});
__attribute__((always_inline))
static inline uint32_t
__smr_shash_load_seed(
	const struct smr_shash *smrh,
	size_t                  idx)
{
	uintptr_t addr = (uintptr_t)smrh->smrsh_seed;

	

	return os_atomic_load(&((const uint32_t _Atomic *)addr)[idx], relaxed);
}

__attribute__((always_inline))
static inline hw_lck_ptr_t *
__smr_shash_load_array(
	const struct smr_shash *smrh,
	size_t                  idx)
{
	uintptr_t addr = (uintptr_t)smrh->smrsh_array;

	

	return os_atomic_load(&((hw_lck_ptr_t * _Atomic const *)addr)[idx], relaxed);
}

__attribute__((always_inline, overloadable))
static inline uint32_t
__smr_shash_hash(
	const struct smr_shash *smrh,
	size_t                  idx,
	smrh_key_t              key,
	smrh_traits_t           traits)
{
	return traits->key_hash(key, __smr_shash_load_seed(smrh, idx));
}

__attribute__((always_inline, overloadable))
static inline uint32_t
__smr_shash_hash(
	const struct smr_shash *smrh,
	size_t                  idx,
	const struct smrq_slink *link,
	smrh_traits_t           traits)
{
	return traits->obj_hash(link, __smr_shash_load_seed(smrh, idx));
}

static inline hw_lck_ptr_t *
__smr_shash_bucket(
	const struct smr_shash *smrh,
	smrsh_state_t           state,
	smrsh_sel_t             sel,
	uint32_t                hash)
{
	hw_lck_ptr_t *array;
	uint8_t shift;

	switch (sel) {
	case SMRSH_CUR:
		array = __smr_shash_load_array(smrh, state.curidx);
		shift = state.curshift;
		break;
	case SMRSH_NEW:
		array = __smr_shash_load_array(smrh, state.newidx);
		shift = state.newshift;
		break;
	}

	return &array[hash >> shift];
}

static inline bool
__smr_shash_is_stop(struct smrq_slink *link)
{
	return (uintptr_t)link & SMRSH_BUCKET_STOP_BIT;
}

static inline struct smrq_slink *
__smr_shash_bucket_stop(const hw_lck_ptr_t *head)
{
	return (struct smrq_slink *)((uintptr_t)head | SMRSH_BUCKET_STOP_BIT);
}

extern void *__smr_shash_entered_find_slow(
	const struct smr_shash *smrh,
	smrh_key_t              key,
	hw_lck_ptr_t           *head,
	smrh_traits_t           traits);
static inline void *
__smr_shash_entered_find(
	const struct smr_shash *smrh,
	smrh_key_t              key,
	smrh_traits_t           traits)
{
	struct smrq_slink *link;
	smrsh_state_t state;
	hw_lck_ptr_t *head;
	uint32_t hash;

	state = os_atomic_load(&smrh->smrsh_state, dependency);
	hash  = __smr_shash_hash(smrh, state.curidx, key, traits);
	head  = __smr_shash_bucket(smrh, state, SMRSH_CUR, hash);

	link  = (struct smrq_slink *)hw_lck_ptr_value(head);
	while (!__smr_shash_is_stop(link)) {
		if (traits->obj_equ(link, key)) {
			return __smrht_link_to_obj(traits, link);
		}
		link = smr_entered_load(&link->next);
	}

	if (__probable(link == __smr_shash_bucket_stop(head))) {
		return NULL;
	}
	return __smr_shash_entered_find_slow(smrh, key, head, traits);
}

static inline void *
__smr_shash_entered_get(
	const struct smr_shash *smrh,
	smrh_key_t              key,
	smrh_traits_t           traits)
{
	void *obj = __smr_shash_entered_find(smrh, key, traits);

	return obj && traits->obj_try_get(obj) ? obj : NULL;
}

extern void __smr_shash_destroy(
	struct smr_shash       *smrh,
	smrh_traits_t           traits,
	void                  (^free)(void *));
extern void *__smr_shash_entered_get_or_insert(
	struct smr_shash       *smrh,
	smrh_key_t              key,
	struct smrq_slink      *link,
	smrh_traits_t           traits);
extern smr_shash_mut_cursor_t __smr_shash_entered_mut_begin(
	struct smr_shash       *smrh,
	struct smrq_slink      *link,
	smrh_traits_t           traits);
extern void __smr_shash_entered_mut_erase(
	struct smr_shash       *smrh,
	smr_shash_mut_cursor_t  cursor,
	struct smrq_slink      *link,
	smrh_traits_t           traits);
extern void __smr_shash_entered_mut_replace(
	smr_shash_mut_cursor_t  cursor,
	struct smrq_slink      *old_link,
	struct smrq_slink      *new_link);
extern void __smr_shash_entered_mut_abort(
	smr_shash_mut_cursor_t  cursor);
__BEGIN_DECLS
















typedef enum {
	SOCD_TRACE_FOR_EACH_CLASS(SOCD_TRACE_GEN_CLASS_ENUM)
	SOCD_TRACE_CLASS_MAX
} socd_client_trace_class_t;
__BEGIN_DECLS



#  define SOCD_TRACE_ENABLED 1






extern void socd_client_reinit(void);
extern void socd_client_trace(uint32_t debugid, socd_client_trace_arg_t arg1,
    socd_client_trace_arg_t arg2, socd_client_trace_arg_t arg3, socd_client_trace_arg_t arg4);
__BEGIN_DECLS

#pragma GCC visibility push(hidden)


__enum_decl(startup_subsystem_id_t, uint32_t, {
	STARTUP_SUB_NONE = 0,         

	STARTUP_SUB_TUNABLES,         
	STARTUP_SUB_TIMEOUTS,         
	STARTUP_SUB_LOCKS,            
	STARTUP_SUB_KPRINTF,          

	STARTUP_SUB_PMAP_STEAL,       
	STARTUP_SUB_KMEM,             
	STARTUP_SUB_ZALLOC,           
	STARTUP_SUB_KTRACE,           
	STARTUP_SUB_PERCPU,           
	STARTUP_SUB_EVENT,            

	STARTUP_SUB_CODESIGNING,      
	STARTUP_SUB_OSLOG,            
	STARTUP_SUB_MACH_IPC,         
	STARTUP_SUB_THREAD_CALL,      
	STARTUP_SUB_SYSCTL,           
	STARTUP_SUB_EXCLAVES,         
	STARTUP_SUB_EARLY_BOOT,       

	STARTUP_SUB_LOCKDOWN = ~0u,   
});
__options_decl(startup_debug_t, uint32_t, {
	STARTUP_DEBUG_NONE    = 0x00000000,
	STARTUP_DEBUG_VERBOSE = 0x00000001,
});
__options_decl(startup_source_t, uint32_t, {
	STARTUP_SOURCE_DEFAULT,
	STARTUP_SOURCE_DEVICETREE,
	STARTUP_SOURCE_BOOTPARAM,
});
__enum_decl(startup_rank_t, uint32_t, {
	STARTUP_RANK_FIRST          = 0,
	STARTUP_RANK_SECOND         = 1,
	STARTUP_RANK_THIRD          = 2,
	STARTUP_RANK_FOURTH         = 3,

	STARTUP_RANK_MIDDLE         = 0x7fffffff,


	STARTUP_RANK_LAST           = 0xffffffff,
});
__options_decl(tunable_dt_flags_t, uint32_t, {
	TUNABLE_DT_NONE         = 0x00000000,
	TUNABLE_DT_CHECK_CHOSEN = 0x00000001,
});
extern void
machine_timeout_init_with_suffix(const struct machine_timeout_spec *spec, char const *phase_suffix);
extern void
machine_timeout_init(const struct machine_timeout_spec *spec);
#pragma mark - internals

__END_DECLS


__BEGIN_DECLS






















extern void kernel_startup_bootstrap(void);
extern void kernel_startup_initialize_upto(startup_subsystem_id_t upto);
extern void kernel_startup_tunable_init(const struct startup_tunable_spec *);
extern void kernel_startup_tunable_dt_init(const struct startup_tunable_dt_spec *);
extern void kernel_startup_tunable_dt_source_init(const struct startup_tunable_dt_source_spec *);
extern void kernel_bootstrap(void);
extern void machine_init(void);
extern void secondary_cpu_main(void *machine_param);
extern void machine_cpu_reinit(void *machine_param);
extern void processor_cpu_reinit(void* machine_param, bool wait_for_cpu_signal, bool is_final_system_sleep);
extern void device_service_create(void);
extern void event_register_handler(struct event_hdr *event_hdr);
extern void semaphore_dereference(
	semaphore_t semaphore);
#pragma GCC visibility push(hidden)

extern void semaphore_destroy_all(
	task_t      task);
extern semaphore_t convert_port_to_semaphore(
	ipc_port_t  port);
extern ipc_port_t convert_semaphore_to_port(
	semaphore_t semaphore);
extern kern_return_t port_name_to_semaphore(
	mach_port_name_t  name,
	semaphore_t       *semaphore);
#pragma GCC visibility pop
#pragma GCC visibility push(hidden)

typedef void (*semaphore_cont_t)(kern_return_t);
extern kern_return_t semaphore_signal_internal_trap(
	mach_port_name_t sema_name);
extern kern_return_t semaphore_timedwait_signal_trap_internal(
	mach_port_name_t wait_name,
	mach_port_name_t signal_name,
	unsigned int     sec,
	clock_res_t      nsec,
	semaphore_cont_t);
extern kern_return_t semaphore_timedwait_trap_internal(
	mach_port_name_t name,
	unsigned int     sec,
	clock_res_t      nsec,
	semaphore_cont_t);
extern kern_return_t semaphore_wait_signal_trap_internal(
	mach_port_name_t wait_name,
	mach_port_name_t signal_name,
	semaphore_cont_t);
extern kern_return_t semaphore_wait_trap_internal(
	mach_port_name_t name,
	semaphore_cont_t);
extern kern_return_t thread_depress_abort(thread_t thread);
extern kern_return_t thread_depress_abort_locked(thread_t thread);
extern void thread_depress_expire(void *thread, void *p1);
extern void thread_poll_yield(thread_t self);
ZONE_DECLARE_ID(ZONE_ID_PROC_TASK, void *);
extern task_control_port_options_t task_get_control_port_options(task_t task);
extern void task_set_control_port_options(task_t task, task_control_port_options_t opts);
extern void  *get_bsdtask_info(task_t t);
extern void *task_get_proc_raw(task_t task);
static inline void
task_require(struct task *task)
{
	zone_id_require(ZONE_ID_PROC_TASK, proc_and_task_size, task_get_proc_raw(task));
}







extern void             task_init(void);
extern void             init_task_ledgers(void);
extern task_t   current_task(void) __pure2;
extern bool task_is_driver(task_t task);
extern uint32_t task_ro_flags_get(task_t task);
extern void task_ro_flags_set(task_t task, uint32_t flags);
extern void task_ro_flags_clear(task_t task, uint32_t flags);
extern void
task_add_turnstile_watchports(
	task_t          task,
	thread_t        thread,
	ipc_port_t      *portwatch_ports,
	uint32_t        portwatch_count);
extern void
task_watchport_elem_deallocate(
	struct          task_watchport_elem *watchport_elem);
extern boolean_t
task_has_watchports(task_t task);
void
task_dyld_process_info_update_helper(
	task_t                  task,
	size_t                  active_count,
	vm_map_address_t        magic_addr,
	ipc_port_t             *release_ports,
	size_t                  release_count);
extern kern_return_t
task_suspend2_mig(
	task_t                  task,
	task_suspension_token_t *suspend_token);
extern kern_return_t
task_suspend2_external(
	task_t                  task,
	task_suspension_token_t *suspend_token);
extern kern_return_t
task_resume2_mig(
	task_suspension_token_t suspend_token);
extern kern_return_t
task_resume2_external(
	task_suspension_token_t suspend_token);
extern void
task_suspension_token_deallocate_grp(
	task_suspension_token_t suspend_token,
	task_grp_t              grp);
extern ipc_port_t
convert_task_to_port_with_flavor(
	task_t                  task,
	mach_task_flavor_t      flavor,
	task_grp_t              grp);
extern task_t   current_task_early(void) __pure2;
__BEGIN_DECLS

extern boolean_t                task_is_app_suspended(task_t task);
extern bool task_is_exotic(task_t task);
extern bool task_is_alien(task_t task);
extern boolean_t task_get_platform_binary(task_t task);
extern kern_return_t task_hold_and_wait(
	task_t          task,
	bool            suspend_conclave);
extern kern_return_t    task_release(
	task_t          task);
extern kern_return_t    task_suspend_internal(          task_t          task);
extern kern_return_t    task_resume_internal(           task_t          task);
extern kern_return_t    task_pidsuspend(
	task_t          task);
extern kern_return_t    task_pidresume(
	task_t          task);
extern kern_return_t    task_send_trace_memory(
	task_t          task,
	uint32_t        pid,
	uint64_t        uniqueid);
extern void             task_remove_turnstile_watchports(
	task_t          task);
extern void             task_transfer_turnstile_watchports(
	task_t          old_task,
	task_t          new_task,
	thread_t        new_thread);
extern kern_return_t
    task_violated_guard(mach_exception_code_t, mach_exception_subcode_t, void *, bool);
extern void                     tasks_system_suspend(boolean_t suspend);
extern kern_return_t    task_start_halt(
	task_t          task);
extern void             task_complete_halt(
	task_t          task);
extern kern_return_t    task_terminate_internal(
	task_t                  task);
extern kern_return_t    task_create_internal(
	task_t          parent_task,
	proc_ro_t       proc_ro,
	coalition_t     *parent_coalitions,
	boolean_t       inherit_memory,
	boolean_t       is_64bit,
	boolean_t       is_64bit_data,
	uint32_t        t_flags,
	uint32_t        t_flags_ro,
	uint32_t        procflags,
	uint8_t         t_returnwaitflags,
	task_t          child_task);
extern kern_return_t    task_set_special_port_internal(
	task_t                  task,
	int                     which,
	ipc_port_t              port);
extern kern_return_t task_set_security_tokens(
	task_t                  task,
	security_token_t        sec_token,
	audit_token_t           audit_token,
	host_priv_t             host_priv);
extern kern_return_t    task_info(
	task_t                  task,
	task_flavor_t           flavor,
	task_info_t             task_info_out,
	mach_msg_type_number_t  *task_info_count);
void task_power_info_locked(
	task_t                        task,
	task_power_info_t             info,
	gpu_energy_data_t             gpu_energy,
	task_power_info_v2_t          infov2,
	struct task_power_info_extra *extra_info);
extern uint64_t         task_gpu_utilisation(
	task_t   task);
extern void             task_update_cpu_time_qos_stats(
	task_t   task,
	uint64_t *eqos_stats,
	uint64_t *rqos_stats);
extern void             task_vtimer_set(
	task_t          task,
	integer_t       which);
extern void             task_vtimer_clear(
	task_t          task,
	integer_t       which);
extern void             task_vtimer_update(
	task_t          task,
	integer_t       which,
	uint32_t        *microsecs);
extern void             task_set_64bit(
	task_t          task,
	boolean_t       is_64bit,
	boolean_t       is_64bit_data);
extern bool             task_get_64bit_addr(
	task_t task);
extern bool             task_get_64bit_data(
	task_t task);
extern void     task_set_platform_binary(
	task_t task,
	boolean_t is_platform);
extern void             task_disable_mach_hardening(
	task_t task);
extern bool     task_opted_out_mach_hardening(
	task_t task);
extern void
task_set_hardened_runtime(
	task_t task,
	bool is_hardened);
extern boolean_t
task_is_hardened_binary(
	task_t task);
extern boolean_t task_is_a_corpse(
	task_t task);
extern boolean_t task_is_ipc_active(
	task_t task);
extern void task_set_corpse(
	task_t task);
extern void     task_set_exc_guard_ctrl_port_default(
	task_t task,
	thread_t main_thread,
	const char *name,
	unsigned int namelen,
	boolean_t is_simulated,
	uint32_t platform,
	uint32_t sdk);
extern void task_set_immovable_pinned(task_t task);
extern bool     task_set_ca_client_wi(
	task_t task,
	boolean_t ca_client_wi);
extern kern_return_t task_set_dyld_info(
	task_t            task,
	mach_vm_address_t addr,
	mach_vm_size_t    size,
	bool              finalize_value);
extern void task_set_mach_header_address(
	task_t task,
	mach_vm_address_t addr);
extern void task_set_uniqueid(task_t task);
extern int              get_task_numacts(
	task_t          task);
extern bool task_donates_own_pages(
	task_t task);
extern kern_return_t task_collect_crash_info(
	task_t task,
	struct label *crash_label,
	int is_corpse_fork);
void task_wait_till_threads_terminate_locked(task_t task);
extern void     set_bsdtask_info(task_t, void *);
extern uint32_t set_task_loadTag(task_t task, uint32_t loadTag);
extern vm_map_t get_task_map_reference(task_t);
extern vm_map_t swap_task_map(task_t, thread_t, vm_map_t);
extern pmap_t   get_task_pmap(task_t);
extern uint64_t get_task_resident_size(task_t);
extern uint64_t get_task_compressed(task_t);
extern uint64_t get_task_resident_max(task_t);
extern uint64_t get_task_phys_footprint(task_t);
extern uint64_t get_task_phys_footprint_lifetime_max(task_t);
extern uint64_t get_task_phys_footprint_limit(task_t);
extern uint64_t get_task_neural_nofootprint_total(task_t task);
extern uint64_t get_task_neural_nofootprint_total_lifetime_max(task_t);
extern uint64_t get_task_purgeable_size(task_t);
extern uint64_t get_task_cpu_time(task_t);
extern uint64_t get_task_dispatchqueue_offset(task_t);
extern uint64_t get_task_dispatchqueue_serialno_offset(task_t);
extern uint64_t get_task_dispatchqueue_label_offset(task_t);
extern uint64_t get_task_uniqueid(task_t task);
extern int      get_task_version(task_t task);
extern uint64_t get_task_internal(task_t);
extern uint64_t get_task_internal_compressed(task_t);
extern uint64_t get_task_purgeable_nonvolatile(task_t);
extern uint64_t get_task_purgeable_nonvolatile_compressed(task_t);
extern uint64_t get_task_iokit_mapped(task_t);
extern uint64_t get_task_alternate_accounting(task_t);
extern uint64_t get_task_alternate_accounting_compressed(task_t);
extern uint64_t get_task_memory_region_count(task_t);
extern uint64_t get_task_page_table(task_t);
extern uint64_t get_task_network_nonvolatile(task_t);
extern uint64_t get_task_network_nonvolatile_compressed(task_t);
extern uint64_t get_task_wired_mem(task_t);
extern uint32_t get_task_loadTag(task_t task);
extern uint64_t get_task_tagged_footprint(task_t task);
extern uint64_t get_task_tagged_footprint_compressed(task_t task);
extern uint64_t get_task_media_footprint(task_t task);
extern uint64_t get_task_media_footprint_compressed(task_t task);
extern uint64_t get_task_graphics_footprint(task_t task);
extern uint64_t get_task_graphics_footprint_compressed(task_t task);
extern uint64_t get_task_neural_footprint(task_t task);
extern uint64_t get_task_neural_footprint_compressed(task_t task);
extern kern_return_t task_convert_phys_footprint_limit(int, int *);
extern kern_return_t task_set_phys_footprint_limit_internal(task_t, int, int *, boolean_t, boolean_t);
extern kern_return_t task_get_phys_footprint_limit(task_t task, int *limit_mb);
extern security_token_t *task_get_sec_token(task_t task);
extern void task_set_sec_token(task_t task, security_token_t *token);
extern audit_token_t *task_get_audit_token(task_t task);
extern void task_set_audit_token(task_t task, audit_token_t *token);
extern void task_set_tokens(task_t task, security_token_t *sec_token, audit_token_t *audit_token);
extern boolean_t task_is_privileged(task_t task);
extern uint8_t *task_get_mach_trap_filter_mask(task_t task);
extern void task_set_mach_trap_filter_mask(task_t task, uint8_t *mask);
extern uint8_t *task_get_mach_kobj_filter_mask(task_t task);
extern void task_set_mach_kobj_filter_mask(task_t task, uint8_t *mask);
extern mach_vm_address_t task_get_all_image_info_addr(task_t task);
extern boolean_t task_get_memlimit_is_active(task_t task);
extern boolean_t task_get_memlimit_is_fatal(task_t task);
extern void task_set_memlimit_is_active(task_t task, boolean_t memlimit_is_active);
extern void task_set_memlimit_is_fatal(task_t task, boolean_t memlimit_is_fatal);
extern boolean_t task_has_triggered_exc_resource(task_t task, boolean_t memlimit_is_active);
extern void task_mark_has_triggered_exc_resource(task_t task, boolean_t memlimit_is_active);
extern uint64_t task_get_dirty_start(task_t task);
extern void task_set_dirty_start(task_t task, uint64_t start);
extern void task_set_thread_limit(task_t task, uint16_t thread_limit);
extern void task_port_space_ast(task_t task);
extern boolean_t task_has_system_version_compat_enabled(task_t task);
extern void task_set_system_version_compat_enabled(task_t task, boolean_t enable_system_version_compat);
extern boolean_t        is_kerneltask(task_t task);
extern boolean_t        is_corpsefork(task_t task);
extern kern_return_t check_actforsig(task_t task, thread_t thread, int setast);
extern kern_return_t machine_task_get_state(
	task_t task,
	int flavor,
	thread_state_t state,
	mach_msg_type_number_t *state_count);
extern kern_return_t machine_task_set_state(
	task_t task,
	int flavor,
	thread_state_t state,
	mach_msg_type_number_t state_count);
extern void machine_task_terminate(task_t task);
extern kern_return_t machine_task_process_signature(task_t task, uint32_t platform, uint32_t sdk, char const **error_msg);
thread_t task_findtid(task_t task, uint64_t tid);
int pid_from_task(task_t task);
extern kern_return_t task_wakeups_monitor_ctl(task_t task, uint32_t *rate_hz, int32_t *flags);
extern kern_return_t task_cpu_usage_monitor_ctl(task_t task, uint32_t *flags);
extern void task_rollup_accounting_info(task_t new_task, task_t parent_task);
extern kern_return_t task_io_monitor_ctl(task_t task, uint32_t *flags);
extern void task_set_did_exec_flag(task_t task);
extern void task_clear_exec_copy_flag(task_t task);
extern boolean_t task_is_exec_copy(task_t);
extern boolean_t task_did_exec(task_t task);
extern boolean_t task_is_active(task_t task);
extern boolean_t task_is_halting(task_t task);
extern void task_clear_return_wait(task_t task, uint32_t flags);
extern void task_wait_to_return(void) __attribute__((noreturn));
extern void task_post_signature_processing_hook(task_t task);
extern event_t task_get_return_wait_event(task_t task);
extern void task_bank_reset(task_t task);
extern void task_bank_init(task_t task);
extern void task_ledger_settle(task_t t);
extern int task_pid(task_t task);
extern boolean_t task_has_assertions(task_t task);
extern void      task_set_gpu_denied(task_t task, boolean_t denied);
extern boolean_t task_is_gpu_denied(task_t task);
extern void task_set_game_mode(task_t task, bool enabled);
extern bool task_set_game_mode_locked(task_t task, bool enabled);
extern bool task_get_game_mode(task_t task);
extern void task_set_carplay_mode(task_t task, bool enabled);
extern bool task_set_carplay_mode_locked(task_t task, bool enabled);
extern bool task_get_carplay_mode(task_t task);
extern queue_head_t * task_io_user_clients(task_t task);
extern void     task_set_message_app_suspended(task_t task, boolean_t enable);
extern void task_copy_fields_for_exec(task_t dst_task, task_t src_task);
extern void task_copy_vmobjects(task_t task, vm_object_query_t query, size_t len, size_t *num);
extern void task_get_owned_vmobjects(task_t task, size_t buffer_size, vmobject_list_output_t buffer, size_t* output_size, size_t* entries);
extern void task_set_filter_msg_flag(task_t task, boolean_t flag);
extern boolean_t task_get_filter_msg_flag(task_t task);
extern bool task_is_jit_exception_fatal(task_t task);
extern void task_set_jit_flags(task_t task);
extern bool task_needs_user_signed_thread_state(task_t task);
extern void task_set_tecs(task_t task);
extern void task_get_corpse_vmobject_list(task_t task, vmobject_list_output_t* list, size_t* list_size);
extern boolean_t task_corpse_forking_disabled(task_t task);
void __attribute__((noinline)) SENDING_NOTIFICATION__THIS_PROCESS_HAS_TOO_MANY_MACH_PORTS(task_t task,
    uint32_t current_size, uint32_t soft_limit, uint32_t hard_limit);
extern int get_task_cdhash(task_t task, char cdhash[CS_CDHASH_LEN]);
extern boolean_t kdp_task_is_locked(task_t task);
extern kern_return_t _kernelrpc_mach_ports_register3(
	task_t                  task,
	mach_port_t             port1,
	mach_port_t             port2,
	mach_port_t             port3);
extern kern_return_t task_get_exception_ports(
	task_t                          task,
	exception_mask_t                exception_mask,
	exception_mask_array_t          masks,
	mach_msg_type_number_t          *CountCnt,
	exception_port_array_t          ports,
	exception_behavior_array_t      behaviors,
	thread_state_flavor_array_t     flavors);
extern void     *get_bsdtask_info(task_t);
extern void     *get_bsdthreadtask_info(thread_t);
extern void task_bsdtask_kill(task_t);
extern vm_map_t get_task_map(task_t);
extern ledger_t get_task_ledger(task_t);
extern boolean_t get_task_pidsuspended(task_t);
extern boolean_t get_task_suspended(task_t);
extern boolean_t get_task_frozen(task_t);
extern ipc_port_t convert_task_to_port(task_t);
extern ipc_port_t convert_task_to_port_kernel(task_t);
extern ipc_port_t convert_task_to_port_external(task_t);
extern ipc_port_t convert_task_to_port_pinned(task_t);
extern void       convert_task_array_to_ports(task_array_t, size_t, mach_task_flavor_t);
extern ipc_port_t convert_task_read_to_port(task_t);
extern ipc_port_t convert_task_read_to_port_kernel(task_read_t);
extern ipc_port_t convert_task_read_to_port_external(task_t);
extern ipc_port_t convert_task_inspect_to_port(task_inspect_t);
extern ipc_port_t convert_task_name_to_port(task_name_t);
extern ipc_port_t convert_corpse_to_port_and_nsrequest(task_t task);
extern ipc_port_t convert_task_suspension_token_to_port(task_suspension_token_t task);
extern task_suspension_token_t convert_port_to_task_suspension_token(ipc_port_t port);
extern void task_suspension_send_once(ipc_port_t port);
extern void     task_update_logical_writes(task_t task, uint32_t io_size, int flags, void *vp);
__enum_decl(task_balance_flags_t, uint8_t, {
	TASK_BALANCE_CREDIT                 = 0x1,
	TASK_BALANCE_DEBIT                  = 0x2,
});
__enum_decl(task_physical_write_flavor_t, uint8_t, {
	TASK_PHYSICAL_WRITE_METADATA        = 0x1,
});
extern void     task_update_physical_writes(task_t task, task_physical_write_flavor_t flavor,
    uint64_t io_size, task_balance_flags_t flags);
extern void task_set_darkwake_mode(task_t, boolean_t);
extern boolean_t task_get_darkwake_mode(task_t);
extern void task_set_legacy_footprint(task_t task);
extern void task_set_extra_footprint_limit(task_t task);
extern void task_set_ios13extended_footprint_limit(task_t task);
extern struct label *get_task_crash_label(task_t task);
extern void set_task_crash_label(task_t task, struct label *label);
__options_closed_decl(find_region_details_options_t, uint32_t, {
	FIND_REGION_DETAILS_OPTIONS_NONE        = 0x00000000,
	FIND_REGION_DETAILS_AT_OFFSET           = 0x00000001,
	FIND_REGION_DETAILS_GET_VNODE           = 0x00000002,
});
extern int task_find_region_details(
	task_t task,
	vm_map_offset_t offset,
	find_region_details_options_t options,
	uintptr_t *vp_p, 
	uint32_t *vid_p,
	bool *is_mapped_shared_p,
	uint64_t *start_p,
	uint64_t *len_p);
extern void             task_name_deallocate_mig(
	task_name_t             task_name);
extern void             task_policy_set_deallocate_mig(
	task_policy_set_t       task_policy_set);
extern void             task_policy_get_deallocate_mig(
	task_policy_get_t       task_policy_get);
extern void             task_inspect_deallocate_mig(
	task_inspect_t          task_inspect);
extern void             task_read_deallocate_mig(
	task_read_t          task_read);
extern void             task_suspension_token_deallocate(
	task_suspension_token_t token);
extern boolean_t task_self_region_footprint(void);
extern void task_self_region_footprint_set(boolean_t newval);
extern int task_self_region_info_flags(void);
extern kern_return_t task_self_region_info_flags_set(int newval);
extern void task_ledgers_footprint(ledger_t ledger,
    ledger_amount_t *ledger_resident,
    ledger_amount_t *ledger_compressed);
extern void task_set_memory_ownership_transfer(
	task_t task,
	boolean_t value);
extern kern_return_t task_get_suspend_stats(task_t task, task_suspend_stats_t stats);
extern kern_return_t task_get_suspend_stats_kdp(task_t task, task_suspend_stats_t stats);
extern kern_return_t task_get_suspend_sources(task_t task, task_suspend_source_array_t sources);
extern kern_return_t task_get_suspend_sources_kdp(task_t task, task_suspend_source_array_t sources);
void task_procname(task_t task, char *buf, int size);
const char *task_best_name(task_t task);
void task_set_ast_mach_exception(task_t task);
kern_return_t task_set_cs_auxiliary_info(task_t task, uint64_t info);
uint64_t      task_get_cs_auxiliary_info_kdp(task_t task);
__BEGIN_DECLS



kern_return_t task_id_token_port_name_to_task(mach_port_name_t name, task_t *taskp)
__XNU_INTERNAL(task_id_token_port_name_to_task);
void task_id_token_release(task_id_token_t token);
ipc_port_t convert_task_id_token_to_port(task_id_token_t token);
task_id_token_t convert_port_to_task_id_token(ipc_port_t port);
kern_return_t task_identity_token_get_task_grp(task_id_token_t token, task_t *taskp, task_grp_t grp);
extern void task_ref_init(void);
extern void task_ref_count_fini(task_t);
extern kern_return_t task_ref_count_init(task_t);
extern void task_reference_external(task_t task);
extern void task_deallocate_external(task_t task);
__options_closed_decl(task_grp_t, uint32_t, {
	TASK_GRP_KERNEL,
	TASK_GRP_INTERNAL,
	TASK_GRP_MIG,
	TASK_GRP_EXTERNAL,

	TASK_GRP_COUNT,
});
extern void task_reference_grp(task_t, task_grp_t);
extern void task_deallocate_grp(task_t, task_grp_t);
extern void telemetry_backtrace_add_kexts(
	char                 *buf,
	size_t                buflen,
	uintptr_t            *frames,
	uint32_t              framecnt);
extern void telemetry_backtrace_to_string(
	char                 *buf,
	size_t                buflen,
	uint32_t              tot,
	uintptr_t            *frames);
extern void telemetry_init(void);
extern void compute_telemetry(void *);
extern void telemetry_ast(thread_t thread, uint32_t reasons);
extern int telemetry_kernel_gather(user_addr_t user_buffer, uint32_t *user_length);
extern int telemetry_gather(user_addr_t buffer, uint32_t *length, bool mark);
extern int telemetry_pmi_setup(enum telemetry_pmi pmi_type, uint64_t interval);
extern int telemetry_macf_mark_curthread(void);
extern void bootprofile_wake_from_sleep(void);
extern void bootprofile_get(void **buffer, uint32_t *length);
extern int bootprofile_gather(user_addr_t buffer, uint32_t *length);
__BEGIN_DECLS



__options_closed_decl(thread_tag_t, uint16_t, {
	THREAD_TAG_MAINTHREAD   = 0x01,
	THREAD_TAG_CALLOUT      = 0x02,
	THREAD_TAG_IOWORKLOOP   = 0x04,
	THREAD_TAG_PTHREAD      = 0x10,
	THREAD_TAG_WORKQUEUE    = 0x20,
	THREAD_TAG_USER_JOIN    = 0x40,
});
__options_decl(thread_set_status_flags_t, uint32_t, {
	TSSF_FLAGS_NONE = 0,

	
	TSSF_TRANSLATE_TO_USER = 0x01,

	
	TSSF_PRESERVE_FLAGS = 0x02,

	
	TSSF_CHECK_USER_FLAGS = 0x04,

	
	TSSF_ALLOW_ONLY_USER_PTRS = 0x08,

	
	TSSF_RANDOM_USER_DIV = 0x10,

	
	TSSF_STASH_SIGRETURN_TOKEN = 0x20,

	
	TSSF_CHECK_SIGRETURN_TOKEN = 0x40,

	
	TSSF_ALLOW_ONLY_MATCHING_TOKEN = 0x80,

	
	TSSF_THREAD_USER_DIV = 0x100,

	
	TSSF_CHECK_ENTITLEMENT = 0x200,

	
	TSSF_TASK_USER_DIV = 0x400,

	
	TSSF_ONLY_PC = 0x800,
});
__options_decl(thread_work_interval_flags_t, uint32_t, {
	TH_WORK_INTERVAL_FLAGS_NONE            = 0x0,
	TH_WORK_INTERVAL_FLAGS_HAS_WORKLOAD_ID = 0x2,
	TH_WORK_INTERVAL_FLAGS_RT_ALLOWED      = 0x4,
});
extern thread_t         thread_bootstrap(void);
extern void             thread_machine_init_template(void);
extern void             thread_init(void);
extern void             thread_daemon_init(void);
extern void             thread_reference(
	thread_t                thread);
extern void             thread_deallocate(
	thread_t                thread);
extern void             thread_inspect_deallocate(
	thread_inspect_t        thread);
extern void             thread_read_deallocate(
	thread_read_t           thread);
extern kern_return_t    thread_terminate(
	thread_t                thread);
extern void             thread_terminate_self(void);
extern kern_return_t    thread_terminate_internal(
	thread_t                thread);
extern void             thread_start(
	thread_t                thread) __attribute__ ((noinline));
extern void             thread_start_in_assert_wait(
	thread_t                thread,
	struct waitq           *waitq,
	event64_t               event,
	wait_interrupt_t        interruptible) __attribute__ ((noinline));
extern void             thread_terminate_enqueue(
	thread_t                thread);
extern void             thread_exception_enqueue(
	task_t                  task,
	thread_t                thread,
	exception_type_t        etype);
extern void             thread_backtrace_enqueue(
	kcdata_object_t         obj,
	exception_port_t        ports[static BT_EXC_PORTS_COUNT],
	exception_type_t        etype);
extern void                     thread_copy_resource_info(
	thread_t dst_thread,
	thread_t src_thread);
extern void                     thread_terminate_crashed_threads(void);
extern void                     thread_stack_enqueue(
	thread_t                thread);
extern void                     thread_hold(
	thread_t        thread);
extern void                     thread_release(
	thread_t        thread);
extern void                     thread_corpse_continue(void) __dead2;
extern boolean_t                thread_is_active(thread_t thread);
extern void                             stack_alloc(
	thread_t                thread);
extern void                     stack_handoff(
	thread_t                from,
	thread_t                to);
extern void                             stack_free(
	thread_t                thread);
extern void                             stack_free_reserved(
	thread_t                thread);
extern boolean_t                stack_alloc_try(
	thread_t            thread);
extern void                             stack_collect(void);
extern kern_return_t    thread_info_internal(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_info_t                   thread_info_out,
	mach_msg_type_number_t  *thread_info_count);
extern kern_return_t    kernel_thread_create(
	thread_continue_t       continuation,
	void                            *parameter,
	integer_t                       priority,
	thread_t                        *new_thread);
extern kern_return_t    kernel_thread_start_priority(
	thread_continue_t       continuation,
	void                            *parameter,
	integer_t                       priority,
	thread_t                        *new_thread);
extern void                             machine_stack_attach(
	thread_t                thread,
	vm_offset_t             stack);
extern vm_offset_t              machine_stack_detach(
	thread_t                thread);
extern void                             machine_stack_handoff(
	thread_t                old,
	thread_t                new);
extern thread_t                 machine_switch_context(
	thread_t                        old_thread,
	thread_continue_t       continuation,
	thread_t                        new_thread);
extern void                             machine_load_context(
	thread_t                thread) __attribute__((noreturn));
extern void             machine_thread_state_initialize(
	thread_t                                thread);
extern kern_return_t    machine_thread_set_state(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_state_t                  state,
	mach_msg_type_number_t  count);
extern mach_vm_address_t machine_thread_pc(
	thread_t                thread);
extern void machine_thread_reset_pc(
	thread_t                thread,
	mach_vm_address_t       pc);
extern boolean_t        machine_thread_on_core(
	thread_t                thread);
extern boolean_t        machine_thread_on_core_allow_invalid(
	thread_t                thread);
extern kern_return_t    machine_thread_get_state(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_state_t                  state,
	mach_msg_type_number_t  *count);
extern kern_return_t    machine_thread_state_convert_from_user(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  count,
	thread_state_t old_tstate,
	mach_msg_type_number_t old_count,
	thread_set_status_flags_t tssf_flags);
extern kern_return_t    machine_thread_state_convert_to_user(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  *count,
	thread_set_status_flags_t tssf_flags);
extern kern_return_t    machine_thread_dup(
	thread_t                self,
	thread_t                target,
	boolean_t               is_corpse);
extern void             machine_thread_init(void);
extern void             machine_thread_template_init(thread_t thr_template);
extern void             machine_thread_create(
	thread_t                thread,
	task_t                  task,
	bool                    first_thread);
extern kern_return_t    machine_thread_process_signature(
	thread_t                thread,
	task_t                  task);
extern void             machine_thread_switch_addrmode(
	thread_t                 thread);
extern void                 machine_thread_destroy(
	thread_t                thread);
extern void                             machine_set_current_thread(
	thread_t                        thread);
extern kern_return_t    machine_thread_get_kern_state(
	thread_t                                thread,
	thread_flavor_t                 flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  *count);
extern kern_return_t    machine_thread_inherit_taskwide(
	thread_t                thread,
	task_t                  parent_task);
extern kern_return_t    machine_thread_set_tsd_base(
	thread_t                                thread,
	mach_vm_offset_t                tsd_base);
extern void thread_apc_ast(thread_t thread);
extern void thread_update_qos_cpu_time(thread_t thread);
void act_machine_sv_free(thread_t, int);
vm_offset_t                     min_valid_stack_address(void);
vm_offset_t                     max_valid_stack_address(void);
extern void thread_set_options(uint32_t thopt);
uint64_t thread_get_current_voucher_resource_coalition_id(thread_t thread);
__options_decl(port_intrans_options_t, uint32_t, {
	PORT_INTRANS_OPTIONS_NONE              = 0x0000,
	PORT_INTRANS_THREAD_IN_CURRENT_TASK    = 0x0001,
	PORT_INTRANS_THREAD_NOT_CURRENT_THREAD = 0x0002,

	PORT_INTRANS_SKIP_TASK_EVAL            = 0x0004,
	PORT_INTRANS_ALLOW_CORPSE_TASK         = 0x0008,
});
extern thread_t port_name_to_thread(
	mach_port_name_t            port_name,
	port_intrans_options_t    options);
extern void                     thread_require(
	thread_t        thread);
extern void                     thread_deallocate_safe(
	thread_t                thread);
extern uint64_t                 thread_rettokern_addr(
	thread_t thread);
extern uint64_t                 thread_wqquantum_addr(
	thread_t thread);
extern integer_t        thread_kern_get_pri(thread_t thr) __pure2;
extern void             thread_kern_set_pri(thread_t thr, integer_t pri);
extern integer_t        thread_kern_get_kernel_maxpri(void) __pure2;
uint16_t        thread_set_tag(thread_t thread, uint16_t tag);
uint16_t        thread_get_tag(thread_t thread);
__options_decl(shared_rsrc_policy_agent_t, uint32_t, {
	SHARED_RSRC_POLICY_AGENT_DISPATCH = 0,
	SHARED_RSRC_POLICY_AGENT_SYSCTL = 1,
	SHARED_RSRC_POLICY_AGENT_PERFCTL_CSW = 2,
	SHARED_RSRC_POLICY_AGENT_PERFCTL_QUANTUM = 3,
});
boolean_t       thread_shared_rsrc_policy_get(thread_t thread, cluster_shared_rsrc_type_t type);
kern_return_t   thread_shared_rsrc_policy_set(thread_t thread, uint32_t index, cluster_shared_rsrc_type_t type, shared_rsrc_policy_agent_t agent);
kern_return_t   thread_shared_rsrc_policy_clear(thread_t thread, cluster_shared_rsrc_type_t type, shared_rsrc_policy_agent_t agent);
static inline thread_tag_t
thread_set_tag_internal(thread_t thread, thread_tag_t tag)
{
	return os_atomic_or_orig(&thread->thread_tag, tag, relaxed);
}

static inline thread_tag_t
thread_get_tag_internal(thread_t thread)
{
	return thread->thread_tag;
}

uint64_t        thread_last_run_time(thread_t thread);
extern kern_return_t    thread_state_initialize(
	thread_t                                thread);
extern kern_return_t    thread_setstatus(
	thread_t                                thread,
	int                                             flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  count);
extern kern_return_t    thread_setstatus_from_user(
	thread_t                                thread,
	int                                             flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  count,
	thread_state_t                  old_tstate,
	mach_msg_type_number_t  old_count,
	thread_set_status_flags_t flags);
extern kern_return_t    thread_getstatus(
	thread_t                                thread,
	int                                             flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  *count);
extern void main_thread_set_immovable_pinned(thread_t thread);
extern kern_return_t    thread_getstatus_to_user(
	thread_t                                thread,
	int                                             flavor,
	thread_state_t                  tstate,
	mach_msg_type_number_t  *count,
	thread_set_status_flags_t flags);
extern kern_return_t    thread_create_with_continuation(
	task_t task,
	thread_t *new_thread,
	thread_continue_t continuation);
extern kern_return_t main_thread_create_waiting(task_t    task,
    thread_continue_t              continuation,
    event_t                        event,
    thread_t                       *new_thread);
extern kern_return_t    thread_create_workq_waiting(
	task_t                  task,
	thread_continue_t       thread_return,
	thread_t                *new_thread,
	bool                    is_permanently_bound);
extern  void    thread_yield_internal(
	mach_msg_timeout_t      interval);
extern void thread_yield_to_preemption(void);
extern void thread_depress_timer_setup(thread_t self);
extern int thread_get_cpulimit(int *action, uint8_t *percentage, uint64_t *interval_ns);
extern int thread_set_cpulimit(int action, uint8_t percentage, uint64_t interval_ns);
extern uint64_t thread_cpulimit_remaining(uint64_t now);
extern bool thread_cpulimit_interval_has_expired(uint64_t now);
extern void thread_cpulimit_restart(uint64_t now);
extern void thread_read_times(
	thread_t         thread,
	time_value_t    *user_time,
	time_value_t    *system_time,
	time_value_t    *runnable_time);
extern void thread_read_times_unsafe(
	thread_t         thread,
	time_value_t    *user_time,
	time_value_t    *system_time,
	time_value_t    *runnable_time);
extern uint64_t         thread_get_runtime_self(void);
extern void                     thread_setuserstack(
	thread_t                thread,
	mach_vm_offset_t        user_stack);
extern user_addr_t         thread_adjuserstack(
	thread_t                thread,
	int                             adjust);
extern void                     thread_setentrypoint(
	thread_t                thread,
	mach_vm_offset_t        entry);
extern kern_return_t    thread_set_tsd_base(
	thread_t        thread,
	mach_vm_offset_t tsd_base);
extern kern_return_t    thread_setsinglestep(
	thread_t                thread,
	int                     on);
extern kern_return_t    thread_userstack(
	thread_t,
	int,
	thread_state_t,
	unsigned int,
	mach_vm_offset_t *,
	int *,
	boolean_t);
extern kern_return_t    thread_entrypoint(
	thread_t,
	int,
	thread_state_t,
	unsigned int,
	mach_vm_offset_t *);
extern kern_return_t    thread_userstackdefault(
	mach_vm_offset_t *,
	boolean_t);
extern kern_return_t    thread_wire_internal(
	host_priv_t             host_priv,
	thread_t                thread,
	boolean_t               wired,
	boolean_t               *prev_state);
extern kern_return_t    thread_dup(thread_t);
extern kern_return_t thread_dup2(thread_t, thread_t);
extern void             thread_sched_call(
	thread_t                thread,
	sched_call_t    call);
extern boolean_t        thread_is_static_param(
	thread_t                thread);
extern task_t   get_threadtask(thread_t) __pure2;
extern task_t   get_threadtask_early(thread_t) __pure2;
extern void             set_thread_pagein_error(thread_t, int);
extern event_t          workq_thread_init_and_wq_lock(task_t, thread_t);
extern thread_ro_t      get_thread_ro_unchecked(thread_t) __pure2;
extern thread_ro_t      get_thread_ro(thread_t) __pure2;
extern thread_ro_t      current_thread_ro_unchecked(void) __pure2;
extern thread_ro_t      current_thread_ro(void) __pure2;
extern void             clear_thread_ro_proc(thread_t);
extern struct uthread  *get_bsdthread_info(thread_t) __pure2;
extern thread_t         get_machthread(struct uthread *) __pure2;
extern uint64_t         uthread_tid(struct uthread *) __pure2;
extern user_addr_t      thread_get_sigreturn_token(thread_t thread);
extern uint32_t         thread_get_sigreturn_diversifier(thread_t thread);
extern void             uthread_init(task_t, struct uthread *, thread_ro_t, int);
extern void             uthread_cleanup_name(struct uthread *uthread);
extern void             uthread_cleanup(struct uthread *, thread_ro_t);
extern void             uthread_cred_ref(struct ucred *);
extern void             uthread_cred_free(struct ucred *);
extern void             uthread_destroy(struct uthread *);
extern void             uthread_reset_proc_refcount(struct uthread *);
extern void             uthread_set_exec_data(struct uthread *uth, struct image_params *imgp);
extern bool             uthread_is64bit(struct uthread *uth) __pure2;
extern mach_port_name_t  uthread_joiner_port(struct uthread *);
extern user_addr_t       uthread_joiner_address(struct uthread *);
extern void              uthread_joiner_wake(task_t task, struct uthread *);
extern boolean_t        thread_should_halt(
	thread_t                thread);
extern boolean_t        thread_should_abort(
	thread_t);
extern bool current_thread_in_kernel_fault(void);
extern int is_64signalregset(void);
extern void act_set_kperf(thread_t);
extern void act_set_astledger(thread_t thread);
extern void act_set_astledger_async(thread_t thread);
extern void act_set_io_telemetry_ast(thread_t);
extern void act_set_macf_telemetry_ast(thread_t);
extern void act_set_astproc_resource(thread_t);
extern vm_offset_t thread_get_kernel_stack(thread_t);
extern kern_return_t thread_process_signature(thread_t thread, task_t task);
extern uint32_t dtrace_get_thread_predcache(thread_t);
extern int64_t dtrace_get_thread_vtime(thread_t);
extern int64_t dtrace_get_thread_tracing(thread_t);
extern uint16_t dtrace_get_thread_inprobe(thread_t);
extern int dtrace_get_thread_last_cpu_id(thread_t);
extern vm_offset_t dtrace_get_kernel_stack(thread_t);
extern void dtrace_set_thread_predcache(thread_t, uint32_t);
extern void dtrace_set_thread_vtime(thread_t, int64_t);
extern void dtrace_set_thread_tracing(thread_t, int64_t);
extern void dtrace_set_thread_inprobe(thread_t, uint16_t);
extern void dtrace_thread_bootstrap(void);
extern void dtrace_thread_didexec(thread_t);
extern int64_t dtrace_calc_thread_recent_vtime(thread_t);
extern kern_return_t    thread_set_wq_state32(
	thread_t          thread,
	thread_state_t    tstate);
extern kern_return_t    thread_set_wq_state64(
	thread_t          thread,
	thread_state_t    tstate);
extern void mach_exception_ast(thread_t);
extern void fd_guard_ast(thread_t,
    mach_exception_code_t, mach_exception_subcode_t);
extern void vn_guard_ast(thread_t,
    mach_exception_code_t, mach_exception_subcode_t);
extern void mach_port_guard_ast(thread_t,
    mach_exception_code_t, mach_exception_subcode_t);
extern void virt_memory_guard_ast(thread_t,
    mach_exception_code_t, mach_exception_subcode_t);
extern void thread_ast_mach_exception(thread_t,
    int, exception_type_t, mach_exception_code_t, mach_exception_subcode_t, bool, bool);
extern void thread_guard_violation(thread_t,
    mach_exception_code_t, mach_exception_subcode_t, bool);
extern void thread_update_io_stats(thread_t, int size, int io_flags);
extern kern_return_t    thread_set_voucher_name(mach_port_name_t name);
extern kern_return_t thread_get_voucher_origin_pid(thread_t thread, int32_t *pid);
extern kern_return_t thread_get_voucher_origin_proximate_pid(thread_t thread,
    int32_t *origin_pid, int32_t *proximate_pid);
extern kern_return_t thread_get_current_voucher_origin_pid(int32_t *pid);
extern void thread_enable_send_importance(thread_t thread, boolean_t enable);
extern kern_return_t    machine_thread_siguctx_pointer_convert_to_user(
	thread_t thread,
	user_addr_t *uctxp);
extern void machine_tecs(thread_t thr);
extern int machine_csv(cpuvn_e cve);
extern kern_return_t    machine_thread_function_pointers_convert_from_user(
	thread_t thread,
	user_addr_t *fptrs,
	uint32_t count);
uint64_t thread_get_last_wait_duration(thread_t thread);
extern thread_t ctid_get_thread(ctid_t ctid);
extern thread_t ctid_get_thread_unsafe(ctid_t ctid);
extern ctid_t thread_get_ctid(thread_t thread);
extern void thread_floor_boost_set_promotion_locked(thread_t thread);
extern thread_pri_floor_t thread_priority_floor_start(void);
extern void thread_priority_floor_end(thread_pri_floor_t *token);
extern void thread_mtx_lock(thread_t thread);
extern void thread_mtx_unlock(thread_t thread);
extern uint64_t thread_dispatchqaddr(
	thread_t thread);
bool thread_is_eager_preempt(thread_t thread);
void thread_set_eager_preempt(thread_t thread);
void thread_clear_eager_preempt(thread_t thread);
void thread_set_honor_qlimit(thread_t thread);
void thread_clear_honor_qlimit(thread_t thread);
extern ipc_port_t convert_thread_to_port(thread_t);
extern ipc_port_t convert_thread_to_port_pinned(thread_t);
extern ipc_port_t convert_thread_inspect_to_port(thread_inspect_t);
extern ipc_port_t convert_thread_read_to_port(thread_read_t);
extern void       convert_thread_array_to_ports(thread_act_array_t, size_t, mach_thread_flavor_t);
extern boolean_t is_external_pageout_thread(void);
extern boolean_t is_vm_privileged(void);
extern boolean_t set_vm_privilege(boolean_t);
extern kern_allocation_name_t thread_set_allocation_name(kern_allocation_name_t new_name);
extern void *thread_iokit_tls_get(uint32_t index);
extern void thread_iokit_tls_set(uint32_t index, void * data);
extern int thread_self_region_page_shift(void);
extern void thread_self_region_page_shift_set(int pgshift);
extern kern_return_t thread_create_immovable(task_t task, thread_t *new_thread);
extern kern_return_t thread_terminate_pinned(thread_t thread);
extern kern_return_t thread_get_ipc_propagate_attr(thread_t thread, struct thread_attr_for_ipc_propagation *attr);
extern size_t thread_get_current_exec_path(char *path, size_t size);
extern void
thread_get_thread_name(thread_t th, char* name);
extern processor_t thread_get_runq(thread_t thread);
extern processor_t thread_get_runq_locked(thread_t thread);
extern void thread_set_runq_locked(thread_t thread, processor_t new_runq);
extern void thread_clear_runq(thread_t thread);
extern void thread_clear_runq_locked(thread_t thread);
extern void thread_assert_runq_null(thread_t thread);
extern void thread_assert_runq_nonnull(thread_t thread);
extern bool thread_supports_cooperative_workqueue(thread_t thread);
extern void thread_arm_workqueue_quantum(thread_t thread);
extern void thread_disarm_workqueue_quantum(thread_t thread);
extern void thread_evaluate_workqueue_quantum_expiry(thread_t thread);
extern bool thread_has_expired_workqueue_quantum(thread_t thread, bool should_trace);
extern kern_return_t thread_get_exception_ports(
	thread_t                        thread,
	exception_mask_t                exception_mask,
	exception_mask_array_t          masks,
	mach_msg_type_number_t          *CountCnt,
	exception_port_array_t          ports,
	exception_behavior_array_t      behaviors,
	thread_state_flavor_array_t     flavors);
extern kern_return_t thread_get_special_port(
	thread_inspect_t         thread,
	int                      which,
	ipc_port_t              *portp);
extern boolean_t thread_has_thread_name(thread_t th);
extern void thread_set_thread_name(thread_t th, const char* name);
extern uint64_t thread_tid(thread_t thread) __pure2;
extern void thread_reference(
	thread_t        thread);
extern void thread_deallocate(
	thread_t        thread);
extern kern_return_t    kernel_thread_start(
	thread_continue_t       continuation,
	void                    *parameter,
	thread_t                *new_thread);
__BEGIN_DECLS


extern boolean_t        thread_call_enter(
	thread_call_t           call);
extern boolean_t        thread_call_enter1(
	thread_call_t           call,
	thread_call_param_t     param1);
extern boolean_t        thread_call_enter_delayed(
	thread_call_t           call,
	uint64_t                deadline);
extern boolean_t        thread_call_enter1_delayed(
	thread_call_t           call,
	thread_call_param_t     param1,
	uint64_t                deadline);
extern boolean_t        thread_call_enter_delayed_with_leeway(
	thread_call_t           call,
	thread_call_param_t     param1,
	uint64_t                deadline,
	uint64_t                leeway,
	uint32_t                flags);
extern boolean_t        thread_call_cancel(
	thread_call_t           call);
extern boolean_t        thread_call_cancel_wait(
	thread_call_t           call);
extern thread_call_t    thread_call_allocate(
	thread_call_func_t      func,
	thread_call_param_t     param0);
extern thread_call_t    thread_call_allocate_with_priority(
	thread_call_func_t      func,
	thread_call_param_t     param0,
	thread_call_priority_t  pri);
extern thread_call_t    thread_call_allocate_with_options(
	thread_call_func_t      func,
	thread_call_param_t     param0,
	thread_call_priority_t  pri,
	thread_call_options_t   options);
extern thread_call_t
thread_call_allocate_with_qos(thread_call_func_t        func,
    thread_call_param_t       param0,
    int                       qos_tier,
    thread_call_options_t     options);
extern boolean_t
thread_call_wait_once(thread_call_t call);
extern boolean_t        thread_call_free(
	thread_call_t           call);
boolean_t               thread_call_isactive(
	thread_call_t call);
__END_DECLS



__enum_closed_decl(thread_call_index_t, uint16_t, {
	THREAD_CALL_INDEX_INVALID       = 0,    
	THREAD_CALL_INDEX_HIGH          = 1,
	THREAD_CALL_INDEX_KERNEL        = 2,
	THREAD_CALL_INDEX_USER          = 3,
	THREAD_CALL_INDEX_LOW           = 4,
	THREAD_CALL_INDEX_KERNEL_HIGH   = 5,
	THREAD_CALL_INDEX_QOS_UI        = 6,
	THREAD_CALL_INDEX_QOS_IN        = 7,
	THREAD_CALL_INDEX_QOS_UT        = 8,
	THREAD_CALL_INDEX_MAX           = 9,    
});
__options_closed_decl(thread_call_flags_t, uint16_t, {
	THREAD_CALL_ALLOC               = 0x0001,       
	THREAD_CALL_WAIT                = 0x0002,       
	THREAD_CALL_DELAYED             = 0x0004,       
	THREAD_CALL_RUNNING             = 0x0008,       
	THREAD_CALL_SIGNAL              = 0x0010,       
	THREAD_CALL_ONCE                = 0x0020,       
	THREAD_CALL_RESCHEDULE          = 0x0040,       
	THREAD_CALL_RATELIMITED         = 0x0080,       
	THREAD_CALL_FLAG_CONTINUOUS     = 0x0100,       
	THREAD_CALL_INITIALIZED         = 0x0200,       
});
extern void             thread_call_setup(
	thread_call_t                   call,
	thread_call_func_t              func,
	thread_call_param_t             param0);
extern void             thread_call_setup_with_options(
	thread_call_t                   call,
	thread_call_func_t              func,
	thread_call_param_t             param0,
	thread_call_priority_t          pri,
	thread_call_options_t           options);
extern void             thread_call_delayed_timer_rescan_all(void);
extern uint64_t         thread_call_get_armed_deadline(thread_call_t call);
__BEGIN_DECLS



extern void             thread_call_func_delayed(
	thread_call_func_t              func,
	thread_call_param_t             param,
	uint64_t                        deadline);
extern void             thread_call_func_delayed_with_leeway(
	thread_call_func_t              func,
	thread_call_param_t             param,
	uint64_t                deadline,
	uint64_t                leeway,
	uint32_t                flags);
extern boolean_t        thread_call_func_cancel(
	thread_call_func_t      func,
	thread_call_param_t     param,
	boolean_t               cancel_all);
extern void adjust_cont_time_thread_calls(void);
extern void thread_call_start_iotes_invocation(thread_call_t call);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)



typedef union hw_lck_ticket_s {
	struct {
		uint8_t         lck_type;
		uint8_t         lck_valid  : 1;
		uint8_t         lck_is_pv  : 1;
		uint8_t         lck_unused : 6;
		union {
			struct {
				uint8_t cticket;
				uint8_t nticket;
			};
			uint16_t tcurnext;
		};
	};
	uint32_t lck_value;
} hw_lck_ticket_t;
extern void hw_lck_ticket_init_locked(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp);
extern void hw_lck_ticket_destroy(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp);
extern void hw_lck_ticket_invalidate(
	hw_lck_ticket_t        *tlock);
extern bool hw_lck_ticket_held(
	hw_lck_ticket_t        *tlock) __result_use_check;
extern void hw_lck_ticket_lock(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp);
extern void hw_lck_ticket_lock_nopreempt(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp);
extern hw_lock_status_t hw_lck_ticket_lock_to(
	hw_lck_ticket_t        *tlock,
	hw_spin_policy_t        policy,
	lck_grp_t              *grp);
extern hw_lock_status_t hw_lck_ticket_lock_nopreempt_to(
	hw_lck_ticket_t        *tlock,
	hw_spin_policy_t        policy,
	lck_grp_t              *grp);
extern bool hw_lck_ticket_lock_try(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp) __result_use_check;
extern bool hw_lck_ticket_lock_try_nopreempt(
	hw_lck_ticket_t        *tlock,
	lck_grp_t              *grp) __result_use_check;
extern void hw_lck_ticket_unlock(
	hw_lck_ticket_t        *tlock);
extern void hw_lck_ticket_unlock_nopreempt(
	hw_lck_ticket_t        *tlock);
extern bool hw_lck_ticket_reserve(
	hw_lck_ticket_t        *tlock,
	uint32_t               *ticket,
	lck_grp_t              *grp) __result_use_check;
extern bool hw_lck_ticket_reserve_nopreempt(
	hw_lck_ticket_t        *tlock,
	uint32_t               *ticket,
	lck_grp_t              *grp) __result_use_check;
extern hw_lock_status_t hw_lck_ticket_reserve_allow_invalid(
	hw_lck_ticket_t        *tlock,
	uint32_t               *ticket,
	lck_grp_t              *grp) __result_use_check;
extern hw_lock_status_t hw_lck_ticket_wait(
	hw_lck_ticket_t        *tlock,
	uint32_t                ticket,
	hw_spin_policy_t        policy,
	lck_grp_t             *grp);
extern hw_lock_status_t hw_lck_ticket_lock_allow_invalid(
	hw_lck_ticket_t        *tlock,
	hw_spin_policy_t        policy,
	lck_grp_t              *grp);
extern void hw_lck_ticket_unlock_kick_pv(
	hw_lck_ticket_t        *tlock,
	uint8_t                 value);
extern void hw_lck_ticket_lock_wait_pv(
	hw_lck_ticket_t         *tlock,
	uint8_t                  value);
extern bool kdp_lck_ticket_is_acquired(
	lck_ticket_t            *tlock) __result_use_check;
extern void lck_ticket_lock_nopreempt(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp);
extern bool lck_ticket_lock_try(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp) __result_use_check;
extern bool lck_ticket_lock_try_nopreempt(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp) __result_use_check;
extern void lck_ticket_unlock_nopreempt(
	lck_ticket_t            *tlock);
extern __exported void lck_ticket_init(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp);
extern __exported void lck_ticket_destroy(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp);
extern __exported void lck_ticket_lock(
	lck_ticket_t            *tlock,
	lck_grp_t               *grp);
extern __exported void lck_ticket_unlock(
	lck_ticket_t            *tlock);
extern __exported void lck_ticket_assert_owned(
	const lck_ticket_t            *tlock);
extern __exported void lck_ticket_assert_not_owned(
	const lck_ticket_t            *tlock);
void timer_init(timer_t timer);
void timer_start(timer_t timer, uint64_t tstamp);
void timer_stop(timer_t timer, uint64_t tstamp);
void timer_update(timer_t timer, uint64_t tstamp);
extern void timer_call_queue_init(mpqueue_head_t *);
extern boolean_t        timer_call_enter1(
	timer_call_t            call,
	timer_call_param_t      param1,
	uint64_t                deadline,
	uint32_t                flags);
extern boolean_t        timer_call_enter_with_leeway(
	timer_call_t            call,
	timer_call_param_t      param1,
	uint64_t                deadline,
	uint64_t                leeway,
	uint32_t                flags,
	boolean_t               ratelimited);
extern boolean_t        timer_call_cancel(
	timer_call_t    call);
extern timer_call_t     timer_call_alloc(
	timer_call_func_t       func,
	timer_call_param_t      param0);
extern void             timer_call_free(
	timer_call_t            call);
extern void             timer_call_setup(
	timer_call_t            call,
	timer_call_func_t       func,
	timer_call_param_t      param0);
extern int timer_get_user_idle_level(void);
extern kern_return_t timer_set_user_idle_level(int ilevel);
extern void timer_call_init(void);
uint64_t timer_call_past_deadline_timer_handle(uint64_t deadline,
    uint64_t ctime);
uint64_t running_timers_deadline(processor_t processor);
bool running_timers_expire(processor_t processor, uint64_t now);
void running_timer_setup(processor_t processor, enum running_timer timer,
    void *param, uint64_t deadline, uint64_t now);
void running_timers_sync(void);
void running_timer_enter(processor_t processor, enum running_timer timer,
    void *param, uint64_t deadline, uint64_t now);
void running_timer_clear(processor_t processor, enum running_timer timer);
void running_timer_cancel(processor_t processor, enum running_timer timer);
void running_timers_activate(processor_t processor);
void running_timers_deactivate(processor_t processor);
extern mpqueue_head_t * timer_queue_assign(
	uint64_t                deadline);
extern uint64_t         timer_call_slop(
	uint64_t                deadline,
	uint64_t                armtime,
	uint32_t                urgency,
	thread_t                arming_thread,
	boolean_t               *rlimited);
extern boolean_t        timer_resort_threshold(uint64_t);
extern void             timer_queue_cancel(
	mpqueue_head_t          *queue,
	uint64_t                deadline,
	uint64_t                new_deadline);
extern mpqueue_head_t * timer_queue_cpu(
	int                     cpu);
extern void             timer_call_cpu(
	int                     cpu,
	void                    (*fn)(void *),
	void                    *arg);
extern void             timer_call_nosync_cpu(
	int                     cpu,
	void                    (*fn)(void *),
	void                    *arg);
extern uint64_t         timer_call_slop(
	uint64_t                deadline,
	uint64_t                armtime,
	uint32_t                urgency,
	thread_t                arming_thread,
	boolean_t               *rlimited);
extern uint64_t         timer_queue_expire(
	mpqueue_head_t          *queue,
	uint64_t                deadline);
extern uint64_t         timer_queue_expire_with_options(
	mpqueue_head_t *,
	uint64_t,
	boolean_t);
extern void             timer_queue_shutdown(int target_cpu,
    mpqueue_head_t          *queue,
    mpqueue_head_t          *new_queue);
extern int              timer_queue_migrate(
	mpqueue_head_t          *from,
	mpqueue_head_t          *to);
extern void             timer_intr(int inuser, uint64_t iaddr);
extern void             timer_resync_deadlines(void);
extern void             timer_queue_expire_local(void *arg);
extern void             timer_set_deadline(uint64_t deadline);
extern uint32_t         timer_queue_migrate_cpu(int target_cpu);
extern void             timer_queue_trace(
	mpqueue_head_t          *queue);
extern void             timer_queue_trace_cpu(int cpu);
extern uint64_t         timer_sysctl_get(int oid);
extern kern_return_t    timer_sysctl_set(int oid, uint64_t value);
__BEGIN_DECLS
__enum_decl(trap_telemetry_ca_event_t, uint8_t, {
	
	TRAP_TELEMETRY_CA_EVENT_NONE = 0,

	TRAP_TELEMETRY_CA_EVENT_KERNEL_BRK = 1,

	TRAP_TELEMETRY_CA_EVENT_INTERNAL = 2,

	
	TRAP_TELEMETRY_CA_EVENT_COUNT,
});
extern bool
trap_telemetry_report_exception(
	trap_telemetry_type_t trap_type,
	uint64_t trap_code,
	trap_telemetry_options_s options,
	void *saved_state);
extern bool
trap_telemetry_report_simulated_trap(
	trap_telemetry_type_t trap_type,
	uint64_t trap_code,
	trap_telemetry_options_s options);
extern bool
trap_telemetry_report_simulated_trap_with_backtrace(
	trap_telemetry_type_t trap_type,
	uint64_t trap_code,
	trap_telemetry_options_s options,
	uintptr_t fault_pc,
	uintptr_t *frames,
	size_t frames_valid_count);
extern void
trap_telemetry_init(void);
extern struct kernel_brk_descriptor brk_descriptors_end[]
__SECTION_END_SYM("__DATA_CONST", "__brk_desc");
const static inline struct kernel_brk_descriptor *
find_brk_descriptor_by_comment(uint16_t comment)
{
	for (kernel_brk_descriptor_t des = brk_descriptors; des < brk_descriptors_end; des++) {
		if (comment >= des->base && comment <= des->max) {
			return des;
		}
	}

	return NULL;
}



__enum_decl(trap_telemetry_kernel_soft_error_code_t, uint64_t, {
	
	TRAP_TELEMETRY_KERNEL_SOFT_ERROR_VM_KERNEL_MAX_ALLOC_SIZE = 0,
});
__BEGIN_DECLS





typedef enum __attribute__((packed)) turnstile_type {
	TURNSTILE_NONE = 0,
	TURNSTILE_KERNEL_MUTEX = 1,
	TURNSTILE_ULOCK = 2,
	TURNSTILE_PTHREAD_MUTEX = 3,
	TURNSTILE_SYNC_IPC = 4,
	TURNSTILE_WORKLOOPS = 5,
	TURNSTILE_WORKQS = 6,
	TURNSTILE_KNOTE = 7,
	TURNSTILE_SLEEP_INHERITOR = 8,
	TURNSTILE_EPOCH_KERNEL = 9,
	TURNSTILE_EPOCH_USER = 10,
	TURNSTILE_TOTAL_TYPES = 11,
} turnstile_type_t;
#pragma GCC visibility push(hidden)


typedef enum __attribute__((flag_enum)) turnstile_stats_update_flags {
	TSU_FLAGS_NONE = 0,
	TSU_TURNSTILE_BLOCK_COUNT = 0x1,
	TSU_REGULAR_WAITQ_BLOCK_COUNT = 0x2,
	TSU_PRI_PROPAGATION = 0x4,
	TSU_NO_INHERITOR = 0x8,
	TSU_NO_TURNSTILE = 0x10,
	TSU_NO_PRI_CHANGE_NEEDED = 0x20,
	TSU_THREAD_RUNNABLE = 0x40,
	TSU_ABOVE_UI_PRI_CHANGE = 0x80,
	TSU_THREAD_ARG = 0x100,
	TSU_TURNSTILE_ARG = 0x200,
	TSU_BOOST_ARG = 0x400,
} turnstile_stats_update_flags_t;
SLIST_HEAD(turnstile_list, turnstile);
void
turnstiles_init(void);
uint32_t
turnstile_compact_id_get(void);
void
turnstile_compact_id_put(uint32_t cid);
void
turnstile_reference(struct turnstile *turnstile);
void
turnstile_deallocate(struct turnstile *turnstile);
void
turnstile_waitq_add_thread_priority_queue(
	struct waitq* wq,
	thread_t thread);
boolean_t
turnstile_recompute_priority_locked(
	struct turnstile *turnstile);
boolean_t
turnstile_recompute_priority(
	struct turnstile *turnstile);
int
turnstile_workq_proprietor_of_max_turnstile(
	struct turnstile *turnstile,
	uintptr_t *proprietor);
int
turnstile_workloop_pusher_info(
	struct turnstile *turnstile,
	thread_t *thread,
	ipc_port_t *port,
	struct knote **knote_out);
void
turnstile_cleanup(void);
void
turnstile_update_thread_priority_chain(thread_t thread);
void
turnstile_update_inheritor_locked(struct turnstile *turnstile);
int
thread_get_inheritor_turnstile_base_priority(thread_t thread);
int
thread_get_inheritor_turnstile_sched_priority(thread_t thread);
boolean_t
turnstile_has_waiters(struct turnstile *turnstile);
void
turnstile_stats_update(
	int hop __assert_only,
	turnstile_stats_update_flags_t flags __assert_only,
	turnstile_inheritor_t inheritor __assert_only);
#pragma GCC visibility pop




unsigned
turnstile_hash_bucket_lock(uintptr_t proprietor, uint32_t *index_proprietor, turnstile_type_t type);
void
turnstile_hash_bucket_unlock(uintptr_t proprietor, uint32_t *index_proprietor, turnstile_type_t type, unsigned s);
void
turnstile_complete(
	uintptr_t proprietor,
	struct turnstile **tstore,
	struct turnstile **turnstile,
	turnstile_type_t type);
bool
turnstile_complete_compact_id(
	uintptr_t proprietor,
	struct turnstile *turnstile,
	turnstile_type_t type);
void
turnstile_complete_hash(
	uintptr_t proprietor,
	turnstile_type_t type);
void
turnstile_update_inheritor(
	struct turnstile *turnstile,
	turnstile_inheritor_t new_inheritor,
	turnstile_update_flags_t flags);
void
turnstile_update_inheritor_complete(
	struct turnstile *turnstile,
	turnstile_update_complete_flags_t flags);
void
turnstile_kernel_update_inheritor_on_wake_locked(
	struct turnstile *turnstile,
	turnstile_inheritor_t new_inheritor,
	turnstile_update_flags_t flags);
extern void workq_reference(struct workqueue *wq);
extern void workq_deallocate_safe(struct workqueue *wq);
extern bool workq_is_current_thread_updating_turnstile(struct workqueue *wq);
extern void workq_schedule_creator_turnstile_redrive(struct workqueue *wq,
    bool locked);
extern void             ux_handler_init(void);
extern void             ux_handler_setup(void);
extern boolean_t        is_ux_handler_port(mach_port_t port);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma GCC visibility push(hidden)


__options_decl(waitq_wakeup_flags_t, uint32_t, {
	WAITQ_WAKEUP_DEFAULT    = 0x0000,
	WAITQ_UPDATE_INHERITOR  = 0x0001,
	WAITQ_PROMOTE_PRIORITY  = 0x0002,
	WAITQ_UNLOCK            = 0x0004,
	WAITQ_KEEP_LOCKED       = 0x0000,
	WAITQ_HANDOFF           = 0x0008,
	WAITQ_ENABLE_INTERRUPTS = 0x0010,
});
static inline waitq_type_t
waitq_type(waitq_t wq)
{
	return wq.wq_q->waitq_type;
}

static inline bool
waitq_same(waitq_t wq1, waitq_t wq2)
{
	return wq1.wq_q == wq2.wq_q;
}

static inline bool
waitq_is_null(waitq_t wq)
{
	return wq.wq_q == NULL;
}


extern bool waitq_wait_possible(thread_t thread);
static inline bool
waitq_preposts(waitq_t wq)
{
	switch (waitq_type(wq)) {
	case WQT_PORT:
	case WQT_SELECT:
		return true;
	default:
		return false;
	}
}

static inline bool
waitq_irq_safe(waitq_t waitq)
{
	switch (waitq_type(waitq)) {
	case WQT_QUEUE:
	case WQT_TURNSTILE:
		return true;
	default:
		return false;
	}
}

static inline bool
waitq_valid(waitq_t waitq)
{
	return waitq.wq_q && waitq.wq_q->waitq_interlock.lck_valid;
}


extern struct waitq *_global_eventq(char *event, size_t event_length);
static inline waitq_wakeup_flags_t
waitq_flags_splx(spl_t spl_level)
{
	return spl_level ? WAITQ_ENABLE_INTERRUPTS : WAITQ_WAKEUP_DEFAULT;
}

#pragma mark locking


extern void waitq_lock(waitq_t wq);
extern void waitq_unlock(waitq_t wq);
extern bool waitq_is_valid(waitq_t wq);
extern void waitq_invalidate(waitq_t wq);
extern bool waitq_held(waitq_t wq) __result_use_check;
extern bool waitq_lock_allow_invalid(waitq_t wq) __result_use_check;
extern bool waitq_lock_reserve(waitq_t wq, uint32_t *ticket) __result_use_check;
extern void waitq_lock_wait(waitq_t wq, uint32_t ticket);
extern bool waitq_lock_try(waitq_t wq) __result_use_check;
#pragma mark assert_wait / wakeup


extern wait_result_t waitq_assert_wait64(
	waitq_t                 waitq,
	event64_t               wait_event,
	wait_interrupt_t        interruptible,
	uint64_t                deadline);
extern wait_result_t waitq_assert_wait64_leeway(
	waitq_t                 waitq,
	event64_t               wait_event,
	wait_interrupt_t        interruptible,
	wait_timeout_urgency_t  urgency,
	uint64_t                deadline,
	uint64_t                leeway);
extern kern_return_t waitq_wakeup64_one(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern kern_return_t waitq_wakeup64_nthreads(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags,
	uint32_t                nthreads);
extern kern_return_t waitq_wakeup64_all(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern thread_t waitq_wakeup64_identify(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern kern_return_t waitq_wakeup64_thread(
	struct waitq           *waitq,
	event64_t               wake_event,
	thread_t                thread,
	wait_result_t           result);
#pragma mark Mach-only assert_wait / wakeup


extern void waitq_clear_promotion_locked(
	waitq_t                 waitq,
	thread_t                thread);
extern bool waitq_pull_thread_locked(
	waitq_t                 waitq,
	thread_t                thread);
extern wait_result_t waitq_assert_wait64_locked(
	waitq_t                 waitq,
	event64_t               wait_event,
	wait_interrupt_t        interruptible,
	wait_timeout_urgency_t  urgency,
	uint64_t                deadline,
	uint64_t                leeway,
	thread_t                thread);
extern kern_return_t waitq_wakeup64_all_locked(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern kern_return_t waitq_wakeup64_nthreads_locked(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags,
	uint32_t                nthreads);
extern kern_return_t waitq_wakeup64_one_locked(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern thread_t waitq_wakeup64_identify_locked(
	waitq_t                 waitq,
	event64_t               wake_event,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern void waitq_resume_identified_thread(
	waitq_t                 waitq,
	thread_t                thread,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern void waitq_resume_and_bind_identified_thread(
	waitq_t                 waitq,
	thread_t                thread,
	processor_t             processor,
	wait_result_t           result,
	waitq_wakeup_flags_t    flags);
extern kern_return_t waitq_wakeup64_thread_and_unlock(
	struct waitq           *waitq,
	event64_t               wake_event,
	thread_t                thread,
	wait_result_t           result);
#pragma mark waitq links


extern waitq_link_t waitq_link_alloc(
	waitq_type_t            type);
extern void waitq_link_free(
	waitq_type_t            type,
	waitq_link_t            link);
extern void waitq_link_free_list(
	waitq_type_t            type,
	waitq_link_list_t      *list);
#pragma mark wait queues lifecycle


extern void waitq_init(
	waitq_t                 waitq,
	waitq_type_t            type,
	int                     policy);
extern void waitq_deinit(
	waitq_t                 waitq);
#pragma mark port wait queues and port set waitq sets


extern kern_return_t waitq_link_locked(
	struct waitq           *waitq,
	struct waitq_set       *wqset,
	waitq_link_t           *link);
extern kern_return_t waitq_link_prepost_locked(
	struct waitq           *waitq,
	struct waitq_set       *wqset);
extern waitq_link_t waitq_unlink_locked(
	struct waitq           *waitq,
	struct waitq_set       *wqset);
extern void waitq_unlink_all_locked(
	struct waitq           *waitq,
	struct waitq_set       *except_wqset,
	waitq_link_list_t      *free_l);
extern void waitq_set_unlink_all_locked(
	struct waitq_set       *wqset,
	waitq_link_list_t      *free_l);
extern void waitq_set_foreach_member_locked(
	struct waitq_set       *wqset,
	void                  (^cb)(struct waitq *));
__options_decl(wqs_prepost_flags_t, uint32_t, {
	WQS_PREPOST_PEEK = 0x1,
	WQS_PREPOST_LOCK = 0x2,
});
extern void waitq_clear_prepost_locked(
	struct waitq           *waitq);
extern void ipc_pset_prepost(
	struct waitq_set       *wqset,
	struct waitq           *waitq);
extern struct select_set *select_set_alloc(void);
extern void select_set_free(
	struct select_set      *selset);
extern void select_set_link(
	struct waitq           *waitq,
	struct select_set      *selset,
	waitq_link_t           *linkp);
extern void select_set_reset(
	struct select_set      *selset);
extern void select_waitq_wakeup_and_deinit(
	struct waitq           *waitq,
	event64_t               wake_event,
	wait_result_t           result);
extern kern_return_t workload_config_init(workload_config_ctx_t *ctx);
extern bool workload_config_initialized(const workload_config_ctx_t *ctx);
extern void workload_config_free(workload_config_ctx_t *ctx);
extern kern_return_t workload_config_insert(workload_config_ctx_t *ctx,
    const char *id, const char *phase, const workload_config_t *config);
extern kern_return_t workload_config_set_default(workload_config_ctx_t *ctx,
    const char *id, const char *phase);
extern kern_return_t workload_config_lookup(const char *id, const char *phase,
    workload_config_t *config);
extern kern_return_t workload_config_lookup_default(const char *id,
    workload_config_t *config);
extern void workload_config_iterate(bool (^cb)(const char *id,
    const void *phases));
extern void workload_config_phases_iterate(const void *phases,
    bool (^cb)(const char *phase, const bool is_default,
    const workload_config_t *config));
extern kern_return_t workload_config_get_flags(workload_config_flags_t *flags);
extern kern_return_t workload_config_clear_flag(workload_config_ctx_t *ctx,
    workload_config_flags_t flag);
extern bool workload_config_available(void);
extern kern_return_t
kern_work_interval_create(thread_t thread, struct kern_work_interval_create_args *create_params);
extern kern_return_t
kern_work_interval_get_flags_from_port(mach_port_name_t port_name, uint32_t*flags);
extern kern_return_t
kern_port_name_to_work_interval(mach_port_name_t name,
    struct work_interval **work_interval);
extern kern_return_t
kern_work_interval_get_policy(struct work_interval *work_interval,
    integer_t *policy,
    integer_t *priority);
extern void
kern_work_interval_release(struct work_interval *work_interval);
extern kern_return_t
kern_work_interval_explicit_join(thread_t thread, struct work_interval *work_interval);
extern kern_return_t
kern_work_interval_destroy(thread_t thread, uint64_t work_interval_id);
extern kern_return_t
kern_work_interval_join(thread_t thread, mach_port_name_t port_name);
extern kern_return_t
kern_work_interval_notify(thread_t thread, struct kern_work_interval_args* kwi_args);
extern kern_return_t
kern_work_interval_set_name(mach_port_name_t port_name, char *name, size_t len);
extern kern_return_t
kern_work_interval_set_workload_id(mach_port_name_t port_name,
    struct kern_work_interval_workload_id_args *workload_id_args);
bool work_interval_port_type_render_server(mach_port_name_t port_name);
extern kern_return_t work_interval_thread_terminate(thread_t thread);
extern int work_interval_get_priority(thread_t thread);
__enum_closed_decl(wi_class_t, uint8_t, {
	WI_CLASS_NONE              = 0,
	WI_CLASS_DISCRETIONARY     = 1,
	WI_CLASS_BEST_EFFORT       = 2,
	WI_CLASS_APPLICATION       = 3,
	WI_CLASS_SYSTEM            = 4,
	WI_CLASS_SYSTEM_CRITICAL   = 5,
	WI_CLASS_REALTIME          = 6,
	WI_CLASS_REALTIME_CRITICAL = 7,
	WI_CLASS_APP_SUPPORT       = 8,

	WI_CLASS_COUNT
});
__options_decl(zone_create_flags_t, uint64_t, {
	
	ZC_NONE                 = 0x00000000,

	
	ZC_SEQUESTER            = 0x00000001,
	
	ZC_NOSEQUESTER          = 0x00000002,

	
	ZC_CACHING              = 0x00000010,
	
	ZC_NOCACHING            = 0x00000020,

	
	ZC_READONLY             = 0x00800000,

	
	ZC_PERCPU               = 0x01000000,

	
	ZC_ZFREE_CLEARMEM       = 0x02000000,

	
	ZC_NOGC                 = 0x04000000,

	
	ZC_NOENCRYPT            = 0x08000000,

	
	ZC_ALIGNMENT_REQUIRED   = 0x10000000,

	
	ZC_NOGZALLOC            = 0x20000000,

	
	ZC_NOCALLOUT            = 0x40000000,

	
	ZC_DESTRUCTIBLE         = 0x80000000,

	
	ZC_SHARED_DATA          = 0x0040000000000000,

	
	ZC_OBJ_CACHE            = 0x0080000000000000,

	
	ZC_PGZ_USE_GUARDS       = 0x0100000000000000,

	
	ZC_NO_TBI_TAG             = 0x0200000000000000,

	
	ZC_KALLOC_TYPE          = 0x0400000000000000,

	
	ZC_NOPGZ                = 0x0800000000000000,

	
	ZC_DATA                 = 0x1000000000000000,

	
	ZC_VM                   = 0x2000000000000000,

	
	ZC_KASAN_NOQUARANTINE   = 0x4000000000000000,

	
	ZC_KASAN_NOREDZONE      = 0x8000000000000000,
});
__enum_decl(zone_create_ro_id_t, zone_id_t, {
	ZC_RO_ID_SANDBOX,
	ZC_RO_ID_PROFILE,
	ZC_RO_ID_PROTOBOX,
	ZC_RO_ID_SB_FILTER,
	ZC_RO_ID_AMFI_OSENTITLEMENTS,
	ZC_RO_ID__LAST = ZC_RO_ID_AMFI_OSENTITLEMENTS,
});
extern zone_t   zone_create(
	const char             *name __unsafe_indexable,
	vm_size_t               size,
	zone_create_flags_t     flags);
extern vm_size_t    zone_get_elem_size(zone_t zone);
extern zone_id_t   zone_create_ro(
	const char             *name __unsafe_indexable,
	vm_size_t               size,
	zone_create_flags_t     flags,
	zone_create_ro_id_t     zc_ro_id);
extern void     zdestroy(
	zone_t          zone);
extern void     zone_require(
	zone_t          zone,
	void           *addr __unsafe_indexable);
extern void     zone_require_ro(
	zone_id_t       zone_id,
	vm_size_t       elem_size,
	void           *addr __unsafe_indexable);
__options_decl(zalloc_flags_t, uint32_t, {
	
	Z_WAITOK        = 0x0000,
	Z_NOWAIT        = 0x0001,
	Z_NOPAGEWAIT    = 0x0002,
	Z_ZERO          = 0x0004,
	Z_REALLOCF      = 0x0008,

	Z_SET_NOTEARLY = 0x0040,
	Z_SPRAYQTN      = 0x0080,
	Z_KALLOC_ARRAY  = 0x0100,
	Z_MAY_COPYINMAP = 0x0800,
	Z_VM_TAG_BT_BIT = 0x1000,
	Z_PCPU          = 0x2000,
	Z_NOZZC         = 0x4000,
	Z_NOFAIL        = 0x8000,

	
	Z_NOWAIT_ZERO          = Z_NOWAIT | Z_ZERO,
	Z_WAITOK_ZERO          = Z_WAITOK | Z_ZERO,
	Z_WAITOK_ZERO_NOFAIL   = Z_WAITOK | Z_ZERO | Z_NOFAIL,
	Z_WAITOK_ZERO_SPRAYQTN = Z_WAITOK | Z_ZERO | Z_SPRAYQTN,

	Z_KPI_MASK             = Z_WAITOK | Z_NOWAIT | Z_NOPAGEWAIT | Z_ZERO,
	Z_ZERO_VM_TAG_BT_BIT   = Z_ZERO | Z_VM_TAG_BT_BIT,
	
	Z_VM_TAG_MASK   = 0xffff0000,

});
extern void *__unsafe_indexable kalloc_type_impl_internal(
	kalloc_type_view_t  kt_view,
	zalloc_flags_t      flags);
extern void kfree_type_impl_internal(
	kalloc_type_view_t kt_view,
	void               *ptr __unsafe_indexable);
static inline void *__unsafe_indexable
kalloc_type_impl(
	kalloc_type_view_t      kt_view,
	zalloc_flags_t          flags)
{
	void *__unsafe_indexable addr = kalloc_type_impl_internal(kt_view, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}




__attribute__((malloc))
extern void *__unsafe_indexable zalloc(
	zone_t          zone);
__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
zalloc(zone_view_t view)
{
	return zalloc((zone_t)view);
}

__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
zalloc(kalloc_type_view_t kt_view)
{
	return (kalloc_type_impl)(kt_view, Z_WAITOK);
}


__attribute__((malloc))
extern void *__unsafe_indexable zalloc_noblock(
	zone_t          zone);
__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
zalloc_noblock(zone_view_t view)
{
	return zalloc_noblock((zone_t)view);
}

__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
zalloc_noblock(kalloc_type_view_t kt_view)
{
	return (kalloc_type_impl)(kt_view, Z_NOWAIT);
}


__attribute__((malloc))
extern void *__unsafe_indexable zalloc_flags(
	zone_t          zone,
	zalloc_flags_t  flags);
__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
__zalloc_flags(
	zone_t          zone,
	zalloc_flags_t  flags)
{
	void *__unsafe_indexable addr = (zalloc_flags)(zone, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}

__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
__zalloc_flags(
	zone_view_t     view,
	zalloc_flags_t  flags)
{
	return __zalloc_flags((zone_t)view, flags);
}

__attribute__((malloc))
__attribute__((overloadable))
static inline void *__unsafe_indexable
__zalloc_flags(
	kalloc_type_view_t  kt_view,
	zalloc_flags_t      flags)
{
	void *__unsafe_indexable addr = (kalloc_type_impl)(kt_view, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}

__attribute__((malloc))
static inline void *__header_indexable
zalloc_flags_buf(
	zone_t          zone,
	zalloc_flags_t  flags)
{
	void *__unsafe_indexable addr = __zalloc_flags(zone, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return __unsafe_forge_bidi_indexable(void *, addr, zone_get_elem_size(zone));
}



__attribute__((malloc))
extern void *__unsafe_indexable zalloc_id(
	zone_id_t       zid,
	zalloc_flags_t  flags);
__attribute__((malloc))
static inline void *__unsafe_indexable
__zalloc_id(
	zone_id_t       zid,
	zalloc_flags_t  flags)
{
	void *__unsafe_indexable addr = (zalloc_id)(zid, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}



__attribute__((malloc))
extern void *__unsafe_indexable zalloc_ro(
	zone_id_t       zone_id,
	zalloc_flags_t  flags);
__attribute__((malloc))
static inline void *__unsafe_indexable
__zalloc_ro(
	zone_id_t       zone_id,
	zalloc_flags_t  flags)
{
	void *__unsafe_indexable addr = (zalloc_ro)(zone_id, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}



extern void zalloc_ro_mut(
	zone_id_t       zone_id,
	void           *elem __unsafe_indexable,
	vm_offset_t     offset,
	const void     *new_data __sized_by(new_data_size),
	vm_size_t       new_data_size);
__enum_decl(zro_atomic_op_t, uint32_t, {
	ZRO_ATOMIC_OR_8      = 0x00000010 | 1,
	ZRO_ATOMIC_OR_16     = 0x00000010 | 2,
	ZRO_ATOMIC_OR_32     = 0x00000010 | 4,
	ZRO_ATOMIC_OR_64     = 0x00000010 | 8,

	ZRO_ATOMIC_XOR_8     = 0x00000020 | 1,
	ZRO_ATOMIC_XOR_16    = 0x00000020 | 2,
	ZRO_ATOMIC_XOR_32    = 0x00000020 | 4,
	ZRO_ATOMIC_XOR_64    = 0x00000020 | 8,

	ZRO_ATOMIC_AND_8     = 0x00000030 | 1,
	ZRO_ATOMIC_AND_16    = 0x00000030 | 2,
	ZRO_ATOMIC_AND_32    = 0x00000030 | 4,
	ZRO_ATOMIC_AND_64    = 0x00000030 | 8,

	ZRO_ATOMIC_ADD_8     = 0x00000040 | 1,
	ZRO_ATOMIC_ADD_16    = 0x00000040 | 2,
	ZRO_ATOMIC_ADD_32    = 0x00000040 | 4,
	ZRO_ATOMIC_ADD_64    = 0x00000040 | 8,

	ZRO_ATOMIC_XCHG_8    = 0x00000050 | 1,
	ZRO_ATOMIC_XCHG_16   = 0x00000050 | 2,
	ZRO_ATOMIC_XCHG_32   = 0x00000050 | 4,
	ZRO_ATOMIC_XCHG_64   = 0x00000050 | 8,

	
	ZRO_ATOMIC_OR_LONG   = ZRO_ATOMIC_LONG(OR),
	ZRO_ATOMIC_XOR_LONG  = ZRO_ATOMIC_LONG(XOR),
	ZRO_ATOMIC_AND_LONG  = ZRO_ATOMIC_LONG(AND),
	ZRO_ATOMIC_ADD_LONG  = ZRO_ATOMIC_LONG(ADD),
	ZRO_ATOMIC_XCHG_LONG = ZRO_ATOMIC_LONG(XCHG),
});
extern uint64_t zalloc_ro_mut_atomic(
	zone_id_t       zone_id,
	void           *elem __unsafe_indexable,
	vm_offset_t     offset,
	zro_atomic_op_t op,
	uint64_t        value);
extern void    zalloc_ro_clear(
	zone_id_t       zone_id,
	void           *elem __unsafe_indexable,
	vm_offset_t     offset,
	vm_size_t       size);
extern void     zfree_id(
	zone_id_t       zone_id,
	void           *addr __unsafe_indexable);
extern void     zfree_ro(
	zone_id_t       zone_id,
	void           *addr __unsafe_indexable);
extern void     zfree(
	zone_t          zone,
	void           *elem __unsafe_indexable);
__attribute__((overloadable))
static inline void
zfree(
	zone_view_t     view,
	void           *elem __unsafe_indexable)
{
	zfree((zone_t)view, elem);
}

__attribute__((overloadable))
static inline void
zfree(
	kalloc_type_view_t   kt_view,
	void                *elem __unsafe_indexable)
{
	return kfree_type_impl(kt_view, elem);
}





__zalloc_deprecated("use zone_create()")
extern zone_t   zinit(
	vm_size_t       size,           
	vm_size_t       maxmem,         
	vm_size_t       alloc,          
	const char      *name __unsafe_indexable);
#pragma mark: implementation details


#pragma mark - XNU only interfaces


#pragma GCC visibility push(hidden)

#pragma mark XNU only: zalloc (extended)




__attribute__((malloc))
extern void *__sized_by(size) zalloc_permanent_tag(
	vm_size_t       size,
	vm_offset_t     align_mask,
	vm_tag_t        tag)
__attribute__((__diagnose_if__((align_mask & (align_mask + 1)),
    "align mask looks invalid", "error")));
extern void zalloc_first_proc_made(void);
extern void zalloc_iokit_lockdown(void);
#pragma mark XNU only: per-cpu allocations










extern void *__zpercpu zalloc_percpu(
	zone_or_view_t  zone_or_view,
	zalloc_flags_t  flags);
static inline void *__zpercpu
__zalloc_percpu(
	zone_or_view_t  zone_or_view,
	zalloc_flags_t  flags)
{
	void *__unsafe_indexable addr = (zalloc_percpu)(zone_or_view, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}



extern void     zfree_percpu(
	zone_or_view_t  zone_or_view,
	void *__zpercpu addr);
extern void *__zpercpu zalloc_percpu_permanent(
	vm_size_t       size,
	vm_offset_t     align_mask);
extern void     zone_enable_smr(
	zone_t                  zone,
	struct smr             *smr,
	zone_smr_free_cb_t      free_cb);
extern void     zfree_smr(
	zone_t          zone,
	void           *elem __unsafe_indexable);
extern void     zfree_id_smr(
	zone_id_t       zone_id,
	void           *addr __unsafe_indexable);
#pragma mark XNU only: zone creation (extended)


__enum_decl(zone_reserved_id_t, zone_id_t, {
	ZONE_ID__ZERO,

	ZONE_ID_PERMANENT,
	ZONE_ID_PERCPU_PERMANENT,

	ZONE_ID_THREAD_RO,
	ZONE_ID_MAC_LABEL,
	ZONE_ID_PROC_RO,
	ZONE_ID_PROC_SIGACTS_RO,
	ZONE_ID_KAUTH_CRED,
	ZONE_ID_CS_BLOB,

	ZONE_ID_SANDBOX_RO,
	ZONE_ID_PROFILE_RO,
	ZONE_ID_PROTOBOX,
	ZONE_ID_SB_FILTER,
	ZONE_ID_AMFI_OSENTITLEMENTS,

	ZONE_ID__FIRST_RO = ZONE_ID_THREAD_RO,
	ZONE_ID__FIRST_RO_EXT = ZONE_ID_SANDBOX_RO,
	ZONE_ID__LAST_RO_EXT = ZONE_ID_AMFI_OSENTITLEMENTS,
	ZONE_ID__LAST_RO = ZONE_ID__LAST_RO_EXT,

	ZONE_ID_PMAP,
	ZONE_ID_VM_MAP,
	ZONE_ID_VM_MAP_ENTRY,
	ZONE_ID_VM_MAP_HOLES,
	ZONE_ID_VM_MAP_COPY,
	ZONE_ID_VM_PAGES,
	ZONE_ID_IPC_PORT,
	ZONE_ID_IPC_PORT_SET,
	ZONE_ID_IPC_KMSG,
	ZONE_ID_IPC_VOUCHERS,
	ZONE_ID_PROC_TASK,
	ZONE_ID_THREAD,
	ZONE_ID_TURNSTILE,
	ZONE_ID_SEMAPHORE,
	ZONE_ID_SELECT_SET,
	ZONE_ID_FILEPROC,


	ZONE_ID__FIRST_DYNAMIC,
});
zone_t zone_by_id(
	size_t                  zid) __pure2;
const char *__unsafe_indexable zone_name(
	zone_t                  zone);
const char *__unsafe_indexable zone_heap_name(
	zone_t                  zone);
extern zone_t   zone_create_ext(
	const char             *name __unsafe_indexable,
	vm_size_t               size,
	zone_create_flags_t     flags,
	zone_id_t               desired_zid,
	void                  (^extra_setup)(zone_t));
extern void     zone_id_require(
	zone_id_t               zone_id,
	vm_size_t               elem_size,
	void                   *addr __unsafe_indexable);
extern void     zone_id_require_aligned(
	zone_id_t               zone_id,
	void                   *addr __unsafe_indexable);
extern void     zone_set_exhaustible(
	zone_t                  zone,
	vm_size_t               max_elements,
	bool                    exhausts_by_design);
extern void     zone_raise_reserve(
	zone_or_view_t          zone_or_view,
	uint16_t                min_elements);
extern void     zone_fill_initially(
	zone_t                  zone,
	vm_size_t               nelems);
extern void zone_drain(
	zone_t                  zone);
extern void zone_get_stats(
	zone_t                  zone,
	struct zone_basic_stats *stats);
EVENT_DECLARE(ZONE_EXHAUSTED, zone_exhausted_cb_t);
#pragma mark XNU only: zone views


__enum_decl(zone_kheap_id_t, uint8_t, {
	KHEAP_ID_NONE,
	KHEAP_ID_EARLY,
	KHEAP_ID_DATA_BUFFERS,
	KHEAP_ID_DATA_SHARED,
	KHEAP_ID_KT_VAR,
});
static inline bool
zone_is_data_kheap(zone_kheap_id_t kheap_id)
{
	return kheap_id == KHEAP_ID_DATA_BUFFERS || kheap_id == KHEAP_ID_DATA_SHARED;
}






#pragma mark XNU only: batched allocations


typedef 


extern void zstack_push(
	zstack_t               *stack,
	void                   *elem);
void *zstack_pop(
	zstack_t               *stack);
static inline uint32_t
zstack_count(zstack_t stack)
{
	return stack.z_count;
}


static inline bool
zstack_empty(zstack_t stack)
{
	return zstack_count(stack) == 0;
}

static inline zstack_t
zstack_load_and_erase(zstack_t *stackp)
{
	zstack_t stack = *stackp;

	*stackp = (zstack_t){ };
	return stack;
}


extern void zfree_nozero(
	zone_id_t               zone_id,
	void                   *elem __unsafe_indexable);
extern zstack_t zalloc_n(
	zone_id_t               zone_id,
	uint32_t                count,
	zalloc_flags_t          flags);
extern void zfree_n(
	zone_id_t               zone_id,
	zstack_t                stack);
extern void zfree_nozero_n(
	zone_id_t               zone_id,
	zstack_t                stack);
#pragma mark XNU only: cached objects


typedef const 



extern void *__unsafe_indexable zcache_mark_valid(
	zone_t                  zone,
	void                    *elem __unsafe_indexable);
static inline void *
zcache_mark_valid_single(
	zone_t                  zone,
	void                    *elem)
{
	return __unsafe_forge_single(void *, zcache_mark_valid(zone, elem));
}

static inline void *__header_bidi_indexable
zcache_mark_valid_indexable(
	zone_t                  zone,
	void                    *elem __header_bidi_indexable)
{
	return zcache_transpose_bounds((char *)elem, (char *)zcache_mark_valid(zone, elem));
}


extern void *__unsafe_indexable zcache_mark_invalid(
	zone_t                  zone,
	void                    *elem __unsafe_indexable);
static inline void *
zcache_mark_invalid_single(
	zone_t                  zone,
	void                    *elem)
{
	return __unsafe_forge_single(void *, zcache_mark_invalid(zone, elem));
}

static inline void *__header_bidi_indexable
zcache_mark_invalid_indexable(
	zone_t                  zone,
	void                    *elem __header_bidi_indexable)
{
	return zcache_transpose_bounds((char *)elem, (char *)zcache_mark_invalid(zone, elem));
}




extern zstack_t zcache_alloc_n(
	zone_id_t               zone_id,
	uint32_t                count,
	zalloc_flags_t          flags,
	zone_cache_ops_t        ops);
extern void zcache_free(
	zone_id_t               zone_id,
	void                   *addr __unsafe_indexable,
	zone_cache_ops_t        ops);
extern void zcache_free_n(
	zone_id_t               zone_id,
	zstack_t                stack,
	zone_cache_ops_t        ops);
extern void zcache_drain(
	zone_id_t               zone_id);
#pragma mark XNU only: PGZ support







#pragma mark XNU only: misc & implementation details



extern void     zone_create_startup(
	struct zone_create_startup_spec *spec);
extern void zone_view_startup_init(
	struct zone_view_startup_spec *spec);
extern void zone_userspace_reboot_checks(void);
static inline zalloc_flags_t
__zone_flags_mix_tag(zalloc_flags_t flags, vm_tag_t tag)
{
	return (flags & Z_VM_TAG_MASK) ? flags : Z_VM_TAG(flags, (uint32_t)tag);
}




extern unsigned zpercpu_count(void) __pure2;
extern kern_return_t zone_map_jetsam_set_limit(uint32_t value);
extern kern_return_t
mach_memory_info_sample(
	mach_zone_name_t *names,
	mach_zone_info_t *info,
	int              *coalesce,
	unsigned int     *zonesCnt,
	mach_memory_info_t *memoryInfo,
	unsigned int       memoryInfoCnt,
	bool               redact_info);
extern void     zone_gc_trim(void);
extern void     zone_gc_drain(void);
__BEGIN_DECLS

#pragma GCC visibility push(hidden)




typedef 




typedef struct zone_magazine *zone_magazine_t;
static inline uint32_t
Z_FAST_QUO(uint64_t offs, uint64_t magic)
{
	return (offs * magic) >> 32;
}


static inline uint32_t
Z_FAST_MOD(uint64_t offs, uint64_t magic, uint64_t size)
{
	uint32_t lowbits = (uint32_t)(offs * magic);

	return (lowbits * size) >> 32;
}


static inline bool
Z_FAST_ALIGNED(uint64_t offs, uint32_t magic)
{
	return (uint32_t)(offs * magic) < magic;
}









typedef 















__options_decl(kalloc_type_options_t, uint64_t, {
	
	KT_OPTIONS_ACCT                         = 0x00000001,
	
	KT_OPTIONS_DEBUG                        = 0x00000002,
	
	KT_OPTIONS_LOOSE_FREE                   = 0x00000004,
});
__enum_decl(kt_var_heap_id_t, uint32_t, {
	
	KT_VAR_DATA_HEAP,
	
	KT_VAR_PTR_HEAP0,
	KT_VAR_PTR_HEAP1,
	
	KT_VAR__FIRST_FLEXIBLE_HEAP,
});
__enum_decl(zone_submap_idx_t, uint32_t, {
	Z_SUBMAP_IDX_VM,
	Z_SUBMAP_IDX_READ_ONLY,
	Z_SUBMAP_IDX_GENERAL_0,
	Z_SUBMAP_IDX_DATA,

	Z_SUBMAP_IDX_COUNT,
});
__abortlike
extern void zone_invalid_panic(zone_t zone);
__pure2
static inline zone_id_t
zone_index(zone_t z)
{
	unsigned long delta;
	uint64_t quo;

	delta = (unsigned long)z - (unsigned long)zone_array;
	if (delta >= MAX_ZONES * sizeof(*z)) {
		zone_invalid_panic(z);
	}
	quo = Z_FAST_QUO(delta, Z_MAGIC_QUO(sizeof(*z)));
	__builtin_assume(quo < MAX_ZONES);
	return (zone_id_t)quo;
}

__pure2
static inline bool
zone_is_ro(zone_t zone)
{
	return zone >= &zone_array[ZONE_ID__FIRST_RO] &&
	       zone <= &zone_array[ZONE_ID__LAST_RO];
}

static inline bool
zone_addr_size_crosses_page(mach_vm_address_t addr, mach_vm_size_t size)
{
	return atop(addr ^ (addr + size - 1)) != 0;
}

__pure2
static inline uint16_t
zone_elem_redzone(zone_t zone)
{
}

__pure2
static inline uint16_t
zone_elem_inner_offs(zone_t zone)
{
	return zone->z_elem_offs;
}

__pure2
static inline uint16_t
zone_elem_outer_offs(zone_t zone)
{
	return zone_elem_inner_offs(zone) - zone_elem_redzone(zone);
}

__pure2
static inline vm_offset_t
zone_elem_inner_size(zone_t zone)
{
	return zone->z_elem_size;
}

__pure2
static inline vm_offset_t
zone_elem_outer_size(zone_t zone)
{
	return zone_elem_inner_size(zone) + zone_elem_redzone(zone);
}

__pure2
static inline zone_security_flags_t
zone_security_config(zone_t z)
{
	zone_id_t zid = zone_index(z);
	return zone_security_array[zid];
}

static inline uint32_t
zone_count_free(zone_t zone)
{
	return zone->z_elems_free + zone->z_recirc.zd_full * _zc_mag_size;
}

static inline uint32_t
zone_count_allocated(zone_t zone)
{
	return zone->z_elems_avail - zone_count_free(zone);
}

static inline vm_size_t
zone_scale_for_percpu(zone_t zone, vm_size_t size)
{
	if (zone->z_percpu) {
		size *= zpercpu_count();
	}
	return size;
}

static inline vm_size_t
zone_size_wired(zone_t zone)
{
	
	vm_size_t size = ptoa(os_atomic_load(&zone->z_wired_cur, relaxed));
	return zone_scale_for_percpu(zone, size);
}

static inline vm_size_t
zone_size_free(zone_t zone)
{
	return zone_scale_for_percpu(zone,
	           zone_elem_inner_size(zone) * zone_count_free(zone));
}


static inline vm_size_t
zone_size_allocated(zone_t zone)
{
	return zone_scale_for_percpu(zone,
	           zone_elem_inner_size(zone) * zone_count_allocated(zone));
}

static inline vm_size_t
zone_size_wasted(zone_t zone)
{
	return zone_size_wired(zone) - zone_scale_for_percpu(zone,
	           zone_elem_outer_size(zone) * zone->z_elems_avail);
}

__pure2
static inline bool
zone_exhaustible(zone_t zone)
{
	return zone->z_wired_max != ~0u;
}

__pure2
static inline bool
zone_exhausted(zone_t zone)
{
	return zone->z_wired_cur >= zone->z_wired_max;
}


extern void zone_set_sig_eq(zone_t zone, zone_id_t sig_eq);
extern zone_id_t zone_get_sig_eq(zone_t zone);
static inline vm_size_t
zone_stats_get_mem_allocated(zone_stats_t stats)
{
	return stats->zs_mem_allocated;
}


extern uint64_t get_zones_collectable_bytes(void);
__enum_closed_decl(zone_gc_level_t, uint32_t, {
	ZONE_GC_TRIM,
	ZONE_GC_DRAIN,
	ZONE_GC_JETSAM,
});
extern void     zone_gc(zone_gc_level_t level);
extern void     compute_zone_working_set_size(void *);
extern void     get_zone_map_size(uint64_t *current_size, uint64_t *capacity);
extern void     get_largest_zone_info(char *zone_name, size_t zone_name_len, uint64_t *zone_size);
extern void     zone_bootstrap(void);
extern void     zone_enable_caching(zone_t zone);
__startup_func
extern vm_offset_t zone_early_mem_init(
	vm_size_t       size);
__startup_func
extern vm_size_t zone_get_early_alloc_size(
	const char          *name __unused,
	vm_size_t            elem_size,
	zone_create_flags_t  flags,
	vm_size_t            min_elems);
__startup_func
extern void     zone_cram_early(
	zone_t          zone,
	vm_offset_t     newmem,
	vm_size_t       size);
extern bool     zone_maps_owned(
	vm_address_t    addr,
	vm_size_t       size);
extern void     zone_map_sizes(
	vm_map_size_t  *psize,
	vm_map_size_t  *pfree,
	vm_map_size_t  *plargest_free);
extern bool
zone_map_nearing_exhaustion(void);
static inline vm_tag_t
zalloc_flags_get_tag(zalloc_flags_t flags)
{
	return (vm_tag_t)((flags & Z_VM_TAG_MASK) >> Z_VM_TAG_SHIFT);
}

extern struct kalloc_result zalloc_ext(
	zone_t          zone,
	zone_stats_t    zstats,
	zalloc_flags_t  flags);
extern void     zfree_ext(
	zone_t          zone,
	zone_stats_t    zstats,
	void           *addr,
	uint64_t        combined_size);
extern zone_id_t zone_id_for_element(
	void           *addr,
	vm_size_t       esize);
extern void zone_element_bounds_check(
	vm_address_t    addr,
	vm_size_t       len);
extern vm_size_t zone_element_size(
	void           *addr,
	zone_t         *z,
	bool            clear_oob,
	vm_offset_t    *oob_offs);
extern bool zone_spans_ro_va(
	vm_offset_t     addr_start,
	vm_offset_t     addr_end);
static inline uint64_t
__zalloc_ro_mut_atomic(vm_offset_t dst, zro_atomic_op_t op, uint64_t value)
{

	switch (op) {
		__ZALLOC_RO_MUT_OP(OR, or_orig);
		__ZALLOC_RO_MUT_OP(XOR, xor_orig);
		__ZALLOC_RO_MUT_OP(AND, and_orig);
		__ZALLOC_RO_MUT_OP(ADD, add_orig);
		__ZALLOC_RO_MUT_OP(XCHG, xchg);
	default:
		panic("%s: Invalid atomic operation: %d", __func__, op);
	}

}


extern bool     zone_owns(
	zone_t          zone,
	void           *addr);
__pure2
extern vm_map_t zone_submap(
	zone_security_flags_t   zsflags);
static inline void
zone_lock(zone_t zone)
{
	hw_lck_ticket_lock(&zone->z_lock, &zone_locks_grp);
}

static inline void
zone_unlock(zone_t zone)
{
	hw_lck_ticket_unlock(&zone->z_lock);
}


int track_this_zone(const char *zonename, const char *logname);
kern_return_t kperf_sample(struct kperf_sample *sbuf,
    struct kperf_context *ctx,
    unsigned actionid,
    unsigned sample_flags);
void kperf_sample_user(struct kperf_usample *sbuf, struct kperf_context *ctx,
    unsigned int actionid, unsigned int sample_flags);
bool kperf_action_has_non_system(unsigned actionid);
bool kperf_action_has_thread(unsigned int actionid);
bool kperf_action_has_task(unsigned int actionid);
void kperf_action_reset(void);
unsigned kperf_action_get_count(void);
int kperf_action_set_count(unsigned count);
int kperf_action_set_samplers(unsigned int actionid, uint32_t samplers);
int kperf_action_get_samplers(unsigned int actionid, uint32_t *samplers_out);
int kperf_action_set_userdata(unsigned int actionid, uint32_t userdata);
int kperf_action_get_userdata(unsigned int actionid, uint32_t *userdata_out);
int kperf_action_set_ucallstack_depth(unsigned int actionid, uint32_t depth);
int kperf_action_get_ucallstack_depth(unsigned int actionid, uint32_t * depth_out);
int kperf_action_set_kcallstack_depth(unsigned int actionid, uint32_t depth);
int kperf_action_get_kcallstack_depth(unsigned int actionid, uint32_t * depth_out);
int kperf_action_set_filter(unsigned int actionid, int pid);
int kperf_action_get_filter(unsigned int actionid, int *pid_out);
void kperf_kdebug_handler(uint32_t debugid, uintptr_t *starting_fp);
int kperf_kdbg_cswitch_get(void);
int kperf_kdbg_cswitch_set(int newval);
int kperf_ast_pend(thread_t thread, uint32_t flags, unsigned int actionid);
void kperf_ast_set_callstack_depth(thread_t thread, uint32_t depth);
void kperf_kcallstack_sample(struct kp_kcallstack *cs, struct kperf_context *);
void kperf_kcallstack_log(struct kp_kcallstack *cs);
void kperf_continuation_sample(struct kp_kcallstack *cs, struct kperf_context *);
void kperf_backtrace_sample(struct kp_kcallstack *cs, struct kperf_context *context);
void kperf_ucallstack_sample(struct kp_ucallstack *cs, struct kperf_context *);
int kperf_ucallstack_pend(struct kperf_context *, uint32_t depth,
    unsigned int actionid);
void kperf_ucallstack_log(struct kp_ucallstack *cs);
void kperf_kdebug_setup(void);
void kperf_kdebug_reset(void);
boolean_t kperf_kdebug_should_trigger(uint32_t debugid);
int kperf_kdebug_set_action(int action_id);
int kperf_kdebug_get_action(void);
int kperf_kdebug_set_n_debugids(uint32_t n_debugids_in);
int kperf_kdebug_set_filter(user_addr_t user_filter, uint32_t user_size);
uint32_t kperf_kdebug_get_filter(struct kperf_kdebug_filter **filter);
uint32_t kperf_get_thread_ast(thread_t thread);
void kperf_set_thread_ast(thread_t thread, uint32_t flags);
boolean_t kperf_thread_get_dirty(thread_t thread);
void kperf_thread_set_dirty(thread_t thread, boolean_t dirty);
void kperf_setup(void);
extern void kperf_init_early(void);
bool kperf_is_sampling(void);
int kperf_enable_sampling(void);
int kperf_disable_sampling(void);
int kperf_port_to_pid(mach_port_name_t portname);
#pragma mark - external callbacks


void kperf_init(void);
extern __attribute__((noinline)) void kperf_thread_ast_handler(thread_t thread);
void kperf_on_cpu_update(void);
static inline void
kperf_on_cpu(thread_t thread, thread_continue_t continuation,
    uintptr_t *starting_fp)
{
	extern boolean_t kperf_on_cpu_active;
	void kperf_on_cpu_internal(thread_t thread, thread_continue_t continuation,
	    uintptr_t *starting_fp);

	if (__improbable(kperf_on_cpu_active)) {
		kperf_on_cpu_internal(thread, continuation, starting_fp);
	}
}


static inline void
kperf_off_cpu(thread_t thread)
{
	extern unsigned int kperf_lazy_cpu_action;
	void kperf_lazy_off_cpu(thread_t thread);

	if (__improbable(kperf_lazy_cpu_action != 0)) {
		kperf_lazy_off_cpu(thread);
	}
}


static inline void
kperf_make_runnable(thread_t thread, int interrupt)
{
	extern unsigned int kperf_lazy_cpu_action;
	void kperf_lazy_make_runnable(thread_t thread, bool interrupt);

	if (__improbable(kperf_lazy_cpu_action != 0)) {
		kperf_lazy_make_runnable(thread, interrupt);
	}
}

static inline void
kperf_running_setup(processor_t processor, uint64_t now)
{
	if (kperf_is_sampling()) {
		extern void kptimer_running_setup(processor_t, uint64_t now);
		kptimer_running_setup(processor, now);
	}
}


static inline void
kperf_interrupt(void)
{
	extern unsigned int kperf_lazy_cpu_action;
	extern void kperf_lazy_cpu_sample(thread_t thread, unsigned int flags,
	    bool interrupt);

	if (__improbable(kperf_lazy_cpu_action != 0)) {
		kperf_lazy_cpu_sample(NULL, 0, true);
	}
}


static inline void
kperf_kdebug_callback(uint32_t debugid, uintptr_t *starting_fp)
{
	extern boolean_t kperf_kdebug_active;
	void kperf_kdebug_handler(uint32_t debugid, uintptr_t *starting_fp);

	if (__improbable(kperf_kdebug_active)) {
		kperf_kdebug_handler(debugid, starting_fp);
	}
}


void kperf_timer_expire(void *param0, void *param1);
extern void kperf_reset(void);
void kperf_kernel_configure(const char *config);
int kperf_bless_pid(pid_t newpid);
void kptimer_init(void);
uint64_t kptimer_min_period_abs(bool pet);
unsigned int kptimer_get_count(void);
int kptimer_set_count(unsigned int count);
int kptimer_get_period(unsigned int timerid, uint64_t *period_out);
int kptimer_set_period(unsigned int timerid, uint64_t period);
int kptimer_get_action(unsigned int timerid, uint32_t *actionid_out);
int kptimer_set_action(unsigned int timer, uint32_t actionid);
int kptimer_set_pet_timerid(unsigned int timerid);
unsigned int kptimer_get_pet_timerid(void);
void kptimer_pet_enter(uint64_t sampledur_abs);
void kptimer_start(void);
void kptimer_stop(void);
void kptimer_stop_curcpu(void);
void kptimer_curcpu_up(void);
void kptimer_expire(processor_t processor, int cpuid, uint64_t now);
void kptimer_reset(void);
void kperf_lazy_reset(void);
void kperf_lazy_off_cpu(thread_t thread);
void kperf_lazy_make_runnable(thread_t thread, bool in_interrupt);
void kperf_lazy_wait_sample(thread_t thread,
    thread_continue_t continuation, uintptr_t *starting_fp);
void kperf_lazy_cpu_sample(thread_t thread, unsigned int flags, bool interrupt);
int kperf_lazy_get_wait_action(void);
int kperf_lazy_get_cpu_action(void);
int kperf_lazy_set_wait_action(int action_id);
int kperf_lazy_set_cpu_action(int action_id);
uint64_t kperf_lazy_get_wait_time_threshold(void);
uint64_t kperf_lazy_get_cpu_time_threshold(void);
int kperf_lazy_set_wait_time_threshold(uint64_t threshold);
int kperf_lazy_set_cpu_time_threshold(uint64_t threshold);
extern void kperf_meminfo_sample(task_t, struct meminfo *);
extern void kperf_meminfo_log(struct meminfo *mi);
void kppet_config(unsigned int actionid);
void kppet_reset(void);
void kppet_on_cpu(thread_t thread, thread_continue_t continuation,
    uintptr_t *starting_frame);
void kppet_mark_sampled(thread_t thread);
void kppet_wake_thread(void);
int kppet_get_idle_rate(void);
int kppet_set_idle_rate(int new_idle_rate);
int kppet_get_lightweight_pet(void);
int kppet_set_lightweight_pet(int on);
void kppet_lightweight_active_update(void);
void kppet_set_period(uint64_t period);
void kperf_task_snapshot_sample(task_t task, struct kperf_task_snapshot *tksn);
void kperf_task_snapshot_log(struct kperf_task_snapshot *tksn);
void kperf_task_info_log(struct kperf_context *ctx);
void kperf_thread_info_sample(struct kperf_thread_info *,
    struct kperf_context *);
void kperf_thread_info_log(struct kperf_thread_info *);
void kperf_thread_scheduling_sample(struct kperf_thread_scheduling *,
    struct kperf_context *);
void kperf_thread_scheduling_log(struct kperf_thread_scheduling *);
void kperf_thread_snapshot_sample(struct kperf_thread_snapshot *,
    struct kperf_context *);
void kperf_thread_snapshot_log(struct kperf_thread_snapshot *);
void kperf_thread_dispatch_sample(struct kperf_thread_dispatch *,
    struct kperf_context *);
int kperf_thread_dispatch_pend(struct kperf_context *, unsigned int actionid);
void kperf_thread_dispatch_log(struct kperf_thread_dispatch *);
void kperf_thread_inscyc_log(struct kperf_context *);
extern int     atoi(const char *);
extern char    *itoa(int, char *);
extern void     free(void *);
extern void     *malloc(size_t);
extern void     *realloc(void *, size_t);
extern char     *getenv(const char *);
extern void     exit(int);
extern long int strtol(const char *, char **, int);
extern unsigned long int strtoul(const char *, char **, int);
__options_decl(coalition_gpu_energy_t, uint32_t, {
	CGE_SELF    = 0x1,
	CGE_BILLED  = 0x2,
	CGE_OTHERS  = 0x4,
});
extern bool coalition_add_to_gpu_energy(uint64_t coalition_id, coalition_gpu_energy_t which, uint64_t energy);
static inline __attribute__((__always_inline__)) int
strings_are_equal(const char* a, const char* b)
{
	while (*a && *b) {
		if (*a != *b) {
			return 0;
		}
		++a;
		++b;
	}
	return *a == *b;
}



union ChainedFixupPointerOnDisk {
	uint64_t raw64;
	struct dyld_chained_ptr_64_kernel_cache_rebase fixup64;
};
OS_ENUM(exclaves_sensor_status, uint32_t,
    EXCLAVES_SENSOR_STATUS_ALLOWED = 1,
    EXCLAVES_SENSOR_STATUS_DENIED = 2,
    EXCLAVES_SENSOR_STATUS_CONTROL = 3,
    EXCLAVES_SENSOR_STATUS_PENDING = 4,
    );
OS_CLOSED_OPTIONS(exclaves_buffer_perm, uint32_t,
    EXCLAVES_BUFFER_PERM_READ = 1,
    EXCLAVES_BUFFER_PERM_WRITE = 2,
    );
OS_ENUM(exclaves_boot_stage, uint32_t,
    EXCLAVES_BOOT_STAGE_NONE = ~0u,
    EXCLAVES_BOOT_STAGE_2 = 0, 
    EXCLAVES_BOOT_STAGE_EXCLAVECORE = 0,
    EXCLAVES_BOOT_STAGE_EXCLAVEKIT = 100,

    
    EXCLAVES_BOOT_STAGE_FAILED = 200,
    );
OS_ENUM(exclaves_status, uint8_t,
    EXCLAVES_STATUS_NOT_STARTED = 0x00, 
    EXCLAVES_STATUS_AVAILABLE = 0x01,
    EXCLAVES_STATUS_FAILED = 0xFE,      
    EXCLAVES_STATUS_NOT_SUPPORTED = 0xFF,
    );
OS_CLOSED_OPTIONS(exclaves_requirement, uint64_t,


    
    EXCLAVES_R_STACKSHOT    = 0x04,

    
    EXCLAVES_R_LOG_SERVER   = 0x08,

    
    EXCLAVES_R_EIC          = 0x10,

    
    EXCLAVES_R_CONCLAVE     = 0x20,

    
    EXCLAVES_R_EXCLAVEKIT   = 0x40,

    
    EXCLAVES_R_CONCLAVE_RESOURCES = 0x80,

    
    EXCLAVES_R_STORAGE      = 0x100,

    
    EXCLAVES_R_TEST_PERF    = 0x200,

    
    EXCLAVES_R_TEST_STRESS  = 0x400,

    );
extern kern_return_t
exclaves_thread_terminate(thread_t thread);
extern bool
exclaves_booted(void);
extern size_t
exclaves_ipc_buffer_count(void);
OS_ENUM(exclaves_clock_type, uint8_t,
    EXCLAVES_CLOCK_ABSOLUTE = 0,
    EXCLAVES_CLOCK_CONTINUOUS = 1,
    );
extern void
exclaves_update_timebase(exclaves_clock_type_t type, uint64_t offset);
exclaves_boot_stage_t
exclaves_get_boot_stage(void);
bool
exclaves_boot_supported(void);
kern_return_t
exclaves_boot_wait(exclaves_boot_stage_t);
__options_closed_decl(exclaves_priv_t, unsigned int, {
	EXCLAVES_PRIV_CONCLAVE_HOST  = 0x1,  
	EXCLAVES_PRIV_CONCLAVE_SPAWN = 0x2,  
	EXCLAVES_PRIV_KERNEL_DOMAIN  = 0x4,  
	EXCLAVES_PRIV_BOOT           = 0x8,  
});
extern bool
exclaves_has_priv(task_t task, exclaves_priv_t priv);
extern bool
exclaves_has_priv_vnode(void *vnode, int64_t off, exclaves_priv_t priv);
uint32_t exclaves_stack_offset(const uintptr_t *out_addr, size_t nframes,
    bool slid_addresses);
extern bool exclaves_inspection_is_initialized(void);
extern kern_return_t exclaves_scheduler_request_watchdog_panic(void);
static inline Exclaves_L4_Word_t
Exclaves_L4_MessageTag_Mrs(Exclaves_L4_MessageTag_t tag)
{
	return Exclaves_L4_Word(Exclaves_L4_BfxW(tag, Exclaves_L4_MessageTag_Mrs_Base,
	           Exclaves_L4_MessageTag_Mrs_Bits));
}

static inline Exclaves_L4_Word_t
Exclaves_L4_MessageTag_Crs(Exclaves_L4_MessageTag_t tag)
{
	return Exclaves_L4_Word(Exclaves_L4_BfxW(tag, Exclaves_L4_MessageTag_Crs_Base,
	           Exclaves_L4_MessageTag_Crs_Bits));
}

static inline Exclaves_L4_Word_t
Exclaves_L4_MessageTag_Unwrapped(Exclaves_L4_MessageTag_t tag)
{
	return Exclaves_L4_Word(Exclaves_L4_BfxW(tag, Exclaves_L4_MessageTag_Unwrapped_Base,
	           Exclaves_L4_MessageTag_Unwrapped_Bits));
}

static inline Exclaves_L4_Word_t
Exclaves_L4_MessageTag_Label(Exclaves_L4_MessageTag_t tag)
{
	return Exclaves_L4_Word(Exclaves_L4_BfxW(tag, Exclaves_L4_MessageTag_Label_Base,
	           Exclaves_L4_MessageTag_Label_Bits));
}

static inline Exclaves_L4_MessageTag_t
Exclaves_L4_MessageTag(Exclaves_L4_Word_t mrs, Exclaves_L4_Word_t crs, Exclaves_L4_Word_t label,
    Exclaves_L4_Bool_t nonblocking)
{
	Exclaves_L4_Word_t tag = (
		Exclaves_L4_BfW(Exclaves_L4_MessageTag_Mrs_Base,
		Exclaves_L4_MessageTag_Mrs_Bits, Exclaves_L4_Word(mrs)) |
		Exclaves_L4_BfW(Exclaves_L4_MessageTag_Crs_Base,
		Exclaves_L4_MessageTag_Crs_Bits, Exclaves_L4_Word(crs)) |
		Exclaves_L4_BfW(Exclaves_L4_MessageTag_NonBlocking_Base,
		Exclaves_L4_MessageTag_NonBlocking_Bits, Exclaves_L4_Word(nonblocking)) |
		Exclaves_L4_BfW(Exclaves_L4_MessageTag_Label_Base,
		Exclaves_L4_MessageTag_Label_Bits, Exclaves_L4_Word(label)));

	return (Exclaves_L4_MessageTag_t) tag;
}










typedef 




enum {
	Exclaves_L4_Ipc_Mr_Tag,
	Exclaves_L4_Ipc_Mr_Badge,
	Exclaves_L4_Ipc_Mr_Message
};
static inline Exclaves_L4_IpcBuffer_t *
Exclaves_L4_IpcBuffer(Exclaves_L4_Void_t)
{
	return Exclaves_L4_IpcBuffer_Ptr(exclaves_get_ipc_buffer());
}





static inline Exclaves_L4_Word_t
Exclaves_L4_GetMr(Exclaves_L4_Word32_t mr)
{
	return Exclaves_L4_IpcBuffer()->mr[mr];
}

static inline Exclaves_L4_Void_t
Exclaves_L4_SetMr(Exclaves_L4_Word32_t mr, Exclaves_L4_Word_t word)
{
	Exclaves_L4_IpcBuffer()->mr[mr] = word;
}

static inline Exclaves_L4_Void_t
Exclaves_L4_SetMrs(Exclaves_L4_Word32_t mr, Exclaves_L4_Word32_t count,
    Exclaves_L4_Word_t * __counted_by(count)words)
{
	Exclaves_L4_IpcBuffer_t *ipcb = Exclaves_L4_IpcBuffer();

	for (Exclaves_L4_Word32_t offset = 0; offset < count; offset++) {
		ipcb->mr[mr + offset] = words[offset];
	}
}

static inline Exclaves_L4_Void_t
Exclaves_L4_GetMrs(Exclaves_L4_Word32_t mr, Exclaves_L4_Word32_t count,
    Exclaves_L4_Word_t * __counted_by(count)words)
{
	Exclaves_L4_IpcBuffer_t *ipcb = Exclaves_L4_IpcBuffer();

	for (Exclaves_L4_Word32_t offset = 0; offset < count; offset++) {
		words[offset] = ipcb->mr[mr + offset];
	}
}

static inline Exclaves_L4_Word_t
Exclaves_L4_GetCr(Exclaves_L4_Word32_t cr, Exclaves_L4_Bool_t dst)
{
	if (dst == Exclaves_L4_True) {
		return Exclaves_L4_IpcBuffer()->dcr[cr];
	} else {
		return Exclaves_L4_IpcBuffer()->scr[cr];
	}
}

static inline Exclaves_L4_Void_t
Exclaves_L4_SetCr(Exclaves_L4_Word32_t cr, Exclaves_L4_Word_t word, Exclaves_L4_Bool_t dst)
{
	if (dst == Exclaves_L4_True) {
		Exclaves_L4_IpcBuffer()->dcr[cr] = word;
	} else {
		Exclaves_L4_IpcBuffer()->scr[cr] = word;
	}
}

static inline Exclaves_L4_MessageTag_t
Exclaves_L4_GetMessageTag(Exclaves_L4_Void_t)
{
	return (Exclaves_L4_MessageTag_t) (Exclaves_L4_GetMr(Exclaves_L4_Ipc_Mr_Tag));
}

static inline Exclaves_L4_Void_t
Exclaves_L4_SetMessageTag(Exclaves_L4_MessageTag_t tag)
{
	Exclaves_L4_SetMr(Exclaves_L4_Ipc_Mr_Tag, Exclaves_L4_Word(tag));
}

static inline Exclaves_L4_Word_t
Exclaves_L4_GetMessageMr(Exclaves_L4_Word32_t mr)
{
	return Exclaves_L4_GetMr(Exclaves_L4_Ipc_Mr_Message + mr);
}

static inline Exclaves_L4_Void_t
Exclaves_L4_SetMessageMr(Exclaves_L4_Word32_t mr, Exclaves_L4_Word_t word)
{
	Exclaves_L4_SetMr((Exclaves_L4_Ipc_Mr_Message + mr), word);
}












typedef enum : uint64_t {
	
	EXCLAVES_XNUPROXY_EXCLAVE_HELLOEXCLAVE = 0,
	
	EXCLAVES_XNUPROXY_EXCLAVE_USERAPP,
	
	EXCLAVES_XNUPROXY_EXCLAVE_RESERVED,
	
	EXCLAVES_XNUPROXY_EXCLAVE_HELLODRIVERS,
	
	EXCLAVES_XNUPROXY_EXCLAVE_HELLOSTORAGE,
	
	EXCLAVES_XNUPROXY_EXCLAVE_USERAPP2,
	
	EXCLAVES_XNUPROXY_EXCLAVE_USERAPP3,
	
	EXCLAVES_XNUPROXY_EXCLAVE_AUDIODRIVER,
	
	EXCLAVES_XNUPROXY_EXCLAVE_HELLODRIVERINTERRUPTS,
	
	EXCLAVES_XNUPROXY_EXCLAVE_EXCLAVEDRIVERKIT,
	
	EXCLAVES_XNUPROXY_EXCLAVE_SECURERTBUDDY_AOP,
	
	EXCLAVES_XNUPROXY_EXCLAVE_SECURERTBUDDY_DCP,
	
	EXCLAVES_XNUPROXY_EXCLAVE_CONCLAVECONTROL,
	
	EXCLAVES_XNUPROXY_EXCLAVE_CONCLAVEDEBUG,
	
	EXCLAVES_XNUPROXY_EXCLAVE_SECURERTBUDDY_AOP_EDK,
	
	EXCLAVES_XNUPROXY_EXCLAVE_SECURERTBUDDY_DCP_EDK,
} exclaves_xnuproxy_exclaves_t;
#pragma pack(push, 4)



#pragma pack(pop)

typedef struct host_basic_info  host_basic_info_data_t;
void host_statistics_init(void);
extern kern_return_t    set_sched_stats_active(
	boolean_t active);
extern kern_return_t    get_sched_statistics(
	struct _processor_statistics_np *out,
	uint32_t *count);
#pragma pack(push, 4)


typedef 




typedef 


typedef 


typedef 

#pragma pack(pop)








extern void kmod_panic_dump(vm_offset_t * addr, unsigned int dump_cnt);
__BEGIN_DECLS
cpu_type_t                      cpu_type(void);
cpu_subtype_t           cpu_subtype(void);
cpu_threadtype_t        cpu_threadtype(void);
__BEGIN_DECLS
cpu_type_t                      slot_type(
	int             slot_num);
cpu_subtype_t           slot_subtype(
	int             slot_num);
cpu_threadtype_t        slot_threadtype(
	int             slot_num);
__options_decl(kern_clock_id_t, uint32_t, {
	KERN_CLOCK_MACH_ABSOLUTE_TIME = 1,
});
__options_decl(mach_eventlink_create_option_t, uint32_t, {
	MELC_OPTION_NONE         = 0,
	MELC_OPTION_NO_COPYIN    = 0x1,
	MELC_OPTION_WITH_COPYIN  = 0x2,
});
__options_decl(mach_eventlink_associate_option_t, uint32_t, {
	MELA_OPTION_NONE              = 0,
	MELA_OPTION_ASSOCIATE_ON_WAIT = 0x1,
});
__options_decl(mach_eventlink_disassociate_option_t, uint32_t, {
	MELD_OPTION_NONE = 0,
});
__options_decl(mach_eventlink_signal_wait_option_t, uint32_t, {
	MELSW_OPTION_NONE    = 0,
	MELSW_OPTION_NO_WAIT = 0x1,
});
__BEGIN_DECLS

uint64_t                        mach_absolute_time(void);
uint64_t                        mach_approximate_time(void);
uint64_t                        mach_continuous_time(void);
uint64_t                        mach_continuous_approximate_time(void);
uint64_t                        mach_continuous_speculative_time(void);
#pragma pack(push, 1)

typedef 
typedef mach_voucher_attr_recipe_data_t *mach_voucher_attr_recipe_t;
#pragma pack(pop)




typedef mach_port_t                     mach_voucher_attr_manager_t;
__BEGIN_DECLS
#pragma GCC visibility push(hidden)


extern kern_return_t memory_object_init(
	memory_object_t memory_object,
	memory_object_control_t memory_control,
	memory_object_cluster_size_t memory_object_page_size);
extern kern_return_t memory_object_terminate(
	memory_object_t memory_object);
extern kern_return_t memory_object_data_request(
	memory_object_t memory_object,
	memory_object_offset_t offset,
	memory_object_cluster_size_t length,
	vm_prot_t desired_access,
	memory_object_fault_info_t fault_info);
extern kern_return_t memory_object_data_return(
	memory_object_t memory_object,
	memory_object_offset_t offset,
	memory_object_cluster_size_t size,
	memory_object_offset_t *resid_offset,
	int *io_error,
	boolean_t dirty,
	boolean_t kernel_copy,
	int upl_flags);
extern kern_return_t memory_object_data_initialize(
	memory_object_t memory_object,
	memory_object_offset_t offset,
	memory_object_cluster_size_t size);
extern kern_return_t memory_object_map(
	memory_object_t memory_object,
	vm_prot_t prot);
extern kern_return_t memory_object_last_unmap(
	memory_object_t memory_object);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)

extern kern_return_t memory_object_change_attributes(
	memory_object_control_t memory_control,
	memory_object_flavor_t flavor,
	memory_object_info_t attributes,
	mach_msg_type_number_t attributesCnt);
extern kern_return_t memory_object_lock_request(
	memory_object_control_t memory_control,
	memory_object_offset_t offset,
	memory_object_size_t size,
	memory_object_offset_t *resid_offset,
	integer_t *io_errno,
	memory_object_return_t should_return,
	integer_t flags,
	vm_prot_t lock_value);
extern kern_return_t memory_object_destroy(
	memory_object_control_t                         memory_control,
	vm_object_destroy_reason_t   reason);
extern kern_return_t memory_object_upl_request(
	memory_object_control_t memory_control,
	memory_object_offset_t offset,
	upl_size_t size,
	upl_t *upl,
	upl_page_info_array_t page_list,
	mach_msg_type_number_t *page_listCnt,
	integer_t cntrl_flags,
	integer_t tag);
extern kern_return_t memory_object_cluster_size(
	memory_object_control_t control,
	memory_object_offset_t *start,
	vm_size_t *length,
	uint32_t *io_streaming,
	memory_object_fault_info_t fault_info);
__exported
extern kern_return_t memory_object_page_op(
	memory_object_control_t memory_control,
	memory_object_offset_t offset,
	integer_t ops,
	uint32_t *phys_entry,
	integer_t *flags);
extern kern_return_t memory_object_range_op(
	memory_object_control_t memory_control,
	memory_object_offset_t offset_beg,
	memory_object_offset_t offset_end,
	integer_t ops,
	integer_t *range);
__BEGIN_DECLS
extern void memory_object_reference(memory_object_t object);
extern void memory_object_deallocate(memory_object_t object);
extern boolean_t memory_object_backing_object(
	memory_object_t mem_obj,
	memory_object_offset_t offset,
	vm_object_t *backing_object,
	vm_object_offset_t *backing_offset);
extern void memory_object_control_reference(memory_object_control_t control);
extern void memory_object_control_deallocate(memory_object_control_t control);
extern int  memory_object_control_uiomove(memory_object_control_t, memory_object_offset_t, void *, int, int, int, int);
_Static_assert(sizeof(struct upl_page_info) == 8, "sizeof(struct upl_page_info) doesn't match expectation");
__BEGIN_DECLS

extern void            *upl_get_internal_vectorupl(upl_t);
extern upl_page_info_t *upl_get_internal_vectorupl_pagelist(upl_t);
extern upl_page_info_t *upl_get_internal_page_list(upl_t upl);
extern ppnum_t          upl_phys_page(upl_page_info_t *upl, int index);
extern boolean_t        upl_device_page(upl_page_info_t *upl);
extern boolean_t        upl_speculative_page(upl_page_info_t *upl, int index);
extern void     upl_clear_dirty(upl_t upl, boolean_t value);
extern void     upl_set_referenced(upl_t upl, boolean_t value);
extern void     upl_range_needed(upl_t upl, int index, int count);
__END_DECLS


__BEGIN_DECLS

extern boolean_t        upl_page_present(upl_page_info_t *upl, int index);
extern boolean_t        upl_dirty_page(upl_page_info_t *upl, int index);
extern boolean_t        upl_valid_page(upl_page_info_t *upl, int index);
extern void             upl_deallocate(upl_t upl);
extern void             upl_mark_decmp(upl_t upl);
extern void             upl_unmark_decmp(upl_t upl);
extern boolean_t        upl_has_wired_pages(upl_t upl);
void upl_page_set_mark(upl_page_info_t *upl, int index, boolean_t v);
boolean_t upl_page_get_mark(upl_page_info_t *upl, int index);
extern int mach_msg_priority_is_pthread_priority(mach_msg_priority_t pri);
extern mach_msg_priority_t mach_msg_priority_encode(
	mach_msg_qos_t override_qos,
	mach_msg_qos_t qos,
	int relpri);
extern mach_msg_qos_t mach_msg_priority_overide_qos(mach_msg_priority_t pri);
extern mach_msg_qos_t mach_msg_priority_qos(mach_msg_priority_t pri);
extern int mach_msg_priority_relpri(mach_msg_priority_t pri);
__enum_decl(mach_msg_type_name_t, unsigned int, {
	MACH_MSG_TYPE_NONE            =  0,     
	MACH_MSG_TYPE_MOVE_RECEIVE    = 16,     
	MACH_MSG_TYPE_MOVE_SEND       = 17,     
	MACH_MSG_TYPE_MOVE_SEND_ONCE  = 18,     
	MACH_MSG_TYPE_COPY_SEND       = 19,     
	MACH_MSG_TYPE_MAKE_SEND       = 20,     
	MACH_MSG_TYPE_MAKE_SEND_ONCE  = 21,     
});
#pragma pack(push, 4)

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 

typedef 


typedef union {
	mach_msg_port_descriptor_t            port;
	mach_msg_ool_descriptor32_t           out_of_line;
	mach_msg_ool_ports_descriptor32_t     ool_ports;
	mach_msg_type_descriptor_t            type;
	mach_msg_guarded_port_descriptor32_t  guarded_port;
} mach_msg_descriptor_t;
static inline mach_msg_descriptor_type_t
mach_msg_kdescriptor_type(const mach_msg_kdescriptor_t *kdesc)
{
	return kdesc->kdesc_header.type;
}

typedef 

static inline mach_msg_kbase_t *
mach_msg_header_to_kbase(mach_msg_header_t *hdr)
{
	return __container_of(hdr, mach_msg_kbase_t, msgb_header);
}




typedef unsigned int mach_msg_trailer_type_t;
#pragma pack(pop)






static inline mach_msg_size_t
mach_round_msg(mach_msg_size_t x)
{
	if (round_msg_overflow(x, &x)) {
		panic("round msg overflow");
	}
	return x;
}





typedef natural_t mach_msg_type_size_t;
__options_decl(mach_msg_option64_t, uint64_t, {
	MACH64_MSG_OPTION_NONE                 = 0x0ull,
	
	MACH64_SEND_MSG                        = MACH_SEND_MSG,
	MACH64_RCV_MSG                         = MACH_RCV_MSG,

	MACH64_RCV_LARGE                       = MACH_RCV_LARGE,
	MACH64_RCV_LARGE_IDENTITY              = MACH_RCV_LARGE_IDENTITY,

	MACH64_SEND_TIMEOUT                    = MACH_SEND_TIMEOUT,
	MACH64_SEND_OVERRIDE                   = MACH_SEND_OVERRIDE,
	MACH64_SEND_INTERRUPT                  = MACH_SEND_INTERRUPT,
	MACH64_SEND_NOTIFY                     = MACH_SEND_NOTIFY,
	MACH64_SEND_ALWAYS                     = MACH_SEND_ALWAYS,
	MACH64_SEND_IMPORTANCE                 = MACH_SEND_IMPORTANCE,
	MACH64_SEND_KERNEL                     = MACH_SEND_KERNEL,
	MACH64_SEND_FILTER_NONFATAL            = MACH_SEND_FILTER_NONFATAL,
	MACH64_SEND_TRAILER                    = MACH_SEND_TRAILER,
	MACH64_SEND_NOIMPORTANCE               = MACH_SEND_NOIMPORTANCE,
	MACH64_SEND_NODENAP                    = MACH_SEND_NODENAP,
	MACH64_SEND_SYNC_OVERRIDE              = MACH_SEND_SYNC_OVERRIDE,
	MACH64_SEND_PROPAGATE_QOS              = MACH_SEND_PROPAGATE_QOS,

	MACH64_SEND_SYNC_BOOTSTRAP_CHECKIN     = MACH_SEND_SYNC_BOOTSTRAP_CHECKIN,

	MACH64_RCV_TIMEOUT                     = MACH_RCV_TIMEOUT,

	MACH64_RCV_INTERRUPT                   = MACH_RCV_INTERRUPT,
	MACH64_RCV_VOUCHER                     = MACH_RCV_VOUCHER,

	MACH64_RCV_GUARDED_DESC                = MACH_RCV_GUARDED_DESC,
	MACH64_RCV_SYNC_WAIT                   = MACH_RCV_SYNC_WAIT,
	MACH64_RCV_SYNC_PEEK                   = MACH_RCV_SYNC_PEEK,

	MACH64_MSG_STRICT_REPLY                = MACH_MSG_STRICT_REPLY,
	

	
	MACH64_MSG_VECTOR                      = 0x0000000100000000ull,
	
	MACH64_SEND_KOBJECT_CALL               = 0x0000000200000000ull,
	
	MACH64_SEND_MQ_CALL                    = 0x0000000400000000ull,
	
	MACH64_SEND_ANY                        = 0x0000000800000000ull,
	
	MACH64_SEND_DK_CALL                    = 0x0000001000000000ull,

	
	MACH64_POLICY_KERNEL_EXTENSION         = 0x0002000000000000ull,
	MACH64_POLICY_FILTER_NON_FATAL         = 0x0004000000000000ull,
	MACH64_POLICY_FILTER_MSG               = 0x0008000000000000ull,
	MACH64_POLICY_DEFAULT                  = 0x0010000000000000ull,
	MACH64_POLICY_SIMULATED                = 0x0020000000000000ull,
	MACH64_POLICY_HARDENED                 = 0x0080000000000000ull,
	MACH64_POLICY_RIGID                    = 0x0100000000000000ull,
	MACH64_POLICY_PLATFORM                 = 0x0200000000000000ull,
	MACH64_POLICY_KERNEL                   = MACH64_SEND_KERNEL,

	
	MACH64_POLICY_NEEDED_MASK              = (
		MACH64_POLICY_SIMULATED |
		MACH64_POLICY_TRANSLATED |
		MACH64_POLICY_DEFAULT |
		MACH64_POLICY_HARDENED |
		MACH64_POLICY_RIGID |
		MACH64_POLICY_PLATFORM |
		MACH64_POLICY_KERNEL),

	
	MACH64_POLICY_MASK                     = (
		MACH64_POLICY_KERNEL_EXTENSION |
		MACH64_POLICY_FILTER_NON_FATAL |
		MACH64_POLICY_FILTER_MSG |
		MACH64_POLICY_NEEDED_MASK),

	
	MACH64_RCV_LINEAR_VECTOR               = 0x1000000000000000ull,
	
	MACH64_RCV_STACK                       = 0x2000000000000000ull,
	
	MACH64_MACH_MSG2                       = 0x8000000000000000ull
});
__BEGIN_DECLS


__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_overwrite(
	mach_msg_header_t *msg,
	mach_msg_option_t option,
	mach_msg_size_t send_size,
	mach_msg_size_t rcv_size,
	mach_port_name_t rcv_name,
	mach_msg_timeout_t timeout,
	mach_port_name_t notify,
	mach_msg_header_t *rcv_msg,
	mach_msg_size_t rcv_limit);
extern void mig_dealloc_reply_port(mach_port_t reply_port);
extern void mig_put_reply_port(mach_port_t reply_port);
extern int mig_strncpy(char     *dest, const char *src, int     len);
extern int mig_strncpy_zerofill(char    *dest, const char *src, int     len);
extern void *mig_user_allocate(vm_size_t size);
extern void mig_user_deallocate(char *data, vm_size_t size);
__BEGIN_DECLS

extern mach_port_name_t mk_timer_create(void);
extern kern_return_t    mk_timer_destroy(
	mach_port_name_t        name);
extern kern_return_t    mk_timer_arm(
	mach_port_name_t        name,
	uint64_t                expire_time);
extern kern_return_t    mk_timer_cancel(
	mach_port_name_t        name,
	uint64_t               *result_time);
extern kern_return_t    mk_timer_arm_leeway(
	mach_port_name_t        name,
	uint64_t                mk_timer_flags,
	uint64_t                mk_timer_expire_time,
	uint64_t                mk_timer_leeway);
static inline boolean_t
ipc_port_valid(ipc_port_t port)
{
	return port != IPC_PORT_DEAD && port;
}

typedef ipc_port_t              mach_port_t;
__enum_closed_decl(mach_port_right_t, uint32_t, {
	MACH_PORT_RIGHT_SEND            = 0,
	MACH_PORT_RIGHT_RECEIVE         = 1,
	MACH_PORT_RIGHT_SEND_ONCE       = 2,
	MACH_PORT_RIGHT_PORT_SET        = 3,
	MACH_PORT_RIGHT_DEAD_NAME       = 4,
	MACH_PORT_RIGHT_LABELH          = 5, 
	MACH_PORT_RIGHT_NUMBER          = 6, 
});
__options_closed_decl(mach_port_type_t, uint32_t, {
	MACH_PORT_TYPE_NONE             = 0,
	MACH_PORT_TYPE_SEND             = MACH_PORT_TYPE(MACH_PORT_RIGHT_SEND),
	MACH_PORT_TYPE_RECEIVE          = MACH_PORT_TYPE(MACH_PORT_RIGHT_RECEIVE),
	MACH_PORT_TYPE_SEND_ONCE        = MACH_PORT_TYPE(MACH_PORT_RIGHT_SEND_ONCE),
	MACH_PORT_TYPE_PORT_SET         = MACH_PORT_TYPE(MACH_PORT_RIGHT_PORT_SET),
	MACH_PORT_TYPE_DEAD_NAME        = MACH_PORT_TYPE(MACH_PORT_RIGHT_DEAD_NAME),
	
	MACH_PORT_TYPE_EX_RECEIVE       = MACH_PORT_TYPE(MACH_PORT_RIGHT_LABELH),

	
	MACH_PORT_TYPE_DNREQUEST        = 0x80000000,
	MACH_PORT_TYPE_SPREQUEST        = 0x40000000,
	MACH_PORT_TYPE_SPREQUEST_DELAYED = 0x20000000,
});
__BEGIN_DECLS

extern  kern_return_t   semaphore_signal(semaphore_t semaphore);
extern  kern_return_t   semaphore_signal_all(semaphore_t semaphore);
extern  kern_return_t   semaphore_wait(semaphore_t semaphore);
extern  kern_return_t   semaphore_timedwait(semaphore_t semaphore,
    mach_timespec_t wait_time);
extern  kern_return_t   semaphore_wait_deadline(semaphore_t semaphore,
    uint64_t deadline);
extern  kern_return_t   semaphore_wait_noblock(semaphore_t semaphore);
extern  kern_return_t   semaphore_wait_signal(semaphore_t wait_semaphore,
    semaphore_t signal_semaphore);
extern  kern_return_t   semaphore_timedwait_signal(semaphore_t wait_semaphore,
    semaphore_t signal_semaphore,
    mach_timespec_t wait_time);
extern  kern_return_t   semaphore_signal_thread(semaphore_t semaphore,
    thread_t thread);
void post_sys_powersource(int);
extern uint32_t rsr_get_version(void);
extern void rsr_bump_version(void);
#pragma pack(push, 4)




typedef struct task_basic_info_32       task_basic_info_32_data_t;
__options_decl(task_control_port_options_t, uint32_t, {
	TASK_CONTROL_PORT_OPTIONS_NONE     = 0x00000000,

	TASK_CONTROL_PORT_PINNED_SOFT      = 0x00000001,
	TASK_CONTROL_PORT_PINNED_HARD      = 0x00000002,
	TASK_CONTROL_PORT_IMMOVABLE_SOFT   = 0x00000004,
	TASK_CONTROL_PORT_IMMOVABLE_HARD   = 0x00000008,
});
extern kern_return_t
    qos_latency_policy_validate(task_latency_qos_t);
extern kern_return_t
    qos_throughput_policy_validate(task_throughput_qos_t);
extern uint32_t
    qos_extract(uint32_t);
extern uint32_t
    qos_latency_policy_package(uint32_t);
extern uint32_t
    qos_throughput_policy_package(uint32_t);
__BEGIN_DECLS
void thread_group_join_io_storage(void);
void thread_group_join_perf_controller(void);
static inline int OS_WARN_RESULT
mach_vm_round_page_overflow(mach_vm_offset_t in, mach_vm_offset_t *out)
{
	return round_page_overflow(in, out);
}















extern vm_size_t        mem_size;
__BEGIN_DECLS
vm_offset_t ml_static_slide(vm_offset_t vaddr);
vm_offset_t ml_static_unslide(vm_offset_t vaddr);
__END_DECLS


static inline bool
vm_is_addr_slid(vm_offset_t addr)
{
	const vm_offset_t stripped_addr = (vm_offset_t)VM_KERNEL_STRIP_PTR(addr);
	const bool is_slid_kern_addr =
	    (stripped_addr >= vm_kernel_slid_base) && (stripped_addr < vm_kernel_slid_top);

}












static inline int
mach_vm_size_unit(mach_vm_size_t size)
{
	uint32_t bits = 64u - (uint32_t)__builtin_clzll((size / 10) | 1);

	return "BKMGTPE"[bits / 10];
}

static inline uint32_t
mach_vm_size_pretty(mach_vm_size_t size)
{
	uint32_t bits = 64u - (uint32_t)__builtin_clzll((size / 10) | 1);

	return (uint32_t)(size >> (bits - bits % 10));
}

static inline mach_vm_offset_t
mach_vm_round_page(mach_vm_offset_t x)
{
	if (round_page_overflow(x, &x)) {
		panic("overflow detected");
	}
	return x;
}

static inline vm_offset_t
round_page(vm_offset_t x)
{
	if (round_page_overflow(x, &x)) {
		panic("overflow detected");
	}
	return x;
}

static inline mach_vm_offset_t
round_page_64(mach_vm_offset_t x)
{
	if (round_page_overflow(x, &x)) {
		panic("overflow detected");
	}
	return x;
}

static inline uint32_t
round_page_32(uint32_t x)
{
	if (round_page_overflow(x, &x)) {
		panic("overflow detected");
	}
	return x;
}



typedef 








static inline vm_offset_t
vm_pack_pointer(vm_offset_t ptr, vm_packing_params_t params)
{
	if (ptr != 0) {
		ptr = vm_memtag_canonicalize_kernel(ptr);
	}

	if (!params.vmpp_base_relative) {
		return ptr >> params.vmpp_shift;
	}
	if (ptr) {
		return (ptr - params.vmpp_base) >> params.vmpp_shift;
	}
	return (vm_offset_t)0;
}


static inline vm_offset_t
vm_unpack_pointer(vm_offset_t packed, vm_packing_params_t params)
{
	if (!params.vmpp_base_relative) {
		intptr_t addr = (intptr_t)packed;
		addr <<= __WORDSIZE - params.vmpp_bits;
		addr >>= __WORDSIZE - params.vmpp_bits - params.vmpp_shift;
		return vm_memtag_load_tag((vm_offset_t)addr);
	}
	if (packed) {
		return vm_memtag_load_tag((packed << params.vmpp_shift) + params.vmpp_base);
	}
	return (vm_offset_t)0;
}


static inline vm_offset_t
vm_packing_max_packable(vm_packing_params_t params)
{
	if (!params.vmpp_base_relative) {
		return VM_MAX_KERNEL_ADDRESS;
	}

	vm_offset_t ptr = params.vmpp_base +
	    (((1ul << params.vmpp_bits) - 1) << params.vmpp_shift);

	return ptr >= params.vmpp_base ? ptr : VM_MAX_KERNEL_ADDRESS;
}


__abortlike
extern void
vm_packing_pointer_invalid(vm_offset_t ptr, vm_packing_params_t params);
static inline void
vm_verify_pointer_packable(vm_offset_t ptr, vm_packing_params_t params)
{
	if (ptr != 0) {
		ptr = vm_memtag_canonicalize_kernel(ptr);
	}

	if (ptr & ((1ul << params.vmpp_shift) - 1)) {
		vm_packing_pointer_invalid(ptr, params);
	}
	if (!params.vmpp_base_relative || ptr == 0) {
		return;
	}
	if (ptr <= params.vmpp_base || ptr > vm_packing_max_packable(params)) {
		vm_packing_pointer_invalid(ptr, params);
	}
}



extern void
vm_packing_verify_range(
	const char         *subsystem,
	vm_offset_t         min_address,
	vm_offset_t         max_address,
	vm_packing_params_t params);
__BEGIN_DECLS






__enum_decl(mach_vm_reclaim_action_t, uint8_t, {
	VM_RECLAIM_FREE       = 1,
	VM_RECLAIM_DEALLOCATE = 2,
});
__enum_decl(mach_vm_reclaim_state_t, uint32_t, {
	VM_RECLAIM_UNRECLAIMED = 1,
	VM_RECLAIM_FREED       = 2,
	VM_RECLAIM_DEALLOCATED = 3,
	VM_RECLAIM_BUSY        = 4,
});
__enum_decl(mach_vm_reclaim_error_t, mach_error_t, {
	VM_RECLAIM_SUCCESS             = ERR_SUCCESS,
	VM_RECLAIM_INVALID_ARGUMENT    = err_vm_reclaim(1),
	VM_RECLAIM_NOT_SUPPORTED       = err_vm_reclaim(2),
	VM_RECLAIM_INVALID_REGION_SIZE = err_vm_reclaim(3),
	VM_RECLAIM_INVALID_CAPACITY    = err_vm_reclaim(4),
	VM_RECLAIM_INVALID_ID          = err_vm_reclaim(5),
	VM_RECLAIM_RESOURCE_SHORTAGE   = err_vm_reclaim(6),
	VM_RECLAIM_INVALID_RING        = err_vm_reclaim(7),
});
#pragma pack(push, 4)




typedef uint32_t vm32_object_id_t;
#pragma pack(pop)


typedef int *vm_page_info_t;
kern_return_t vm_stats(void *info, unsigned int *count);
__enum_decl(virtual_memory_guard_exception_code_t, uint32_t, {
	kGUARD_EXC_DEALLOC_GAP  = 1,
	kGUARD_EXC_RECLAIM_COPYIO_FAILURE = 2,
	kGUARD_EXC_SEC_LOOKUP_DENIED = 3,
	kGUARD_EXC_RECLAIM_INDEX_FAILURE = 4,
	kGUARD_EXC_SEC_RANGE_DENIED = 6,
	kGUARD_EXC_SEC_ACCESS_FAULT = 7,
	kGUARD_EXC_RECLAIM_DEALLOCATE_FAILURE = 8,
	kGUARD_EXC_SEC_COPY_DENIED = 16,
	kGUARD_EXC_SEC_SHARING_DENIED = 32,
	kGUARD_EXC_SEC_ASYNC_ACCESS_FAULT = 64,
});
static inline bool
vm_guard_is_sec_access(uint32_t flavor)
{
	return flavor == kGUARD_EXC_SEC_ACCESS_FAULT ||
	       flavor == kGUARD_EXC_SEC_ASYNC_ACCESS_FAULT;
}

static inline bool
vm_guard_is_sec_policy(uint32_t flavor)
{
	return flavor == kGUARD_EXC_SEC_LOOKUP_DENIED ||
	       flavor == kGUARD_EXC_SEC_RANGE_DENIED ||
	       flavor == kGUARD_EXC_SEC_COPY_DENIED ||
	       flavor == kGUARD_EXC_SEC_SHARING_DENIED;
}


__enum_decl(vm_map_range_id_t, uint8_t, {
	KMEM_RANGE_ID_NONE,
	KMEM_RANGE_ID_PTR_0,
	KMEM_RANGE_ID_PTR_1,
	KMEM_RANGE_ID_PTR_2,
	KMEM_RANGE_ID_SPRAYQTN,
	KMEM_RANGE_ID_IOKIT,
	KMEM_RANGE_ID_DATA,

	KMEM_RANGE_ID_FIRST   = KMEM_RANGE_ID_PTR_0,
	KMEM_RANGE_ID_NUM_PTR = KMEM_RANGE_ID_PTR_2,
	KMEM_RANGE_ID_MAX     = KMEM_RANGE_ID_DATA,

	
	UMEM_RANGE_ID_DEFAULT = 0, 
	UMEM_RANGE_ID_HEAP,        
	UMEM_RANGE_ID_FIXED,       
	UMEM_RANGE_ID_LARGE_FILE,

	
	UMEM_RANGE_ID_MAX     = UMEM_RANGE_ID_LARGE_FILE,

});
__options_decl(vm_map_create_options_t, uint32_t, {
	VM_MAP_CREATE_DEFAULT          = 0x00000000,
	VM_MAP_CREATE_PAGEABLE         = 0x00000001,
	VM_MAP_CREATE_CORPSE_FOOTPRINT = 0x00000002,
	VM_MAP_CREATE_DISABLE_HOLELIST = 0x00000004,
	VM_MAP_CREATE_NEVER_FAULTS     = 0x00000008,
});
__options_decl(mach_vm_range_flags_t, uint64_t, {
	MACH_VM_RANGE_NONE      = 0x000000000000,
});
__enum_decl(mach_vm_range_tag_t, uint16_t, {
	MACH_VM_RANGE_DEFAULT,
	MACH_VM_RANGE_DATA,
	MACH_VM_RANGE_FIXED,
});
#pragma pack(1)

typedef 

#pragma pack()

typedef mach_vm_range_recipe_v1_t    mach_vm_range_recipe_t;
extern int vmrtf_extract(uint64_t, boolean_t, unsigned long, void *, unsigned long *);
extern unsigned int vmrtfaultinfo_bufsz(void);
VM_GENERATE_UNSAFE_WRAPPER(uint64_t, vm_addr_struct_t);
VM_GENERATE_UNSAFE_WRAPPER(uint64_t, vm_size_struct_t);
VM_GENERATE_UNSAFE_WRAPPER(uint32_t, vm32_addr_struct_t);
VM_GENERATE_UNSAFE_WRAPPER(uint32_t, vm32_size_struct_t);
VM_GENERATE_UNSAFE_ADDR(mach_vm_address_t, mach_vm_address_ut);
VM_GENERATE_UNSAFE_ADDR(mach_vm_offset_t, mach_vm_offset_ut);
VM_GENERATE_UNSAFE_SIZE(mach_vm_size_t, mach_vm_size_ut);
VM_GENERATE_UNSAFE_ADDR(vm_address_t, vm_address_ut);
VM_GENERATE_UNSAFE_ADDR(vm_offset_t, vm_offset_ut);
VM_GENERATE_UNSAFE_SIZE(vm_size_t, vm_size_ut);
VM_GENERATE_UNSAFE_ADDR(vm_map_address_t, vm_map_address_ut);
VM_GENERATE_UNSAFE_ADDR(vm_map_offset_t, vm_map_offset_ut);
VM_GENERATE_UNSAFE_SIZE(vm_map_size_t, vm_map_size_ut);
VM_GENERATE_UNSAFE_ADDR(memory_object_offset_t, memory_object_offset_ut);
VM_GENERATE_UNSAFE_SIZE(memory_object_size_t, memory_object_size_ut);
VM_GENERATE_UNSAFE_ADDR(vm_object_offset_t, vm_object_offset_ut);
VM_GENERATE_UNSAFE_SIZE(vm_object_size_t, vm_object_size_ut);
VM_GENERATE_UNSAFE_ADDR(pointer_t, pointer_ut);
VM_GENERATE_UNSAFE_ADDR32(vm32_address_t, vm32_address_ut);
VM_GENERATE_UNSAFE_ADDR32(vm32_offset_t, vm32_offset_ut);
VM_GENERATE_UNSAFE_SIZE32(vm32_size_t, vm32_size_ut);
VM_GENERATE_UNSAFE_TYPE(vm_prot_t, vm_prot_ut);
VM_GENERATE_UNSAFE_TYPE(vm_inherit_t, vm_inherit_ut);
VM_GENERATE_UNSAFE_TYPE(vm_behavior_t, vm_behavior_ut);
extern  void    commpage_populate( void );
extern  void    commpage_text_populate( void );
__BEGIN_DECLS

#pragma GCC visibility push(hidden)


bool ml_cpu_can_exit(int cpu_id);
void ml_cpu_begin_state_transition(int cpu_id);
void ml_cpu_end_state_transition(int cpu_id);
void ml_cpu_begin_loop(void);
void ml_cpu_end_loop(void);
int ml_early_cpu_max_number(void);
void ml_cpu_power_enable(int cpu_id);
void ml_cpu_power_disable(int cpu_id);
void cpu_event_register_callback(cpu_callback_t fn, void *param);
void cpu_event_unregister_callback(cpu_callback_t fn);
void ml_broadcast_cpu_event(enum cpu_event event, unsigned int cpu_or_cluster);
unsigned long long ml_io_read(uintptr_t iovaddr, int iovsz);
unsigned int ml_io_read8(uintptr_t iovaddr);
unsigned int ml_io_read16(uintptr_t iovaddr);
unsigned int ml_io_read32(uintptr_t iovaddr);
unsigned long long ml_io_read64(uintptr_t iovaddr);
void ml_io_write(uintptr_t vaddr, uint64_t val, int size);
void ml_io_write8(uintptr_t vaddr, uint8_t val);
void ml_io_write16(uintptr_t vaddr, uint16_t val);
void ml_io_write32(uintptr_t vaddr, uint32_t val);
void ml_io_write64(uintptr_t vaddr, uint64_t val);
OS_WARN_RESULT
int ml_io_increase_timeouts(uintptr_t iovaddr_base, unsigned int size, uint32_t read_timeout_us, uint32_t write_timeout_us);
OS_WARN_RESULT
int ml_io_reset_timeouts(uintptr_t iovaddr_base, unsigned int size);
OS_WARN_RESULT
int ml_io_increase_timeouts_phys(vm_offset_t iopaddr_base, unsigned int size,
    uint32_t read_timeout_us, uint32_t write_timeout_us);
OS_WARN_RESULT
int ml_io_reset_timeouts_phys(vm_offset_t iopaddr_base, unsigned int size);
void ml_get_cluster_type_name(cluster_type_t cluster_type, char *name,
    size_t name_size);
unsigned int ml_get_cluster_count(void);
bool ml_addr_in_non_xnu_stack(uintptr_t addr);
void ml_map_cpus_to_clusters(uint8_t *table);
void ml_task_post_signature_processing_hook(task_t task);
void __hib_assert(const char *file, int line, const char *expression) __attribute__((noreturn));
uintptr_t pal_hib_map(pal_hib_map_type_t virt, uint64_t phys);
void pal_hib_restore_pal_state(uint32_t *src);
void pal_hib_init(void);
void pal_hib_write_hook(void);
void pal_hib_resume_init(pal_hib_ctx_t *palHibCtx, hibernate_page_list_t *map, uint32_t *nextFree);
void pal_hib_restored_page(pal_hib_ctx_t *palHibCtx, pal_hib_restore_stage_t stage, ppnum_t ppnum);
void pal_hib_patchup(pal_hib_ctx_t *palHibCtx);
void pal_hib_teardown_pmap_structs(addr64_t *unneeded_start, addr64_t *unneeded_end);
void pal_hib_rebuild_pmap_structs(void);
void pal_hib_decompress_page(void *src, void *dst, void *scratch, unsigned int compressedSize);
extern void __static_if_key_delta(
	static_if_key_t         key,
	int                     delta);
__attribute__((always_inline))
static inline unsigned long
__static_if_entry_patch_point(static_if_entry_t sie)
{
}

extern void ml_static_if_entry_patch(
	static_if_entry_t     ent,
	int                     branch);
extern void ml_static_if_flush_icache(void);
extern void static_if_init(const char *args);
extern int bcmp_impl(const void *, const void *, size_t) asm("_bcmp");
extern int memcmp_impl(const void *, const void *, size_t) asm("_memcmp");
extern unsigned long memcmp_zero_ptr_aligned_impl(const void *addr, size_t size) asm("_memcmp_zero_ptr_aligned");
extern void *bzero_impl(void *, const void *, size_t) asm("_bzero");
extern void *memset_impl(void *, int c, size_t) asm("_memset");
extern void *memcpy_impl(void *, const void *, size_t) asm("_memcpy");
extern void *memmove_impl(void *, const void *, size_t) asm("_memmove");
extern void bcopy_impl(const void *, void *, size_t) asm("_bcopy");
extern size_t strlen_impl(const char *) asm("_strlen");
extern size_t strnlen_impl(const char *s, size_t) asm("_strnlen");
extern int strprefix_impl(const char *, const char *) asm("_strprefix");
extern int strcmp_impl(const char *, const char *) asm("_strcmp");
extern int strncmp_impl(const char *, const char *, size_t) asm("_strncmp");
extern int strlcmp_impl(const char *, const char *, size_t) asm("_strlcmp");
extern int strbufcmp_impl(const char *, size_t, const char *, size_t) asm("_strbufcmp");
extern int strcasecmp_impl(const char *, const char *) asm("_strcasecmp");
extern int strncasecmp_impl(const char *, const char *, size_t) asm("_strncasecmp");
extern int strlcasecmp_impl(const char *, const char *, size_t) asm("_strlcasecmp");
extern int strbufcasecmp_impl(const char *, size_t, const char *, size_t) asm("_strbufcasecmp");
extern char *strchr_impl(const char *, int) asm("_strchr");
extern char *strrchr_impl(const char *, int) asm("_strrchr");
extern char *strnstr_impl(const char *s, const char *, size_t) asm("_strnstr");
extern size_t strlcat_impl(char *, const char *, size_t) asm("_strlcat");
extern const char *strbufcat_impl(char *, size_t, const char *, size_t) asm("_strbufcat");
extern size_t strlcpy_impl(char *, const char *, size_t) asm("_strlcpy");
extern const char *strbufcpy_impl(char *, size_t, const char *, size_t) asm("_strbufcpy");
extern char *strncpy_impl(char *, const char *, size_t) asm("_strncpy");
extern char *strncat_impl(char *, const char *, size_t) asm("_strncat");
extern void machine_task_init(task_t new_task, task_t parent_task, boolean_t memory_inherit);
#pragma pack(4)


typedef 

typedef 

typedef 


typedef 

typedef vm_info_object_t *vm_info_object_array_t;
<h2>bootstrap_arguments</h2>
<hr>
<p>
<strong>Function</strong> - Return a set of arguments to the bootstrap task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   bootstrap_arguments</strong>
                <strong>(mach_port_t</strong>                          <var>bootstrap</var>,
                 <strong>task_t</strong>                                    <var>task</var>,
                 <strong>pointer_t</strong>                            <var>pointer_t</var>,
                 <strong>mach_msg_type_number_t</strong>  <var>mach_msg_type_number_t</var><strong>);
<h2>bootstrap_completed</h2>
<hr>
<p>
<strong>Server Interface</strong> - Inform bootstrap server that
initialization is complete.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   bootstrap_completed</strong>
                <strong>(mach_port_t</strong>                     <var>bootstrap_port</var>,
                 <strong>task_t</strong>                                    <var>task</var><strong>);
<h2>bootstrap_environment</h2>
<hr>
<p>
<strong>Function</strong> - Return to the bootstrap task an array of strings specifying the task's environment.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   bootstrap_environment</strong>
                <strong>(mach_port_t</strong>                          <var>bootstrap</var>,
                 <strong>task_t</strong>                                    <var>task</var>,
                 <strong>pointer_t</strong>                            <var>pointer_t</var>,
                 <strong>mach_msg_type_number_t</strong>  <var>mach_msg_type_number_t</var><strong>);
<h2>bootstrap_ports</h2>
<hr>
<p>
<strong>Function</strong> - Return send rights to the system's control ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   bootstrap_ports</strong>
                <strong>(mach_port_t</strong>                          <var>bootstrap</var>,
                 <strong>bootstrap</strong>                         <var>host_control</var>,
                 <strong>host_control</strong>                     <var>device_master</var>,
                 <strong>device_master</strong>                <var>root_wired_ledger</var>,
                 <strong>root_wired_ledger</strong>            <var>root_paged_ledger</var>,
                 <strong>bootstrap</strong>                             <var>security</var><strong>);
<h2>catch_exception_raise</h2>
<hr>
<p>
<strong>Server Interface</strong> - Handles the occurrence of an exception within a thread.

<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   catch_exception_raise</strong>
                <strong>(mach_port_t</strong>                          <var>exception_port</var>,
                 <strong>mach_port_t</strong>                                  <var>thread</var>,
                 <strong>mach_port_t</strong>                                    <var>task</var>,
                 <strong>exception_type_t</strong>                          <var>exception</var>,
                 <strong>exception_data_t</strong>                               <var>code</var>,
                 <strong>mach_msg_type_number_t</strong>                   <var>code_count</var><strong>);
</strong>
</pre>
<p>
<strong>catch_exception_raise_state</strong>
expanded form:
<pre>
<strong>kern_return_t   catch_exception_raise_state</strong>
                <strong>(mach_port_t</strong>                          <var>exception_port</var>,
                 <strong>exception_type_t</strong>                          <var>exception</var>,
                 <strong>exception_data_t</strong>                               <var>code</var>,
                 <strong>mach_msg_type_number_t</strong>                   <var>code_count</var>,
                 <strong>int *</strong>                                        <var>flavor</var>,
                 <strong>thread_state_t</strong>                             <var>in_state</var>,
                 <strong>mach_msg_type_number_t</strong>               <var>in_state_count</var>,
                 <strong>thread_state_t</strong>                            <var>out_state</var>,
                 <strong>mach_msg_type_number_t *</strong>            <var>out_state_count</var><strong>);
</strong>
</pre>
<p>
<strong>catch_exception_raise_state_identity</strong>
expanded form:
<pre>
<strong>kern_return_t   catch_exception_raise_state_identity</strong>
                <strong>(mach_port_t</strong>                          <var>exception_port</var>,
                 <strong>mach_port_t</strong>                                  <var>thread</var>,
                 <strong>mach_port_t</strong>                                    <var>task</var>,
                 <strong>exception_type_t</strong>                          <var>exception</var>,
                 <strong>exception_data_t</strong>                               <var>code</var>,
                 <strong>mach_msg_type_number_t</strong>                   <var>code_count</var>,
                 <strong>int *</strong>                                        <var>flavor</var>,
                 <strong>thread_state_t</strong>                             <var>in_state</var>,
                 <strong>mach_msg_type_number_t</strong>               <var>in_state_count</var>,
                 <strong>thread_state_t</strong>                            <var>out_state</var>,
                 <strong>mach_msg_type_number_t *</strong>            <var>out_state_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<dt> <var>exception_port</var>
<dd>
[in exception (receive) right] The port to which the exception
notification was sent.
<p>
<dt> <var>thread</var>
<dd>
[in thread-self send right] The thread self port for the thread taking the 
exception.
<p>
<dt> <var>task</var>
<dd>
[in task-self send right] The task self port for the task containing the 
thread taking the exception.
<p>
<dt> <var>exception</var>
<dd>
[in scalar] The type of the exception.
The machine independent values raised by all implementations are:
     <dl>
<p>
<dt> EXC_BAD_ACCESS
<dd>
Could not access memory. subcode contains the bad memory 
address.
<p>
<dt> EXC_BAD_INSTRUCTION
<dd>
Instruction failed. Illegal or undefined instruction or operand.
<p>
<dt> EXC_ARITHMETIC
<dd>
Arithmetic exception;
codes 0x10000 
- 0x1FFFF reserved for OS emulation.
<p>
<dt> EXC_BREAKPOINT
<dd>
Trace, breakpoint, etc. Details in subcode field.
<p>
<dt> EXC_SYSCALL
<dd>
System call requested. Details in subcode field.
<p>
<dt> EXC_MACH_SYSCALL
<dd>
System call with a number in the Mach call range requested. 
Details in subcode field.
     </dl
<p>
<dt> <var>code</var>
<dd>
[in scalar] A machine dependent array indicating a particular instance 
of exception.
<p>
<dt> <var>code_count</var>
<dd>
[in scalar] The size of the buffer (in natural-sized units).
<p>
<dt> <var>flavor</var>
<dd>
[pointer to in/out scalar] On input, the type of state included as selected
when the exception port was set. On output, the type of state being 
returned.
<p>
<dt> <var>in_state</var>
<dd>
[pointer to in structure] State information of the thread at the time of 
the exception.
<p>
<dt> <var>in_state_count</var>
<dd>
[in scalar] The size of the in state buffer (in natural-sized units).
<p>
<dt> <var>out_state</var>
<dd>
[out structure] The state the thread will have if continued from the 
point of the exception. The maximum size of this array is 
THREAD_STATE_MAX.
<p>
<dt> <var>out_state_count</var>
<dd>
[pointer to out scalar] The size of the out state buffer (in natural-sized units).
     </dl>
<h3>DESCRIPTION</h3>
<p>
A <strong>catch_exception_raise</strong> function is called by
<strong>exc_server</strong> as the result of a
kernel message indicating that an exception occurred within a thread. 
The <var>exception_port</var> parameter specifies the port named via
a previous call to <strong>thread_set_exception_ports</strong> or
<strong>task_set_exception_ports</strong>
as the port that responds when the thread takes an
exception.
<p>
The alternate message forms (the format being selected when the exception port 
was set) allow for selected thread state to be included.

<h3>NOTES</h3>
<p>
When an exception occurs in a thread, the thread sends an exception message to 
its exception port, blocking in the kernel waiting for the receipt of a reply. It is 
assumed that some task is listening
(most likely with <strong>mach_msg_server</strong>) to this 
port, using the <strong>exc_server</strong> function
to decode the messages and then call the 
linked in <strong>catch_exception_raise</strong>.
It is the job of <strong>catch_exception_raise</strong> to handle
the exception and decide the course of action for thread.
     <p>
If the thread should continue from the point of exception, 
<strong>catch_exception_raise</strong> would return KERN_SUCCESS. This causes a reply 
message to be sent to the kernel, which will allow the thread to continue from 
the point of the exception.
If some other action should be taken by thread, the following actions should be 
performed by <strong>catch_exception_raise</strong>:
     <dl>
       <dt> <strong>thread_suspend</strong>
       <dd>
		 This keeps the thread from proceeding after the next step.
	    <p>
<dt> <strong>thread_abort</strong>
		 <dd>
		  This aborts the message receive operation currently blocking 
the thread.
		      <p>
<dt> <strong>thread_set_state</strong>
     <dd>
	  (if using the <strong>catch_exception_raise</strong> form). Set the 
thread's state so that it continues doing something else.
	  <p>
	  <dt> <strong>thread_resume</strong>
	       <dd>
		    Let the thread start running from its new state.
</dl>
Returning a value other than KERN_SUCCESS insures that no reply message
will be sent.
sent. (Actually, the kernel uses a send once right to send the exception
message, which <strong>thread_abort</strong> destroys, so replying to the message is harmless.)
The thread can always be destroyed with <strong>thread_terminate</strong>.
<p>
A thread can have two exception ports active for it: its thread type specific exception
port and the task type specific exception port. The kernel will try sending
an exception message to both ports looking for a reply message with a 
return value of KERN_SUCCESS. The kernel tries the thread specific port first, 
then the task specific port. If the return value from the first exception message 
the kernel sends has a return value of KERN_SUCCESS, the thread continues 
(with a possibly modified state). If the return value is not KERN_SUCCESS, 
the kernel tries the second port. If that return value is KERN_SUCCESS, the 
thread continues;
<h2>clock_alarm</h2>
<hr>
<p>
<strong>Function</strong> - Set off an alarm.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_alarm</strong>
                <strong>(clock_t</strong>                             <var>clock_name</var>,
                 <strong>alarm_type_t</strong>                        <var>alarm_type</var>,
                 <strong>tvalspec_t</strong>                          <var>alarm_time</var>,
                 <strong>mach_port_t</strong>                   <var>alarm_reply_port</var><strong>);
<h2>clock_alarm_reply</h2>
<hr>
<p>
<strong>Function</strong> - Ring a preset alarm.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_alarm_reply</strong>
                <strong>(reply_port_t</strong>                  <var>alarm_reply_port</var>,
                 <strong>kern_return_t</strong>                       <var>reply_code</var>,
                 <strong>alarm_type_t</strong>                        <var>alarm_type</var>,
                 <strong>tvalspec_t</strong>                           <var>wake_time</var><strong>);
<h2>clock_get_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Return attributes of a clock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_get_attributes</strong>
                <strong>(clock_t</strong>                             <var>clock_name</var>,
                 <strong>clock_flavor_t</strong>                          <var>flavor</var>,
                 <strong>clock_attr_t</strong>                         <var>attribute</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>attribute_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>clock_name</var> 
<dd>
[in clock-name send right]
The name (or control) port for the clock.
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
Type of information desired.  Defined values are:
<dl>
<p>
<dt> <strong>CLOCK_GET_TIME_RES</strong>
<dd>
The resolution, in nanoseconds, with which the value returned 
by <strong>clock_get_time</strong> is updated.
<p>
<dt> <strong>CLOCK_MAP_TIME_RES</strong>
<dd>
The resolution, in nanoseconds, with which the value visible 
via <strong>clock_map_time</strong> is updated.
<p>
<dt> <strong>CLOCK_ALARM_CURRES</strong>
<dd>
The resolution, in nanoseconds, at which clock alarm and 
sleep timers are currently serviced.
<p>
<dt> <strong>CLOCK_ALARM_MINRES</strong>
<dd>
The minimum resolution, in nanoseconds, at which clock 
alarm and sleep timers can be serviced.
<p>
<dt> <strong>CLOCK_ALARM_MAXRES</strong>
<dd>
The maximum resolution, in nanoseconds, at which clock 
alarm and sleep timers can be serviced.
</dl>
<p>
<dt> <var>attribute</var> 
<dd>
[out scalar]
The returned attribute.
<p>
<dt> <var>attribute_count</var> 
<dd>
[in/out scalar]
On input, the maximum size of the buffer;
<h2>clock_get_time</h2>
<hr>
<p>
<strong>Function</strong> - Return the current time.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_get_time</strong>
                <strong>(clock_t</strong>                             <var>clock_name</var>,
                 <strong>tvalspec_t</strong>                            <var>cur_time</var><strong>);
<h2>clock_map_time</h2>
<hr>
<p>
<strong>Function</strong> - Return a memory object that maps a clock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_map_time</strong>
                <strong>(clock_t</strong>                             <var>clock_name</var>,
                 <strong>memory_object_t</strong>                   <var>clock_memory</var><strong>);
<h2>clock_reply_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel-generated alarm.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	clock_reply_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>clock_set_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Set a particular clock's attributes.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_set_attributes</strong>
                <strong>(clock_ctrl_t</strong>                     <var>clock_control</var>,
                 <strong>clock_flavor_t</strong>                          <var>flavor</var>,
                 <strong>clock_attr_t</strong>                         <var>attribute</var>,
                 <strong>clock_control</strong>                  <var>attribute_count</var><strong>);
<h2>clock_set_time</h2>
<hr>
<p><strong>Function</strong> - Set the current time.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_set_time</strong>
                <strong>(clock_ctrl_t</strong>                     <var>clock_control</var>,
                 <strong>tvalspec_t</strong>                            <var>new_time</var><strong>);
<h2>clock_sleep</h2>
<hr>
<p>
<strong>System Trap</strong> - Sleep until a given time.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   clock_sleep</strong>
                <strong>(mach_port_t</strong>                         <var>clock_name</var>,
                 <strong>sleep_type_t</strong>                        <var>sleep_type</var>,
                 <strong>tvalspec_t</strong>                          <var>sleep_time</var>,
                 <strong>clock_name</strong>                           <var>wake_time</var><strong>);
<h2>default_pager_add_segment</h2>
<hr>
<p>
<strong>Server Interface</strong> - Add additional backing storage for a default pager.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/default_pager_object.h&gt</strong>

<strong>kern_return_t   default_pager_add_segment</strong>
                <strong>(mach_port_t</strong>                      <var>backing_store</var>,
                 <strong>mach_port_t</strong>                             <var>device</var>,
                 <strong>recnum_t</strong>                                <var>offset</var>,
                 <strong>recnum_t</strong>                                 <var>count</var>,
                 <strong>int</strong>                                <var>record_size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>backing_store</var>
<dd>
[in backing store (receive) right] The backing store port.
<p>
<dt> <var>device</var>
<dd>
[in device port] The port for the device containing the backing storage 
partition.
<p>
<dt> <var>offset</var>
<dd>
[in scalar] The offset, in <var>record_size units</var>, to the beginning of the
backing storage on the device.
<p>
<dt> <var>count</var>
<dd>
[in scalar] The number of <var>record_size</var> units
in the partition/segment.
<p>
<dt> <var>record_size</var>
<dd>
[in scalar] The size, in bytes, of the storage device record.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>default_pager_add_segment</strong> function is called to add a partition to
a default pager's backing storage (i.e. expand the amount of backing
storage available to a memory manager). The kernel does not make
this call itself (which is why it can be a synchronous call);
<h2>default_pager_info</h2>
<hr>
<p>
<strong>Server Interface</strong> - Furnish caller with information about the pager's paging partition.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   default_pager_info</strong>
                <strong>(mach_port_t</strong>                              <var>pager</var>,
                 <strong>default_pager_info_t</strong>                      <var>info</var><strong>);
</strong>


<strong>kern_return_t   seqnos_default_pager_info</strong>
                <strong>(mach_port_t</strong>                              <var>pager</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>default_pager_info_t</strong>                     <var>*info</var><strong>);
</strong>

</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>pager</var> 
<dd>
[in default-pager (receive) right]
The default memory manager service 
port.
<p>
<dt> <var>seqno</var> 
<dd>
[in scalar]
The sequence number of this message relative to the pager 
port.
<p>
<dt> <var>info</var> 
<dd>
[out structure]
Total and free space consumption.
</dl>
<h3>DESCRIPTION</h3>
<p>
A <strong>default_pager_info</strong> function is called as the result
of a message requesting 
that the default memory manager return information concerning the default
pager's paging partitions.  The kernel does not make this call
itself (which is why it 
can be a synchronous call);
<h2>device_close</h2>
<hr>
<p>
<strong>Function</strong> - De-establish a connection to a device.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt device/device.h&gt</strong>

<strong>kern_return_t	device_close</strong>
		<strong>(mach_port_t</strong>	<var>device</var><strong>);
<h2>device_get_status</h2>
<hr>
<p>
<strong>Function</strong> - Return the current device status.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt device/device.h&gt</strong>

<strong>kern_return_t   device_get_status</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>dev_flavor_t</strong>                            <var>flavor</var>,
                 <strong>dev_status_t</strong>                            <var>status</var>,
                 <strong>mach_msg_type_number_t</strong>           <var>*status_count</var><strong>);
<h2>device_map</h2>
<hr>
<p>
<strong>Function</strong> - Establish the specified device's memory manager.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt device/device.h&gt</strong>

<strong>kern_return_t   device_map</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>vm_prot_t</strong>                                 <var>prot</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>mach_port_t</strong>                          <var>mem_obj_t</var>,
                 <strong>boolean_t</strong>                                <var>unmap</var><strong>);
<h2>device_open</h2>
<hr>
<p>
<strong>Function</strong> - Establish a connection to a device.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h&gt</strong>

<strong>kern_return_t   device_open</strong>
                <strong>(mach_port_t</strong>                        <var>master_port</var>,
                 <strong>mach_port_t</strong>                             <var>ledger</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>security_token_t</strong>                   <var>security_id</var>,
                 <strong>dev_name_t</strong>                                <var>name</var>,
                 <strong>mach_port_t</strong>                             <var>device</var><strong>);
</strong>


<strong>#include&ltdevice/device_request.h&gt</strong>

<strong>kern_return_t   device_open_request</strong>
                <strong>(mach_port_t</strong>                        <var>master_port</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>mach_port_t</strong>                             <var>ledger</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>security_token_t</strong>                   <var>security_id</var>,
                 <strong>dev_name_t</strong>                                <var>name</var><strong>);
</strong>


<strong>kern_return_t   ds_device_open_reply</strong>
                <strong>(mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>mach_port_t</strong>                             <var>device</var><strong>);
<h2>device_read</h2>
<hr>
<p>
<strong>Function</strong> - Read a sequence of bytes from a specific device.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h&gt</strong>

<strong>kern_return_t   device_read</strong>
                <strong>(device_t</strong>                                <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var>,
                 <strong>io_buf_ptr_t</strong>                      <var>io_buf_ptr_t</var>,
                 <strong>mach_msg_type_number_t</strong>  <var>mach_msg_type_number_t</var><strong>);
</strong>


<strong>#include&ltdevice/device_request.h&gt</strong>

<strong>kern_return_t   device_read_request</strong>
                <strong>(mach_port_t</strong>                            <var>device</var>,
                 <strong>mach_port_t</strong>                        <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
</strong>



<strong>kern_return_t   ds_device_read_reply</strong>
                <strong>(mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>io_buf_ptr_t</strong>                              <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
<h2>device_read_async</h2>
<hr>
<p>
<strong>System Trap</strong> - Read a sequence of bytes from a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   device_read_async</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                              <var>queue</var>,
                 <strong>mach_port_t</strong>                         <var>request_id</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
<h2>device_read_async_inband</h2>
<hr>
<p>
<strong>System Trap</strong> - Read a sequence of bytes "inband" from a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   device_read_async_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                              <var>queue</var>,
                 <strong>mach_port_t</strong>                         <var>request_id</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
<h2>device_read_inband</h2>
<hr>
<p>
<strong>Function</strong> - Read a sequence of bytes "inband" from a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h&gt</strong>

<strong>kern_return_t   device_read_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var>,
                 <strong>io_buf_ptr_inband_t</strong>                       <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>             <var>*data_count</var><strong>);
</strong>


<strong>#include&ltdevice/device_request.h&gt</strong>

<strong>kern_return_t   device_read_request_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
</strong>


<strong>kern_return_t   ds_device_read_reply_inband</strong>
                <strong>(mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>io_buf_ptr_inband_t</strong>                       <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
<h2>device_read_overwrite</h2>
<hr>
<p><strong>System Trap</strong> -- Read a sequence of bytes from a specific device into my address space.
<h3>SYNOPSIS</h3>
<pre>

<strong>kern_return_t   device_read_overwrite</strong>
                <strong>(mach_port_t</strong>                               <var>device</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var>,
                 <strong>io_buf_pointer_t</strong>                          <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
</strong>



<strong>kern_return_t   device_read_overwrite_request</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var>,
                 <strong>io_buf_pointer_t</strong>                          <var>data</var><strong>);
</strong>



<strong>kern_return_t   ds_device_read_reply_overwrite</strong>
                <strong>(mach_port_t</strong>                        <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>io_buf_len_t</strong>                        <var>data_count</var><strong>);
<h2>device_reply_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle incoming data from kernel device driver.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	device_reply_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_msg</var><strong>);
<h2>device_set_filter</h2>
<hr>
<p>
<strong>Function</strong> - Name an input filter for a device.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt device/device.h&gt</strong>
<strong>#include&lt device/net_status.h&gt</strong>

<strong>kern_return_t   device_set_filter</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                       <var>receive_port</var>,
                 <strong>mach_msg_type_name_t</strong>         <var>receive_port_type</var>,
                 <strong>int</strong>                                   <var>priority</var>,
                 <strong>filter_array_t</strong>                          <var>filter</var>,
                 <strong>mach_msg_type_number_t</strong>             <var>filter_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>device</var> 
<dd>
[in device send right]
A device port
<p>
<dt> <var>receive_port</var> 
<dd>
[in filter send or receive (to be converted to send) right]
The port to
receive the input data that is selected by the filter.
<p>
<dt> <var>receive_port_type</var> 
<dd>
[in scalar]
IPC type of the send right provided to the device;
<h2>device_set_status</h2>
<hr>
<p>
<strong>Function</strong> - Set device status.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt device/device.h&gt</strong>

<strong>kern_return_t   device_set_status</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>dev_flavor_t</strong>                            <var>flavor</var>,
                 <strong>dev_status_t</strong>                            <var>status</var>,
                 <strong>mach_msg_type_number_t</strong>            <var>status_count</var><strong>);
<h2>device_write</h2>
<hr>
<p>
<strong>Function</strong> - Write a sequence of bytes to a specific device.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h&gt</strong>

<strong>kern_return_t   device_write</strong>
                <strong>(device_t</strong>                                <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_t</strong>                              <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var>,
                 <strong>io_buf_len_t</strong>                      <var>io_buf_len_t</var><strong>);
</strong>


<strong>#include&ltdevice/device_request.h&gt</strong>

<strong>kern_return_t   device_write_request</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_t</strong>                              <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
</strong>


<strong>kern_return_t   ds_device_write_reply</strong>
                <strong>(mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>io_buf_len_t</strong>                     <var>bytes_written</var><strong>);
<h2>device_write_async</h2>
<hr>
<p>
<strong>System Trap</strong> - Write a sequence of bytes to a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   device_write_async</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                              <var>queue</var>,
                 <strong>mach_port_t</strong>                         <var>request_id</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_t</strong>                              <var>data</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
<h2>device_write_async_inband</h2>
<hr>
<p>
<strong>System Trap</strong> - Write a sequence of bytes "inband" to a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   device_write_async_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                              <var>queue</var>,
                 <strong>mach_port_t</strong>                         <var>request_id</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_t</strong>                              <var>data</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var><strong>);
<h2>device_write_inband</h2>
<hr>
<p>
<strong>Function</strong> - Write a sequence of bytes "inband" to a device object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h (device_write_inband)&gt</strong>

<strong>kern_return_t   device_write_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_inband_t</strong>                       <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var>,
                 <strong>io_buf_len_t</strong>                      <var>io_buf_len_t</var><strong>);
</strong>


<strong>#include&ltdevice/device_request.h&gt</strong>

<strong>kern_return_t   device_write_request_inband</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_ptr_inband_t</strong>                       <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
</strong>


<strong>kern_return_t   ds_device_write_reply_inband</strong>
                <strong>(mach_port_t</strong>                         <var>reply_port</var>,
                 <strong>kern_return_t</strong>                      <var>return_code</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_writte</var><strong>);
<h2>do_mach_notify_port_deleted</h2>
<hr>
<p>
<strong>Server Interface</strong> - Handle the current instance of a port-deleted notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   do_mach_notify_port_deleted</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
</strong>


<strong>kern_return_t   do_seqnos_mach_notify_port_deleted</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
<h2>do_mach_notify_port_destroyed</h2>
<hr>
<p>
<strong>Server Interface</strong> - Handle the current instance of a port-destroyed notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   do_mach_notify_port_destroyed</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_receive_t</strong>                       <var>name</var><strong>);
</strong>


<strong>kern_return_t   do_seqnos_mach_notify_port_destroyed</strong>
                <strong>(mach_port_t</strong>                             <var>notify</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>mach_port_receive_t</strong>                       <var>name</var><strong>);
<h2>do_mach_notify_dead_name</h2>
<hr>
<strong>Server Interface</strong> - Handle the current instance of a dead-name notification.
<p>
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   do_mach_notify_dead_name</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
</strong>


<strong>kern_return_t   do_seqnos_mach_notify_dead_name</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
<h2>do_mach_notify_no_senders</h2>
<hr>
<p>
<strong>Server Interface</strong> - Handle the current instance of a no-more-senders notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   do_mach_notify_no_senders</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_mscount_t</strong>                    <var>mscount</var><strong>);
</strong>


<strong>kern_return_t   do_seqnos_mach_notify_no_senders</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>mach_port_mscount_t</strong>                    <var>mscount</var><strong>);
<h2>do_mach_notify_send_once</h2>
<hr>
<p>
<strong>Server Interface</strong> - Handle the current instance of a send-once notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   do_mach_notify_send_once</strong>
                <strong>(notify_port_t</strong>                          <var>notify</var><strong>)</strong>


<strong>kern_return_t   do_seqnos_mach_notify_send_once</strong>
                <strong>(notify_port_t</strong>                           <var>notify</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var><strong>);
<h2>default_pager_backing_store_create</h2>
<hr>
<p>
<strong>Server Interface</strong> - Create a backing storage object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/default_pager_object.h&gt</strong>

<strong>kern_return_t   default_pager_backing_store_create</strong>
                <strong>(mach_port_t</strong>                              <var>pager</var>,
                 <strong>int</strong>                                   <var>priority</var>,
                 <strong>int</strong>                                     <var>clsize</var>,
                 <strong>mach_port_t</strong>                      <var>backing_store</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>pager</var>
<dd>
[in default pager (receive) right] The default pager service port.
<p>
<dt> <var>priority</var>
<dd>
[in scalar] The scheduling priority for the backing store service 
thread(s).
<p>
<dt> <var>clsize</var>
<dd>
[in scalar] The preferred cluster size (in bytes) for the backing
store object.
<p>
<dt> <var>backing_store</var>
<dd>
[out backing store (receive) right] The port used to manipulate the
created backing store.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>default_pager_backing_store_create</strong> function is called to create a
new backing storage object. The kernel does not make this call itself
(which is why it can be a synchronous call);
<h2>default_pager_backing_store_delete</h2>
<hr>
<p>
<strong>Server Interface</strong> - Delete a backing storage object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/default_pager_object.h&gt</strong>

<strong>kern_return_t   default_pager_backing_store_delete</strong>
                <strong>(mach_port_t</strong>                      <var>backing_store</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>backing_store</var>
<dd>
[in backing store (receive) right] The backing store port created by 
default_pager_backing_store_create.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>default_pager_backing_store_delete</strong> function is called to destroy a
backing storage object created by
<strong>default_pager_backing_store_create</strong>. The kernel does not make this call
itself (which is why it can be a synchronous call);
<h2>default_pager_backing_store_info</h2>
<hr>
<p>
<strong>Server Interface</strong> - Return information about a backing storage object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt default_pager/mach/default_pager_types.h&gt</strong>

<strong>kern_return_t   default_pager_backing_store_info</strong>
                <strong>(mach_port_t</strong>                      <var>backing_store</var>,
                 <strong>backing_store_flavor_t</strong>                  <var>flavor</var>,
                 <strong>backing_store_info_t</strong>                      <var>info</var>,
                 <strong>mach_msg_type_number_t</strong>                    <var>size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>backing_store</var>
<dd>
[in backing store (receive) right] The backing store port for which
information is desired.
<p>
<dt> <var>flavor</var>
<dd>
[in scalar] The type of information to be returned. Valid values are:
<p>
<dt> <var>BACKING_STORE_BASIC_INFO</var>
<dd>
Statistical and space used/available information. It includes 
the priority and cluster size that was provided in the 
default_pager_backing_store_create call.
<p>
<dt> <var>info</var>
<dd>
[pointer to in structure] The data structure that will be filled in with the 
information provided for the requested flavor.
<p>
<dt> <var>size</var>
<dd>
[pointer to in/out scalar] On input, the maximum size of the info data 
structure;
on output, the actual size of the returned data.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>default_pager_backing_store_info</strong> function is called to obtain
information about a backing storage object created by
<strong>default_pager_backing_store_create</strong>. The kernel does not make this call
itself (which is why it can be a synchronous call);
<h2>default_pager_object_create</h2>
<hr>
<p>
<strong>Server Interface</strong> - Initialize a non-persistent memory object suitable for sharing between tasks.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   default_pager_object_create</strong>
                <strong>(mach_port_t</strong>                              <var>pager</var>,
                 <strong>memory_object_t</strong>                 <var>*memory_object</var>,
                 <strong>vm_size_t</strong>                          <var>object_size</var><strong>);
</strong>


<strong>kern_return_t   seqnos_default_pager_object_create</strong>
                <strong>(mach_port_t</strong>                              <var>pager</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_t</strong>                 <var>*memory_object</var>,
                 <strong>vm_size_t</strong>                          <var>object_size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>pager</var> 
<dd>
[in default-pager (receive) right]
The default memory manager service 
port.
<p>
<dt> <var>seqno</var> 
<dd>
[in scalar]
The sequence number of this message relative to the <var>pager</var> 
port.
<p>
<dt> <var>memory_object</var> 
<dd>
[out memory-object send right]
A memory object port (with full access) for the memory object.
<p>
<dt> <var>object_size</var> 
<dd>
[in scalar]
The maximum size for the memory object.
</dl>
<h3>DESCRIPTION</h3>
<p>
A <strong>default_pager_object_create</strong> function is called as
the result of a message
requesting that the default memory manager create and return a (shared) memory 
object which is suitable for use with <strong>vm_map</strong>.  This memory object has 
the same properties as does a memory object provided by 
<strong>vm_allocate</strong>: its initial 
contents are zero and the backing contents are temporary in that they do not
persist after the memory object is destroyed.  The memory object
is suitable for use 
as non-permanent shared memory.  The kernel does not make this call itself 
(which is why it can be a synchronous call);
<h2>device_read_overwrite_async</h2>
<hr>
<p>
<strong>System Trap</strong> - Read a sequence of bytes from a device object into the caller's 
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   device_read_overwrite_async</strong>
                <strong>(mach_port_t</strong>                             <var>device</var>,
                 <strong>mach_port_t</strong>                              <var>queue</var>,
                 <strong>mach_port_t</strong>                         <var>request_id</var>,
                 <strong>dev_mode_t</strong>                                <var>mode</var>,
                 <strong>recnum_t</strong>                                <var>recnum</var>,
                 <strong>io_buf_len_t</strong>                      <var>bytes_wanted</var>,
                 <strong>io_buf_ptr_t</strong>                            <var>buffer</var><strong>);
<h2>etap_get_info</h2>
<hr>
<p>
<strong>Function</strong> - Map ETAP buffers and tables into server's address space.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/etap.h&gt</strong>

<strong>kern_return_t   etap_get_info</strong>
                <strong>(host_priv_t</strong>                          <var>priv_port</var>,
                 <strong>int</strong>                                        <var>int</var>,
                 <strong>int</strong>                                        <var>int</var>,
                 <strong>vm_offset_t</strong>                        <var>vm_offset_t</var>,
                 <strong>vm_offset_t</strong>                        <var>vm_offset_t</var>,
                 <strong>int</strong>                                        <var>int</var>,
                 <strong>int</strong>                                        <var>int</var>,
                 <strong>int</strong>                                        <var>int</var>,
                 <strong>int</strong>                                        <var>int</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>priv_port</var>
<dd>
the name of the Server's privileged device port (server_device_port),
     granting the Server access to this service. 
<p>
<dt> <var>et_entries</var>
<dd>
used to return number of entries in the event table.
<p>
<dt> <var>st_entries</var>
<dd>
used to return number of entries in the subsystem table.
<p>
<dt> <var>et_offset</var>
<dd>
used to return the event table's page offset.
<p>
<dt> <var>st_offset</var>
<dd>
used to return the subsystem table's page offset.
<p>
<dt> <var>cb_width</var>
<dd>
returns the current cumulative buffer interval width.
<p>
<dt> <var>mb_size</var>
<dd>
returns the size of the monitored buffers, 
<p>
<dt> <var>mb_entries</var>
<dd>
returns the maximum number of entries in a monitored buffer.
<p>
<dt> <var>mb_cpus</var>
<dd>
returns the number of allocated monitored buffers (or supported CPUs).
</dl>
<h3>DESCRIPTION</h3>
<p>
The  <strong>etap_get_info</strong> interface provides the user space
etap server or daemon with configuration and location information pertaining
to the kernel's internal ETAP buffers;
<h2>etap_probe</h2>
<hr>
<p>
<strong>System Trap</strong> - Record event data in the kernel's
ETAP buffer(s).
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/etap.h&gt</strong>

<strong>kern_return_t   etap_probe</strong>
                <strong>(int</strong>                                   <var>event_id</var>,
                 <strong>event_id</strong>                             <var>data_size</var>,
                 <strong>etap_data_t</strong>                        <var>etap_data_t</var><strong>);
<h2>etap_trace_event</h2>
<hr>
<p>
<strong>System Trap</strong> -
manipulate event probes and lock event tracing.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/etap.h&gt</strong>

<strong>kern_return_t   etap_trace_event</strong>
                <strong>(etap_data_t</strong>                               <var>mode</var>,
                 <strong>mode</strong>                                      <var>type</var>,
                 <strong>boolean_t</strong>                               <var>enable</var>,
                 <strong>enable</strong>                                   <var>nargs</var>,
                 <strong>mode</strong>                                      <var>mode</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>mode</var>
<dd>
indicates the desired trace flavor or reset option and may be one,
or more, of the following:
<ul>
<p>
  <li>
ETAP_CUMULATIVE: cumulative lock event trace mode.
     <p>
<li>
ETAP_MONITORED: monitored event trace mode (for probes or locks)
<p>
<li>
ETAP_RESET: reset mode, clears all trace status and cumulative buffer entries
</ul>
<p>
<dt> <var>type</var>
<dd>
used when measuring lock event contention or durations
and may be one, or more, of the following:
<ul>
<p>
  <li>
ETAP_CONTENT
<p>
  <li>
ETAP_DURATION
</ul>
<p>
<dt> <var>enable</var>
<dd>
a boolean value  indicattin whether the event trace operation is
to be enabled (TRUE) or disabled (FALSE). 
<p>
<dt> <var>nargs</var>
<dd>
specifies how many arguments are passed in the args array.
<p>
<dt> <var>args</var>
<dd>
an array, each element of which is a character string
representing a specific subsystem or event type. These values must
correspond to the ones the kernel uses to represent the same
subsystems and event types. The maximum length of a character string
is EVENT_NAME_LENGTH (defined in <strong>mach/etap.h</strong>).
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>etap_trace_event</strong> system call is used to enable
and disable kernel event probes (of a specified type) and all modes of lock event
tracing. The call also supports a reset option, where the cumulative
buffer data and all event type tracing is reset to zero. When the
reset option is used, a new interval width can also be defined, using
the <var>nargs</var> parameter.
<p>
To reset the ETAP instrumentation,
the system call would utilize the mode parameter, passing the value of
ETAP_RESET (All other parameters may equal NULL). If, at the time of
reset, the <var>nargs</var> parameter is assigned a value, then the
cumulative buffer interval width will be adjusted to be the size of
that value.  For example, the following system call would reset the
ETAP instrumentation and adjust the cumulative buffer's interval width
to 100ms:
<pre>
         etap_trace_event(ETAP_RESET, 0, 0, 100, 0);
<h2>etap_trace_thread</h2>
<hr>
<p>
<strong>Function</strong> - Set a thread's ETAP trace status.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/etap.h&gt</strong>

<strong>kern_return_t   etap_trace_thread</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var>,
                 <strong>boolean_t</strong>                               <var>active</var><strong>);
<h2>evc_wait</h2>
<hr>
<p>
<strong>System Trap</strong> - Wait for a kernel (device) signalled event.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	evc_wait</strong>
		<strong>(unsigned int</strong>	<var>event</var><strong>);
<h2>exc_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel-reported thread exception.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	exc_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>host_default_memory_manager</h2>
<hr>
<p>
<strong>Function</strong> - Establish the official connection between the kernel and its default pager task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_default_memory_manager</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>mach_port_make_send_t</strong>          <var>default_manager</var>,
                 <strong>vm_size_t</strong>                         <var>cluster_size</var><strong>);
<h2>host_adjust_time</h2>
<hr>
<p>
<strong>Function</strong> - Gradually change the time.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t   host_adjust_time</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>time_value_t</strong>                    <var>new_adjustment</var>,
                 <strong>time_value_t</strong>                    <var>old_adjustment</var><strong>);
<h2>host_get_boot_info</h2>
<hr>
<p>
<strong>Function</strong> - Return operator boot information.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_get_boot_info</strong>
                <strong>(host_priv_t</strong>                          <var>priv_host</var>,
                 <strong>kernel_boot_info_t</strong>                   <var>boot_info</var><strong>);
<h2>host_get_clock_control</h2>
<hr>
<p>
<strong>Function</strong> - Return a send right to a kernel clock's control port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_get_clock_control</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>clock_id_t</strong>                                  <var>id</var>,
                 <strong>clock_ctrl_t</strong>                     <var>clock_control</var><strong>);
<h2>host_get_clock_service</h2>
<hr>
<p>
<strong>Function</strong> - Return a send right to a kernel clock's service port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_get_clock_service</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>clock_id_t</strong>                                  <var>id</var>,
                 <strong>clock_t</strong>                             <var>clock_name</var><strong>);
<h2>host_get_time</h2>
<hr>
<p>
<strong>Function</strong> - Return the current time.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t   host_get_time</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>time_value_t</strong>                      <var>current_time</var><strong>);
<h2>host_info</h2>
<hr>
<p>
<strong>Function</strong> - Return information about a host.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_info</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>host_flavor_t</strong>                           <var>flavor</var>,
                 <strong>host_info_t</strong>                          <var>host_info</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>host_info_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>host</var> 
<dd>
[in host-name send right]
The name (or control) port for the host for 
which information is to be obtained.
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of statistics desired:
<dl>
<p>
<dt> <strong>HOST_BASIC_INFO</strong>
<dd>
Basic information (number of processors, amount of
memory).  The returned structure is <strong>host_basic_info</strong>.
<p>
<dt> <strong>HOST_SCHED_INFO</strong>
<dd>
Basic restrictions of the kernel's scheduling, minimum
quantum and time-out value.  The returned structure is 
<strong>host_sched_info</strong>.
<p>
<dt> <strong>HOST_RESOURCE_SIZES</strong>
<dd>
This interface feature is not implemented in OSF/1 R1.3.
Size of significant kernel structures, as a ledger would
consider them when limiting kernel resource consumption.  The
returned structure is <strong>kernel_resource_sizes</strong>.
</dl>
<p>
<dt> <var>host_info</var> 
<dd>
[out structure]
Statistics about the specified host.
<p>
<dt> <var>host_info_count</var> 
<dd>
[in/out scalar]
On input, the maximum size of the buffer;
<h2>host_kernel_version</h2>
<hr>
<p>
<strong>Function</strong> - Return kernel version information for a host.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_kernel_version</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>kernel_version_t</strong>                       <var>version</var><strong>);
<h2>host_page_size</h2>
<hr>
<p>
<strong>Function</strong> - Provide the system's virtual page size.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_page_size</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>vm_size_t</strong>                            <var>page_size</var><strong>);
<h2>host_processors</h2>
<hr>
<p>
<strong>Function</strong> - Return a list of send rights representing all processor ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_processors</strong>
                <strong>(host_t</strong>                               <var>host_priv</var>,
                 <strong>processor_port_array_t</strong>          <var>processor_list</var>,
                 <strong>host_priv</strong>                      <var>processor_count</var><strong>);
<h2>host_processor_sets</h2>
<hr>
<p>
<strong>Function</strong> - Return a list of send rights representing all processor set name ports.

<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_processor_sets</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>processor_set_name_port_array_t</strong><var>processor_set_name_list</var>,
                 <strong>host_priv</strong>             <var>processor_set_name_count</var><strong>);
<h2>host_processor_set_priv</h2>
<hr>
<p>
<strong>Function</strong> - Translate a processor set name port
  into a processor set control port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_processor_set_priv</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>processor_set_name_t</strong>                  <var>set_name</var>,
                 <strong>processor_set_t</strong>                  <var>processor_set</var><strong>);
<h2>host_processor_slots</h2>
<hr>
<p>
<strong>Function</strong> - Return a list of numbers that map processor slots to active processors.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_processor_slots</strong>
                <strong>(host_t</strong>                                    <var>host</var>,
                 <strong>processor_slot_t</strong>                         <var>slots</var>,
                 <strong>host</strong>                                     <var>count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>host</var> 
<dd>
[in host-name send right]
The name (or control) port for the host for 
which information is to be obtained.
<p>
<dt> <var>slots</var> 
<dd>
[out array of <var>processor_slot_t</var>]
An array of the processor slot numbers 
for active processors.
<p>
<dt> <var>count</var> 
<dd>
[pointer to in/out scalar]
On input, the maximum size of the slots
buffer;
<h2>host_reboot</h2>
<hr>
<p>
<strong>Function</strong> - Reboot this host.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_reboot</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>int</strong>                                    <var>options</var><strong>);
<h2>host_security_create_task_token</h2>
<hr>
<p>
<strong>Function</strong> - Create a new task with an explicit security token.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_security_create_task_token</strong>
                <strong>(host_security_t</strong>                  <var>host_security</var>,
                 <strong>task_t</strong>                             <var>parent_task</var>,
                 <strong>security_token_t</strong>                <var>security_token</var>,
                 <strong>audit_token_t</strong>                      <var>audit_token</var>,
                 <strong>ledger_port_array_t</strong>                    <var>ledgers</var>,
                 <strong>boolean_t</strong>                       <var>inherit_memory</var>,
                 <strong>task_t</strong>                             <var>child_task</var><strong>);
<h2>host_security_set_task_token</h2>
<hr>
<p>
<strong>Function</strong> - Change the target task's security token.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_security_set_task_token</strong>
                <strong>(host_security_t</strong>                  <var>host_security</var>,
                 <strong>task_t</strong>                                    <var>task</var>,
                 <strong>security_token_t</strong>                <var>security_token</var>,
                 <strong>audit_token_t</strong>                      <var>audit_token</var>,
                 <strong>host_t</strong>                                    <var>host</var><strong>);
<h2>host_set_time</h2>
<hr>
<p>
<strong>Function</strong> - Sets the time.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t   host_set_time</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>time_value_t</strong>                          <var>new_time</var><strong>);
<h2>host_statistics</h2>
<hr>
<p>
<strong>Function</strong> - Return statistics for a host.<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   host_statistics</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>host_flavor_t</strong>                           <var>flavor</var>,
                 <strong>host_info_t</strong>                          <var>host_info</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>host_info_count</var><strong>);
<h2>i386_get_ldt</h2>
<hr>
<p>
<strong>Function</strong> - Return per-thread segment descriptors.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   i386_get_ldt</strong>
                <strong>(thread_act_t</strong>                        <var>target_act</var>,
                 <strong>int</strong>                             <var>first_selector</var>,
                 <strong>int</strong>                              <var>desired_count</var>,
                 <strong>descriptor_list_t</strong>                    <var>desc_list</var><strong>);
<h2>i386_io_port_add</h2>
<hr>
<p>
<strong>Function</strong> - Permit target thread to invoke operations on the specified device.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   i386_io_port_add</strong>
                <strong>(thread_act_t</strong>                        <var>target_act</var>,
                 <strong>device_t</strong>                                <var>device</var><strong>);
<h2>i386_io_port_list</h2>
<hr>
<p>
<strong>Function</strong> - List the devices that permit target thread to invoke operations.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   i386_io_port_list</strong>
                <strong>(thread_act_t</strong>                        <var>target_act</var>,
                 <strong>device_list_t</strong>                      <var>device_list</var><strong>);
<h2>i386_io_port_remove</h2>
<hr>
<p>
<strong>Function</strong> - Disable target thread's ability to invoke operations on the
specified device.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   i386_io_port_remove</strong>
                <strong>(thread_act_t</strong>                        <var>target_act</var>,
                 <strong>device_t</strong>                                <var>device</var><strong>);
<h2>i386_set_ldt</h2>
<hr>
<p>
<strong>Function</strong> - Set per-thread segment descriptors.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   i386_set_ldt</strong>
                <strong>(thread_act_t</strong>                        <var>target_act</var>,
                 <strong>int</strong>                             <var>first_selector</var>,
                 <strong>descriptor_list_t</strong>                    <var>desc_list</var><strong>);
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Mach Kernel Interface Reference Manual</title>
</head>
<body>
<h3>Mach IPC Interface</h3>
<blockquote>
<p>
Mach IPC presents itself in a few forms: message queues, lock-sets, 
and semaphores (more may be added in the future). &nbsp;
<h2>io_done_queue_create</h2>
<hr>
<p>
<strong>Function</strong> - Create an <strong>io_done_queue</strong> kernel object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   io_done_queue_create</strong>
                <strong>(mach_port_t</strong>                               <var>host</var>,
                 <strong>mach_port_t</strong>                               <var>queue</var><strong>);
<h2>io_done_queue_terminate</h2>
<hr>
<p>
<strong>Function</strong> - Terminate an io_done_queue kernel object.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltdevice/device.h&gt</strong>

<strong>kern_return_t	io_done_queue_terminate</strong>
		<strong>(mach_port_t</strong>	<var>queue</var><strong>);
<h2>io_done_queue_wait</h2>
<hr>
<p>
<strong>System Trap</strong> - Wait on an io_done_queue kernel object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	io_done_queue_wait</strong>
		<strong>(mach_port_t</strong>             <var>queue</var>,
		<strong>io_done_result_t</strong>       <var>*result</var><strong>);
<h2>ledger_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a subordinate ledger.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_create</strong>
                <strong>(ledger_port_t</strong>                    <var>parent_ledger</var>,
                 <strong>ledger_port_t</strong>                    <var>ledger_ledger</var>,
                 <strong>ledger_port_t</strong>                     <var>child_ledger</var>,
                 <strong>ledger_item_t</strong>                         <var>transfer</var><strong>);
<h2>ledger_get_remote</h2>
<hr>
<p>
<strong>Function</strong> - Return send right to specified host's remote ledger port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_get_remote</strong>
                <strong>(ledger_port_t</strong>                           <var>ledger</var>,
                 <strong>host_t</strong>                               <var>host_name</var>,
                 <strong>ledger</strong>                            <var>service_port</var><strong>);
</strong>


<strong>kern_return_t   ledger_return_remote</strong>
                <strong>(ledger_port_t</strong>                           <var>ledger</var>,
                 <strong>host_t</strong>                               <var>host_name</var>,
                 <strong>ledger</strong>                            <var>service_port</var><strong>);
<h2>ledger_read</h2>
<hr>
<p>
<strong>Function</strong> - Return the ledger limit and balance.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_read</strong>
                <strong>(ledger_port_t</strong>                           <var>ledger</var>,
                 <strong>ledger_item_t</strong>                          <var>balance</var>,
                 <strong>ledger_item_t</strong>                          <var>maximum</var><strong>);
<h2>ledger_set_remote</h2>
<hr>
<p>
<strong>Function</strong> - Set this host's remote ledger port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_set_remote</strong>
                <strong>(ledger_port_t</strong>                           <var>ledger</var>,
                 <strong>ledger_port_t</strong>                     <var>service_port</var><strong>);
<h2>ledger_terminate</h2>
<hr>
<p>
<strong>Function</strong> - Destroy a ledger.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_terminate</strong>
                <strong>(ledger_port_t</strong>                           <var>ledger</var><strong>);
<h2>ledger_transfer</h2>
<hr>
<p>
<strong>Function</strong> - Transfer resources from a parent ledger to a child.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   ledger_transfer</strong>
                <strong>(ledger_port_t</strong>                    <var>parent_ledger</var>,
                 <strong>ledger_port_t</strong>                     <var>child_ledger</var>,
                 <strong>ledger_item_t</strong>                         <var>transfer</var><strong>);
<h2>lock_acquire</h2>
<hr>
<p>
<strong>Function</strong> - Acquire access rights to a lock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_acquire</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>lock_handoff</h2>
<hr>
<p>
<strong>Function</strong> - Hand-off ownership of a lock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_handoff</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>lock_handoff_accept</h2>
<hr>
<p>
<strong>Function</strong> - Accept a lock hand-off.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_handoff_accept</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>lock_make_stable</h2>
<hr>
<p>
<strong>Function</strong> - Stabilize the state of the specified lock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_make_stable</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>lock_release</h2>
<hr>
<p>
<strong>Function</strong> - Release ownership of a lock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_release</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>lock_set_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a new lock set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_set_create</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                      <var>locks</var>,
                 <strong>int</strong>                                     <var>policy</var><strong>);
<h2>lock_set_destroy</h2>
<hr>
<p>
<strong>Function</strong> - Destroy a lock set and its associated locks.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_set_destroy</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>lock_set_t</strong>                            <var>lock_set</var><strong>);
<h2>lock_try</h2>
<hr>
<p>
<strong>Function</strong> - Attempt to acquire access rights to a lock.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   lock_try</strong>
                <strong>(lock_set_t</strong>                            <var>lock_set</var>,
                 <strong>int</strong>                                    <var>lock_id</var><strong>);
<h2>mach_host_self</h2>
<hr>
<p>
<strong>System Trap</strong> - Returns the host self port
<h3>SYNOPSIS</h3>
<pre>
host_name_port_t   mach_host_self( void );
<h2>mach_msg</h2>
<hr>
<p>
<strong>System Trap</strong> / <strong>Function</strong> - Send and/or receive a message from the target port.
<h3>SYNOPSIS</h3>
<pre>
<strong>mach_msg_return_t   mach_msg</strong>
                    <strong>(mach_msg_header_t</strong>                <var>msg</var>,
                     <strong>mach_msg_option_t</strong>             <var>option</var>,
                     <strong>mach_msg_size_t</strong>            <var>send_size</var>,
                     <strong>mach_msg_size_t</strong>        <var>receive_limit</var>,
                     <strong>mach_port_t</strong>             <var>receive_name</var>,
                     <strong>mach_msg_timeout_t</strong>           <var>timeout</var>,
                     <strong>mach_port_t</strong>                   <var>notify</var><strong>);
</strong>

<strong>mach_msg_return_t   mach_msg_overwrite</strong>
                    <strong>(mach_msg_header_t*</strong>          <var>send_msg</var>,
                     <strong>mach_msg_option_t</strong>             <var>option</var>,
                     <strong>mach_msg_size_t</strong>            <var>send_size</var>,
                     <strong>mach_msg_size_t</strong>        <var>receive_limit</var>,
                     <strong>mach_port_t</strong>             <var>receive_name</var>,
                     <strong>mach_msg_timeout_t</strong>           <var>timeout</var>,
                     <strong>mach_port_t</strong>                   <var>notify</var>,
                     <strong>mach_msg_header_t</strong>       <var>*receive_msg</var>,
                     <strong>mach_msg_size_t</strong>     <var>receive_msg_size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>msg</var>
<dd>
[pointer to in/out structure containing random and reply rights] A
message buffer used by <strong>mach_msg</strong> both for send and receive. This must
be naturally aligned.
<p>
<dt> <var>send_msg</var>
<dd>
[pointer to in structure containing random and reply rights] The mes-
sage buffer to be sent. This must be naturally aligned.
<p>
<dt> <var>option</var>
<dd>
[in scalar] Message options are bit values, combined with bitwise-or.
One or both of MACH_SEND_MSG and MACH_RCV_MSG should be used. Other
options act as modifiers.
<p>
<dt> <var>send_size</var>
<dd>
[in scalar] When sending a message, specifies the size of the message
buffer to be sent (the size of the header and body) in
bytes. Otherwise zero should be supplied.
<p>
<dt> <var>receive_limit</var>
<dd>
[in scalar] When receiving a message, specifies the maximum size of
the msg or receive_msg buffer in bytes. Otherwise zero should be sup-
plied.
<p>
<dt> <var>receive_name</var>
<dd>
[in random right] When receiving a message, specifies the port or port
set. Otherwise MACH_PORT_NULL should be supplied.
<p>
<dt> <var>timeout</var>
<dd>
[in scalar] When using the MACH_SEND_TIMEOUT and MACH_RCV_TIMEOUT
options, specifies the time in milliseconds to wait before giving
up. Otherwise MACH_MSG_TIMEOUT_NONE should be supplied.
<p>
<dt> <var>notify</var>
<dd>
[in notify receive right] When using the MACH_SEND_CANCEL and
MACH_RCV_NOTIFY options, specifies the port used for the
notification. Otherwise MACH_PORT_NULL should be supplied.
<p>
<dt> <var>receive_msg</var>
<dd>
[pointer to in/out structure] A message buffer into which a message
(header and body) will be received. This must be naturally aligned. By
default (<strong>mach_msg</strong>), any received message will overwrite the send
message buffer. This buffer is in/out only if the MACH_RCV_OVERWRITE
option is used;
otherwise this buffer is out only.
<p>
<dt> <var>receive_msg_size</var>
<dd>
[in scalar] When using the MACH_RCV_OVERWRITE option, specifies the
size (in bytes) of the receive "message" that is to be used by
<strong>mach_msg</strong> to indicate the disposition of received out-of-line regions.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>mach_msg</strong> system call sends and receives Mach messages. Mach
messages contain data, which can include port rights and addresses of
large regions of memory. <strong>mach_msg</strong> uses the same buffer for sending and
receiving a message;
the other calls permit separate send and receive
buffers (although they may be specified to be the same).
If the option argument contains MACH_SEND_MSG, the call sends a
message.  The <var>send_size</var> argument specifies the size of the message
buffer (header and body) to send. The msgh_remote_port field of the
message header specifies the destination of the message.
If the option argument contains MACH_RCV_MSG, it receives a
message. The receive_limit argument specifies the size of a buffer
that will receive the message;
messages that are larger are not
received. The receive_name argument specifies the port or port set
from which to receive.
<p>
If the option argument contains both MACH_SEND_MSG and MACH_RCV_MSG,
then <strong>mach_msg</strong> does both send and receive operations (in that
order). If the send operation encounters an error (any return code
other than MACH_MSG_SUCCESS), the call returns immediately
without attempting the receive operation. Semantically the combined
call is equivalent to separate send and receive calls, but it saves
a system call and enables other internal optimizations.

If the option argument specifies neither MACH_SEND_MSG nor
MACH_RCV_MSG, <strong>mach_msg</strong> does nothing.
Some options, like MACH_SEND_TIMEOUT and MACH_RCV_TIMEOUT, share a
supporting argument. If these options are used together, they make
independent use of the supporting argument's value.
<h3>NOTES</h3>
<p>
The Mach kernel provides message-oriented, capability-based
inter-process communication. The inter-process communication (IPC)
primitives efficiently support many different styles of interaction,
including remote procedure calls, object-oriented distributed
programming, streaming of data, and sending very large amounts of
data.
<h4>Major Concepts</h4>
<p>
The IPC primitives operate on three abstractions: messages, ports, and
port sets.  User tasks access all other kernel services and
abstractions via the IPC primitives.
<p>
The message primitives let tasks send and receive messages. Tasks send
messages to ports. Messages sent to a port are delivered reliably
(messages may not be lost) and are received in the order in which they
were sent via send rights by a given sending task (or a given
kernel). (Messages sent to send-once rights are unordered.)
<p>
Messages
contain a fixed-size header and a variable-sized message body
containing kernel and user data, and a variable-size trailer of kernel
appended message attributes. The header describes the destination
and the size of the message (header plus body). The message body
contains descriptions of additional port rights to be transmitted,
descriptions of "out-of-line" memory regions to be sent and a
variable amount of user data, which typically includes type conversion
information. The out-of-line memory regions (including out-of-line
port arrays) are (typically) disjoint from the message body.
The IPC implementation makes use of the VM system to efficiently
transfer large amounts of data. The message can contain the addresses
of regions of the sender's address space which should be transferred
as part of the message.
<p>
When a task receives a message containing
such out-of-line regions of data, the data can appear in unused
portions or overwrite an existing portion of the receiver's address
space (depending on the requested receive options). Under favorable
circumstances, the transmission of out-of-line data is optimized so
that sender and receiver share the physical pages of data
copy-on-write, and no actual data copy occurs unless the pages are
written. Regions of memory up to 4 gigabytes may be sent in this
manner.
<p>
Ports hold a queue of messages. Tasks operate on a port to send and
receive messages by exercising capabilities (rights) for the
port. Multiple tasks can hold send rights for a port.
Tasks can also
hold send-once rights, which grant the ability to send a single
message. Only one task can hold the receive capability (receive
right) for a port.
<p>
Port rights can be transferred between tasks via
messages. The sender of a message can specify in the message that the
message contains a port right. If a message contains a receive right
for a port, the receive right is removed from the sender of the
message and transferred to the receiver of the
message. While the receive right is in transit, tasks holding send
rights can still send messages to the port, and they are queued until
a task acquires the receive right and uses it to receive the messages.
<p>
Tasks can receive messages from ports and port sets. The port set
abstraction allows a single thread to wait for a message from any of
several ports. Tasks manipulate port sets with a port set name,
which is taken from the same name space as are the port rights. The
port-set name may not be transferred in a message. A port set holds
receive rights, and a receive operation on a port set blocks waiting
for a message sent to any of the constituent ports. A port may not be-
long to more than one port set, and if a port is a member of a port
set, the holder of the receive right can't receive directly from the
port.
<p>
Port rights are a secure, location-independent way of naming
ports. The port queue is a protected data structure, only accessible
via the kernel's exported message primitives. Rights are also
protected by the kernel;
there is no way for a malicious user task to
guess a port's internal name and send a message to a port to which it
shouldn't have access. Port rights do not carry any location in-
formation. When a receive right for a port moves from task to task,
and even between tasks on different machines, the send rights for
the port remain unchanged and continue to function.
<h4>Port Rights</h4>
<p>
Each task has its own space of port rights. Port rights are named with
positive (unsigned) integers. For all architectures, sizeof
(mach_port_t) = sizeof (mach_port_name_t) = sizeof (void*) and so user
space addresses may be used as port names, except for the reserved
values MACH_PORT_NULL (0) and MACH_PORT_DEAD (all 1 bits). When the
kernel chooses a name for a new right, however, it is free to pick any
unused name (one which denotes no right) in the space.
<p>
There are three basic kinds of rights: receive rights, send rights and
send-once rights. A port name can name any of these types of rights,
or name a port-set, be a dead name, or name nothing. Dead names are
not capabilities. They act as place-holders to prevent a name from
being otherwise used.
<p>
A port is destroyed, or dies, when its receive right is
de-allocated. When a port dies, send and send-once rights for the port
turn into dead names. Any messages queued at the port are destroyed,
which de-allocates the port rights and out-of-line memory in the
messages.
<p>
Each send-once right held by a task has a different name. In contrast,
when a task holds send rights or a receive right for a port, the
rights share a single name.
<p>
Tasks may hold multiple user-references for send rights. When a task
receives a send right which it already holds, the kernel increments
the right's user-reference count. When a task de-allocates a send
right, the kernel decrements its user-reference count, and the task
only loses the send right when the count goes to zero.
<p>
Send-once rights always have a user reference count of one. Tasks may
hold multiple user references for dead names.
Each send-once right generated guarantees the receipt of a single
message, either a message sent to that send-once right or, if the
send-once right is in any way destroyed, a send-once notification.
<p>
A message can carry port rights;
the msgh_remote or msgh_local fields
in the message header or the disposition field in a message body
descriptor specify the type of port right and how the port right is to
be extracted from the caller. The values MACH_PORT_NULL and
MACH_PORT_DEAD are valid in place of a port right in a message body.
<p>
In a sent message, the following mach_msg_type_name_t values denote
port rights:
<dl>
<dt> MACH_MSG_TYPE_MAKE_SEND
     <dd>
The message will carry a send right, but the caller must supply a
receive right. The send right is created from the receive right, and the
receive right's make-send count is incremented.
<dt> MACH_MSG_TYPE_COPY_SEND
     <dd>
The message will carry a send right, and the caller must supply a send 
right. The user reference count for the supplied send right is not 
changed. The caller may also supply a dead name and the receiving 
task will get MACH_PORT_DEAD.
<dt> MACH_MSG_TYPE_MOVE_SEND
     <dd>
The message will carry a send right, and the caller must supply a send
right. The user reference count for the supplied send right is
decremented, and the right is destroyed if the count becomes
zero. Unless a receive right remains, the name becomes available for
recycling. The caller may also supply a dead name, which loses a user
reference, and the receiving task will get MACH_PORT_DEAD.
<dt> MACH_MSG_TYPE_MAKE_SEND_ONCE
<dd>
The message will carry a send-once right, but the caller must supply a
receive right. The send-once right is created from the receive right.
Note that send once rights can only be created from the receive right.
<dt> MACH_MSG_TYPE_MOVE_SEND_ONCE
     <dd>
The message will carry a send-once right, and the caller must supply a
send-once right. The caller loses the supplied send-once right. The
caller may also supply a dead name, which loses a user reference,
and the receiving task will get MACH_PORT_DEAD.
<dt> MACH_MSG_TYPE_MOVE_RECEIVE
     <dd>
The message will carry a receive right, and the caller must supply a
receive right. The caller loses the supplied receive right, but
retains any send rights with the same name. The make-send count and
sequence number of the receive right are reset to zero and
no-more-senders notification requests are cancelled (with a
send-once notification being sent to the no-more-senders notification
right), but the port retains other attributes like queued messages
and extant send and send-once rights.
If a message carries a send or send-once right, and the port dies
while the message is in transit, then the receiving task will get
MACH_PORT_DEAD instead of a right.
	  </dl>
	  <p>
The following mach_msg_type_name_t values in a received message
indicate that it carries port rights:
	  <dl>
<dt> MACH_MSG_TYPE_PORT_SEND
     <dd>
This value is an alias for MACH_MSG_TYPE_MOVE_SEND. The 
message carried a send right. If the receiving task already has send and/
or receive rights for the port, then that name for the port will be reused. 
Otherwise, the right will have a new, previously unused, name. If the 
task already has send rights, it gains a user reference for the right (un-
less this would cause the user-reference count to overflow). Otherwise, 
it acquires send rights, with a user-reference count of one.
<dt> MACH_MSG_TYPE_PORT_SEND_ONCE
     <dd>
This value is an alias for MACH_MSG_TYPE_MOVE_SEND_ONCE. The message
carried a send-once right. The right will have a new, previously
unused, name.
<dt> MACH_MSG_TYPE_PORT_RECEIVE
     <dd>
This value is an alias for MACH_MSG_TYPE_MOVE_RECEIVE.  The message
carried a receive right. If the receiving task already has send rights
for the port, then that name for the port will be reused;
otherwise,
the right will have a new, previously unused name.
	  </dl>
	  <p>
It is also possible to send a (nearly unbounded) array of port rights
"out-of-line". All of the rights named by the array must be of the
same type. The array is physically copied with the message body
proper. The array of port right (names) can be received by the
receiver using the same options available for out-of-line data
reception described below.
<h4>Memory</h4>
	  <p>
A message can contain one or more regions of the sender's address
space which are to be transferred as part of the message. The message
carries a logical copy of the memory. For this "out-of-line" memory,
the kernel can copy the data or use virtual memory techniques to defer
any actual page copies unless the sender or the receiver modifies
the data, the physical pages remain shared.
<p>
	  The sender of the message must explicitly request an out-of-line
transfer. Such a region is described as an arbitrary region of the
sender's address space. The sender always sees this memory as being
copied to the receiver.
<p>
	  For each region, the sender has a de-allocate option. If the option is
set and the out-of-line memory region is not null, then the region is
implicitly de-allocated from the sender, as if by vm_deallocate. In
particular, the start address is truncated down and the end address
rounded up so that every page overlapped by the memory region is
de-allocated (thereby possibly de-allocating more memory than is
effectively transmitted). The use of this option effectively changes
the memory copy to a memory movement. Aside from possibly optimizing
the sender's use of memory, the de-allocation option allows the kernel
to more efficiently handle the transfer of memory.
	  <p>
For each region, the sender has the choice of permitting the kernel to
choose a transmission strategy or the choice of requiring physical
copy:
	  <dl>
<dt> MACH_MSG_VIRTUAL_COPY
     <dd>
In a sent message, this flag allows the kernel to choose any mechanism
to transmit the data. For large regions, this involves constructing a
virtual copy of the pages containing the region. The portion of the
first page preceding the data and the portion of the last page
following the data are not copied (and will appear as zero if the
virtual copy is dynamically allocated in the receiver).
	  <p>
In a received message, this flag indicates that the kernel transmitted
a virtual copy. Access to the received memory may involve interactions
with the memory manager managing the sender's original data. Integri-
ty-conscious receivers should exercise caution when dealing with out-
of-line memory from un-trustworthy sources. Receivers concerned about
deterministic access time should also exercise caution. The dynamic
allocation option guarantees that the virtual copy will not be di-
rectly referenced during the act of receiving the message.
<dt> MACH_MSG_PHYSICAL_COPY
     <dd>
In a sent message, this flag requires that the kernel construct an
actual copy of the memory (either into wired kernel memory or default
memory managed space). There is a (fairly large) limit on the amount
of data that can be physically copied in a message. Port arrays always
assume this option when sent.
	  <p>
In a received message, this flag indicates that the kernel did
transmit a physical copy.
	  </dl>
	  <p>
The receiver has two options for the reception of out-of-line memory
(or "out-of-line" port arrays): allocation and overwrite.
In the absence of the MACH_RCV_OVERWRITE option, all out-of-line re-
gions are dynamically allocated. Allocated out-of-line memory arrives
somewhere in the receiver's address space as new memory. It has the
same inheritance and protection attributes as newly vm_allocate'ed
memory. The receiver has the responsibility of de-allocating (with
vm_deallocate) the memory when it is no longer needed. If the message
contains more than one region, each will be allocated its own region,
not necessarily contiguously. If the sender's data was transmitted as
a virtual copy the allocated region will have the same data alignment
within the page;
otherwise, the received data will appear starting at
the beginning of a page.
	  <p>
If the MACH_RCV_OVERWRITE option is set, the receiver can specify how
each received region is to be processed (dynamically allocated as
described above, or written over existing memory). With this option,
the contents of the receive buffer (receive_msg) are examined by the
kernel. The kernel scans the descriptors in the receive buffer
"message" to determine how to handle each out-of-line region. (Note:
whereas receive_limit is the maximum size of the receive buffer,
receive_msg_size is the amount filled in with this "message".) The
kernel uses each out-of-line data descriptor (in order) to specify
the processing for each received data region in turn, each out-of-line
port array descriptor is used correspondingly. (Intermingled port
descriptors are ignored when matching descriptors between the
incoming message and the receive buffer list.)
<p>
The copy option in the
matching descriptor specifies the processing:
	  <dl>
<dt> MACH_MSG_OVERWRITE
     <dd>
This flag indicates that the region should write over a specified
region of the receiver's address space, as indicated by the address
and size/ count fields of the descriptor. The full range overwritten
must already exist (be allocated or mapped) in the receiver's address
space. Depending on the nature of the data transmission this
overwrite may involve virtual memory manipulations or it may involve
actual data copy.
<dt> MACH_MSG_ALLOCATE
     <dd>
This flag indicates that the region is to be dynamically allocated. No
other descriptor values are relevant.
	  </dl>
	  <p>
If not enough descriptors appear in the receive buffer to describe all
received regions, additional regions are dynamically allocated. If
the receiver specifies more descriptors than there are regions in the
received message, the additional descriptors are ignored (and do not
appear in the final received message).
	  <p>
Note that the receive buffer descriptors will be overwritten:
The size fields in descriptors will be updated (when scanned, they
specified the maximum sizes of regions, when received, they specify
the actual sizes of received regions).
The copy fields in descriptors will be updated (when scanned, they
specified allocate versus overwrite, when received, they indicate
whether the region was physically or virtually copied).
The descriptors may appear in different positions (given intermingled
port descriptors).
Descriptors that were not used (because there were not that many
received regions) will be discarded.
	  <p>
Null out-of-line memory is legal. If the out-of-line region size is
zero, then the region's specified address is ignored. A receive
allocated null out-of-line memory region always has a zero address.
Unaligned addresses and region sizes that are not page multiples are
legal. A received message can also contain regions with unaligned
addresses and sizes which are not multiples of the page size.
<h4>Message Send</h4>
	  <p>
The send operation queues a message to a port. The message carries a
copy of the caller's data. After the send, the caller can freely
modify the message buffer or the out-of-line memory regions and the
message contents will remain unchanged.
	  	  <p>
The message carries with it the security ID of the sender, which the
receiver can request in the message trailer.
	  	  <p>
Message delivery is reliable and sequenced. Reception of a message
guarantees that all messages previously sent to the port by a single
task (or a single kernel) via send rights have been received and that
they are received in the order in which they were sent. Messages sent
to send-once rights are unordered.
	  <p>
If the destination port's queue is full, several things can happen. If
the message is sent to a send-once right (msgh_remote_port carries a
send-once right), then the kernel ignores the queue limit and delivers
the message. Otherwise the caller blocks until there is room in the
queue, unless the MACH_SEND_TIMEOUT option is used. If a port has
several blocked senders, then any of them may queue the next message
when space in the queue becomes available, with the proviso that a
blocked sender will not be indefinitely starved.
These options modify MACH_SEND_MSG. If MACH_SEND_MSG is not also
specified, they are ignored.
<dl>
<dt> MACH_SEND_TIMEOUT
     <dd>
The timeout argument should specify a maximum time (in milliseconds)
for the call to block before giving up. If the message can't be queued
before the timeout interval elapses, then the call returns
MACH_SEND_TIMED_OUT. A zero timeout is legitimate.
<dt> MACH_SEND_INTERRUPT
     <dd>
If specified, the <strong>mach_msg</strong> call will return 
MACH_SEND_INTERRUPTED if a software interrupt aborts the call. 
Otherwise, the send operation will be retried.
<dt> MACH_SEND_TRAILER
     <dd>
If set, the kernel, instead of determining the message attributes
itself, will accept a formatted message trailer from the sender. The
supplied trailer must be of the latest version supported by the
kernel, and must contain all message attributes defined by the
kernel. Only tasks with a security ID of KERNEL_SECURITY_ID can use
this option;
the intended use of this option is in support of the
Net Message server. The trailer must follow the message in memory as
it would appear in a received message. (The send_size argument to
<strong>mach_msg</strong> still indicates the size of the message proper, not including
this trailer.)
	  </dl>
	  <p>
The queueing of a message carrying receive rights may create a
circular loop of receive rights and messages, which can never be
received. For example, a message carrying a receive right can be
sent to that receive right. This situation is not an error, but the
kernel will garbage-collect such loops, destroying the messages.
Some return codes, like MACH_SEND_TIMED_OUT, imply that the message
was almost sent, but could not be queued. In these situations, the
kernel tries to return the message contents to the caller with a
pseudo-receive operation. This prevents the loss of port rights or
memory which only exist in the message, for example, a receive right
which was moved into the message, or out-of-line memory sent with
the de-allocate option.
	  <p>
The intent of the pseudo-receive operation is to restore, as best as
possible, the state prior to attempting the send. This involves
restoring the port rights and out-of-line memory regions contained in
the message. The port right names and out-of-line addresses in the
message send buffer are updated to reflect the new values resulting
from their effective reception. The pseudo-receive handles the des-
tination and reply rights as any other rights;
they are not reversed
as is the appearance in a normal received message. Also, no trailer is
appended to the message. After the pseudo-receive, the message is
ready to be resent. If the message is not resent, note that
out-of-line memory regions may have moved and some port rights may
have changed names.
	  <p>
Although unlikely, the pseudo-receive operation may encounter resource
shortages. This is similar to a MACH_RCV_BODY_ERROR return code from
a receive operation. When this happens, the normal send return codes
are augmented with the MACH_MSG_IPC_SPACE, MACH_MSG_VM_SPACE,
MACH_MSG_IPC_KERNEL and MACH_MSG_VM_KERNEL bits to indicate the
nature of the resource shortage.
<h4>Message Receive</h4>
	  <p>
The receive operation de-queues a message from a port. The receiving
task acquires the port rights and out-of-line memory regions carried
in the message.
The receive_name argument specifies a port or port set from which to
receive. If a port is specified, the caller must possess the receive
right for the port and the port must not be a member of a port set. If
no message is present, the call blocks, subject to the
MACH_RCV_TIMEOUT option.
<p>
If a port set is specified, the call will receive a message sent to
any of the member ports. It is permissible for the port set to have
no member ports, and ports may be added and removed while a receive
from the port set is in progress. The received message can come from
any of the member ports which have messages, with the proviso that a
member port with messages will not be indefinitely starved. The
msgh_local_port field in the received message header specifies from
which port in the port set the message came.
<p>
The receive_limit argument specifies the size of the caller's message
buffer (which must be big enough for the message header, body and
trailer);
the trailer's length is given by
the msgh_trailer_size field within the trailer. The receiver of a
message is given a choice as to what trailer format is desired, and,
within that format, which of the leading trailer attributes are
desired (that is, to get trailer element three, the receiver must also
accept elements one and two). For any given trailer format (of which
there is currently only one), the trailer is compatibly extended by
adding additional elements to the end.
<p>
Received messages are stamped (in the trailer) with a sequence number,
taken from the port from which the message was received. (Messages
received from a port set are stamped with a sequence number from the
appropriate member port.) Newly created ports start with a zero
sequence number, and the sequence number is reset to zero whenever the
port's receive right moves between tasks.  When a message is de-queued
from the port, it is stamped with the port's sequence number and the
port's sequence number is then incremented. (Note that this occurs
whether or not the receiver requests the sequence number in the trail-
er.) The de-queue and increment operations are atomic, so that
multiple threads receiving messages from a port can use the msgh_seqno
field to reconstruct the original order of the messages.
<p>
The destination and reply ports are reversed in a received message
header. The msgh_local_port field carries the name of the destination
port, from which the message was received, and the msgh_remote_port
field carries the reply port right. The bits in msgh_bits are also
reversed. The MACH_MSGH_BITS_LOCAL bits have a value of
MACH_MSG_TYPE_PORT_SEND_ONCE or MACH_MSG_TYPE_PORT_SEND depending on
the type of right to which the message was sent. The
MACH_MSGH_BITS_REMOTE bits describe the reply port right.
<p>
A received message can contain port rights and out-of-line memory. The
msgh_local_port field does not carry a port right;
the act of
receiving the message consumes the send or send-once right for the
destination port. The msgh_remote_port field does carry a port right,
and the message can carry additional port rights and memory if the
MACH_MSGH_BITS_COMPLEX bit is set. Received port rights and memory
should be consumed or de-allocated in some fashion.
In almost all cases, msgh_local_port will specify the name of a
receive right, either receive_name, or, if receive_name is a port
set, a member of receive_name.
<p>
If other threads are concurrently
manipulating the receive right, the situation is more complicated. If
the receive right is renamed during the call, then msgh_local_port
specifies the right's new name. If the caller loses the receive right
after the message was de-queued from it, then <strong>mach_msg</strong> will proceed
instead of returning MACH_RCV_PORT_DIED. If the receive right was
destroyed, then msgh_local_port specifies MACH_PORT_DEAD. If the
receive right still exists, but isn't held by the caller, then
msgh_local_port specifies MACH_PORT_NULL.
<p>
The following options modify MACH_RCV_MSG. If MACH_RCV_MSG is not also
specified, they are ignored.
<dl>
<dt> MACH_RCV_TIMEOUT
     <dd>
The timeout argument should specify a maximum time (in milliseconds)
for the call to block before giving up. If no message arrives before
the timeout interval elapses, then the call returns
MACH_RCV_TIMED_OUT. A zero timeout is legitimate.
<dt> MACH_RCV_NOTIFY
     <dd>
The notify argument should specify a receive right for a notify
port. If receiving the reply port creates a new port right in the
caller, then the notify port is used to request a dead-name
notification for the new port right.
<dt> MACH_RCV_INTERRUPT
     <dd>
If specified, the <strong>mach_msg</strong> call will return MACH_RCV_INTERRUPTED if a
software interrupt aborts the call.  Otherwise, the receive operation
will be retried.
<dt> MACH_RCV_OVERWRITE
     <dd>
If specified, the message buffer specified by receive_msg (or msg), of 
length receive_msg_size, will be scanned for out-of-line descriptors to 
specify the processing to be done when receiving out-of-line regions. 
This option is only allowed for <strong>mach_msg_overwrite</strong>.
<dt> MACH_RCV_LARGE
     <dd>
If the message is larger than receive_limit or an out-of-line region
is larger than the size allowed by a corresponding receive descriptor
(MACH_RCV_OVERWRITE), the message remains queued instead of being
destroyed. If the header, trailer and body would not fit into
receive_limit, only the message header (mach_msg_header) and trailer
header (mach_msg_trailer) are returned with the actual size of the
message returned in the msgh_size field, the actual size of the
trailer returned in the msgh_trailer_size field and an error return
value of MACH_RCV_TOO_LARGE. If receive_limit is sufficient but an
out-of-line descriptor is not, the message header, trailer and body
are received, with out-of-line descriptors set to indicate the
nature and size of the out-of-line regions, with an error return of
MACH_RCV_SCATTER_SMALL. No out-of-line regions or port rights
(including the reply right) will be received. If this option is not
specified, messages too large will be de-queued and then destroyed;
</strong>
</pre>
<h3>FIELDS</h3>
<dl>
<dt> <var>msgh_bits</var>
<dd>
This field specifies the following properties of the message:
<dl>
  <p>
<dt> <strong>MACH_MSGH_BITS_REMOTE_MASK</strong>
<dd>
Encodes <var>mach_msg_type_name_t</var> values that specify the port 
rights in the <var>msgh_remote_port</var> field.  The value must specify 
a send or send-once right for the destination of the message.
     <p>
<dt> <strong>MACH_MSGH_BITS_LOCAL_MASK</strong>
<dd>
Encodes <var>mach_msg_type_name_t</var> values that specify the port 
rights in the <var>msgh_local_port</var> field.  If the value doesn't
specify a send or send-once right for the message's reply port, it 
must be zero and <var>msgh_local_port</var> must be <strong>MACH_PORT_NULL</strong>.
     <p>
<dt> <strong>MACH_MSGH_BITS_COMPLEX</strong>
<dd>
The complex bit must be specified if the message body
contains additional port rights or out-of-line memory regions.
     <p>
<dt> <strong>MACH_MSGH_BITS_REMOTE</strong>(<var>bits</var>)
<dd>
This macro returns the appropriate <var>mach_msg_type_name_t</var> 
values, given a <var>msgh_bits</var> value.
     <p>
<dt> <strong>MACH_MSGH_BITS_LOCAL</strong>(<var>bits</var>)
<dd>
This macro returns the appropriate <var>mach_msg_type_name_t</var> 
values, given a <var>msgh_bits</var> value.
     <p>
<dt> <strong>MACH_MSGH_BITS</strong>(<var>remote</var>, <var>local</var>)
<dd>
This macro constructs a value for <var>msgh_bits</var>, given two 
<var>mach_msg_type_name_t</var> values. 
</dl>
<p>
<dt> <var>msgh_size</var>
<dd>
This field is ignored on send (the size to send is specified by the
<var>send_size</var> parameter to <strong>mach_msg</strong>);
<h2>mach_ports_lookup</h2>
<hr>
<p>
<strong>Function</strong> - Provide caller with an array of the target task's well-known ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_ports_lookup</strong>
                <strong>(task_t</strong>                             <var>target_task</var>,
                 <strong>mach_port_array_t</strong>                <var>init_port_set</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>init_port_count</var><strong>);
<h2>mach_ports_register</h2>
<hr>
<p>
<strong>Function</strong> - Register an array of well-known ports on behalf of the target task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_ports_register</strong>
                <strong>(task_t</strong>                             <var>target_task</var>,
                 <strong>mach_port_array_t</strong>                <var>init_port_set</var>,
                 <strong>target_task</strong>              <var>init_port_array_count</var><strong>);
<h2>mach_port_allocate</h2>
<hr>
<p>
<strong>Function</strong> - Create caller-specified type of port right.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_allocate</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_right_t</strong>                        <var>right</var>,
                 <strong>mach_port_name_t</strong>                         <var>*name</var><strong>);
<h2>mach_port_allocate_full</h2>
<hr>
<p>
<strong>Function</strong> - Create a port right with full Mach port semantics.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_allocate_full</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_right_t</strong>                        <var>right</var>,
                 <strong>subsystem_t</strong>                          <var>subsystem</var>,
                 <strong>mach_port_qos_t</strong>                            <var>qos</var>,
                 <strong>task</strong>                                      <var>name</var><strong>);
<h2>mach_port_allocate_name</h2>
<hr>
<p>
<strong>Function</strong> - Create a port right with the caller-specified name.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_allocate_name</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_right_t</strong>                        <var>right</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
<h2>mach_port_allocate_qos</h2>
<hr>
<p>
<strong>Function</strong> - Allocate a port with specified "quality of service."
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	mach_port_allocate_qos</strong>
		<strong>(ipc_space_t</strong>	<var>task</var>,
		<strong>mach_port_right_t</strong>	<var>right</var>,
		<strong>mach_port_qos_t</strong>	<var>qos</var>,
		<strong>mach_port_name_t*</strong>	<var>name</var><strong>);
<h2>mach_port_deallocate</h2>
<hr>
<p>
<strong>Function</strong> - Decrement the target port right's user reference count.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_deallocate</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
<h2>mach_port_destroy</h2>
<hr>
<p>
<strong>Function</strong> - Deallocate all port rights associated with specified name.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_destroy</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var><strong>);
charset=iso-8859-1">
   <meta name="GENERATOR" content="Mozilla/4.73 (Macintosh;
<b>(ipc_space_t</b>&nbsp;
<h2>mach_port_extract_right</h2>
<hr>
<p>
<strong>Function</strong> - Remove the specified right from the target task and return it to the caller.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_extract_right</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_msg_type_name_t</strong>              <var>desired_type</var>,
                 <strong>mach_port_poly_t</strong>                        <var>*right</var>,
                 <strong>mach_msg_type_name_</strong>             <var>*acquired_type</var><strong>);
<h2>mach_port_get_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Return information about target port as specified by the caller.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_get_attributes</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_flavor_t</strong>                      <var>flavor</var>,
                 <strong>mach_port_info_t</strong>                     <var>port_info</var>,
                 <strong>mach_msg_type_number_t</strong>        <var>*port_info_count</var><strong>);
<h2>mach_port_get_refs</h2>
<hr>
<p>
<strong>Function</strong> - Return the current count of user references on the target port right.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_get_refs</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_right_t</strong>                        <var>right</var>,
                 <strong>mach_port_urefs_t</strong>                        <var>*refs</var><strong>);
<h2>mach_port_get_set_status</h2>
<hr>
<p>
<strong>Function</strong> - Return the port right names contained in the target port set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_get_set_status</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_name_array_t</strong>                <var>*members</var>,
                 <strong>task</strong>                                     <var>count</var><strong>);
charset=iso-8859-1">
   <meta name="GENERATOR" content="Mozilla/4.73 (Macintosh;
<b>(ipc_space_t</b>&nbsp;
<h2>mach_port_insert_right</h2>
<hr>
<p>
<strong>Function</strong> - Insert the specified port right into the target task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_insert_right</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_poly_t</strong>                         <var>right</var>,
                 <strong>mach_msg_type_name_t</strong>                <var>right_type</var><strong>);
<h2>mach_port_mod_refs</h2>
<hr>
<p>
<strong>Function</strong> - Modify the specified port right's count of user references.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_mod_refs</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_right_t</strong>                        <var>right</var>,
                 <strong>mach_port_delta_t</strong>                        <var>delta</var><strong>);
charset=iso-8859-1">
   <meta name="GENERATOR" content="Mozilla/4.73 (Macintosh;
<b>(ipc_space_t</b>&nbsp;
<h2>mach_port_names</h2>
<hr>
<p>
<strong>Function</strong> - Return information about a task's port name space.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_names</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_array_t</strong>                  <var>*names</var>,
                 <strong>mach_msg_type_number_t</strong>               <var>*namesCnt</var>,
                 <strong>mach_port_type_array_</strong>                   <var>*types</var>,
                 <strong>mach_msg_type_number_t</strong>               <var>*typesCnt</var><strong>);
<h2>mach_port_set_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Set the target port's attributes.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_set_attributes</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_flavor_t</strong>                      <var>flavor</var>,
                 <strong>mach_port_info_t</strong>                     <var>port_info</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>port_info_count</var><strong>);
<h2>mach_port_set_mscount</h2>
<hr>
<p>
<strong>Function</strong> - Change the target port's make-send count.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_set_mscount</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_mscount_t</strong>                    <var>mscount</var><strong>);
<h2>mach_port_set_seqno</h2>
<hr>
<p>
<strong>Function</strong> - Change the current value of the target port's sequence number.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_set_seqno</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var><strong>);
<h2>mach_port_type</h2>
<hr>
<p>
<strong>Function</strong> - Return the characteristics of the target port name.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_type</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_port_type_t</strong>                         <var>ptype</var><strong>);
<h2>mach_rpc_trap</h2>
<hr>
<p>
<strong>System Trap</strong> - Real-Time RPC trap.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/rpc.h&gt</strong>

<strong>mach_rpc_return_t   mach_rpc_trap</strong>
                     <strong>(mach_port_t</strong>                    <var>dest_port</var>,
                      <strong>mach_rpc_id_t</strong>                <var>routine_num</var>,
                      <strong>mach_rpc_signature_t</strong>       <var>signature_ptr</var>,
                      <strong>mach_rpc_size_t</strong>           <var>signature_size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>dest_port</var>
<dd>
[in send right] The port representing the destination of the RPC
     (usually a registered subsystem established by a call to
     <strong>mach_port_allocate_subsystem</strong>).
<p>
<dt> <var>routine_num</var>
<dd>
[in scalar] Identifier of the server work function.
<p>
<dt> <var>signature_ptr</var>
<dd>
[in pointer] Pointer to the call's <strong>mach_rpc_signature</strong> structure.
<p>
<dt> <var>signature_size</var>
<dd>
[in scalar] Size, in bytes, of the call's <strong>mach_rpc_signature</strong> structure.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>mach_rpc_trap</strong> system trap is the entry point for the
invoke side of the Mach RPC service used to transfer control to an RPC server.
The trap is accessed via the MIG generated
<strong>MACH_RPC</strong> macro
which is invoked transparently when the <strong>mach_rpc</strong> feature is enabled.
This function is not designed for use directly by the user. It is
automatically generated by MIG to handle a function call.
<p>
For a
complete description of this functionality, refer to: Burke, Edward,
Michael Condict, David Mitchell, Franklin Reynolds, Peter Watkins,
William Willcox, "RPC Design for Real-Time Mach," OSF Research
Institute, Cambridge, MA.
<h3>NOTES</h3>
<p>
This interface is experimental and therefore subject to change.
<h3>RETURN VALUES</h3>
<dl>
<p>
<dt> <strong>KERN_FAILURE</strong>
<dd>
Either the argument copyin failed, there were too many arguments, or
the argument copyout failed.
<p>
<dt> <strong>KERN_INVALID_ARGUMENT</strong>
<dd>
The dest_port, signature_ptr, and/or the subsystem associated with the
dest_port is invalid;
<h2>mach_subsystem_create</h2>
<hr>
<p>
<strong>Function</strong> - Register information about an RPC subsystem.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_subsystem_create</strong>
                <strong>(task_t</strong>                             <var>target_task</var>,
                 <strong>user_subsystem_t</strong>                   <var>user_subsys</var>,
                 <strong>mach_msg_type_number_t</strong>          <var>user_subsysCnt</var>,
                 <strong>subsystem_t</strong>                        <var>subsystem_t</var><strong>);
<h2>mach_thread_self</h2>
<hr>
<p>
<strong>System Trap</strong> - Returns the thread self port.

<h3>SYNOPSIS</h3>
<p>
<strong>#include <mach/mach_traps.h></strong>
<pre>
<strong>thread_port_t   mach_thread_self( void );
</strong>
</pre>
<h3>FIELDS</h3>
<dl>
<dt> <var>mtv_time</var>
<dd>
Clock time.
<p>
<dt> <var>mtv_csec</var>
<dd>
A field used to synchronize with the kernel's setting of the time.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>mapped_tvalspec</strong> structure defines the format of the 
current-time structure 
maintained by the kernel and visible through a mapped clock 
(<strong>clock_map_time</strong>).  The data in this structure is updated at the 
clock's current resolution and contains the same <strong>tvalspec</strong> value that 
would be returned by <strong>clock_get_time</strong>.
<h3>NOTES</h3>
<p>
Because of the race between the referencing of the multiple fields
in the clock 
value and the kernel's setting them, they should be referenced as follows:
<p>
<pre>
   <strong>tvalspec_t* ts;
</strong>
   <strong>do</strong>
   <strong>{</strong>
              <strong>ts-> tv_sec = mtime -> mtv_time.tv_sec;</strong>
              <strong>ts  -> tv_nsec = mtime -> mtv_time.tv_nsec;</strong>
   <strong>} while (ts  -> tv_sec != mtime -> mtv_csec);
<h2>memory_object_create</h2>
<hr>
<p>
<strong>Function</strong> - Request that the default pager handle management requests on the specified memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_create</strong>
                <strong>(memory_object_t</strong>                          <var>pager</var>,
                 <strong>memory_object_t</strong>              <var>new_memory_object</var>,
                 <strong>vm_size_t</strong>                      <var>new_object_size</var>,
                 <strong>memory_object_control_t</strong>            <var>new_control</var>,
                 <strong>vm_size_t</strong>                        <var>new_page_size</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_create</strong>
                <strong>(memory_object_t</strong>                          <var>pager</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_t</strong>              <var>new_memory_object</var>,
                 <strong>vm_size_t</strong>                      <var>new_object_size</var>,
                 <strong>memory_object_control_t</strong>            <var>new_control</var>,
                 <strong>vm_size_t</strong>                        <var>new_page_size</var><strong>);
<h2>memory_object_data_error</h2>
<hr>
<p>
<strong>Function</strong> - An error prevents the supply of previously requested data.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_error</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>kern_return_t</strong>                           <var>reason</var><strong>);
<h2>memory_object_data_request</h2>
<hr>
<p>
<strong>Server Interface</strong> - Request that memory manager page-in specified data.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_request</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>vm_prot_t</strong>                       <var>desired_access</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_data_request</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>vm_prot_t</strong>                       <var>desired_access</var><strong>);
<h2>memory_object_data_return</h2>
<hr>
<p>
<strong>Server Interface</strong> - Return memory object data to the appropriate memory manager.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_return</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>pointer_t</strong>                                 <var>data</var>,
                 <strong>boolean_t</strong>                                <var>dirty</var>,
                 <strong>boolean_t</strong>                          <var>kernel_copy</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_data_return</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>pointer_t</strong>                                 <var>data</var>,
                 <strong>boolean_t</strong>                                <var>dirty</var>,
                 <strong>boolean_t</strong>                          <var>kernel_copy</var><strong>);
<h2>memory_object_data_supply</h2>
<hr>
<p>
<strong>Function</strong> - Provide kernel with data previously requested by the kernel's Memory Management facility.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_supply</strong>
                <strong>(mem_object_control_port_t</strong>       <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>pointer_t</strong>                                 <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var>,
                 <strong>boolean_t</strong>                           <var>deallocate</var>,
                 <strong>vm_prot_t</strong>                           <var>lock_value</var>,
                 <strong>boolean_t</strong>                             <var>precious</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var><strong>);
<h2>memory_object_data_unlock</h2>
<hr>
<p>
<strong>Server Interface</strong> - Request that the memory manager change current access permission on the specified memory object's data.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_unlock</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>vm_prot_t</strong>                       <var>desired_access</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_data_unlock</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>vm_prot_t</strong>                       <var>desired_access</var><strong>);
<h2>memory_object_destroy</h2>
<hr>
<p>
<strong>Function</strong> - Shut down a memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_destroy</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>kern_return_t</strong>                           <var>reason</var><strong>);
<h2>memory_object_init</h2>
<hr>
<p>
<strong>Server Interface</strong> - Initializes a memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_init</strong>
		<strong>(memory_object_t</strong>                <var>memory_object</var>,
		 <strong>memory_object_control_t       </strong><var>memory_control</var>,
		 <strong>vm_size_t</strong>            <var>memory_object_page_size</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_init</strong>
		<strong>(memory_object_t</strong>                <var>memory_object</var>,
		 <strong>mach_port_seqno_t</strong>                      <var>seqno</var>,
		 <strong>memory_object_control_t</strong>       <var>memory_control</var>,
		 <strong>vm_size_t</strong>            <var>memory_object_page_size</var><strong>);
<h2>memory_object_lock_request</h2>
<hr>
<p>
<strong>Function</strong> - Restrict access to memory object data.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_lock_request</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>memory_object_return_t</strong>           <var>should_return</var>,
                 <strong>boolean_t</strong>                         <var>should_flush</var>,
                 <strong>vm_prot_t</strong>                           <var>lock_value</var>,
                 <strong>mach_port_t</strong>                         <var>reply_port</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>memory_control</var> 
<dd>
[in memory-cache-control send right]
The memory cache control port 
to be used by the memory manager for cache management requests. 
This port is provided by the kernel in a <strong>memory_object_init</strong> call.
<p>
<dt> <var>offset</var> 
<dd>
[in scalar]
The offset within the memory object, in bytes.
<p>
<dt> <var>size</var> 
<dd>
[in scalar]
The number of bytes of data (starting at <var>offset</var>) to be
affected.  The number must convert to an integral number of memory object 
pages.
<p>
<dt> <var>should_return</var> 
<dd>
[in scalar]
Clean indicator.  Values are:
<dl>
<p>
<dt> <strong>MEMORY_OBJECT_RETURN_NONE</strong>
<dd>
Don't return any pages.  If <var>should_flush</var> is <strong>TRUE</strong>, pages will 
be discarded.
<p>
<dt> <strong>MEMORY_OBJECT_RETURN_DIRTY</strong>
<dd>
Return only dirty (modified) pages.  If <var>should_flush</var> is <strong>TRUE</strong>, 
precious pages will be discarded;
<h2>memory_object_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel operation request aimed at a given memory manager.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	memory_object_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>memory_object_synchronize</h2>
<hr>
<p>
<strong>Server Interface</strong> - Forward a client's request to synchronize data with its image in backing store.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_synchronize</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_offset_t</strong>                             <var>length</var>,
                 <strong>memory_object</strong>                       <var>sync_flags</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_synchronize</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_offset_t</strong>                             <var>length</var>,
                 <strong>memory_object</strong>                       <var>sync_flags</var><strong>);
<h2>memory_object_terminate</h2>
<hr>
<p>
<strong>Server Interface</strong> - Relinquish access to a memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_terminate</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var><strong>);
</strong>



<strong>kern_return_t   seqnos_memory_object_terminate</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var><strong>);
<h2>memory_object_change_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Modify caller-specified subset of attributes representing target memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_change_attributes</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>memory_object_flavor_t</strong>                  <var>flavor</var>,
                 <strong>memory_object_info_t</strong>                <var>attributes</var>,
                 <strong>attributes</strong>                    <var>attributes_count</var>,
                 <strong>mach_port_t</strong>                           <var>reply_to</var><strong>);
<h2>memory_object_change_completed</h2>
<hr>
<p>
<strong>Server Interface</strong> - Notify memory manager that kernel has updated memory object attributes as requested.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_change_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>memory_object_flavor_t</strong>                  <var>flavor</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_change_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>memory_object_flavor_t</strong>                  <var>flavor</var><strong>);
<h2>memory_object_data_initialize</h2>
<hr>
<p>
<strong>Server Interface</strong> - Request that the default pager record initialization information for specified memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_initialize</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>pointer_t</strong>                                 <var>data</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_data_initialize</strong>
                <strong>(memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>pointer_t</strong>                                 <var>data</var><strong>);
<h2>memory_object_data_unavailable</h2>
<hr>
<p>
<strong>Function</strong> - Instruct kernel to zero-fill pages as requested data does not exist.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_data_unavailable</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>memory_control</var> 
<dd>
[in memory-cache-control send right]
The memory cache control port 
to be used by the memory manager for cache management requests. 
This port is provided by the kernel in a <strong>memory_object_init</strong> or a 
<strong>memory_object_create</strong> call.
<p>
<dt> <var>offset</var> 
<dd>
[in scalar]
The offset within the memory object, in bytes.
<p>
<dt> <var>size</var> 
<dd>
[in scalar]
The number of bytes of data (starting at <var>offset</var>).  The number 
must convert to an integral number of memory object pages.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>memory_object_data_unavailable</strong> function indicates
that the memory 
manager cannot provide the kernel with the data requested for
the given region. 
Instead, the kernel should provide the data for this region.
<p>
A memory manager can use this call in any of the following situations:
<ul>
<li>
When the object was created by the kernel 
(via <strong>memory_object_create</strong>) and 
the kernel has not yet provided data for the region (via either
<strong>memory_object_data_initialize</strong> or <strong>memory_object_data_return</strong>).
In this case, the
object is a temporary memory object;
<h2>memory_object_default_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel operation request targeted for the default pager.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	memory_object_default_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>memory_object_get_attributes</h2>
<hr>
<p>
<strong>Function</strong> - Return current attributes for a memory object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_get_attributes</strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>memory_object_flavor_t</strong>                  <var>flavor</var>,
                 <strong>memory_object_info_t</strong>                <var>attributes</var>,
                 <strong>mach_msg_type_number_t</strong>        <var>attributes_count</var><strong>);
<h2>memory_object_lock_completed</h2>
<hr>
<p>
<strong>Server Interface</strong> - Report to memory manager that a previous consistency control request has been handled.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_lock_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_lock_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var><strong>);
<h2>memory_object_supply_completed</h2>
<hr>
<p>
<strong>Server Interface</strong> - Return results associated with the kernel's handling of a particular memory manager request.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_supply_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>kern_return_t</strong>                           <var>result</var>,
                 <strong>vm_offset_t</strong>                       <var>error_offset</var><strong>);
</strong>


<strong>kern_return_t   seqnos_memory_object_supply_completed</strong>
                <strong>(memory_object_t</strong>                     <var>reply_port</var>,
                 <strong>mach_port_seqno_t</strong>                        <var>seqno</var>,
                 <strong>memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_size_t</strong>                               <var>length</var>,
                 <strong>kern_return_t</strong>                           <var>result</var>,
                 <strong>vm_offset_t</strong>                       <var>error_offset</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>reply_port</var> 
<dd>
[in reply (receive) right]
The port supplied in the corresponding
<strong>memory_object_data_supply</strong> call.
<p>
<dt> <var>seqno</var> 
<dd>
[in scalar]
The sequence number of this message relative to the port 
named in the <strong>memory_object_data_supply</strong> call.
<p>
<dt> <var>memory_control</var> 
<dd>
[in memory-cache-control send right]
The memory cache control port 
to be used for a response by the memory manager.  If the memory
object has been supplied to more than one kernel, this parameter
identifies the kernel that is making the call.
<p>
<dt> <var>offset</var> 
<dd>
[in scalar]
The offset within the memory object from the corresponding 
data supply call.
<p>
<dt> <var>length</var> 
<dd>
[in scalar]
The number of bytes accepted.  The number converts to an 
integral number of memory object pages.
<p>
<dt> <var>result</var> 
<dd>
[in scalar]
A kernel return code indicating the result of the supply
operation, possibly <strong>KERN_SUCCESS</strong>.  <strong>KERN_MEMORY_PRESENT</strong> is 
currently the only error returned;
<h2>memory_object_synchronize_completed</h2>
<hr>
<p>
<strong>Function</strong> - Inform the kernel that synchronized data has been processed.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   memory_object_synchronize_completed </strong>
                <strong>(memory_object_control_t</strong>         <var>memory_control</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>vm_offset_t</strong>                             <var>length</var><strong>);
<h2>mach_port_allocate_subsystem</h2>
<hr>
<p>
<strong>Function</strong> - Create a port right associated with the caller-specified subsystem.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_allocate_subsystem</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>subsystem_t</strong>                             <var>subsys</var>,
                 <strong>mach_port_name_t</strong>              <var>mach_port_name_t</var><strong>);
<h2>mach_port_request_notification</h2>
<hr>
<p>
<strong>Function</strong> - Request notification of the specified port event type.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   mach_port_request_notification</strong>
                <strong>(ipc_space_t</strong>                               <var>task</var>,
                 <strong>mach_port_name_t</strong>                          <var>name</var>,
                 <strong>mach_msg_id_t</strong>                          <var>variant</var>,
                 <strong>mach_port_mscount_t</strong>                       <var>sync</var>,
                 <strong>mach_port_send_once_t</strong>                   <var>notify</var>,
                 <strong>mach_msg_type_name_t</strong>               <var>notify_type</var>,
                 <strong>mach_port_send_once_t</strong>                <var>*previous</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>task</var> 
<dd>
[in task send right]
The task holding the specified right.
<p>
<dt> <var>name</var> 
<dd>
[in scalar]
The task's name for the right.
<p>
<dt> <var>variant</var> 
<dd>
[in scalar]
The type of notification.
<p>
<dt> <var>sync</var> 
<dd>
[in scalar]
Some variants use this value to overcome race conditions.
<p>
<dt> <var>notify</var> 
<dd>
[in notify send-once or receive (to be converted to send-once) right]
A 
send-once right, to which the notification will be sent.
<p>
<dt> <var>notify_type</var> 
<dd>
[in scalar]
IPC type of the <var>notify</var> right;
<h2>notify_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle the next kernel-generated IPC notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	notify_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>processor_assign</h2>
<hr>
<p>
<strong>Function</strong> - Assign a processor to a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_assign</strong>
		<strong>(processor_t</strong>	<var>processor</var>,
		<strong>processor_set_t</strong>	<var>new_set</var>,
		<strong>boolean_t</strong>	<var>wait</var><strong>);
<h2>processor_control</h2>
<hr>
<p>
<strong>Function</strong> - Perform caller-specified operation on target processor. (Protected Interface.)
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_control</strong>
		<strong>(processor_t</strong>	<var>processor</var>,
		<strong>processor_info_t</strong>	<var>cmd</var>,
		<strong>mach_msg_type_number_t*</strong>	<var>count</var><strong>);
<h2>processor_exit</h2>
<hr>
<p>
<strong>Function</strong> - Exit a processor.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_exit</strong>
		<strong>(processor_t</strong>	<var>processor</var><strong>);
<h2>processor_get_assignment</h2>
<hr>
<p>
<strong>Function</strong> - Get current assignment for a processor.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_get_assignment</strong>
		<strong>(processor_t</strong>	<var>processor</var>,
		<strong>processor_set_name_t</strong>	<var>assigned_set</var><strong>);
<h2>processor_info</h2>
<hr>
<p>
<strong>Function</strong> - Return information about a processor.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_info</strong>
		<strong>(processor_t</strong>	<var>processor</var>,
		<strong>processor_flavor_t</strong>	<var>flavor</var>,
		<strong>host_t</strong>	<var>host</var>,
		<strong>processor_info_t</strong>	<var>processor_info</var>,
		<strong>mach_msg_type_number_t</strong>	<var>processor_info_count</var><strong>);
<h2>processor_set_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a new processor set object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_create</strong>
		<strong>(host_t</strong>	<var>host_name</var>,
		<strong>processor_set_t</strong>	<var>new_set</var>,
		<strong>processor_set_name_t</strong>	<var>new_name</var><strong>);
<h2>processor_set_default</h2>
<hr>
<p>
<strong>Function</strong> - Return the default processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_default</strong>
		<strong>(host_t</strong>	<var>host</var>,
		<strong>processor_set_name_t</strong>	<var>default_set_name</var><strong>);
<h2>processor_set_destroy</h2>
<hr>
<p>
<strong>Function</strong> - Destroy the target processor set object.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_destroy</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var><strong>);
<h2>processor_set_info</h2>
<hr>
<p>
<strong>Function</strong> - Return processor set state according to caller-specified flavor.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_info</strong>
		<strong>(processor_set_name_t</strong>	<var>processor_set_name</var>,
		<strong>int</strong>	<var>flavor</var>,
		<strong>host_t</strong>	<var>host</var>,
		<strong>processor_set_info_t</strong>	<var>processor_set_info</var>,
		<strong>mach_msg_type_number_t</strong>	<var>processor_set_info_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<dt> <var>processor_set_name</var> 
<dd>
[in processor-set-name send right]
A processor set name (or control) 
port for which information is desired.
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of information requested.
<dl>
<dt> <strong>PROCESSOR_SET_BASIC_INFO</strong>
<dd>
Basic information concerning the processor set (number of
assigned processors and default policy).  The returned structure 
is defined by <strong>processor_set_basic_info</strong>.
<dt> <strong>PROCESSOR_SET_TIMESHARE_DEFAULT</strong>
<dd>
The base attributes for the timeshare scheduling policy.  The 
returned structure is <strong>policy_timeshare_base</strong>.
<dt> <strong>PROCESSOR_SET_FIFO_DEFAULT</strong>
<dd>
The base attributes for the FIFO scheduling policy.  The
returned structure is <strong>policy_fifo_base</strong>.
<dt> <strong>PROCESSOR_SET_RR_DEFAULT</strong>
<dd>
The base attributes for the round-robin scheduling policy.  The 
returned structure is <strong>policy_rr_base</strong>.
<dt> <strong>PROCESSOR_SET_TIMESHARE_LIMITS</strong>
<dd>
Limits on the allowed timeshare policy attributes.  The
returned structure is defined by <strong>policy_timeshare_limit</strong>.
<dt> <strong>PROCESSOR_SET_RR_LIMITS</strong>
<dd>
Limits on the allowed round robin policy attributes.  The
returned structure is defined by <strong>policy_rr_limit</strong>.
<dt> <strong>PROCESSOR_SET_FIFO_LIMITS</strong>
<dd>
Limits on the allowed first-in, first-out policy attributes.  The 
returned structure is defined by <strong>policy_fifo_limit</strong>.
<dt> <strong>PROCESSOR_SET_ENABLED_POLICIES</strong>
<dd>
The set of enabled policies.  The returned data is a bit-vector.
</dl>
<dt> <var>host</var> 
<dd>
[out host-name send right]
The name port for the host on which the 
processor set resides.
<dt> <var>processor_set_info</var> 
<dd>
[out structure]
Information about the processor set.
<dt> <var>processor_set_info_count</var> 
<dd>
[in/out scalar]
On input, the maximum size of the buffer;
<h2>processor_set_max_priority</h2>
<hr>
<p>
<strong>Function</strong> - Sets the maximum scheduling priority for a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t	processor_set_max_priority</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var>,
		<strong>int</strong>	<var>priority</var>,
		<strong>boolean_t</strong>	<var>change_threads</var><strong>);
<h2>processor_set_statistics</h2>
<hr>
<p>
<strong>Function</strong> - Return scheduling statistics for a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_statistics</strong>
		<strong>(processor_set_t</strong>	<var>processor_set_control</var>,
		<strong>processor_set_flavor_t</strong>	<var>flavor</var>,
		<strong>processor_set_info_t</strong>	<var>processor_set_info</var>,
		<strong>mach_msg_type_number_t</strong>	<var>processor_set_info_count</var><strong>);
<h2>processor_set_tasks</h2>
<hr>
<p>
<strong>Function</strong> - Return a list of pointers to all tasks currently assigned to the target processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_tasks</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var>,
		<strong>task_port_array_t</strong>	<var>task_list</var>,
		<strong>mach_msg_type_number_t*</strong>	<var>task_count</var><strong>);
<h2>processor_set_threads</h2>
<hr>
<p>
<strong>Function</strong> - Return a list of pointers to all threads currently assigned to the target processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_threads</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var>,
		<strong>thread_port_array_t</strong>	<var>thread_list</var>,
		<strong>mach_msg_type_number_t*</strong>	<var>thread_count</var><strong>);
<h2>processor_start</h2>
<hr>
<p>
<strong>Function</strong> - Start a processor.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&ltmach/mach_host.h&gt</strong>

<strong>kern_return_t	processor_start</strong>
		<strong>(processor_t</strong>	<var>processor</var><strong>);
<h2>prof_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle the next kernel-generated PC sample message.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	prof_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>processor_set_policy_control</h2>
<hr>
<p>
<strong>Function</strong> - Set target processor set's scheduling policy state.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	processor_set_policy_control</strong>
		<strong>(processor_set_t</strong>	<var>processor_set_control</var>,
		<strong>processor_set_flavor_t</strong>	<var>flavor</var>,
		<strong>processor_set_info_t</strong>	<var>policy_info</var>,
		<strong>mach_msg_type_number_t*</strong>	<var>policy_info_count</var>,
		<strong>boolean_t</strong>	<var>change_tasks_threads</var><strong>);
<h2>processor_set_policy_disable</h2>
<hr>
<p>
<strong>Function</strong> - Disables a scheduling policy for a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t	processor_set_policy_disable</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var>,
		<strong>int</strong>	<var>policy</var>,
		<strong>boolean_t</strong>	<var>change_threads</var><strong>);
<h2>processor_set_policy_enable</h2>
<hr>
<p>
<strong>Function</strong> - Enables a scheduling policy for a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lt mach/mach_host.h&gt</strong>

<strong>kern_return_t	processor_set_policy_enable</strong>
		<strong>(processor_set_t</strong>	<var>processor_set</var>,
		<strong>int</strong>	<var>policy</var><strong>);
<h2>receive_samples</h2>
<p>
Server Interface - Handles the occurrence of a PC sampling message

<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   receive_samples</strong>
                <strong>(mach_port_t</strong>                     <var>sample_port</var>,
                 <strong>sample_array_t</strong>                      <var>samples</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>sample_count</var><strong>);
<h2>semaphore_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a new semaphore.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t	semaphore_create</strong>
		<strong>(task_t</strong>                   <var>task</var>,
		 <strong>semaphore_t</strong>        <var>*semaphore</var>,
		 <strong>int</strong>                    <var>policy</var>,
		 <strong>int</strong>                     <var>value</var><strong>);
<h2>semaphore_destroy</h2>
<hr>
<p>
<strong>Function</strong> - Destroy a semaphore.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   semaphore_destroy</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>semaphore_t</strong>                          <var>semaphore</var><strong>);
<h2>semaphore_signal</h2>
<hr>
<p>
<strong>Function</strong> - Increments the semaphore count.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   semaphore_signal</strong>
                <strong>(semaphore_t</strong>                          <var>semaphore</var><strong>);
<h2>semaphore_signal_all</h2>
<hr>
<p>
<strong>Function</strong> - Wake up all threads blocked on a semaphore.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   semaphore_signal_all</strong>
                <strong>(semaphore_t</strong>                          <var>semaphore</var><strong>);
<h2>semaphore_wait</h2>
<hr>
<p>
<strong>Function</strong> - Wait on the specified semaphore.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   semaphore_wait</strong>
                <strong>(semaphore_t</strong>                          <var>semaphore</var><strong>);
<h2>seqnos_notify_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle the next kernel-generated IPC notification.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	seqnos_notify_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>seqnos_memory_object_default_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel operation request targeted for the default pager.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	seqnos_memory_object_default_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>seqnos_memory_object_server</h2>
<hr>
<p>
<strong>Function</strong> - Handle kernel operation request aimed at a given memory manager.
<h3>SYNOPSIS</h3>
<pre>
<strong>boolean_t	seqnos_memory_object_server</strong>
		<strong>(mach_msg_header_t</strong>	<var>request_msg</var>,
		<strong>mach_msg_header_t</strong>	<var>reply_ms</var><strong>);
<h2>task_assign</h2>
<hr>
<p>
<strong>Function</strong> - Assign a task to a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_assign</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>processor_set_t</strong>                  <var>processor_set</var>,
                 <strong>boolean_t</strong>                       <var>assign_threads</var><strong>);
<h2>task_assign_default</h2>
<hr>
<p>
<strong>Function</strong> - Assign a task to the default processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_assign_default</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>boolean_t</strong>                       <var>assign_threads</var><strong>);
<h2>task_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a new task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_create</strong>
                <strong>(task_t</strong>                             <var>parent_task</var>,
                 <strong>ledger_port_array_t</strong>                    <var>ledgers</var>,
                 <strong>int</strong>                               <var>ledger_count</var>,
                 <strong>boolean_t</strong>                       <var>inherit_memory</var>,
                 <strong>task_t</strong>                              <var>child_task</var><strong>);
<h2>task_get_assignment</h2>
<hr>
<p>
<strong>Function</strong> - Return the processor set to which a task is assigned.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_get_assignment</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>processor_set_name_t</strong>             <var>processor_set</var><strong>);
<h2>task_get_emulation_vector</h2>
<hr>
<p>
<strong>Function</strong> - Return an array identifying the target task's user-level system call handlers. 
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_get_emulation_vector</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                               <var>vector_start</var>,
                 <strong>emulation_vector_t</strong>            <var>emulation_vector</var>,
                 <strong>mach_msg_type_number_t*</strong> <var>emulation_vector_count</var><strong>);
<h2>task_get_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Return send rights to the target task's exception ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_get_exception_ports</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>exception_mask_array_t</strong>     <var>old_exception_masks</var>,
                 <strong>old_exception_masks</strong>        <var>old_exception_count</var>,
                 <strong>exception_port_array_t</strong>     <var>old_exception_ports</var>,
                 <strong>exception_behavior_array_t</strong>       <var>old_behaviors</var>,
                 <strong>exception_flavor_array_t</strong>           <var>old_flavors</var><strong>);
<h2>task_get_special_port</h2>
<hr>
<p>
<strong>Function</strong> - Return a send write to the indicated special port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_get_special_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>task</strong>                              <var>special_port</var><strong>);
</strong>


<strong>Macro Forms:</strong>


<strong>kern_return_t   task_get_bootstrap_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>task</strong>                              <var>special_port</var><strong>);
</strong>


<strong>kern_return_t   task_get_kernel_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>task</strong>                              <var>special_port</var><strong>);
</strong>


<strong>kern_return_t   task_get_host_name_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>task</strong>                              <var>special_port</var><strong>);
<h2>task_info</h2>
<hr>
<p>
<strong>Function</strong> - Return per-task information according to specified flavor.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_info</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>task_flavor_t</strong>                           <var>flavor</var>,
                 <strong>task_info_t</strong>                          <var>task_info</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>task_info_count</var><strong>);
<h2>task_policy</h2>
<hr>
<p>
<strong>Function</strong> - Set target task's default scheduling policy state.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_policy</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>policy_t</strong>                                <var>policy</var>,
                 <strong>policy_base_t</strong>                             <var>base</var>,
                 <strong>base</strong>                                <var>base_count</var>,
                 <strong>boolean_t</strong>                            <var>set_limit</var>,
                 <strong>boolean_t</strong>                       <var>change_threads</var><strong>);
<h2>task_resume</h2>
<hr>
<p>
<strong>Function</strong> - Decrement the target task's suspend count.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_resume</strong>
                <strong>(task_t</strong>         <var>task</var><strong>);
<h2>task_sample</h2>
<hr>
<p>
<strong>Function</strong> - Sample the target task's thread program counters periodically.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_sample</strong>
                <strong>(task_t</strong>                             <var>sample_task</var>,
                 <strong>mach_port_make_send_t</strong>               <var>reply_port</var><strong>);
<h2>task_set_emulation</h2>
<hr>
<p>
<strong>Function</strong> - Establish a user-level handler for a system call.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_emulation</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>vm_address_t</strong>                  <var>routine_entry_pt</var>,
                 <strong>int</strong>                             <var>syscall_number</var><strong>);
<h2>task_set_emulation_vector</h2>
<hr>
<p>
<strong>Function</strong> - Establish the target task's user-level system call handlers. 
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_emulation_vector</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                               <var>vector_start</var>,
                 <strong>emulation_vector_t</strong>            <var>emulation_vector</var>,
                 <strong>mach_msg_type_number_t</strong>  <var>emulation_vector_count</var><strong>);
<h2>task_set_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Set target task's exception ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_exception_ports</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>mach_port_t</strong>                     <var>exception_port</var>,
                 <strong>exception_behavior_t</strong>                  <var>behavior</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>task</var> 
<dd>
[in task send right]
The task for which to set the ports.
<p>
<dt> <var>exception_types</var> 
<dd>
[in scalar]
A flag word indicating the types of exceptions for which the 
exception port applies:
<dl>
<p>
<dt> <strong>EXC_MASK_BAD_ACCESS</strong>
<dd>
Could not access memory.
<p>
<dt> <strong>EXC_MASK_BAD_INSTRUCTION</strong>
<dd>
Instruction failed.  Illegal or undefined instruction or operand.
<p>
<dt> <strong>EXC_MASK_ARITHMETIC</strong>
<dd>
Arithmetic exception
<p>
<dt> <strong>EXC_MASK_EMULATION</strong>
<dd>
Emulation instruction.  Emulation support instruction
encountered.
<p>
<dt> <strong>EXC_MASK_SOFTWARE</strong>
<dd>
Software generated exception.
<p>
<dt> <strong>EXC_MASK_BREAKPOINT</strong>
<dd>
Trace, breakpoint, etc.
<p>
<dt> <strong>EXC_MASK_SYSCALL</strong>
<dd>
System call requested.
<p>
<dt> <strong>EXC_MASK_MACH_SYSCALL</strong>
<dd>
System call with a number in the Mach call range requested.
<p>
<dt> <strong>EXC_MASK_RPC_ALERT	</strong>
<dd>
Exceptional condition encountered during execution of RPC.
</dl>
<p>
<dt> <var>exception_port</var> 
<dd>
[in exception send right]
The exception port for all selected exception 
types.
<p>
<dt> <var>behavior</var> 
<dd>
[in scalar]
The type of exception message to be sent.  Defined types are:
<dl>
<p>
<dt> <strong>EXCEPTION_DEFAULT</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.
<p>
<dt> <strong>EXCEPTION_STATE</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.
<p>
<dt> <strong>EXCEPTION_STATE_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.  Mark the exception port (and associated
exceptions) as protected.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.  Mark the exception port 
(and associated exceptions) as protected.
</dl>
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of state to be sent with the exception message. 
These types are defined in <strong>&ltmach/thread_states.h&gt</strong>.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>task_set_exception_ports</strong> function sets a specified
set of exception ports belonging to <var>task</var>.  A task exception port 
is used when a thread specific exception port returns a non-success reply.
<h3>NOTES</h3>
<p>
If the value of the <strong>EXC_MACH_SYSCALL</strong> exception class exception port is 
the host name port, Mach kernel traps are executed by the kernel as expected;
<H2>task_set_info</h2>
<hr>
<p>
<strong>Function</strong> - Set task-specific information state.
<h3>SYNOPSIS</h3>
<pre>
<strong>#include&lttask_info.h&gt</strong>

<strong>kern_return_t   task_set_info</strong>
                <strong>(task_t</strong>                             <var>target_task</var>,
                 <strong>task_flavor_t</strong>                           <var>flavor</var>,
                 <strong>task_info_t</strong>                          <var>task_info</var><strong>);
<h2>task_set_policy</h2>
<hr>
<p>
<strong>Function</strong> - Set target task's default scheduling policy state. (Protected Interface.)
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_policy</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>processor_set_t</strong>                  <var>processor_set</var>,
                 <strong>policy_t</strong>                                <var>policy</var>,
                 <strong>policy_base_t</strong>                             <var>base</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>base_count</var>,
                 <strong>policy_limit_t</strong>                           <var>limit</var>,
                 <strong>mach_msg_type_number_t</strong>             <var>limit_count</var>,
                 <strong>boolean_t</strong>                       <var>change_threads</var><strong>);
<h2>task_set_port_space</h2>
<hr>
<p>
<strong>Function</strong> - Set the size of the target  task's port name space table.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_port_space</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                              <var>table_entries</var><strong>);
<h2>task_set_special_port</h2>
<hr>
<p>
<strong>Function</strong> - Set the indicated special port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_set_special_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
</strong>


<strong>Macro forms:</strong>


<strong>kern_return_t   task_set_bootstrap_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
</strong>


<strong>kern_return_t   task_set_kernel_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
</strong>


<strong>kern_return_t   task_set_host_name_port</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>task</var> 
<dd>
[in task send right]
The port for the task for which to set the port.
<p>
<dt> <var>which_port</var> 
<dd>
[in scalar]
The special port to be set.  Valid values are:
<dl>
<p>
<dt> <strong>TASK_BOOTSTRAP_PORT</strong>
<dd>
[bootstrap send right] The task's bootstrap port.  Used to send 
messages requesting return of other system service ports.
<p>
<dt> <strong>TASK_KERNEL_PORT</strong>
<dd>
[task-self send right] The task's kernel port.  Used by the
kernel to receive messages to manipulate the task. This is the movable task 
port and different from the one returned by <strong>mach_task_self</strong> 
(immovable). Setting this special port does not change the identity of the 
kernel port that names the task;
<h2>task_suspend</h2>
<hr>
<p>
<strong>Function</strong> - Suspend the target task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_suspend</strong>
                <strong>(task_t</strong>          <var>task</var><strong>);
<h2>task_swap_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Set target task's exception ports, returning the previous exception ports.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_swap_exception_ports</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>mach_port_t</strong>                     <var>exception_port</var>,
                 <strong>exception_behavior_t</strong>                  <var>behavior</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var>,
                 <strong>exception_mask_array_t</strong>     <var>old_exception_masks</var>,
                 <strong>old_exception_masks</strong>        <var>old_exception_count</var>,
                 <strong>exception_port_array_t</strong>     <var>old_exception_ports</var>,
                 <strong>exception_behavior_array_t</strong>       <var>old_behaviors</var>,
                 <strong>exception_flavor_array_t</strong>           <var>old_flavors</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>task</var> 
<dd>
[in task send right]
The task for which to set the ports.
<p>
<dt> <var>exception_types</var> 
<dd>
[in scalar]
A flag word indicating the types of exceptions for which the 
exception port applies:
<dl>
<p>
<dt> <strong>EXC_MASK_BAD_ACCESS</strong>
<dd>
Could not access memory.
<p>
<dt> <strong>EXC_MASK_BAD_INSTRUCTION</strong>
<dd>
Instruction failed.  Illegal or undefined instruction or operand.
<p>
<dt> <strong>EXC_MASK_ARITHMETIC</strong>
<dd>
Arithmetic exception
<p>
<dt> <strong>EXC_MASK_EMULATION</strong>
<dd>
Emulation instruction.  Emulation support instruction
encountered.
<p>
<dt> <strong>EXC_MASK_SOFTWARE</strong>
<dd>
Software generated exception.
<p>
<dt> <strong>EXC_MASK_BREAKPOINT</strong>
<dd>
Trace, breakpoint, etc.
<p>
<dt> <strong>EXC_MASK_SYSCALL</strong>
<dd>
System call requested.
<p>
<dt> <strong>EXC_MASK_MACH_SYSCALL</strong>
<dd>
System call with a number in the Mach call range requested.
<p>
<dt> <strong>EXC_MASK_RPC_ALERT	</strong>
<dd>
Exceptional condition encountered during execution of RPC.
</dl>
<p>
<dt> <var>exception_port</var> 
<dd>
[in exception send right]
The exception port for all selected exception 
types.
<p>
<dt> <var>behavior</var> 
<dd>
[in scalar]
The type of exception message to be sent.  Defined types are:
<dl>
<p>
<dt> <strong>EXCEPTION_DEFAULT</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.
<p>
<dt> <strong>EXCEPTION_STATE</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.
<p>
<dt> <strong>EXCEPTION_STATE_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.  Mark the exception port (and associated
exceptions) as protected.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.  Mark the exception port 
(and associated exceptions) as protected.
</dl>
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of state to be sent with the exception message. 
These types are defined in <strong>&ltmach/thread_states.h&gt</strong>.
<p>
<dt> <var>old_exception_masks</var> 
<dd>
[out array of <var>exception_mask_t</var>]
An array, each element being a mask 
specifying for which exception types the corresponding element of the 
other arrays apply.
<p>
<dt> <var>old_exception_count</var> 
<dd>
[pointer to in/out scalar]
On input, the maximum size of the array
buffers;
<h2>task_terminate</h2>
<hr>
<p>
<strong>Function</strong> - Terminate the target task and deallocate its resources.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_terminate</strong>
                <strong>(task_t</strong>            <var>task</var><strong>);
<h2>task_threads</h2>
<hr>
<p>
<strong>Function</strong> - Return the target task's list of threads.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   task_threads</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>thread_act_port_array_t</strong>            <var>thread_list</var>,
                 <strong>mach_msg_type_number_t*</strong>           <var>thread_count</var><strong>);
<h2>thread_abort</h2>
<hr>
<p>
<strong>Function</strong> - Abort a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_abort</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var><strong>);
<h2>thread_abort_safely</h2>
<hr>
<p>
<strong>Function</strong> - Abort a thread, restartably.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_abort_safely</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var><strong>);
<h2>thread_activation_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a thread activation.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_activation_create</strong>
                <strong>(task_t</strong>                                    <var>task</var>,
                 <strong>mach_port_name_t</strong>                      <var>RPC_port</var>,
                 <strong>vm_offset_t</strong>                         <var>user_stack</var>,
                 <strong>vm_size_t</strong>                           <var>stack_size</var>,
                 <strong>thread_act_t</strong>                      <var>thread_act_t</var><strong>);
<h2>thread_assign</h2>
<hr>
<p>
<strong>Function</strong> - Assign a thread to a processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_assign</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>processor_set_t</strong>                  <var>processor_set</var><strong>);
<h2>thread_assign_default</h2>
<hr>
<p>
<strong>Function</strong> - Assign a thread to the default processor set.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_assign_default</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var><strong>);
<h2>thread_create</h2>
<hr>
<p>
<strong>Function</strong> - Create a thread within a task.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_create</strong>
                <strong>(task_t</strong>                             <var>parent_task</var>,
                 <strong>thread_act_t</strong>                      <var>child_thread</var><strong>);
<h2>thread_create_running</h2>
<hr>
<p>
<strong>Function</strong> - Optimized creation of a running thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_create_running</strong>
                <strong>(task_t</strong>                             <var>parent_task</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var>,
                 <strong>thread_state_t</strong>                           <var>state</var>,
                 <strong>thread_act_t</strong>                      <var>child_thread</var><strong>);
<h2>thread_depress_abort</h2>
<hr>
<p>
<strong>Function</strong> - Cancel thread scheduling depression.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_depress_abort</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var><strong>);
<h2>thread_get_assignment</h2>
<hr>
<p>
<strong>Function</strong> - Return the processor set to which a thread is assigned.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_get_assignment</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>processor_set_name_t</strong>             <var>processor_set</var><strong>);
<h2>thread_get_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Return a send right to an exception port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_get_exception_ports</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>exception_mask_array_t</strong>     <var>old_exception_masks</var>,
                 <strong>old_exception_masks</strong>        <var>old_exception_count</var>,
                 <strong>exception_port_array_t</strong>     <var>old_exception_ports</var>,
                 <strong>exception_behavior_array_t</strong>       <var>old_behaviors</var>,
                 <strong>exception_flavor_array_t</strong>           <var>old_flavors</var><strong>);
<h2>thread_get_special_port</h2>
<hr>
<p>
<strong>Function</strong> - Return a send right to the caller-specified special port.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_get_special_port</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>thread</strong>                            <var>special_port</var><strong>);
</strong>
</pre>

<h4>Macro form:</h4>

<pre>
<strong>kern_return_t   thread_get_kernel_port</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>thread</strong>                            <var>special_port</var><strong>);
<h2>thread_get_state</h2>
<hr>
<p>
<strong>Function</strong> - Return the execution state for a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_get_state</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var>,
                 <strong>thread_state_t</strong>                       <var>old_state</var>,
                 <strong>mach_msg_type_number_t</strong>         <var>old_state_count</var><strong>);
on output, the 
size returned (in natural-sized units).
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>thread_get_state</strong> function returns the execution
state (for example, the
machine registers) for <var>target_thread</var>. flavor specifies the type
of state information 
returned.
<p>
The format of the data returned is machine specific;
<h2>thread_info</h2>
<hr>
<p>
<strong>Function</strong> - Return information about a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_info</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var>,
                 <strong>thread_flavor_t</strong>                         <var>flavor</var>,
                 <strong>thread_info_t</strong>                      <var>thread_info</var>,
                 <strong>mach_msg_type_number_t</strong>       <var>thread_info_count</var><strong>);
<h2>thread_policy</h2>
<hr>
<p>
<strong>Function</strong> - Set target thread's scheduling policy state.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_policy</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>policy_t</strong>                                <var>policy</var>,
                 <strong>policy_base_t</strong>                             <var>base</var>,
                 <strong>base</strong>                                <var>base_count</var>,
                 <strong>boolean_t</strong>                            <var>set_limit</var><strong>);
<h2>thread_resume</h2>
<hr>
<p>
<strong>Function</strong> - Resume a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_resume</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var><strong>);
<h2>thread_sample</h2>
<hr>
<p>
<strong>Function</strong> - Perform periodic PC sampling for a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_sample</strong>
                <strong>(thread_act_t</strong>                     <var>sample_thread</var>,
                 <strong>mach_port_make_send_t</strong>               <var>reply_port</var><strong>);
<h2>thread_set_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Set exception ports for a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_set_exception_ports</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>mach_port_t</strong>                     <var>exception_port</var>,
                 <strong>exception_behavior_t</strong>                  <var>behavior</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>thread</var> 
<dd>
[in thread send right]
The thread for which to set the ports.
<p>
<dt> <var>exception_types</var> 
<dd>
[in scalar]
A flag word indicating the types of exceptions for which the 
exception port applies:
<dl>
<p>
<dt> <strong>EXC_MASK_BAD_ACCESS</strong>
<dd>
Could not access memory.
<p>
<dt> <strong>EXC_MASK_BAD_INSTRUCTION</strong>
<dd>
Instruction failed.  Illegal or undefined instruction or operand.
<p>
<dt> <strong>EXC_MASK_ARITHMETIC</strong>
<dd>
Arithmetic exception.
<p>
<dt> <strong>EXC_MASK_EMULATION</strong>
<dd>
Emulation instruction.  Emulation support instruction
encountered.
<p>
<dt> <strong>EXC_MASK_SOFTWARE</strong>
<dd>
Software generated exception.
<p>
<dt> <strong>EXC_MASK_BREAKPOINT</strong>
<dd>
Trace, breakpoint, etc.
<p>
<dt> <strong>EXC_MASK_SYSCALL</strong>
<dd>
System call requested.
<p>
<dt> <strong>EXC_MASK_MACH_SYSCALL</strong>
<dd>
System call with a number in the Mach call range requested.
</dl>
<p>
<dt> <var>exception_port</var> 
<dd>
[in exception send right]
The exception port for all selected exception 
types.
<p>
<dt> <var>behavior</var> 
<dd>
[in scalar]
The type of exception message to be sent.  Defined types are:
<dl>
<p>
<dt> <strong>EXCEPTION_DEFAULT</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.
<p>
<dt> <strong>EXCEPTION_DEFAULT_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.  Mark the exception port (and associated exceptions) 
as protected.
<p>
<dt> <strong>EXCEPTION_STATE</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.
<p>
<dt> <strong>EXCEPTION_STATE_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.  Mark the exception port (and associated
exceptions) as protected.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.  Mark the exception port 
(and associated exceptions) as protected.
</dl>
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of state to be sent with the exception message. 
These types are defined in \*L<mach/thread_states.h>\*O.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>thread_set_exception_ports</strong> function sets a specified
set of exception 
ports belonging to <var>thread</var>.
<h3>NOTES</h3>
<p>
If the value of the <strong>EXC_MACH_SYSCALL</strong> exception class exception port is 
the host name port, Mach kernel traps are executed by the kernel as expected;
<h2>thread_set_policy</h2>
<hr>
<p>
<strong>Function</strong> - Set target thread's scheduling policy state. (Protected Interface.)
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_set_policy</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>processor_set_t</strong>                  <var>processor_set</var>,
                 <strong>policy_t</strong>                                <var>policy</var>,
                 <strong>policy_base_t</strong>                             <var>base</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>base_count</var>,
                 <strong>policy_limit_t</strong>                           <var>limit</var>,
                 <strong>mach_msg_type_number_</strong>              <var>limit_count</var><strong>);
<h2>thread_set_special_port</h2>
<hr>
<p>
<strong>Function</strong> - Set caller-specified special port belonging to the target thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_set_special_port</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>int</strong>                                 <var>which_port</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
</strong>
</pre>

<h4>Macro form:</h4>
<pre>
<strong>kern_return_t   thread_set_kernel_port</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>mach_port_t</strong>                       <var>special_port</var><strong>);
<h2>thread_set_state</h2>
<hr>
<p>
<strong>Function</strong> - Set the target thread's user-mode execution state.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_set_state</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var>,
                 <strong>thread_state_t</strong>                       <var>new_state</var>,
                 <strong>target_thread</strong>                  <var>new_state_count</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>target_thread</var> 
<dd>
[in thread send right]
The thread for which to set the execution state. 
The calling thread cannot specify itself.
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of state to set.  Valid values correspond to
supported machine architecture features.
<p>
<dt> <var>new_state</var> 
<dd>
[pointer to in structure]
State information for the specified thread.
<p>
<dt> <var>new_state_count</var> 
<dd>
[in scalar]
The size of the buffer (in natural-sized units).
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>thread_set_state</strong> function sets the execution state
(for example, the
machine registers) for <var>target_thread</var>.  <var>flavor</var> specifies the type
of state to set.
<p>
The format of the state to set is machine specific;
<h2>thread_suspend</h2>
<hr>
<p>
<strong>Function</strong> - Suspend a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_suspend</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var><strong>);
<h2>thread_switch</h2>
<hr>
<p>
<strong>Function</strong> - Cause context switch with options.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_switch</strong>
                <strong>(mach_port_t</strong>                         <var>new_thread</var>,
                 <strong>int</strong>                                     <var>option</var>,
                 <strong>mach_msg_timeout_t</strong>                        <var>time</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>new_thread</var> 
<dd>
[in thread send right]
Thread to which the processor should switch
context.
<p>
<dt> <var>option</var> 
<dd>
[in scalar]
Options applicable to the context switch.
<p>
<dt> <var>time</var> 
<dd>
[in scalar]
Time duration during which the thread should be affected by 
<var>option</var>.
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>thread_switch</strong> function provides low-level access
to the scheduler's
context switching code.  <var>new_thread</var> is a hint that implements
hand-off scheduling. 
The operating system will attempt to switch directly to the new thread 
(bypassing the normal logic that selects the next thread to run) 
if possible.  Since this is a hint, it may be incorrect;
it is ignored if it 
doesn't specify a thread on the same host as the current thread or if 
the scheduler cannot switch to that thread (i.e., 
not runable or already running on another processor).  In this
case, the normal 
logic to select the next thread to run is used;
the current thread
may continue 
running if there is no other appropriate thread to run.
<p>
The <var>option</var> argument specifies the interpretation and use of <var>time</var>.
The possible values (from \*L<mach/thread_switch.h>\*O) are:
<dl>
<dt> <strong>SWITCH_OPTION_NONE</strong>
<dd>
The <var>time</var> argument is ignored.
<dt> <strong>SWITCH_OPTION_WAIT</strong>
<dd>
The thread is blocked for the specified <var>time</var>.  This wait cannot be
canceled by <strong>thread_resume</strong>;
only <strong>thread_abort</strong> can terminate this wait.
<dt> <strong>SWITCH_OPTION_DEPRESS</strong>
<dd>
The thread's scheduling attributes are temporarily set so as to provide 
it with the lowest possible service for duration <var>time</var>.  The scheduling
depression is aborted when <var>time</var> has passed, when the current thread is 
next run (either via hand-off scheduling or because the processor set 
has nothing better to do), or when <strong>thread_abort</strong> or
<strong>thread_depress_abort</strong> is applied to the current thread.
Changing the thread's scheduling attributes (via <strong>thread_policy</strong>) 
will not affect this depression.
<dt> <strong>SWITCH_OPTION_IDLE</strong>
<dd>
This option is similar to <strong>SWITCH_OPTION_DEPRESS</strong> however, the 
thread's scheduling attributes are temporarily set so as to place it 
at a service level that is below all runnable threads for 
duration <var>time</var>.  The scheduling depression is aborted 
when <var>time</var> has passed, when the current thread is 
next run (either via hand-off scheduling or because the processor set 
has nothing better to do), or when <strong>thread_abort</strong> or
<strong>thread_depress_abort</strong> is applied to the current thread.
Changing the thread's scheduling attributes (via <strong>thread_policy</strong>) 
will not affect this depression.
</dl>
<p>
The minimum time and units of time can be obtained as the <var>min_timeout</var> 
value from the <strong>HOST_SCHED_INFO</strong> flavor of <strong>host_info</strong>.
<h3>NOTES</h3>
<p>
<strong>thread_switch</strong> is often called when the current thread
can proceed no further 
for some reason;
the various options and arguments allow information about 
this reason to be transmitted to the kernel.  The <var>new_thread</var>
argument (hand-off 
scheduling) is useful when the identity of the thread that must make progress
before the current thread runs again is known.  The <strong>SWITCH_OPTION_WAIT</strong>
option is used when the amount of time that the current thread
must wait before it 
can do anything useful can be estimated and is fairly short,
especially when the 
identity of the thread for which this thread must wait is not known.
<h3>CAUTIONS</h3>
<p>
Users should beware of calling <strong>thread_switch</strong> with an
invalid hint (e.g., <strong>THREAD_NULL</strong>) and no option.  
Because the time-sharing scheduler varies the 
priority of threads based on usage, this may result in a waste
of CPU time if the 
thread that must be run is of lower priority.  The use of the 
<strong>SWITCH_OPTION_DEPRESS</strong> option in this situation is highly recommended.
<p>
<strong>thread_switch</strong> ignores policies.  Users relying on the
preemption semantics of a 
fixed time policy should be aware that <strong>thread_switch</strong>
ignores these semantics;
<h2>thread_terminate</h2>
<hr>
<p>
<strong>Function</strong> - Destroy a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_terminate</strong>
                <strong>(thread_act_t</strong>                     <var>target_thread</var><strong>);
<h2>thread_wire</h2>
<hr>
<p>
<strong>Function</strong> - Mark the thread as privileged with respect to kernel resources.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_wire</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>thread_act_t</strong>                            <var>thread</var>,
                 <strong>boolean_t</strong>                                <var>wired</var><strong>);
<h2>thread_swap_exception_ports</h2>
<hr>
<p>
<strong>Function</strong> - Swap exception ports for a thread.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   thread_swap_exception_ports</strong>
                <strong>(thread_act_t</strong>                            <var>thread</var>,
                 <strong>exception_mask_t</strong>               <var>exception_types</var>,
                 <strong>mach_port_t</strong>                     <var>exception_port</var>,
                 <strong>exception_behavior_t</strong>                  <var>behavior</var>,
                 <strong>thread_state_flavor_t</strong>                   <var>flavor</var>,
                 <strong>exception_mask_array_t</strong>     <var>old_exception_masks</var>,
                 <strong>old_exception_masks</strong>        <var>old_exception_count</var>,
                 <strong>exception_port_array_t</strong>     <var>old_exception_ports</var>,
                 <strong>exception_behavior_array_t</strong>       <var>old_behaviors</var>,
                 <strong>exception_flavor_array_t</strong>           <var>old_flavors</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>thread</var> 
<dd>
[in thread send right]
The thread for which to set the ports.
<p>
<dt> <var>exception_types</var> 
<dd>
[in scalar]
A flag word indicating the types of exceptions for which the 
exception port applies:
<dl>
<p>
<dt> <strong>EXC_MASK_BAD_ACCESS</strong>
<dd>
Could not access memory.
<p>
<dt> <strong>EXC_MASK_BAD_INSTRUCTION</strong>
<dd>
Instruction failed.  Illegal or undefined instruction or operand.
<p>
<dt> <strong>EXC_MASK_ARITHMETIC</strong>
<dd>
Arithmetic exception
<p>
<dt> <strong>EXC_MASK_EMULATION</strong>
<dd>
Emulation instruction.  Emulation support instruction
encountered.
<p>
<dt> <strong>EXC_MASK_SOFTWARE</strong>
<dd>
Software generated exception.
<p>
<dt> <strong>EXC_MASK_BREAKPOINT</strong>
<dd>
Trace, breakpoint, etc.
<p>
<dt> <strong>EXC_MASK_SYSCALL</strong>
<dd>
System call requested.
<p>
<dt> <strong>EXC_MASK_MACH_SYSCALL</strong>
<dd>
System call with a number in the Mach call range requested.
</dl>
<p>
<dt> <var>exception_port</var> 
<dd>
[in exception send right]
The exception port for all selected exception 
types.
<p>
<dt> <var>behavior</var> 
<dd>
[in scalar]
Control of the behavior of the exception processing.  Defined 
types are:
<dl>
<p>
<dt> <strong>EXCEPTION_DEFAULT</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.
<p>
<dt> <strong>EXCEPTION_DEFAULT_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise</strong> message including the thread 
identity.  Mark the exception port (and associated exceptions) 
as protected.
<p>
<dt> <strong>EXCEPTION_STATE</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.
<p>
<dt> <strong>EXCEPTION_STATE_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state</strong> message including the 
thread state.  Mark the exception port (and associated
exceptions) as protected.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.
<p>
<dt> <strong>EXCEPTION_STATE_IDENTITY_PROTECTED</strong>
<dd>
Send a <strong>catch_exception_raise_state_identity</strong> message
including the thread identity and state.  Mark the exception port 
(and associated exceptions) as protected.
</dl>
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of state to be sent with the exception message. 
These types are defined in \*L<mach/thread_states.h>\*O.
<p>
<dt> <var>old_exception_masks</var> 
<dd>
[out array of <var>exception_mask_t</var>]
An array, each element being a mask 
specifying for which exception types the corresponding element of the 
other arrays apply.
<p>
<dt> <var>old_exception_count</var> 
<dd>
[pointer to in/out scalar]
On input, the maximum size of the array
buffers;
<h2>vm_allocate</h2>
<hr>
<p>
<strong>Function</strong> - Allocate a region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_allocate</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>boolean_t</strong>                             <var>anywhere</var><strong>);
<h2>vm_behavior_set</h2>
<hr>
<p>
<strong>Function</strong> - Specify expected access patterns for the target VM region.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_behavior_set</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>vm_behavior_t</strong>                         <var>behavior</var><strong>);
<h2>vm_copy</h2>
<hr>
<p>
<strong>Function</strong> - Copy a region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_copy</strong>
                <strong>(vm_task_t</strong>            <var>target_task</var>,
                 <strong>vm_address_t</strong>      <var>source_address</var>,
                 <strong>vm_size_t</strong>                  <var>count</var>,
                 <strong>vm_address_t</strong>        <var>dest_address</var><strong>);
<h2>vm_deallocate</h2>
<hr>
<p>
<strong>Function</strong> - Deallocate a region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_deallocate</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var><strong>);
<h2>vm_inherit</h2>
<hr>
<p>
<strong>Function</strong> - Set a VM region's inheritance attribute.
<p>
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_inherit</strong>
                 <strong>(vm_task_t</strong>                   <var>target_task</var>,
                  <strong>vm_address_t</strong>                    <var>address</var>,
                  <strong>vm_size_t</strong>                          <var>size</var>,
                  <strong>vm_inherit_t</strong>            <var>new_inheritance</var><strong>);
<h2>vm_machine_attribute</h2>
<hr>
<p>
<strong>Function</strong> - Get/set the target memory region's special attributes.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_machine_attribute</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>vm_machine_attribute_t</strong>               <var>attribute</var>,
                 <strong>vm_machine_attribute_val_t</strong>               <var>value</var><strong>);
<h2>vm_map</h2>
<hr>
<p>
<strong>Function</strong> - Map the specified memory object to a region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_map</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>vm_address_t</strong>                              <var>mask</var>,
                 <strong>boolean_t</strong>                             <var>anywhere</var>,
                 <strong>memory_object_t</strong>                  <var>memory_object</var>,
                 <strong>vm_offset_t</strong>                             <var>offset</var>,
                 <strong>boolean_t</strong>                                 <var>copy</var>,
                 <strong>vm_prot_t</strong>                       <var>cur_protection</var>,
                 <strong>vm_prot_t</strong>                       <var>max_protection</var>,
                 <strong>vm_inherit_t</strong>                       <var>inheritance</var><strong>);
<h2>vm_msync</h2>
<hr>
<p>
<strong>Function</strong> - Synchronize the specified region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_msync</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>target_task</strong>                         <var>sync_flags</var><strong>);
<h2>vm_protect</h2>
<hr>
<p>
<strong>Function</strong> - Set access privilege attribute for a region of virtual memory.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_protect</strong>
                 <strong>(vm_task_t</strong>           <var>target_task</var>,
                  <strong>vm_address_t</strong>            <var>address</var>,
                  <strong>vm_size_t</strong>                  <var>size</var>,
                  <strong>boolean_t</strong>           <var>set_maximum</var>,
                  <strong>vm_prot_t</strong>        <var>new_protection</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>target_task</var> 
<dd>
[in task send right]
The port for the task whose address space contains 
the region.
<p>
<dt> <var>address</var> 
<dd>
[in scalar]
The starting address for the region.
<p>
<dt> <var>size</var> 
<dd>
[in scalar]
The number of bytes in the region.
<p>
<dt> <var>set_maximum</var> 
<dd>
[in scalar]
Maximum/current indicator.  If true, the new protection sets 
the maximum protection for the region.  If false, the new protection sets 
the current protection for the region.  If the maximum protection is set 
below the current protection, the current protection is also
     reset to the new 
maximum.
<p>
<dt> <var>new_protection</var> 
<dd>
[in scalar]
The new protection for the region.  Valid values are obtained 
by or'ing together the following values:
<dl>
<p>
<dt> <strong>VM_PROT_READ</strong>
<dd>
Allows read access.
<p>
<dt> <strong>VM_PROT_WRITE</strong>
<dd>
Allows write access.
<p>
<dt> <strong>VM_PROT_EXECUTE</strong>
<dd>
Allows execute access.
</dl>
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>vm_protect</strong> function sets access privileges for
a region within the specified 
task's address space.
The <var>new_protection</var> parameter specifies a combination
of read, write, and 
execute accesses that are allowed (rather than prohibited).
<p>
The region starts at the beginning of the virtual page containing
<var>address</var>;
<h2>vm_read</h2>
<hr>
<p>
<strong>Function</strong> - Read the specified range of target task's address space.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_read</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>size</strong>                                  <var>data_out</var>,
                 <strong>target_task</strong>                         <var>data_count</var><strong>);
</strong>
</pre>

<h4>Overwrite form:</h4>
<pre>
<strong>kern_return_t   vm_read_overwrite</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>pointer_t</strong>                              <var>data_in</var>,
                 <strong>target_task</strong>                         <var>data_count</var><strong>);
on output, the 
size returned (in natural-sized units).
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>vm_read</strong> and <strong>vm_read_overwrite</strong>
functions read a portion of a task's virtual
memory (they enable tasks to read other tasks' memory).
The <strong>vm_read</strong> function returns the data in a dynamically
allocated array of bytes;
<h2>vm_region</h2>
<hr>
<p>
<strong>Function</strong> - Return description of a virtual memory region.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_region</strong>
                 <strong>(vm_task_t</strong>                    <var>target_task</var>,
                  <strong>vm_address_t</strong>                     <var>address</var>,
                  <strong>vm_size_t</strong>                           <var>size</var>,
                  <strong>vm_region_flavor_t</strong>                <var>flavor</var>,
                  <strong>vm_region_info_t</strong>                    <var>info</var>,
                  <strong>mach_msg_type_number_t</strong>        <var>info_count</var>,
                  <strong>memory_object_name_t</strong>         <var>object_name</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>target_task</var> 
<dd>
[in task send right]
The port for the task whose address space contains 
the region.
<p>
<dt> <var>address</var> 
<dd>
[pointer to in/out scalar]
The address at which to start looking for a
region.  The function returns the starting address actually used.
<p>
<dt> <var>size</var> 
<dd>
[out scalar]
The number of bytes in the located region.  The number 
converts to an integral number of virtual pages.
<p>
<dt> <var>flavor</var> 
<dd>
[in scalar]
The type of information to be returned.  Valid values are:
<dl>
<p>
<dt> <strong>VM_REGION_BASIC_INFO</strong>
<dd>
Basic information about the region (size, inheritance, etc.). 
This information is declared by the
     <strong>vm_region_basic_info</strong> structure.
</dl>
<p>
<dt> <var>info</var> 
<dd>
[out structure]
Returned region information.
<p>
<dt> <var>info_count</var> 
<dd>
[in/out scalar]
On input, the maximum size of the buffer;
<h2>vm_remap</h2>
<hr>
<p>
<strong>Function</strong> - Map memory objects in one task's address space to that of another task's.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_remap</strong>
                 <strong>(mach_port_t</strong>	           <var>target_task</var>,
                  <strong>vm_address_t</strong>	        <var>target_address</var>,
                  <strong>vm_size_t</strong>	                  <var>size</var>,
                  <strong>vm_address_t</strong>	                  <var>mask</var>,
                  <strong>boolean_t</strong>	              <var>anywhere</var>,
                  <strong>mach_port_t</strong>	           <var>source_task</var>,
                  <strong>vm_address_t</strong>	        <var>source_address</var>,
                  <strong>boolean_t</strong>	                  <var>copy</var>,
                  <strong>vm_prot_t</strong>	        <var>cur_protection</var>,
                  <strong>vm_prot_t</strong>	        <var>max_protection</var>,
                  <strong>vm_inherit_t</strong>            <var>inheritance</var><strong>);
<h2>vm_wire</h2>
<hr>
<p>
<strong>Function</strong> - Modify the target region's paging characteristics.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_wire</strong>
                <strong>(host_priv_t</strong>                               <var>host</var>,
                 <strong>vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>vm_size_t</strong>                                 <var>size</var>,
                 <strong>vm_prot_t</strong>                         <var>wired_access</var><strong>);
</strong>
</pre>
<h3>PARAMETERS</h3>
<dl>
<p>
<dt> <var>host</var> 
<dd>
[in host-control send right]
The control port for the host for which
information is to be obtained.
<p>
<dt> <var>target_task</var> 
<dd>
[in task send right]
The port for the task whose address space contains 
the region.
<p>
<dt> <var>address</var> 
<dd>
[in scalar]
The starting address for the region.
<p>
<dt> <var>size</var> 
<dd>
[in scalar]
The number of bytes in the region.
<p>
<dt> <var>wired_access</var> 
<dd>
[in scalar]
The pageability of the region. The following values cause
the region to be wired and protected as specified
(values may be combined):
<dl>
<dt> <strong>VM_PROT_READ</strong>
<dt> <strong>VM_PROT_WRITE</strong>
<dt> <strong>VM_PROT_execute</strong>
</dl>
<p>
The following value causes the region to be unwired (made pageable):
<dl>
<dt> <strong>VM_PROT_NONE</strong>
</dl>
</dl>
<h3>DESCRIPTION</h3>
<p>
The <strong>vm_wire</strong> function sets the pageability privileges
for a region within the 
specified task's address space.  <var>wired_access</var> specifies the types
of accesses to 
the memory region which must not suffer from (internal) faults
of any kind after 
this call returns.  A non-null <var>wired_access</var> value indicates that
the page is to be 
"wired" into memory;
<h2>vm_write</h2>
<hr>
<p>
<strong>Function</strong> - Write data to the specified address in the target task's address space.
<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_write</strong>
                <strong>(vm_task_t</strong>                          <var>target_task</var>,
                 <strong>vm_address_t</strong>                           <var>address</var>,
                 <strong>pointer_t</strong>                                 <var>data</var>,
                 <strong>mach_msg_type_number_t</strong>              <var>data_count</var><strong>);
<h2>vm_set_default_memory_manager</h2>
<hr>
<p>
<strong>Function</strong> - Obsolete interface.  Functionality now provided via host_set_default_memory_manager interface.<h3>SYNOPSIS</h3>
<pre>
<strong>kern_return_t   vm_set_default_memory_manager</strong>
                <strong>(host_priv_t</strong>                          <var>host_priv</var>,
                 <strong>mach_port_move_send_t</strong>          <var>default_manager</var><strong>);
void entropy_init(size_t seed_size, uint8_t *seed);
void entropy_collect(void);
int32_t entropy_provide(size_t *entropy_size, void *entropy, void *arg);
uint32_t entropy_filter(uint32_t sample_count, entropy_sample_t *samples, uint32_t filter_count, bitmap_t *filter);
__BEGIN_DECLS



void random_cpu_init(int cpu);
void register_and_init_prng(struct cckprng_ctx *ctx, const struct cckprng_funcs *funcs);
extern void random_bool_init(struct bool_gen * bg);
extern void random_bool_gen_entropy(struct bool_gen * bg, unsigned int * buffer, int count);
extern unsigned int random_bool_gen_bits(struct bool_gen * bg, unsigned int * buffer, unsigned int count, unsigned int numbits);
__BEGIN_DECLS


kern_return_t
KUNCUserNotificationDisplayNotice(
	int             noticeTimeout,
	unsigned        flags,
	char            *iconPath,
	char            *soundPath,
	char            *localizationPath,
	char            *alertHeader,
	char            *alertMessage,
	char            *defaultButtonTitle) __attribute__((deprecated));
kern_return_t
KUNCUserNotificationDisplayAlert(
	int             alertTimeout,
	unsigned        flags,
	char            *iconPath,
	char            *soundPath,
	char            *localizationPath,
	char            *alertHeader,
	char            *alertMessage,
	char            *defaultButtonTitle,
	char            *alternateButtonTitle,
	char            *otherButtonTitle,
	unsigned        *responseFlags) __attribute__((deprecated));
kern_return_t
KUNCExecute(
	char    *executionPath,
	int     openAsUser,
	int     pathExecutionType) __attribute__((deprecated));
KUNCUserNotificationID KUNCGetNotificationID(void) __attribute__((deprecated));
kern_return_t
KUNCUserNotificationDisplayFromBundle(
	KUNCUserNotificationID          notificationID,
	char                            *bundleIdentifier,
	char                            *fileName,
	char                            *fileExtension,
	char                            *messageKey,
	char                            *tokenString,
	KUNCUserNotificationCallBack    callback,
	int                             contextKey) __attribute__((deprecated));
extern kern_return_t
cpm_allocate(vm_size_t size, vm_page_t *list, ppnum_t max_pnum, ppnum_t pnum_mask, boolean_t wire, int flags);
#pragma once


#pragma mark - Building blocks


typedef 
static const size_t lz4_hash_table_size = LZ4_COMPRESS_HASH_ENTRIES * sizeof(lz4_hash_entry_t);
extern void lz4_encode_2gb(uint8_t **dst_ptr, size_t dst_size,
    const uint8_t **src_ptr, const uint8_t *src_begin, size_t src_size,
    lz4_hash_entry_t hash_table[LZ4_COMPRESS_HASH_ENTRIES], int skip_final_literals);
extern int lz4_decode(uint8_t **dst_ptr, uint8_t *dst_begin, uint8_t *dst_end,
    const uint8_t **src_ptr, const uint8_t *src_end);
#pragma mark - Buffer interfaces (LZ4 RAW)

size_t lz4raw_encode_buffer(uint8_t * __restrict dst_buffer, size_t dst_size,
    const uint8_t * __restrict src_buffer, size_t src_size,
    lz4_hash_entry_t hash_table[LZ4_COMPRESS_HASH_ENTRIES]);
size_t lz4raw_decode_buffer(uint8_t * __restrict dst_buffer, size_t dst_size,
    const uint8_t * __restrict src_buffer, size_t src_size,
    void * __restrict work __attribute__((unused)));
extern mach_port_t convert_memory_object_to_port(
	memory_object_t         object);
extern memory_object_t convert_port_to_memory_object(
	mach_port_t             port);
extern upl_t convert_port_to_upl(
	ipc_port_t      port);
extern ipc_port_t convert_upl_to_port( upl_t );
__private_extern__
memory_object_default_t memory_manager_default_reference(void);
__private_extern__
kern_return_t           memory_manager_default_check(void);
__private_extern__
memory_object_control_t memory_object_control_allocate(
	vm_object_t             object);
__private_extern__
void                    memory_object_control_collapse(
	memory_object_control_t *control,
	vm_object_t             object);
__private_extern__
vm_object_t             memory_object_control_to_vm_object(
	memory_object_control_t control);
__private_extern__
vm_object_t             memory_object_to_vm_object(
	memory_object_t mem_obj);
extern void memory_object_control_disable(
	memory_object_control_t *control);
extern boolean_t        memory_object_is_shared_cache(
	memory_object_control_t         control);
extern void             memory_object_mark_for_realtime(
	memory_object_control_t         control,
	bool                            for_realtime);
extern kern_return_t    copypv(
	addr64_t source,
	addr64_t sink,
	unsigned int size,
	int which);
extern boolean_t pmap_has_managed_page(ppnum_t first, ppnum_t last);
extern pmap_t           pmap_create_options(    
	ledger_t        ledger,
	vm_map_size_t   size,
	unsigned int    flags);
extern void *pmap_steal_memory(vm_size_t size, vm_size_t alignment);
extern void *pmap_steal_freeable_memory(vm_size_t size);
extern uint_t pmap_free_pages(void);
extern uint_t pmap_free_pages_span(void);
extern void pmap_startup(vm_offset_t *startp, vm_offset_t *endp);
extern void pmap_init(void);
extern void mapping_adjust(void);
extern void mapping_free_prime(void);
extern boolean_t pmap_next_page(ppnum_t *pnum);
extern boolean_t pmap_next_page_hi(ppnum_t *pnum, boolean_t might_free);
extern void pmap_virtual_space(
	vm_offset_t     *virtual_start,
	vm_offset_t     *virtual_end);
extern pmap_t(pmap_kernel)(void);
extern void             pmap_reference(pmap_t pmap);
extern void             pmap_destroy(pmap_t pmap);
extern void             pmap_switch(pmap_t pmap, thread_t thread);
extern void             pmap_require(pmap_t pmap);
extern kern_return_t    pmap_enter(     
	pmap_t          pmap,
	vm_map_offset_t v,
	ppnum_t         pn,
	vm_prot_t       prot,
	vm_prot_t       fault_type,
	unsigned int    flags,
	boolean_t       wired,
	pmap_mapping_type_t mapping_type);
extern kern_return_t    pmap_enter_options(
	pmap_t pmap,
	vm_map_offset_t v,
	ppnum_t pn,
	vm_prot_t prot,
	vm_prot_t fault_type,
	unsigned int flags,
	boolean_t wired,
	unsigned int options,
	void *arg,
	pmap_mapping_type_t mapping_type);
extern kern_return_t    pmap_enter_options_addr(
	pmap_t pmap,
	vm_map_offset_t v,
	pmap_paddr_t pa,
	vm_prot_t prot,
	vm_prot_t fault_type,
	unsigned int flags,
	boolean_t wired,
	unsigned int options,
	void *arg,
	pmap_mapping_type_t mapping_type);
extern void             pmap_remove_some_phys(
	pmap_t          pmap,
	ppnum_t         pn);
extern void             pmap_lock_phys_page(
	ppnum_t         pn);
extern void             pmap_unlock_phys_page(
	ppnum_t         pn);
extern void             pmap_page_protect(      
	ppnum_t phys,
	vm_prot_t       prot);
extern void             pmap_page_protect_options(      
	ppnum_t phys,
	vm_prot_t       prot,
	unsigned int    options,
	void            *arg);
extern void(pmap_zero_page)(
	ppnum_t         pn);
extern void(pmap_zero_page_with_options)(
	ppnum_t         pn,
	int             options);
extern void(pmap_zero_part_page)(
	ppnum_t         pn,
	vm_offset_t     offset,
	vm_size_t       len);
extern void(pmap_copy_page)(
	ppnum_t         src,
	ppnum_t         dest,
	int             options);
extern void(pmap_copy_part_page)(
	ppnum_t         src,
	vm_offset_t     src_offset,
	ppnum_t         dst,
	vm_offset_t     dst_offset,
	vm_size_t       len);
extern void(pmap_copy_part_lpage)(
	vm_offset_t     src,
	ppnum_t         dst,
	vm_offset_t     dst_offset,
	vm_size_t       len);
extern void(pmap_copy_part_rpage)(
	ppnum_t         src,
	vm_offset_t     src_offset,
	vm_offset_t     dst,
	vm_size_t       len);
extern unsigned int(pmap_disconnect)(   
	ppnum_t         phys);
extern unsigned int(pmap_disconnect_options)(   
	ppnum_t         phys,
	unsigned int    options,
	void            *arg);
extern kern_return_t(pmap_attribute_cache_sync)(      
	ppnum_t         pn,
	vm_size_t       size,
	vm_machine_attribute_t attribute,
	vm_machine_attribute_val_t* value);
extern unsigned int(pmap_cache_attributes)(
	ppnum_t         pn);
extern  void            pmap_set_cache_attributes(
	ppnum_t,
	unsigned int);
extern void            *pmap_map_compressor_page(
	ppnum_t);
extern void             pmap_unmap_compressor_page(
	ppnum_t,
	void*);
__enum_decl(unified_page_list_type_t, uint8_t, {
	
	UNIFIED_PAGE_LIST_TYPE_UPL_ARRAY,
	
	UNIFIED_PAGE_LIST_TYPE_VM_PAGE_LIST,
	
	UNIFIED_PAGE_LIST_TYPE_VM_PAGE_OBJ_Q,
	
	UNIFIED_PAGE_LIST_TYPE_VM_PAGE_FIFO_Q,
});
extern void unified_page_list_iterator_next(unified_page_list_iterator_t *iter);
extern bool unified_page_list_iterator_end(const unified_page_list_iterator_t *iter);
extern ppnum_t unified_page_list_iterator_page(
	const unified_page_list_iterator_t *iter,
	bool *is_fictitious);
extern void pmap_batch_set_cache_attributes(
	const unified_page_list_t *,
	unsigned int);
extern void pmap_sync_page_data_phys(ppnum_t pa);
extern void pmap_sync_page_attributes_phys(ppnum_t pa);
extern bool pmap_verify_free(ppnum_t pn);
extern void(pmap_copy)(                         
	pmap_t          dest,
	pmap_t          source,
	vm_map_offset_t dest_va,
	vm_map_size_t   size,
	vm_map_offset_t source_va);
extern kern_return_t(pmap_attribute)(           
	pmap_t          pmap,
	vm_map_offset_t va,
	vm_map_size_t   size,
	vm_machine_attribute_t  attribute,
	vm_machine_attribute_val_t* value);
extern void             pmap_clear_reference(ppnum_t     pn);
extern boolean_t(pmap_is_referenced)(ppnum_t     pn);
extern void             pmap_set_modify(ppnum_t  pn);
extern void             pmap_clear_modify(ppnum_t pn);
extern boolean_t        pmap_is_modified(ppnum_t pn);
extern unsigned int pmap_get_refmod(ppnum_t pn);
extern void                     pmap_clear_refmod(ppnum_t pn, unsigned int mask);
extern void                     pmap_clear_refmod_options(ppnum_t pn, unsigned int mask, unsigned int options, void *);
extern bool
pmap_clear_refmod_range_options(
	pmap_t pmap,
	vm_map_address_t start,
	vm_map_address_t end,
	unsigned int mask,
	unsigned int options);
extern void pmap_flush_context_init(pmap_flush_context *);
extern void pmap_flush(pmap_flush_context *);
extern void             pmap_protect(   
	pmap_t          map,
	vm_map_offset_t s,
	vm_map_offset_t e,
	vm_prot_t       prot);
extern void             pmap_protect_options(   
	pmap_t          map,
	vm_map_offset_t s,
	vm_map_offset_t e,
	vm_prot_t       prot,
	unsigned int    options,
	void            *arg);
extern void(pmap_pageable)(
	pmap_t          pmap,
	vm_map_offset_t start,
	vm_map_offset_t end,
	boolean_t       pageable);
extern uint64_t pmap_shared_region_size_min(pmap_t map);
extern kern_return_t pmap_nest(pmap_t,
    pmap_t,
    addr64_t,
    uint64_t);
extern kern_return_t pmap_unnest(pmap_t,
    addr64_t,
    uint64_t);
extern kern_return_t pmap_unnest_options(pmap_t,
    addr64_t,
    uint64_t,
    unsigned int);
extern boolean_t pmap_adjust_unnest_parameters(pmap_t, vm_map_offset_t *, vm_map_offset_t *);
extern void             pmap_advise_pagezero_range(pmap_t, uint64_t);
extern boolean_t        pmap_is_noencrypt(ppnum_t);
extern void             pmap_set_noencrypt(ppnum_t pn);
extern void             pmap_clear_noencrypt(ppnum_t pn);
extern void             pmap_change_wiring(     
	pmap_t          pmap,
	vm_map_offset_t va,
	boolean_t       wired);
extern void             pmap_remove(    
	pmap_t          map,
	vm_map_offset_t s,
	vm_map_offset_t e);
extern void             pmap_remove_options(    
	pmap_t          map,
	vm_map_offset_t s,
	vm_map_offset_t e,
	int             options);
extern void             fillPage(ppnum_t pa, unsigned int fill);
extern void pmap_pre_expand(pmap_t pmap, vm_map_offset_t vaddr);
extern kern_return_t pmap_pre_expand_large(pmap_t pmap, vm_map_offset_t vaddr);
extern vm_size_t pmap_query_pagesize(pmap_t map, vm_map_offset_t vaddr);
mach_vm_size_t pmap_query_resident(pmap_t pmap,
    vm_map_offset_t s,
    vm_map_offset_t e,
    mach_vm_size_t *compressed_bytes_p);
extern void pmap_set_vm_map_cs_enforced(pmap_t pmap, bool new_value);
extern bool pmap_get_vm_map_cs_enforced(pmap_t pmap);
extern void pmap_set_jit_entitled(pmap_t pmap);
extern bool pmap_get_jit_entitled(pmap_t pmap);
extern void pmap_set_tpro(pmap_t pmap);
extern bool pmap_get_tpro(pmap_t pmap);
extern void pmap_trim(pmap_t grand, pmap_t subord, addr64_t vstart, uint64_t size);
extern bool pmap_is_nested(pmap_t pmap);
extern kern_return_t pmap_dump_page_tables(pmap_t pmap, void *bufp, void *buf_end, unsigned int level_mask, size_t *bytes_copied);
extern uint32_t pmap_user_va_bits(pmap_t pmap);
extern uint32_t pmap_kernel_va_bits(void);
bool pmap_has_prot_policy(pmap_t pmap, bool translated_allow_execute, vm_prot_t prot);
uint64_t pmap_release_pages_fast(void);
extern kern_return_t pmap_query_page_info(
	pmap_t          pmap,
	vm_map_offset_t va,
	int             *disp);
extern bool pmap_in_ppl(void);
extern uint32_t pmap_lookup_in_static_trust_cache(const uint8_t cdhash[CS_CDHASH_LEN]);
extern bool pmap_lookup_in_loaded_trust_caches(const uint8_t cdhash[CS_CDHASH_LEN]);
extern bool pmap_has_iofilter_protected_write(void);
extern void pmap_iofilter_protected_write(vm_address_t addr, uint64_t value, uint64_t width);
extern void *pmap_claim_reserved_ppl_page(void);
extern void pmap_free_reserved_ppl_page(void *kva);
extern void pmap_ledger_verify_size(size_t);
extern ledger_t pmap_ledger_alloc(void);
extern void pmap_ledger_free(ledger_t);
extern bool pmap_is_bad_ram(ppnum_t ppn);
extern bool pmap_is_page_restricted(ppnum_t pn);
extern bool pmap_is_exotic(pmap_t pmap);
extern int pmap_cs_configuration(void);
extern bool
pmap_performs_stage2_translations(const pmap_t pmap);
extern ppnum_t          kernel_pmap_present_mapping(uint64_t vaddr, uint64_t * pvincr, uintptr_t * pvphysaddr);
__BEGIN_DECLS


bool
pmap_cs_enabled(void);
void*
pmap_image4_pmap_data(
	size_t *allocated_size);
void
pmap_image4_set_nonce(
	const img4_nonce_domain_index_t ndi,
	const img4_nonce_t *nonce);
void
pmap_image4_roll_nonce(
	const img4_nonce_domain_index_t ndi);
errno_t
pmap_image4_copy_nonce(
	const img4_nonce_domain_index_t ndi,
	img4_nonce_t *nonce_out);
errno_t
pmap_image4_execute_object(
	img4_runtime_object_spec_index_t obj_spec_index,
	const img4_buff_t *payload,
	const img4_buff_t *manifest);
errno_t
pmap_image4_copy_object(
	img4_runtime_object_spec_index_t obj_spec_index,
	vm_address_t object_out,
	size_t *object_length);
errno_t
pmap_image4_monitor_trap(
	image4_cs_trap_t selector,
	const void *input_data,
	size_t input_size);
int metacompressor(const uint8_t *in, uint8_t *cdst, int32_t outbufsz,
    uint16_t *codec, void *cscratch, boolean_t *, uint32_t *pop_count_p);
bool metadecompressor(const uint8_t *source, uint8_t *dest, uint32_t csize,
    uint16_t ccodec, void *compressor_dscratch, uint32_t *pop_count_p);
void vm_compressor_algorithm_init(void);
int vm_compressor_algorithm(void);
boolean_t vm_swap_create_file(void);
void vm_swapout_iodone(void *, int);
kern_return_t vm_swap_put_finish(struct swapfile *, uint64_t *, int, boolean_t);
kern_return_t vm_swap_put(vm_offset_t, uint64_t*, uint32_t, c_segment_t, struct swapout_io_completion *);
void vm_swap_flush(void);
void vm_swap_reclaim(void);
void vm_swap_encrypt(c_segment_t);
uint64_t vm_swap_get_used_space(void);
uint64_t vm_swap_get_max_configured_space(void);
void vm_swap_reset_max_segs_tracking(uint64_t *alloced_max, uint64_t *used_max);
extern __startup_func void vm_compressor_swap_init_swap_file_limit(void);
extern void vm_swapfile_open(const char *path, struct vnode **vp);
extern void vm_swapfile_close(uint64_t path, struct vnode *vp);
extern int vm_swapfile_preallocate(struct vnode *vp, uint64_t *size, boolean_t *pin);
extern uint64_t vm_swapfile_get_blksize(struct vnode *vp);
extern uint64_t vm_swapfile_get_transfer_size(struct vnode *vp);
extern int vm_swapfile_io(struct vnode *vp, uint64_t offset, uint64_t start, int npages, int flags, void *upl_ctx);
extern int vm_record_file_write(struct vnode *vp, uint64_t offset, char *buf, int size);
int vm_swap_vol_get_capacity(const char *volume_name, uint64_t *capacity);
uint64_t vm_swap_get_total_space(void);
uint64_t vm_swap_get_free_space(void);
__BEGIN_DECLS


void vm_consider_waking_compactor_swapper(void);
void vm_consider_swapping(void);
void vm_compressor_flush(void);
void c_seg_free(c_segment_t);
void c_seg_free_locked(c_segment_t);
void c_seg_need_delayed_compaction(c_segment_t, boolean_t);
void c_seg_update_task_owner(c_segment_t, task_t);
void vm_compressor_record_warmup_start(void);
void vm_compressor_record_warmup_end(void);
int                     vm_wants_task_throttled(task_t);
extern void             vm_compaction_swapper_do_init(void);
extern void             vm_compressor_swap_init(void);
extern void             vm_swap_free(uint64_t);
extern void             c_seg_swapin_requeue(c_segment_t, boolean_t, boolean_t, boolean_t);
extern int              c_seg_swapin(c_segment_t, boolean_t, boolean_t);
extern void             c_seg_wait_on_busy(c_segment_t);
extern void             c_seg_trim_tail(c_segment_t);
extern void             c_seg_switch_state(c_segment_t, int, boolean_t);
extern void c_seg_insert_into_q(queue_head_t *, c_segment_t);
extern uint64_t vm_compressor_compute_elapsed_msecs(clock_sec_t, clock_nsec_t, clock_sec_t, clock_nsec_t);
uint32_t vm_compressor_get_encode_scratch_size(void) __pure2;
uint32_t vm_compressor_get_decode_scratch_size(void) __pure2;
__BEGIN_DECLS

extern kern_return_t vm_compressor_pager_put(
	memory_object_t                 mem_obj,
	memory_object_offset_t          offset,
	ppnum_t                         ppnum,
	void                            **current_chead,
	char                            *scratch_buf,
	int                             *compressed_count_delta_p,
	vm_compressor_options_t         flags);
extern unsigned int vm_compressor_pager_state_clr(
	memory_object_t         mem_obj,
	memory_object_offset_t  offset);
extern vm_external_state_t vm_compressor_pager_state_get(
	memory_object_t         mem_obj,
	memory_object_offset_t  offset);
extern void vm_compressor_pager_transfer(
	memory_object_t         dst_mem_obj,
	memory_object_offset_t  dst_offset,
	memory_object_t         src_mem_obj,
	memory_object_offset_t  src_offset);
extern memory_object_offset_t vm_compressor_pager_next_compressed(
	memory_object_t         mem_obj,
	memory_object_offset_t  offset);
__enum_closed_decl(vm_decompress_result_t, int, {
	DECOMPRESS_SUCCESS_SWAPPEDIN = 1,
	DECOMPRESS_SUCCESS = 0,
	DECOMPRESS_NEED_BLOCK = -2,
	DECOMPRESS_FIRST_FAIL_CODE = -3,
	DECOMPRESS_FAILED_BAD_Q = -3,
	DECOMPRESS_FAILED_BAD_Q_FREEZE = -4,
	DECOMPRESS_FAILED_ALGO_ERROR = -5,
	DECOMPRESS_FAILED_WKDMD_POPCNT = -6,
	DECOMPRESS_FAILED_UNMODIFIED = -7,
});
extern bool osenvironment_is_diagnostics(void);
extern void vm_compressor_init(void);
extern bool vm_compressor_is_slot_compressed(int *slot);
extern kern_return_t vm_compressor_put(ppnum_t pn, int *slot, void **current_chead, char *scratch_buf, vm_compressor_options_t flags);
extern vm_decompress_result_t vm_compressor_get(ppnum_t pn, int *slot, vm_compressor_options_t flags);
extern int vm_compressor_free(int *slot, vm_compressor_options_t flags);
extern unsigned int vm_compressor_pager_reap_pages(memory_object_t mem_obj, vm_compressor_options_t flags);
extern void vm_compressor_pager_count(memory_object_t mem_obj,
    int compressed_count_delta,
    boolean_t shared_lock,
    vm_object_t object);
extern void vm_compressor_transfer(int *dst_slot_p, int *src_slot_p);
__options_decl(vm_compressor_options_t, uint32_t, {
	C_DONT_BLOCK            = 0x00000001, 
	C_KEEP                  = 0x00000002, 
	C_KDP                   = 0x00000004, 
	C_PAGE_UNMODIFIED       = 0x00000008,
	C_KDP_MULTICPU          = 0x00000010,
});
extern kern_return_t vm_compressor_pager_get(
	memory_object_t         mem_obj,
	memory_object_offset_t  offset,
	ppnum_t                 ppnum,
	int                     *my_fault_type,
	vm_compressor_options_t flags,
	int                     *compressed_count_delta_p);
extern unsigned int vm_compressor_pager_get_count(memory_object_t mem_obj);
static_assert((C_SEG_OFFSET_BITS + C_SLOT_C_SIZE_BITS +
    C_SLOT_C_CODEC_BITS + C_SLOT_C_POPCOUNT_BITS +
    C_SLOT_C_PADDING_BITS + C_SLOT_PACKED_PTR_BITS) % 32 == 0);
void vm_decompressor_lock(void);
void vm_decompressor_unlock(void);
void vm_compressor_delay_trim(void);
void vm_compressor_do_warmup(void);
extern kern_return_t    vm_swap_get(c_segment_t, uint64_t, uint64_t);
extern void kdp_compressor_busy_find_owner(event64_t wait_event, thread_waitinfo_t *waitinfo);
extern kern_return_t vm_compressor_kdp_init(void);
extern void vm_compressor_kdp_teardown(void);
extern bool vm_swap_low_on_space(void);
extern bool vm_swap_out_of_space(void);
void vm_wake_compactor_swapper(void);
extern void             vm_swap_consider_defragmenting(int);
void vm_run_compactor(void);
void vm_thrashing_jetsam_done(void);
uint32_t vm_compression_ratio(void);
uint32_t vm_compressor_pool_size(void);
uint32_t vm_compressor_fragmentation_level(void);
uint32_t vm_compressor_incore_fragmentation_wasted_pages(void);
bool vm_compressor_is_thrashing(void);
bool vm_compressor_swapout_is_ripe(void);
uint32_t vm_compressor_pages_compressed(void);
void vm_compressor_process_special_swapped_in_segments(void);
extern bool vm_compressor_low_on_space(void);
extern bool vm_compressor_compressed_pages_nearing_limit(void);
extern bool vm_compressor_out_of_space(void);
extern unsigned int     vm_page_info(
	hash_info_bucket_t      *info,
	unsigned int            count);
extern kern_return_t
vm_map_with_linking(
	task_t                  task,
	struct mwl_region       *regions,
	uint32_t                region_cnt,
	void                    **link_info,
	uint32_t                link_info_size,
	memory_object_control_t file_control);
__enum_closed_decl(vm_fault_return_t, int, {
	VM_FAULT_SUCCESS            = 0,
	VM_FAULT_RETRY              = 1,
	VM_FAULT_INTERRUPTED        = 2,
	VM_FAULT_MEMORY_SHORTAGE    = 3,
	VM_FAULT_MEMORY_ERROR       = 5,
	
	VM_FAULT_SUCCESS_NO_VM_PAGE = 6,
	VM_FAULT_BUSY               = 7,
});
extern kern_return_t vm_fault(
	vm_map_t        map,
	vm_map_offset_t vaddr,
	vm_prot_t       fault_type,
	boolean_t       change_wiring,
	vm_tag_t        wire_tag,                   
	int             interruptible,
	pmap_t          pmap,
	vm_map_offset_t pmap_addr)
__XNU_INTERNAL(vm_fault)
;
extern kern_return_t vm_pre_fault(
	vm_map_offset_t offset,
	vm_prot_t       prot);
__BEGIN_DECLS




extern vm_fault_return_t vm_fault_page(
	
	vm_object_t     first_object,           
	vm_object_offset_t first_offset,        
	vm_prot_t       fault_type,             
	boolean_t       must_be_resident,        
	boolean_t       caller_lookup,          
	
	vm_prot_t       *protection,            
	vm_page_t       *result_page,           
	
	vm_page_t       *top_page,              
	int             *type_of_fault,         
	
	kern_return_t   *error_code,            
	boolean_t       no_zero_fill,           
	vm_object_fault_info_t fault_info);
extern void vm_fault_cleanup(
	vm_object_t     object,
	vm_page_t       top_page);
extern kern_return_t vm_fault_wire(
	vm_map_t        map,
	vm_map_entry_t  entry,
	vm_prot_t       prot,
	vm_tag_t        wire_tag,
	pmap_t          pmap,
	vm_map_offset_t pmap_addr,
	ppnum_t         *physpage_p);
extern void vm_fault_unwire(
	vm_map_t        map,
	vm_map_entry_t  entry,
	boolean_t       deallocate,
	pmap_t          pmap,
	vm_map_offset_t pmap_addr,
	vm_map_offset_t end_addr);
extern kern_return_t    vm_fault_copy(
	vm_object_t             src_object,
	vm_object_offset_t      src_offset,
	vm_map_size_t           *copy_size,             
	vm_object_t             dst_object,
	vm_object_offset_t      dst_offset,
	vm_map_t                dst_map,
	vm_map_version_t         *dst_version,
	int                     interruptible);
extern kern_return_t vm_fault_enter(
	vm_page_t m,
	pmap_t pmap,
	vm_map_offset_t vaddr,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset,
	vm_prot_t prot,
	vm_prot_t fault_type,
	boolean_t wired,
	vm_tag_t  wire_tag,             
	vm_object_fault_info_t fault_info,
	boolean_t *need_retry,
	int *type_of_fault,
	uint8_t *object_lock_type);
extern kern_return_t vm_pre_fault_with_info(
	vm_map_t                map,
	vm_map_offset_t         offset,
	vm_prot_t               prot,
	vm_object_fault_info_t  fault_info);
__BEGIN_DECLS



extern void vm_fault_init(void);
extern kern_return_t vm_fault_external(
	vm_map_t        map,
	vm_map_offset_t vaddr,
	vm_prot_t       fault_type,
	boolean_t       change_wiring,
	int             interruptible,
	pmap_t          caller_pmap,
	vm_map_offset_t caller_pmap_addr);
extern vm_offset_t kdp_lightweight_fault(
	vm_map_t map,
	vm_offset_t cur_target_addr,
	bool multi_cpu);
extern void vm_fault_disable(void);
extern void vm_fault_enable(void);
extern bool vm_fault_get_disabled(void);
extern boolean_t NEED_TO_HARD_THROTTLE_THIS_TASK(void);
extern void kmem_init(
	vm_offset_t     start,
	vm_offset_t     end);
extern void vm_mem_bootstrap(void);
__BEGIN_DECLS

extern kern_return_t memory_object_iopl_request(
	ipc_port_t              port,
	memory_object_offset_t  offset,
	upl_size_t              *upl_size,
	upl_t                   *upl_ptr,
	upl_page_info_array_t   user_page_list,
	unsigned int            *page_list_count,
	upl_control_flags_t     *flags,
	vm_tag_t                tag);
extern uint32_t         vm_tag_get_kext(vm_tag_t tag, char * name, vm_size_t namelen);
extern void iopl_valid_data(
	upl_t                   upl_ptr,
	vm_tag_t        tag);
extern void               vm_page_set_offset(vm_page_t page, vm_object_offset_t offset);
extern vm_object_offset_t vm_page_get_offset(vm_page_t page);
extern ppnum_t            vm_page_get_phys_page(vm_page_t page);
extern vm_page_t          vm_page_get_next(vm_page_t page);
extern kern_return_t device_data_action(
	uintptr_t               device_handle,
	ipc_port_t              device_pager,
	vm_prot_t               protection,
	vm_object_offset_t      offset,
	vm_size_t               size);
extern kern_return_t device_close(
	uintptr_t     device_handle);
extern boolean_t vm_swap_files_pinned(void);
extern kern_return_t device_pager_populate_object(
	memory_object_t         device,
	memory_object_offset_t  offset,
	ppnum_t                 page_num,
	vm_size_t               size);
extern memory_object_t device_pager_setup(
	memory_object_t,
	uintptr_t,
	vm_size_t,
	int);
extern kern_return_t vm_map_range_physical_size(
	vm_map_t         map,
	vm_map_address_t start,
	mach_vm_size_t   size,
	mach_vm_size_t * phys_size);
extern void vm_panic_hibernate_write_image_failed(
	int err,
	uint64_t file_size_min,
	uint64_t file_size_max,
	uint64_t file_size);
extern kern_return_t mach_make_memory_entry_internal(
	vm_map_t                target_map,
	memory_object_size_ut  *size,
	memory_object_offset_ut offset,
	vm_prot_ut              permission,
	vm_named_entry_kernel_flags_t vmne_kflags,
	ipc_port_t              *object_handle,
	ipc_port_t              parent_handle);
extern kern_return_t
memory_entry_check_for_adjustment(
	vm_map_t                        src_map,
	ipc_port_t                      port,
	vm_map_offset_t         *overmap_start,
	vm_map_offset_t         *overmap_end);
extern kern_return_t memory_entry_purgeable_control_internal(
	ipc_port_t      entry_port,
	vm_purgable_t   control,
	int             *state);
extern kern_return_t mach_memory_entry_get_page_counts(
	ipc_port_t      entry_port,
	unsigned int    *resident_page_count,
	unsigned int    *dirty_page_count);
extern kern_return_t mach_memory_entry_phys_page_offset(
	ipc_port_t              entry_port,
	vm_object_offset_t      *offset_p);
extern kern_return_t mach_memory_entry_map_size(
	ipc_port_t                 entry_port,
	vm_map_t                   map,
	memory_object_offset_ut    offset,
	memory_object_size_ut      size,
	mach_vm_size_t            *map_size);
extern kern_return_t vm_map_enter_mem_object_prefault(
	vm_map_t                map,
	vm_map_offset_ut       *address,
	vm_map_size_ut          size,
	vm_map_offset_ut        mask,
	vm_map_kernel_flags_t   vmk_flags,
	ipc_port_t              port,
	vm_object_offset_ut     offset,
	vm_prot_ut              cur_protection,
	vm_prot_ut              max_protection,
	upl_page_list_ptr_t     page_list,
	unsigned int            page_list_count);
#pragma mark - kernel address obfuscation / hashing for logging


extern void vm_kernel_addrhide(
	vm_offset_t             addr,
	vm_offset_t            *hide_addr);
extern void vm_kernel_addrperm_external(
	vm_offset_t             addr,
	vm_offset_t            *perm_addr);
extern void vm_kernel_unslide_or_perm_external(
	vm_offset_t             addr,
	vm_offset_t            *perm_addr);
extern kern_allocation_name_t kern_allocation_name_allocate(
	const char             *name,
	uint16_t                suballocs);
extern void kern_allocation_name_release(
	kern_allocation_name_t  allocation);
extern const char *kern_allocation_get_name(
	kern_allocation_name_t  allocation);
__stateful_pure
extern mach_vm_size_t mach_vm_range_size(
	const struct mach_vm_range *r);
__attribute__((overloadable, pure))
extern bool mach_vm_range_contains(
	const struct mach_vm_range *r,
	mach_vm_offset_t        addr);
__attribute__((overloadable, pure))
extern bool mach_vm_range_contains(
	const struct mach_vm_range *r,
	mach_vm_offset_t        addr,
	mach_vm_offset_t        size);
__attribute__((overloadable, pure))
extern bool mach_vm_range_intersects(
	const struct mach_vm_range *r1,
	const struct mach_vm_range *r2);
__attribute__((overloadable, pure))
extern bool mach_vm_range_intersects(
	const struct mach_vm_range *r1,
	mach_vm_offset_t        addr,
	mach_vm_offset_t        size);
__pure2
extern bool kmem_range_id_contains(
	kmem_range_id_t         range_id,
	vm_map_offset_t         addr,
	vm_map_size_t           size);
__pure2
extern kmem_range_id_t kmem_addr_get_range(
	vm_map_offset_t         addr,
	vm_map_size_t           size);
extern kmem_range_id_t kmem_adjust_range_id(
	uint32_t                hash);
__startup_func
extern uint16_t kmem_get_random16(
	uint16_t                upper_limit);
__startup_func
extern void kmem_shuffle(
	uint16_t               *shuffle_buf,
	uint16_t                count);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)


#pragma mark - the kmem subsystem



struct mach_memory_info;
__pure2
extern vm_map_size_t kmem_range_id_size(
	kmem_range_id_t         range_id);
__options_decl(kmem_claims_flags_t, uint32_t, {
	KC_NONE         = 0x00000000,
	KC_NO_ENTRY     = 0x00000001,
	KC_NO_MOVE      = 0x00000002,
});
extern void kmem_range_startup_init(
	struct kmem_range_startup_spec *sp);
#pragma mark kmem entry parameters


extern void kmem_entry_validate_guard(
	vm_map_t                map,
	struct vm_map_entry    *entry,
	vm_offset_t             addr,
	vm_size_t               size,
	kmem_guard_t            guard);
extern vm_size_t kmem_size_guard(
	vm_map_t                map,
	vm_offset_t             addr,
	kmem_guard_t            guard);
#pragma mark kmem allocations


__options_decl(kma_flags_t, uint32_t, {
	KMA_NONE            = KMEM_NONE,

	
	KMA_NOFAIL          = KMEM_NOFAIL,
	KMA_NOPAGEWAIT      = KMEM_NOPAGEWAIT,

	
	KMA_VAONLY          = KMEM_VAONLY,
	KMA_PAGEABLE        = KMEM_PAGEABLE,
	KMA_ZERO            = KMEM_ZERO,
	KMA_NOSOFTLIMIT     = KMEM_NOSOFTLIMIT,

	
	KMA_KOBJECT         = KMEM_KOBJECT,
	KMA_COMPRESSOR      = KMEM_COMPRESSOR,

	
	KMA_LOMEM           = KMEM_LOMEM,
	KMA_LAST_FREE       = KMEM_LAST_FREE,
	KMA_DATA            = KMEM_DATA,
	KMA_DATA_SHARED     = KMEM_DATA_SHARED,
	KMA_SPRAYQTN        = KMEM_SPRAYQTN,

	
	KMA_PERMANENT       = KMEM_PERMANENT,
	KMA_GUARD_FIRST     = KMEM_GUARD_FIRST,
	KMA_GUARD_LAST      = KMEM_GUARD_LAST,
	KMA_KSTACK          = KMEM_KSTACK,
	KMA_NOENCRYPT       = KMEM_NOENCRYPT,
	KMA_KASAN_GUARD     = KMEM_KASAN_GUARD,
	KMA_TAG             = KMEM_TAG,
});
extern kmem_return_t kmem_alloc_guard(
	vm_map_t                map,
	vm_size_t               size,
	vm_offset_t             mask,
	kma_flags_t             flags,
	kmem_guard_t            guard) __result_use_check;
static inline kern_return_t
kernel_memory_allocate(
	vm_map_t                map,
	vm_offset_t            *addrp,
	vm_size_t               size,
	vm_offset_t             mask,
	kma_flags_t             flags,
	vm_tag_t                tag)
{
	kmem_guard_t guard = {
		.kmg_tag = tag,
	};
	kmem_return_t kmr;

	kmr = kmem_alloc_guard(map, size, mask, flags, guard);
	if (kmr.kmr_return == KERN_SUCCESS) {
		__builtin_assume(kmr.kmr_address != 0);
	} else {
		__builtin_assume(kmr.kmr_address == 0);
	}
	*addrp = kmr.kmr_address;
	return kmr.kmr_return;
}

static inline kern_return_t
kmem_alloc(
	vm_map_t                map,
	vm_offset_t            *addrp,
	vm_size_t               size,
	kma_flags_t             flags,
	vm_tag_t                tag)
{
	return kernel_memory_allocate(map, addrp, size, 0, flags, tag);
}


extern kmem_return_t kmem_alloc_contig_guard(
	vm_map_t                map,
	vm_size_t               size,
	vm_offset_t             mask,
	ppnum_t                 max_pnum,
	ppnum_t                 pnum_mask,
	kma_flags_t             flags,
	kmem_guard_t            guard);
static inline kern_return_t
kmem_alloc_contig(
	vm_map_t                map,
	vm_offset_t            *addrp,
	vm_size_t               size,
	vm_offset_t             mask,
	ppnum_t                 max_pnum,
	ppnum_t                 pnum_mask,
	kma_flags_t             flags,
	vm_tag_t                tag)
{
	kmem_guard_t guard = {
		.kmg_tag = tag,
	};
	kmem_return_t kmr;

	kmr = kmem_alloc_contig_guard(map, size, mask,
	    max_pnum, pnum_mask, flags, guard);
	if (kmr.kmr_return == KERN_SUCCESS) {
		__builtin_assume(kmr.kmr_address != 0);
	} else {
		__builtin_assume(kmr.kmr_address == 0);
	}
	*addrp = kmr.kmr_address;
	return kmr.kmr_return;
}



__options_decl(kms_flags_t, uint32_t, {
	KMS_NONE            = KMEM_NONE,

	
	KMS_NOFAIL          = KMEM_NOFAIL,
	KMS_NOSOFTLIMIT     = KMEM_NOSOFTLIMIT,

	
	KMS_LAST_FREE       = KMEM_LAST_FREE,
	KMS_DATA            = KMEM_DATA,

	
	KMS_PERMANENT       = KMEM_PERMANENT,
});
extern kmem_return_t kmem_suballoc(
	vm_map_t                parent,
	mach_vm_offset_t       *addr,
	vm_size_t               size,
	vm_map_create_options_t vmc_options,
	int                     vm_flags,
	kms_flags_t             flags,
	vm_tag_t                tag);
#pragma mark kmem reallocation


__options_decl(kmr_flags_t, uint32_t, {
	KMR_NONE            = KMEM_NONE,

	
	KMR_NOPAGEWAIT      = KMEM_NOPAGEWAIT,
	KMR_FREEOLD         = KMEM_FREEOLD,
	KMR_REALLOCF        = KMEM_REALLOCF,

	
	KMR_ZERO            = KMEM_ZERO,

	
	KMR_KOBJECT         = KMEM_KOBJECT,

	
	KMR_LOMEM           = KMEM_LOMEM,
	KMR_LAST_FREE       = KMEM_LAST_FREE,
	KMR_DATA            = KMEM_DATA,
	KMR_DATA_SHARED     = KMEM_DATA_SHARED,
	KMR_SPRAYQTN        = KMEM_SPRAYQTN,

	
	KMR_GUARD_FIRST     = KMEM_GUARD_FIRST,
	KMR_GUARD_LAST      = KMEM_GUARD_LAST,
	KMR_KASAN_GUARD     = KMEM_KASAN_GUARD,
	KMR_TAG             = KMEM_TAG,
});
extern kmem_return_t kmem_realloc_guard(
	vm_map_t                map,
	vm_offset_t             oldaddr,
	vm_size_t               oldsize,
	vm_size_t               newsize,
	kmr_flags_t             flags,
	kmem_guard_t            guard) __result_use_check
__attribute__((diagnose_if(!KMEM_REALLOC_FLAGS_VALID(flags),
    "invalid realloc flags passed", "error")));
static inline bool
kmem_realloc_should_free(
	vm_offset_t             oldaddr,
	kmem_return_t           kmr)
{
	return oldaddr && oldaddr != kmr.kmr_address;
}


#pragma mark kmem free


__options_decl(kmf_flags_t, uint32_t, {
	KMF_NONE            = KMEM_NONE,

	

	

	
	KMF_GUESS_SIZE      = KMEM_GUESS_SIZE,

	
	KMF_GUARD_FIRST     = KMEM_GUARD_FIRST,
	KMF_GUARD_LAST      = KMEM_GUARD_LAST,
	KMF_KASAN_GUARD     = KMEM_KASAN_GUARD,
	KMF_TAG             = KMEM_TAG,
});
extern vm_size_t kmem_free_guard(
	vm_map_t                map,
	vm_offset_t             addr,
	vm_size_t               size,
	kmf_flags_t             flags,
	kmem_guard_t            guard);
__attribute__((overloadable))
static inline void
kmem_free(
	vm_map_t                map,
	vm_offset_t             addr,
	vm_size_t               size,
	kmf_flags_t             flags)
{
	kmem_free_guard(map, addr, size, flags, KMEM_GUARD_NONE);
}

__attribute__((overloadable))
static inline void
kmem_free(
	vm_map_t                map,
	vm_offset_t             addr,
	vm_size_t               size)
{
	kmem_free(map, addr, size, KMF_NONE);
}


#pragma mark kmem population


extern kern_return_t kernel_memory_populate(
	vm_offset_t             addr,
	vm_size_t               size,
	kma_flags_t             flags,
	vm_tag_t                tag);
extern void kernel_memory_depopulate(
	vm_offset_t             addr,
	vm_size_t               size,
	kma_flags_t             flags,
	vm_tag_t                tag);
#pragma mark - VM_FLAGS_* / vm_map_kernel_flags_t conversions


extern int vm_map_kernel_flags_vmflags(
	vm_map_kernel_flags_t    vmk_flags);
__attribute__((overloadable))
extern void vm_map_kernel_flags_set_vmflags(
	vm_map_kernel_flags_t  *vmk_flags,
	int                     vm_flags,
	vm_tag_t                vm_tag);
__attribute__((overloadable))
extern void vm_map_kernel_flags_set_vmflags(
	vm_map_kernel_flags_t  *vmk_flags,
	int                     vm_flags_and_tag);
extern void vm_map_kernel_flags_and_vmflags(
	vm_map_kernel_flags_t   *vmk_flags,
	int                      vm_flags_mask);
extern bool vm_map_kernel_flags_check_vmflags(
	vm_map_kernel_flags_t   vmk_flags,
	int                     vm_flags_mask);
extern bool vm_map_kernel_flags_check_vm_and_kflags(
	vm_map_kernel_flags_t   vmk_flags,
	int                     vm_flags_mask);
#pragma mark - kernel variants of the Mach VM interfaces


extern kern_return_t    mach_vm_allocate_kernel(
	vm_map_t                map,
	mach_vm_offset_ut      *addr_u,
	mach_vm_size_ut         size_u,
	vm_map_kernel_flags_t   vmk_flags);
extern kern_return_t    mach_vm_map_kernel(
	vm_map_t                target_map,
	mach_vm_offset_ut      *address,
	mach_vm_size_ut         size,
	mach_vm_offset_ut       mask,
	vm_map_kernel_flags_t   vmk_flags,
	ipc_port_t              port,
	memory_object_offset_ut offset,
	boolean_t               copy,
	vm_prot_ut              cur_prot,
	vm_prot_ut              max_prot,
	vm_inherit_ut           inheritance);
extern kern_return_t    mach_vm_remap_new_kernel(
	vm_map_t                target_map,
	mach_vm_offset_ut      *address,
	mach_vm_size_ut         size,
	mach_vm_offset_ut       mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_map_t                src_map,
	mach_vm_offset_ut       src_address,
	boolean_t               copy,
	vm_prot_ut             *cur_prot,
	vm_prot_ut             *max_prot,
	vm_inherit_ut           inheritance);
extern kern_return_t    vm_map_wire_kernel(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	vm_prot_ut              prot_u,
	vm_tag_t                tag,
	boolean_t               user_wire);
extern kern_return_t vm_map_purgable_control(
	vm_map_t                map,
	vm_map_offset_ut        address,
	vm_purgable_t           control,
	int                    *state);
extern kern_return_t mach_vm_purgable_control(
	vm_map_t                map,
	mach_vm_offset_ut       address_u,
	vm_purgable_t           control,
	int                    *state);
#pragma mark - map copyio



extern kern_return_t     copyinmap(
	vm_map_t                map,
	vm_map_offset_t         fromaddr,
	void                   *todata __sized_by(length),
	vm_size_t               length);
extern kern_return_t     copyoutmap(
	vm_map_t                map,
	void                   *fromdata __sized_by(length),
	vm_map_offset_t         toaddr,
	vm_size_t               length);
extern kern_return_t     copyoutmap_atomic32(
	vm_map_t                map,
	uint32_t                value,
	vm_map_offset_t         toaddr);
extern kern_return_t     copyoutmap_atomic64(
	vm_map_t                map,
	uint64_t                value,
	vm_map_offset_t         toaddr);
#pragma mark - accounting

#pragma mark accounting: kern allocation name


extern void             kern_allocation_update_size(
	kern_allocation_name_t  allocation,
	int64_t                 delta,
	vm_object_t             object);
extern void             kern_allocation_update_subtotal(
	kern_allocation_name_t  allocation,
	vm_tag_t                subtag,
	int64_t                 delta);
extern vm_tag_t         vm_tag_bt(void);
extern vm_tag_t         vm_tag_alloc(
	vm_allocation_site_t   *site);
extern void             vm_tag_alloc_locked(
	vm_allocation_site_t   *site,
	vm_allocation_site_t  **releasesiteP);
extern void             vm_tag_update_size(
	vm_tag_t                tag,
	int64_t                 delta,
	vm_object_t             object);
extern uint64_t         vm_tag_get_size(
	vm_tag_t                tag);
#pragma mark accounting: diagnostics and query interfaces


extern uint32_t         vm_page_diagnose_estimate(void);
extern kern_return_t    vm_page_diagnose(
	struct mach_memory_info *info __counted_by(num_info),
	unsigned int            num_info,
	uint64_t                zones_collectable_bytes,
	bool                    redact_info);
#pragma mark - init methods


extern void             vm_init_before_launchd(void);
extern memory_object_t device_pager_setup(memory_object_t, uintptr_t, vm_size_t, int);
extern kern_return_t device_pager_populate_object( memory_object_t device,
    memory_object_offset_t offset, ppnum_t page_num, vm_size_t size);
__BEGIN_DECLS


#pragma mark - VM map basics


extern vm_map_t         vm_map_create(
	pmap_t                  pmap,
	vm_map_offset_t         min_off,
	vm_map_offset_t         max_off,
	boolean_t               pageable);
extern void             vm_map_deallocate(
	vm_map_t                map);
extern int              vm_map_page_shift(
	vm_map_t                map) __pure2;
extern vm_map_offset_t  vm_map_page_mask(
	vm_map_t                map) __pure2;
extern int              vm_map_page_size(
	vm_map_t                map) __pure2;
extern vm_map_offset_t  vm_map_round_page_mask(
	vm_map_offset_t         offset,
	vm_map_offset_t         mask) __pure2;
extern vm_map_offset_t  vm_map_trunc_page_mask(
	vm_map_offset_t         offset,
	vm_map_offset_t         mask) __pure2;
extern void vm_map_disable_hole_optimization(
	vm_map_t                map);
#pragma mark - MIG helpers
#pragma GCC visibility push(hidden)


extern vm_map_t         convert_port_entry_to_map(
	ipc_port_t              port) __exported;
extern void             vm_map_inspect_deallocate(
	vm_map_inspect_t        map);
extern void             vm_map_read_deallocate(
	vm_map_read_t           map);
#pragma GCC visibility pop

#pragma mark - vm map wiring


extern kern_return_t    vm_map_unwire(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	boolean_t               user_wire);
#pragma mark - vm map copy





extern kern_return_t    vm_map_copyin(
	vm_map_t                src_map,
	vm_map_address_ut       src_addr,
	vm_map_size_ut          len,
	boolean_t               src_destroy,
	vm_map_copy_t          *copy_result);
extern kern_return_t    vm_map_copyout(
	vm_map_t                dst_map,
	vm_map_address_t       *addr, 
	vm_map_copy_t           copy);
extern void             vm_map_copy_discard(
	vm_map_copy_t           copy);
__BEGIN_DECLS



extern boolean_t vm_map_check_protection(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	vm_prot_ut              protection_u,
	vm_sanitize_caller_t    vm_sanitize_caller);
extern kern_return_t vm_map_wire_impl(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	vm_prot_ut              prot_u,
	vm_tag_t                tag,
	boolean_t               user_wire,
	ppnum_t                *physpage_p,
	vm_sanitize_caller_t    vm_sanitize_caller);
extern kern_return_t vm_map_unwire_impl(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	boolean_t               user_wire,
	vm_sanitize_caller_t    vm_sanitize_caller);
#pragma GCC visibility push(hidden)







struct kmem_page_meta;
extern void kernel_memory_populate_object_and_unlock(
	vm_object_t             object, 
	vm_address_t            addr,
	vm_offset_t             offset,
	vm_size_t               size,
	struct vm_page         *page_list,
	kma_flags_t             flags,
	vm_tag_t                tag,
	vm_prot_t               prot,
	pmap_mapping_type_t     mapping_type);
extern void vm_map_init(void);
extern kern_return_t vm_map_locate_space_anywhere(
	vm_map_t                map,
	vm_map_size_t           size,
	vm_map_offset_t         mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_map_offset_t        *start_inout,
	vm_map_entry_t         *entry_out);
extern kern_return_t vm_map_find_space(
	vm_map_t                map,
	vm_map_address_t        hint_addr,
	vm_map_size_t           size,
	vm_map_offset_t         mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_map_entry_t          *o_entry);
extern void vm_map_clip_start(
	vm_map_t                map,
	vm_map_entry_t          entry,
	vm_map_offset_t         endaddr);
extern void vm_map_clip_end(
	vm_map_t                map,
	vm_map_entry_t          entry,
	vm_map_offset_t         endaddr);
extern boolean_t vm_map_entry_should_cow_for_true_share(
	vm_map_entry_t          entry);
__options_decl(vmr_flags_t, uint32_t, {
	VM_MAP_REMOVE_NO_FLAGS          = 0x000,
	VM_MAP_REMOVE_KUNWIRE           = 0x001,
	VM_MAP_REMOVE_INTERRUPTIBLE     = 0x002,
	VM_MAP_REMOVE_NOKUNWIRE_LAST    = 0x004,
	VM_MAP_REMOVE_NO_MAP_ALIGN      = 0x008,
	VM_MAP_REMOVE_IMMUTABLE         = 0x010,
	VM_MAP_REMOVE_GAPS_FAIL         = 0x020,
	VM_MAP_REMOVE_NO_YIELD          = 0x040,
	VM_MAP_REMOVE_GUESS_SIZE        = 0x080,
	VM_MAP_REMOVE_IMMUTABLE_CODE    = 0x100,
	VM_MAP_REMOVE_TO_OVERWRITE      = 0x200,
});
extern kmem_return_t vm_map_remove_guard(
	vm_map_t                map,
	vm_map_offset_t         start,
	vm_map_offset_t         end,
	vmr_flags_t             flags,
	kmem_guard_t            guard) __result_use_check;
extern kmem_return_t vm_map_remove_and_unlock(
	vm_map_t        map,
	vm_map_offset_t start,
	vm_map_offset_t end,
	vmr_flags_t     flags,
	kmem_guard_t    guard) __result_use_check;
static inline void
vm_map_remove(
	vm_map_t                map,
	vm_map_offset_t         start,
	vm_map_offset_t         end)
{
	vmr_flags_t  flags = VM_MAP_REMOVE_NO_FLAGS;
	kmem_guard_t guard = KMEM_GUARD_NONE;

	(void)vm_map_remove_guard(map, start, end, flags, guard);
}

extern bool kmem_is_ptr_range(vm_map_range_id_t range_id);
extern mach_vm_range_t kmem_validate_range_for_overwrite(
	vm_map_offset_t         addr,
	vm_map_size_t           size);
extern uint32_t kmem_addr_get_slot_idx(
	vm_map_offset_t         start,
	vm_map_offset_t         end,
	vm_map_range_id_t       range_id,
	struct kmem_page_meta **meta,
	uint32_t               *size_idx,
	mach_vm_range_t         slot);
extern void kmem_validate_slot(
	vm_map_offset_t         addr,
	struct kmem_page_meta  *meta,
	uint32_t                size_idx,
	uint32_t                slot_idx);
extern kern_return_t kmem_locate_space(
	vm_map_size_t           size,
	vm_map_range_id_t       range_id,
	bool                    direction,
	vm_map_offset_t        *start_inout,
	vm_map_entry_t         *entry_out);
extern void kmem_free_space(
	vm_map_offset_t         start,
	vm_map_offset_t         end,
	vm_map_range_id_t       range_id,
	mach_vm_range_t         slot);
ppnum_t vm_map_get_phys_page(
	vm_map_t        map,
	vm_offset_t     offset);
extern kern_return_t    vm_map_inherit(
	vm_map_t                map,
	vm_map_offset_ut        start,
	vm_map_offset_ut        end,
	vm_inherit_ut           new_inheritance);
extern kern_return_t    vm_map_protect(
	vm_map_t                map,
	vm_map_offset_ut        start_u,
	vm_map_offset_ut        end_u,
	boolean_t               set_max,
	vm_prot_ut              new_prot_u);
#pragma GCC visibility pop

static inline void
VME_OBJECT_SET(
	vm_map_entry_t entry,
	vm_object_t    object,
	bool           atomic,
	uint32_t       context)
{
	__builtin_assume(((vm_offset_t)object & 3) == 0);

	entry->vme_atomic = atomic;
	entry->is_sub_map = false;
	if (atomic) {
		entry->vme_context = context;
	} else {
		entry->vme_context = 0;
	}

	if (!object) {
		entry->vme_object_or_delta = 0;
	} else if (is_kernel_object(object)) {
		{
			entry->vme_object_or_delta = 0;
		}
	} else {
		entry->vme_object_or_delta = VM_OBJECT_PACK(object);
	}

	entry->vme_kernel_object = is_kernel_object(object);
	entry->vme_resilient_codesign = false;
	entry->used_for_jit = false;
}


static inline void
VME_OFFSET_SET(
	vm_map_entry_t entry,
	vm_object_offset_t offset)
{
	entry->vme_offset = offset >> VME_OFFSET_SHIFT;
	assert3u(VME_OFFSET(entry), ==, offset);
}


static inline void
VME_ALIAS_SET(
	vm_map_entry_t entry,
	unsigned int alias)
{
	assert3u(alias & VME_ALIAS_MASK, ==, alias);
	entry->vme_alias = alias;
}

static inline void
VME_OBJECT_SHADOW(
	vm_map_entry_t entry,
	vm_object_size_t length,
	bool always)
{
	vm_object_t object;
	vm_object_offset_t offset;

	object = VME_OBJECT(entry);
	offset = VME_OFFSET(entry);
	vm_object_shadow(&object, &offset, length, always);
	if (object != VME_OBJECT(entry)) {
		entry->vme_object_or_delta = VM_OBJECT_PACK(object);
		entry->use_pmap = true;
	}
	if (offset != VME_OFFSET(entry)) {
		VME_OFFSET_SET(entry, offset);
	}
}

extern vm_tag_t vmtaglog_tag;
static inline bool
vmtaglog_matches(vm_tag_t tag)
{
	switch (vmtaglog_tag) {
	case VM_KERN_MEMORY_NONE:
		return false;
	case VM_KERN_MEMORY_FIRST_DYNAMIC:
		return tag >= VM_KERN_MEMORY_FIRST_DYNAMIC;
	case VM_KERN_MEMORY_ANY:
		return tag != VM_KERN_MEMORY_NONE;
	default:
		return tag == vmtaglog_tag;
	}
}

static inline void
vme_btref_consider_and_set(__unused vm_map_entry_t entry, __unused void *fp)
{
}

static inline void
vme_btref_consider_and_put(__unused vm_map_entry_t entry)
{
}

extern kern_return_t
vm_map_copy_adjust_to_target(
	vm_map_copy_t           copy_map,
	vm_map_offset_ut        offset,
	vm_map_size_ut          size,
	vm_map_t                target_map,
	boolean_t               copy,
	vm_map_copy_t           *target_copy_map_p,
	vm_map_offset_t         *overmap_start_p,
	vm_map_offset_t         *overmap_end_p,
	vm_map_offset_t         *trimmed_start_p);
__attribute__((always_inline))
int vm_map_lock_read_to_write(vm_map_t map);
__attribute__((always_inline))
boolean_t vm_map_try_lock(vm_map_t map);
__attribute__((always_inline))
boolean_t vm_map_try_lock_read(vm_map_t map);
int vm_self_region_page_shift(vm_map_t target_map);
int vm_self_region_page_shift_safely(vm_map_t target_map);
extern boolean_t        vm_map_lookup_entry_or_next(
	vm_map_t                map,
	vm_map_address_t        address,
	vm_map_entry_t          *entry);
extern void             vm_map_copy_remap(
	vm_map_t                map,
	vm_map_entry_t          where,
	vm_map_copy_t           copy,
	vm_map_offset_t         adjustment,
	vm_prot_t               cur_prot,
	vm_prot_t               max_prot,
	vm_inherit_t            inheritance);
extern kern_return_t    vm_map_lookup_and_lock_object(
	vm_map_t                *var_map,                               
	vm_map_address_t        vaddr,
	vm_prot_t               fault_type,
	int                     object_lock_type,
	vm_map_version_t        *out_version,                           
	vm_object_t             *object,                                
	vm_object_offset_t      *offset,                                
	vm_prot_t               *out_prot,                              
	boolean_t               *wired,                                 
	vm_object_fault_info_t  fault_info,                             
	vm_map_t                *real_map,                              
	bool                    *contended);
extern boolean_t        vm_map_verify(
	vm_map_t                map,
	vm_map_version_t        *version);
extern void             vm_map_simplify_entry(
	vm_map_t        map,
	vm_map_entry_t  this_entry);
extern void             vm_map_simplify(
	vm_map_t                map,
	vm_map_offset_t         start);
extern kern_return_t    vm_map_enter_fourk(
	vm_map_t                map,
	vm_map_offset_t         *address,
	vm_map_size_t           size,
	vm_map_offset_t         mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_object_t             object,
	vm_object_offset_t      offset,
	boolean_t               needs_copy,
	vm_prot_t               cur_protection,
	vm_prot_t               max_protection,
	vm_inherit_t            inheritance);
extern kern_return_t    vm_map_enter(
	vm_map_t                map,
	vm_map_offset_t        *address,
	vm_map_size_t           size,
	vm_map_offset_t         mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_object_t             object,
	vm_object_offset_t      offset,
	boolean_t               needs_copy,
	vm_prot_t               cur_protection,
	vm_prot_t               max_protection,
	vm_inherit_t            inheritance);
extern kern_return_t    vm_map_enter_mem_object(
	vm_map_t                map,
	vm_map_offset_ut       *address,
	vm_map_size_ut          size,
	vm_map_offset_ut        mask,
	vm_map_kernel_flags_t   vmk_flags,
	ipc_port_t              port,
	vm_object_offset_ut     offset,
	boolean_t               needs_copy,
	vm_prot_ut              cur_protection,
	vm_prot_ut              max_protection,
	vm_inherit_ut           inheritance,
	upl_page_list_ptr_t     page_list,
	unsigned int            page_list_count);
extern kern_return_t    vm_map_remap(
	vm_map_t                target_map,
	vm_map_offset_ut       *address,
	vm_map_size_ut          size,
	vm_map_offset_ut        mask,
	vm_map_kernel_flags_t   vmk_flags,
	vm_map_t                src_map,
	vm_map_offset_ut        memory_address,
	boolean_t               copy,
	vm_prot_ut              *cur_protection,
	vm_prot_ut              *max_protection,
	vm_inherit_ut           inheritance);
extern kern_return_t    vm_map_machine_attribute(
	vm_map_t                map,
	vm_map_offset_ut        start,
	vm_map_offset_ut        end,
	vm_machine_attribute_t  attribute,
	vm_machine_attribute_val_t *value);
extern kern_return_t    vm_map_msync(
	vm_map_t                map,
	vm_map_address_ut       address,
	vm_map_size_ut          size,
	vm_sync_t               sync_flags);
extern kern_return_t    vm_map_behavior_set(
	vm_map_t                map,
	vm_map_offset_t         start,
	vm_map_offset_t         end,
	vm_behavior_t           new_behavior);
extern kern_return_t vm_map_region(
	vm_map_t                 map,
	vm_map_offset_ut        *address,
	vm_map_size_ut          *size,
	vm_region_flavor_t       flavor,
	vm_region_info_t         info,
	mach_msg_type_number_t  *count,
	mach_port_t             *object_name);
extern kern_return_t vm_map_region_recurse_64(
	vm_map_t                 map,
	vm_map_offset_ut        *address,
	vm_map_size_ut          *size,
	natural_t               *nesting_depth,
	vm_region_submap_info_64_t info,
	mach_msg_type_number_t  *count);
extern int override_nx(vm_map_t map, uint32_t user_tag);
extern void vm_map_region_top_walk(
	vm_map_entry_t entry,
	vm_region_top_info_t top);
extern void vm_map_region_walk(
	vm_map_t map,
	vm_map_offset_t va,
	vm_map_entry_t entry,
	vm_object_offset_t offset,
	vm_object_size_t range,
	vm_region_extended_info_t extended,
	boolean_t look_for_pages,
	mach_msg_type_number_t count);
extern void vm_map_copy_ledger(
	task_t  old_task,
	task_t  new_task,
	int     ledger_entry);
extern void             vm_map_destroy(
	vm_map_t                map);
extern void             vm_map_require(
	vm_map_t                map);
extern void             vm_map_copy_require(
	vm_map_copy_t           copy);
extern kern_return_t    vm_map_copy_extract(
	vm_map_t                src_map,
	vm_map_address_t        src_addr,
	vm_map_size_t           len,
	boolean_t               copy,
	vm_map_copy_t           *copy_result,   
	vm_prot_t               *cur_prot,      
	vm_prot_t               *max_prot,      
	vm_inherit_t            inheritance,
	vm_map_kernel_flags_t   vmk_flags);
extern kern_return_t    vm_map_copyin_internal(
	vm_map_t                src_map,
	vm_map_address_ut       src_addr_u,
	vm_map_size_ut          len_u,
	int                     flags,
	vm_map_copy_t          *copy_result);
extern boolean_t        vm_map_tpro_enforcement(
	vm_map_t                map);
extern void vm_map_iokit_mapped_region(
	vm_map_t                map,
	vm_size_t               bytes);
extern void vm_map_iokit_unmapped_region(
	vm_map_t                map,
	vm_size_t               bytes);
extern boolean_t first_free_is_valid(vm_map_t);
extern void             vm_map_range_fork(
	vm_map_t                new_map,
	vm_map_t                old_map);
extern int              vm_map_get_user_range(
	vm_map_t                map,
	vm_map_range_id_t       range_id,
	mach_vm_range_t         range);
static inline bool
VM_MAP_IS_EXOTIC(
	vm_map_t map __unused)
{
	if (VM_MAP_PAGE_SHIFT(map) < PAGE_SHIFT ||
	    pmap_is_exotic(map->pmap)) {
		return true;
	}
	return false;
}

static inline bool
VM_MAP_IS_ALIEN(
	vm_map_t map __unused)
{
	
	if (map->is_alien) {
		return true;
	}
	return false;
}

static inline bool
VM_MAP_POLICY_WX_FAIL(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map)) {
		return false;
	}
	return true;
}

static inline bool
VM_MAP_POLICY_WX_STRIP_X(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map)) {
		return true;
	}
	return false;
}

static inline bool
VM_MAP_POLICY_ALLOW_MULTIPLE_JIT(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map) || map->single_jit) {
		return false;
	}
	return true;
}

static inline bool
VM_MAP_POLICY_ALLOW_JIT_RANDOM_ADDRESS(
	vm_map_t map)
{
	return VM_MAP_IS_ALIEN(map);
}

static inline bool
VM_MAP_POLICY_ALLOW_JIT_INHERIT(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map)) {
		return false;
	}
	return true;
}

static inline bool
VM_MAP_POLICY_ALLOW_JIT_SHARING(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map)) {
		return false;
	}
	return true;
}

static inline bool
VM_MAP_POLICY_ALLOW_JIT_COPY(
	vm_map_t map __unused)
{
	if (VM_MAP_IS_ALIEN(map)) {
		return false;
	}
	return true;
}

static inline bool
VM_MAP_POLICY_WRITABLE_SHARED_REGION(
	vm_map_t map __unused)
{
}

static inline void
vm_prot_to_wimg(unsigned int prot, unsigned int *wimg)
{
	switch (prot) {
	case MAP_MEM_NOOP:                      break;
	case MAP_MEM_IO:                        *wimg = VM_WIMG_IO; break;
	case MAP_MEM_COPYBACK:                  *wimg = VM_WIMG_USE_DEFAULT; break;
	case MAP_MEM_INNERWBACK:                *wimg = VM_WIMG_INNERWBACK; break;
	case MAP_MEM_POSTED:                    *wimg = VM_WIMG_POSTED; break;
	case MAP_MEM_POSTED_REORDERED:          *wimg = VM_WIMG_POSTED_REORDERED; break;
	case MAP_MEM_POSTED_COMBINED_REORDERED: *wimg = VM_WIMG_POSTED_COMBINED_REORDERED; break;
	case MAP_MEM_WTHRU:                     *wimg = VM_WIMG_WTHRU; break;
	case MAP_MEM_WCOMB:                     *wimg = VM_WIMG_WCOMB; break;
	case MAP_MEM_RT:                        *wimg = VM_WIMG_RT; break;
	default:                                break;
	}
}

static inline boolean_t
vm_map_always_shadow(vm_map_t map)
{
	if (map->mapped_in_other_pmaps) {
		
		return TRUE;
	}
	return FALSE;
}

extern void
vm_map_sizes(vm_map_t map,
    vm_map_size_t * psize,
    vm_map_size_t * pfree,
    vm_map_size_t * plargest_free);
extern void vm_map_guard_exception(
	vm_map_offset_t         address,
	unsigned                reason);
RB_HEAD(rb_head, vm_map_store);
extern void vm_map_store_init(
	struct vm_map_header   *header);
extern bool vm_map_store_lookup_entry(
	struct _vm_map         *map,
	vm_map_offset_t         address,
	struct vm_map_entry   **entryp);
extern void _vm_map_store_entry_link(
	struct vm_map_header   *header,
	struct vm_map_entry    *after_where,
	struct vm_map_entry    *entry);
extern void vm_map_store_entry_link(
	struct _vm_map         *map,
	struct vm_map_entry    *after_where,
	struct vm_map_entry    *entry,
	vm_map_kernel_flags_t   vmk_flags);
extern void _vm_map_store_entry_unlink(
	struct vm_map_header   *header,
	struct vm_map_entry    *entry,
	bool                    check_permanent);
extern void vm_map_store_entry_unlink(
	struct _vm_map         *map,
	struct vm_map_entry    *entry,
	bool                    check_permanent);
extern void vm_map_store_update_first_free(
	struct _vm_map         *map,
	struct vm_map_entry    *entry,
	bool                    new_entry_creation);
extern void vm_map_store_copy_reset(
	struct vm_map_copy     *copy_map,
	struct vm_map_entry    *entry);
extern bool vm_map_store_has_RB_support(
	struct vm_map_header   *header);
extern struct vm_map_entry *vm_map_store_find_space(
	vm_map_t                map,
	vm_map_offset_t         hint,
	vm_map_offset_t         limit,
	bool                    backwards,
	vm_map_offset_t         guard_offset,
	vm_map_size_t           size,
	vm_map_offset_t         mask,
	vm_map_offset_t        *addr_out);
extern bool first_free_is_valid_ll(
	struct _vm_map         *map);
extern void vm_map_store_init_ll(
	struct vm_map_header   *header);
extern void vm_map_store_entry_link_ll(
	struct vm_map_header   *header,
	struct vm_map_entry    *after_where,
	struct vm_map_entry    *entry);
extern void vm_map_store_entry_unlink_ll(
	struct vm_map_header   *header,
	struct vm_map_entry    *entry);
extern void update_first_free_ll(
	struct _vm_map         *map,
	struct vm_map_entry    *entry);
extern void vm_map_store_copy_reset_ll(
	struct vm_map_copy     *copy_map,
	struct vm_map_entry    *entry,
	int                     nentries);
RB_PROTOTYPE_SC(__private_extern__, rb_head, vm_map_store, entry, rb_node_compare);
extern void vm_map_store_init_rb(
	struct vm_map_header   *header);
extern int rb_node_compare(
	struct vm_map_store    *first,
	struct vm_map_store    *second);
extern bool vm_map_store_lookup_entry_rb(
	struct _vm_map         *map,
	vm_map_offset_t         address,
	struct vm_map_entry   **entryp);
extern void vm_map_store_entry_link_rb(
	struct vm_map_header   *header,
	struct vm_map_entry    *entry);
extern void vm_map_store_entry_unlink_rb(
	struct vm_map_header   *header,
	struct vm_map_entry    *entry);
extern void vm_map_store_copy_reset_rb(
	struct vm_map_copy     *copy_map,
	struct vm_map_entry    *entry,
	int                     nentries);
extern void update_first_free_rb(
	struct _vm_map         *map,
	struct vm_map_entry    *entry,
	bool                    new_entry_creation);
__BEGIN_DECLS

extern void     vm_map_reference(vm_map_t       map);
extern vm_map_t current_map(void);
extern kern_return_t    vm_map_exec(
	vm_map_t                new_map,
	task_t                  task,
	boolean_t               is64bit,
	void                    *fsroot,
	cpu_type_t              cpu,
	cpu_subtype_t           cpu_subtype,
	boolean_t               reslide,
	boolean_t               is_driverkit,
	uint32_t                rsr_version);
static inline vm_map_t
_VME_SUBMAP(
	vm_map_entry_t entry)
{
	__builtin_assume(entry->vme_submap);
	return (vm_map_t)(entry->vme_submap << VME_SUBMAP_SHIFT);
}

static inline void
VME_SUBMAP_SET(
	vm_map_entry_t entry,
	vm_map_t submap)
{
	__builtin_assume(((vm_offset_t)submap & 3) == 0);

	entry->is_sub_map = true;
	entry->vme_submap = (vm_offset_t)submap >> VME_SUBMAP_SHIFT;
}

static inline vm_object_t
_VME_OBJECT(
	vm_map_entry_t entry)
{
	vm_object_t object;

	if (!entry->vme_kernel_object) {
		object = VM_OBJECT_UNPACK(entry->vme_object_or_delta);
		__builtin_assume(!is_kernel_object(object));
	} else {
		object = kernel_object_default;
	}
	return object;
}


static inline vm_object_offset_t
VME_OFFSET(
	vm_map_entry_t entry)
{
	return entry->vme_offset << VME_OFFSET_SHIFT;
}








typedef 






typedef 






ZONE_DECLARE_ID(ZONE_ID_VM_MAP_ENTRY, struct vm_map_entry);
ZONE_DECLARE_ID(ZONE_ID_VM_MAP_HOLES, struct vm_map_links);
ZONE_DECLARE_ID(ZONE_ID_VM_MAP, struct _vm_map);
extern boolean_t        vm_map_lookup_entry(
	vm_map_t                map,
	vm_map_address_t        address,
	vm_map_entry_t          *entry);
extern void             vm_map_reference(
	vm_map_t        map);
extern void             vm_map_inherit_limits(
	vm_map_t                new_map,
	const struct _vm_map   *old_map);
extern vm_map_t         vm_map_fork(
	ledger_t                ledger,
	vm_map_t                old_map,
	int                     options);
extern kern_return_t vm_map_query_volatile(
	vm_map_t        map,
	mach_vm_size_t  *volatile_virtual_size_p,
	mach_vm_size_t  *volatile_resident_size_p,
	mach_vm_size_t  *volatile_compressed_size_p,
	mach_vm_size_t  *volatile_pmap_size_p,
	mach_vm_size_t  *volatile_compressed_pmap_size_p);
extern kern_return_t vm_map_set_cache_attr(
	vm_map_t        map,
	vm_map_offset_t va);
extern void vm_map_copy_footprint_ledgers(
	task_t  old_task,
	task_t  new_task);
extern size_t ml_get_vm_reserved_regions(
	bool                    vm_is64bit,
	const struct vm_reserved_region **regions);
extern void ml_fp_save_area_prealloc(void);
extern  kern_return_t   vm_map_write_user(
	vm_map_t                map,
	void                   *src_p,
	vm_map_offset_ut        dst_addr_u,
	vm_size_ut              size_u);
extern  kern_return_t   vm_map_read_user(
	vm_map_t                map,
	vm_map_offset_ut        src_addr_u,
	void                   *dst_p,
	vm_size_ut              size_u);
extern vm_map_size_t    vm_map_adjusted_size(vm_map_t map);
extern void vm_map_switch_back(vm_map_switch_context_t ctx);
extern boolean_t vm_map_cs_enforcement(
	vm_map_t                map);
extern void vm_map_cs_enforcement_set(
	vm_map_t                map,
	boolean_t               val);
extern void vm_map_cs_debugged_set(
	vm_map_t map,
	boolean_t val);
extern kern_return_t vm_map_cs_wx_enable(vm_map_t map);
extern kern_return_t vm_map_csm_allow_jit(vm_map_t map);
extern void vm_map_will_allocate_early_map(
	vm_map_t               *map_owner);
extern void vm_map_relocate_early_maps(
	vm_offset_t             delta);
extern void vm_map_relocate_early_elem(
	uint32_t                zone_id,
	vm_offset_t             new_addr,
	vm_offset_t             delta);
extern vm_map_t vm_map_create_options(
	pmap_t                  pmap,
	vm_map_offset_t         min_off,
	vm_map_offset_t         max_off,
	vm_map_create_options_t options);
extern boolean_t        vm_kernel_map_is_kernel(vm_map_t map);
extern kern_return_t    vm_map_enter_mem_object_control(
	vm_map_t                target_map,
	vm_map_offset_ut       *address,
	vm_map_size_ut          initial_size,
	vm_map_offset_ut        mask,
	vm_map_kernel_flags_t   vmk_flags,
	memory_object_control_t control,
	vm_object_offset_ut     offset,
	boolean_t               needs_copy,
	vm_prot_ut              cur_protection,
	vm_prot_ut              max_protection,
	vm_inherit_ut           inheritance);
extern void vm_map_setup(vm_map_t map, task_t task);
extern kern_return_t    vm_map_terminate(
	vm_map_t                map);
extern kern_return_t    vm_map_copy_overwrite(
	vm_map_t                dst_map,
	vm_map_address_ut       dst_addr_u,
	vm_map_copy_t           copy,
	vm_map_size_ut          copy_size_u,
	boolean_t               interruptible);
extern boolean_t        vm_map_copy_validate_size(
	vm_map_t                dst_map,
	vm_map_copy_t           copy,
	vm_map_size_t          *size);
extern kern_return_t    vm_map_copyout_size(
	vm_map_t                dst_map,
	vm_map_address_t       *dst_addr, 
	vm_map_copy_t           copy,
	vm_map_size_ut          copy_size);
extern void             vm_map_disable_NX(
	vm_map_t                map);
extern void             vm_map_disallow_data_exec(
	vm_map_t                map);
extern void             vm_map_set_64bit(
	vm_map_t                map);
extern void             vm_map_set_32bit(
	vm_map_t                map);
extern void             vm_map_set_jumbo(
	vm_map_t                map);
extern void             vm_map_set_jit_entitled(
	vm_map_t                map);
extern void             vm_map_set_max_addr(
	vm_map_t                map,
	vm_map_offset_t         new_max_offset,
	bool                    extra_jumbo);
extern boolean_t        vm_map_has_hard_pagezero(
	vm_map_t                map,
	vm_map_offset_t         pagezero_size);
extern void             vm_commit_pagezero_status(vm_map_t      tmap);
extern boolean_t        vm_map_tpro(
	vm_map_t                map);
extern void             vm_map_set_tpro(
	vm_map_t                map);
extern void             vm_map_set_tpro_enforcement(
	vm_map_t                map);
extern boolean_t        vm_map_set_tpro_range(
	vm_map_t                map,
	vm_map_address_t        start,
	vm_map_address_t        end);
extern boolean_t        vm_map_is_64bit(
	vm_map_t                map);
extern kern_return_t    vm_map_raise_max_offset(
	vm_map_t        map,
	vm_map_offset_t new_max_offset);
extern kern_return_t    vm_map_raise_min_offset(
	vm_map_t        map,
	vm_map_offset_t new_min_offset);
extern void vm_map_set_high_start(
	vm_map_t        map,
	vm_map_offset_t high_start);
extern vm_map_offset_t  vm_compute_max_offset(
	boolean_t               is64);
extern void             vm_map_get_max_aslr_slide_section(
	vm_map_t                map,
	int64_t                 *max_sections,
	int64_t                 *section_size);
extern uint64_t         vm_map_get_max_aslr_slide_pages(
	vm_map_t map);
extern uint64_t         vm_map_get_max_loader_aslr_slide_pages(
	vm_map_t map);
extern kern_return_t    vm_map_set_size_limit(
	vm_map_t                map,
	uint64_t                limit);
extern kern_return_t    vm_map_set_data_limit(
	vm_map_t                map,
	uint64_t                limit);
extern void             vm_map_set_user_wire_limit(
	vm_map_t                map,
	vm_size_t               limit);
extern void vm_map_switch_protect(
	vm_map_t                map,
	boolean_t               val);
extern boolean_t        vm_map_page_aligned(
	vm_map_offset_t         offset,
	vm_map_offset_t         mask);
extern bool vm_map_range_overflows(
	vm_map_t                map,
	vm_map_offset_t         addr,
	vm_map_size_t           size);
extern kern_return_t    vm_map_range_configure(
	vm_map_t                map,
	bool                    needs_extra_jumbo_va);
extern void             vm_map_kernel_flags_update_range_id(
	vm_map_kernel_flags_t  *flags,
	vm_map_t                map,
	vm_map_size_t           size);
extern void vm_map_mark_alien(vm_map_t map);
extern void vm_map_single_jit(vm_map_t map);
extern kern_return_t vm_map_page_info(
	vm_map_t                map,
	vm_map_offset_ut        offset,
	vm_page_info_flavor_t   flavor,
	vm_page_info_t          info,
	mach_msg_type_number_t  *count);
extern kern_return_t vm_map_page_range_info_internal(
	vm_map_t                map,
	vm_map_offset_ut        start_offset,
	vm_map_offset_ut        end_offset,
	int                     effective_page_shift,
	vm_page_info_flavor_t   flavor,
	vm_page_info_t          info,
	mach_msg_type_number_t  *count);
static inline int
VM_MAP_PAGE_SHIFT(
	vm_map_t map)
{
	int shift = map ? map->hdr.page_shift : PAGE_SHIFT;
	
	__builtin_assume(shift >= 12 && shift <= 14);
	return shift;
}




extern kern_return_t vm_map_set_page_shift(vm_map_t map, int pageshift);
extern bool vm_map_is_exotic(vm_map_t map);
extern bool vm_map_is_alien(vm_map_t map);
extern pmap_t vm_map_get_pmap(vm_map_t map);
extern void vm_map_guard_exception(vm_map_offset_t gap_start, unsigned reason);
extern bool vm_map_is_corpse_source(vm_map_t map);
extern void vm_map_set_corpse_source(vm_map_t map);
extern void vm_map_unset_corpse_source(vm_map_t map);
extern kern_return_t vm_map_partial_reap(
	vm_map_t map,
	unsigned int *reclaimed_resident,
	unsigned int *reclaimed_compressed);
boolean_t        kdp_vm_map_is_acquired_exclusive(vm_map_t map);
boolean_t        vm_map_entry_has_device_pager(vm_map_t, vm_map_offset_t vaddr);
bool vm_map_is_map_size_valid(vm_map_t target_map, vm_size_t size, bool no_soft_limit);
__BEGIN_DECLS



extern vm_object_t vm_convert_port_to_copy_object(
	ipc_port_t      port);
__BEGIN_DECLS

extern void mach_memory_entry_port_release(ipc_port_t port);
extern vm_named_entry_t mach_memory_entry_from_port(ipc_port_t port);
extern struct vm_named_entry *mach_memory_entry_allocate(ipc_port_t *user_handle_p);
static inline void
VM_OBJECT_SET_PAGER_CREATED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->pager_created = value;
}
static inline void
VM_OBJECT_SET_PAGER_INITIALIZED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->pager_initialized = value;
}
static inline void
VM_OBJECT_SET_PAGER_READY(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->pager_ready = value;
}
static inline void
VM_OBJECT_SET_PAGER_TRUSTED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->pager_trusted = value;
}
static inline void
VM_OBJECT_SET_CAN_PERSIST(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->can_persist = value;
}
static inline void
VM_OBJECT_SET_INTERNAL(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->internal = value;
}
static inline void
VM_OBJECT_SET_PRIVATE(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->private = value;
}
static inline void
VM_OBJECT_SET_PAGEOUT(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->pageout = value;
}
static inline void
VM_OBJECT_SET_ALIVE(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->alive = value;
}
static inline void
VM_OBJECT_SET_PURGABLE(
	vm_object_t object,
	unsigned int value)
{
	vm_object_lock_assert_exclusive(object);
	object->purgable = value;
	assert3u(object->purgable, ==, value);
}
static inline void
VM_OBJECT_SET_PURGEABLE_ONLY_BY_KERNEL(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->purgeable_only_by_kernel = value;
}
static inline void
VM_OBJECT_SET_PURGEABLE_WHEN_RIPE(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->purgeable_when_ripe = value;
}
static inline void
VM_OBJECT_SET_SHADOWED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->shadowed = value;
}
static inline void
VM_OBJECT_SET_TRUE_SHARE(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->true_share = value;
}
static inline void
VM_OBJECT_SET_TERMINATING(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->terminating = value;
}
static inline void
VM_OBJECT_SET_NAMED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->named = value;
}
static inline void
VM_OBJECT_SET_SHADOW_SEVERED(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->shadow_severed = value;
}
static inline void
VM_OBJECT_SET_PHYS_CONTIGUOUS(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->phys_contiguous = value;
}
static inline void
VM_OBJECT_SET_NOPHYSCACHE(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->nophyscache = value;
}
static inline void
VM_OBJECT_SET_FOR_REALTIME(
	vm_object_t object,
	bool value)
{
	vm_object_lock_assert_exclusive(object);
	object->for_realtime = value;
}
static inline void
VM_OBJECT_SET_NO_PAGER_REASON(
	vm_object_t object,
	unsigned int value)
{
	vm_object_lock_assert_exclusive(object);
	object->no_pager_reason = value;
	assert3u(object->no_pager_reason, ==, value);
}



__private_extern__ void         vm_object_bootstrap(void);
__private_extern__ void         vm_object_reaper_init(void);
__private_extern__ vm_object_t  vm_object_allocate(vm_object_size_t size);
__private_extern__ void    _vm_object_allocate(vm_object_size_t size,
    vm_object_t object);
__private_extern__ void vm_object_set_size(
	vm_object_t             object,
	vm_object_size_t        outer_size,
	vm_object_size_t        inner_size);
static inline void
vm_object_reference_locked(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	os_ref_retain_locked_raw(&object->ref_count, &vm_object_refgrp);
}

static inline void
vm_object_reference_shared(vm_object_t object)
{
	vm_object_lock_assert_shared(object);
	os_ref_retain_raw(&object->ref_count, &vm_object_refgrp);
}

__private_extern__ void         vm_object_reference(
	vm_object_t     object);
__private_extern__ void         vm_object_deallocate(
	vm_object_t     object);
__private_extern__ void         vm_object_pmap_protect(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	pmap_t                  pmap,
	vm_map_size_t           pmap_page_size,
	vm_map_offset_t         pmap_start,
	vm_prot_t               prot);
__private_extern__ void         vm_object_pmap_protect_options(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	pmap_t                  pmap,
	vm_map_size_t           pmap_page_size,
	vm_map_offset_t         pmap_start,
	vm_prot_t               prot,
	int                     options);
__private_extern__ void         vm_object_page_remove(
	vm_object_t             object,
	vm_object_offset_t      start,
	vm_object_offset_t      end);
__private_extern__ void         vm_object_deactivate_pages(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	boolean_t               kill_page,
	boolean_t               reusable_page,
	boolean_t               kill_no_write,
	struct pmap             *pmap,

	vm_map_offset_t         pmap_offset);
__private_extern__ void vm_object_reuse_pages(
	vm_object_t             object,
	vm_object_offset_t      start_offset,
	vm_object_offset_t      end_offset,
	boolean_t               allow_partial_reuse);
__private_extern__ kern_return_t vm_object_zero(
	vm_object_t             object,
	vm_object_offset_t      cur_offset,
	vm_object_offset_t      end_offset);
__private_extern__ uint64_t     vm_object_purge(
	vm_object_t              object,
	int                      flags);
__private_extern__ kern_return_t vm_object_purgable_control(
	vm_object_t     object,
	vm_purgable_t   control,
	int             *state);
__private_extern__ kern_return_t vm_object_get_page_counts(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	unsigned int            *resident_page_count,
	unsigned int            *dirty_page_count);
__private_extern__ boolean_t    vm_object_coalesce(
	vm_object_t             prev_object,
	vm_object_t             next_object,
	vm_object_offset_t      prev_offset,
	vm_object_offset_t      next_offset,
	vm_object_size_t        prev_size,
	vm_object_size_t        next_size);
__private_extern__ boolean_t    vm_object_shadow(
	vm_object_t             *object,
	vm_object_offset_t      *offset,
	vm_object_size_t        length,
	boolean_t               always_shadow);
__private_extern__ void         vm_object_collapse(
	vm_object_t             object,
	vm_object_offset_t      offset,
	boolean_t               can_bypass);
__private_extern__ boolean_t    vm_object_copy_quickly(
	vm_object_t             object,
	vm_object_offset_t      src_offset,
	vm_object_size_t        size,
	boolean_t               *_src_needs_copy,
	boolean_t               *_dst_needs_copy);
__private_extern__ kern_return_t        vm_object_copy_strategically(
	vm_object_t             src_object,
	vm_object_offset_t      src_offset,
	vm_object_size_t        size,
	bool                    forking,
	vm_object_t             *dst_object,
	vm_object_offset_t      *dst_offset,
	boolean_t               *dst_needs_copy);
__private_extern__ kern_return_t        vm_object_copy_slowly(
	vm_object_t             src_object,
	vm_object_offset_t      src_offset,
	vm_object_size_t        size,
	boolean_t               interruptible,
	vm_object_t             *_result_object);
__private_extern__ vm_object_t  vm_object_copy_delayed(
	vm_object_t             src_object,
	vm_object_offset_t      src_offset,
	vm_object_size_t        size,
	boolean_t               src_object_shared);
__private_extern__ kern_return_t        vm_object_destroy(
	vm_object_t                                     object,
	vm_object_destroy_reason_t   reason);
__private_extern__ void         vm_object_compressor_pager_create(
	vm_object_t     object);
__private_extern__ vm_external_state_t vm_object_compressor_pager_state_get(
	vm_object_t        object,
	vm_object_offset_t offset);
__private_extern__ void vm_object_compressor_pager_state_clr(
	vm_object_t        object,
	vm_object_offset_t offset);
__private_extern__ kern_return_t vm_object_upl_request(
	vm_object_t             object,
	vm_object_offset_t      offset,
	upl_size_t              size,
	upl_t                   *upl,
	upl_page_info_t         *page_info,
	unsigned int            *count,
	upl_control_flags_t     flags,
	vm_tag_t            tag);
__private_extern__ kern_return_t vm_object_transpose(
	vm_object_t             object1,
	vm_object_t             object2,
	vm_object_size_t        transpose_size);
__private_extern__ boolean_t vm_object_sync(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	boolean_t               should_flush,
	boolean_t               should_return,
	boolean_t               should_iosync);
__private_extern__ kern_return_t vm_object_update(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	vm_object_offset_t      *error_offset,
	int                     *io_errno,
	memory_object_return_t  should_return,
	int                     flags,
	vm_prot_t               prot);
__private_extern__ kern_return_t vm_object_lock_request(
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	memory_object_return_t  should_return,
	int                     flags,
	vm_prot_t               prot);
__private_extern__ vm_object_t  vm_object_memory_object_associate(
	memory_object_t         pager,
	vm_object_t             object,
	vm_object_size_t        size,
	boolean_t               check_named);
__private_extern__ void vm_object_cluster_size(
	vm_object_t             object,
	vm_object_offset_t      *start,
	vm_size_t               *length,
	vm_object_fault_info_t  fault_info,
	uint32_t                *io_streaming);
__private_extern__ kern_return_t vm_object_populate_with_private(
	vm_object_t             object,
	vm_object_offset_t      offset,
	ppnum_t                 phys_page,
	vm_size_t               size);
__private_extern__ void vm_object_change_wimg_mode(
	vm_object_t             object,
	unsigned int            wimg_mode);
extern kern_return_t vm_object_page_op(
	vm_object_t             object,
	vm_object_offset_t      offset,
	int                     ops,
	ppnum_t                 *phys_entry,
	int                     *flags);
extern kern_return_t vm_object_range_op(
	vm_object_t             object,
	vm_object_offset_t      offset_beg,
	vm_object_offset_t      offset_end,
	int                     ops,
	uint32_t                *range);
__private_extern__ void         vm_object_reap_pages(
	vm_object_t object,
	int     reap_type);
__private_extern__ void
vm_object_pageout(
	vm_object_t     object);
__enum_closed_decl(vm_object_wait_reason_t, uint8_t, {
	VM_OBJECT_EVENT_PAGER_INIT = 0,
	VM_OBJECT_EVENT_PAGER_READY = 1,
	VM_OBJECT_EVENT_PAGING_IN_PROGRESS = 2,
	VM_OBJECT_EVENT_MAPPING_IN_PROGRESS = 3,
	VM_OBJECT_EVENT_UNBLOCKED = 4,
	VM_OBJECT_EVENT_PAGING_ONLY_IN_PROGRESS = 5,
	VM_OBJECT_EVENT_PAGEIN_THROTTLE = 6,
});
_Static_assert(VM_OBJECT_EVENT_MAX < 7,
    "vm_object_wait_reason_t must fit in all_wanted");
_Static_assert(VM_OBJECT_EVENT_MAX < offsetof(struct vm_object, Lock),
    "Wait reason collides with vm_object->Lock");
extern wait_result_t vm_object_sleep(
	vm_object_t             object,
	vm_object_wait_reason_t reason,
	wait_interrupt_t        interruptible,
	lck_sleep_action_t      action);
static inline void
vm_object_set_wanted(
	vm_object_t             object,
	vm_object_wait_reason_t reason)
{
	vm_object_lock_assert_exclusive(object);
	assert(reason >= 0 && reason <= VM_OBJECT_EVENT_MAX);

	object->all_wanted |= (1 << reason);
}

static inline bool
vm_object_wanted(
	vm_object_t             object,
	vm_object_wait_reason_t event)
{
	vm_object_lock_assert_held(object);
	assert(event >= 0 && event <= VM_OBJECT_EVENT_MAX);

	return object->all_wanted & (1 << event);
}

extern void vm_object_wakeup(
	vm_object_t             object,
	vm_object_wait_reason_t reason);
static inline void
vm_object_activity_begin(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	VM_PIP_DEBUG_BEGIN(object);
	if (os_inc_overflow(&object->activity_in_progress)) {
		panic("vm_object_activity_begin(%p): overflow\n", object);
	}
}

static inline void
vm_object_activity_end(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	if (os_dec_overflow(&object->activity_in_progress)) {
		panic("vm_object_activity_end(%p): underflow\n", object);
	}
	if (object->paging_in_progress == 0 &&
	    object->activity_in_progress == 0) {
		vm_object_wakeup((object),
		    VM_OBJECT_EVENT_PAGING_IN_PROGRESS);
	}
}

static inline void
vm_object_paging_begin(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	VM_PIP_DEBUG_BEGIN((object));
	if (os_inc_overflow(&object->paging_in_progress)) {
		panic("vm_object_paging_begin(%p): overflow\n", object);
	}
}

static inline void
vm_object_paging_end(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	if (os_dec_overflow(&object->paging_in_progress)) {
		panic("vm_object_paging_end(%p): underflow\n", object);
	}
	
	if (object->paging_in_progress == vm_object_pagein_throttle - 1) {
		vm_object_wakeup(object, VM_OBJECT_EVENT_PAGEIN_THROTTLE);
	}
	if (object->paging_in_progress == 0) {
		vm_object_wakeup(object, VM_OBJECT_EVENT_PAGING_ONLY_IN_PROGRESS);
		if (object->activity_in_progress == 0) {
			vm_object_wakeup((object),
			    VM_OBJECT_EVENT_PAGING_IN_PROGRESS);
		}
	}
}


extern wait_result_t vm_object_paging_wait(vm_object_t object, wait_interrupt_t interruptible);
extern wait_result_t vm_object_paging_only_wait(vm_object_t object, wait_interrupt_t interruptible);
extern wait_result_t vm_object_paging_throttle_wait(vm_object_t object, wait_interrupt_t interruptible);
static inline void
vm_object_mapping_begin(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	assert(!object->mapping_in_progress);
	object->mapping_in_progress = TRUE;
}

static inline void
vm_object_mapping_end(vm_object_t object)
{
	vm_object_lock_assert_exclusive(object);
	assert(object->mapping_in_progress);
	object->mapping_in_progress = FALSE;
	vm_object_wakeup(object,
	    VM_OBJECT_EVENT_MAPPING_IN_PROGRESS);
}

extern wait_result_t vm_object_mapping_wait(vm_object_t object, wait_interrupt_t interruptible);
extern void     vm_object_cache_add(vm_object_t);
extern void     vm_object_cache_remove(vm_object_t);
extern int      vm_object_cache_evict(int, int);
extern void     vm_object_ledger_tag_ledgers(
	vm_object_t object,
	int *ledger_idx_volatile,
	int *ledger_idx_nonvolatile,
	int *ledger_idx_volatile_compressed,
	int *ledger_idx_nonvolatile_compressed,
	int *ledger_idx_composite,
	int *ledger_idx_external_wired,
	boolean_t *do_footprint);
extern kern_return_t vm_object_ownership_change(
	vm_object_t object,
	int new_ledger_tag,
	task_t new_owner,
	int new_ledger_flags,
	boolean_t task_objq_locked);
os_refgrp_decl_extern(vm_object_refgrp);
extern void             vm_object_lock(vm_object_t);
extern bool             vm_object_lock_check_contended(vm_object_t);
extern boolean_t        vm_object_lock_try(vm_object_t);
extern boolean_t        _vm_object_lock_try(vm_object_t);
extern boolean_t        vm_object_lock_avoid(vm_object_t);
extern void             vm_object_lock_shared(vm_object_t);
extern boolean_t        vm_object_lock_yield_shared(vm_object_t);
extern boolean_t        vm_object_lock_try_shared(vm_object_t);
extern void             vm_object_unlock(vm_object_t);
extern boolean_t        vm_object_lock_upgrade(vm_object_t);
extern void             kdp_vm_object_sleep_find_owner(
	event64_t          wait_event,
	block_hint_t       wait_type,
	thread_waitinfo_t *waitinfo);
extern void page_worker_init(void);
__enum_closed_decl(vm_relocate_reason_t, unsigned int, {
	VM_RELOCATE_REASON_CONTIGUOUS,

	VM_RELOCATE_REASON_COUNT,
});
__enum_closed_decl(vm_remove_reason_t, unsigned int, {
	VM_REMOVE_REASON_USE,
	VM_REMOVE_REASON_REBALANCE,

	VM_REMOVE_REASON_COUNT,
});
__enum_closed_decl(vm_memory_class_t, unsigned int, {
	VM_MEMORY_CLASS_REGULAR,

	VM_MEMORY_CLASS_COUNT,
});
__pure2
static inline struct vm_page *
vm_pages_array_internal(void)
{
	extern vm_page_t vm_pages;
	return vm_pages;
}


__pure2
static inline vm_page_t
vm_page_get(uint32_t i)
{
	return VM_FAR_ADD_PTR_UNBOUNDED(vm_pages_array_internal(), i);
}

__pure2
static inline bool
vm_page_in_array(const struct vm_page *m)
{
	return vm_pages_array_internal() <= m && m < vm_pages_end;
}

typedef struct vm_page_with_ppnum *vm_page_with_ppnum_t;
static inline ppnum_t
VM_PAGE_GET_PHYS_PAGE(const struct vm_page *m)
{
	return ((const struct vm_page_with_ppnum *)m)->vmp_phys_page;
}

static inline void
VM_PAGE_INIT_PHYS_PAGE(struct vm_page *m, ppnum_t pnum)
{
	((vm_page_with_ppnum_t)(m))->vmp_phys_page = pnum;
}

static inline void
VM_PAGE_SET_PHYS_PAGE(struct vm_page *m, ppnum_t pnum)
{
	assert(!vm_page_in_array(m) && !m->vmp_canonical);
	((vm_page_with_ppnum_t)(m))->vmp_phys_page = pnum;
}





static inline vm_page_packed_t
vm_page_pack_ptr(uintptr_t p)
{
	if (vm_page_in_array((vm_page_t)p)) {
		ptrdiff_t diff = (vm_page_t)p - vm_pages_array_internal();
		assert((vm_page_t)p == vm_page_get((uint32_t)diff));
		return (vm_page_packed_t)(diff | VM_PAGE_PACKED_FROM_ARRAY);
	}

	VM_ASSERT_POINTER_PACKABLE(p, VM_PAGE_PACKED_PTR);
	vm_offset_t packed = VM_PACK_POINTER(p, VM_PAGE_PACKED_PTR);
	return CAST_DOWN_EXPLICIT(vm_page_packed_t, packed);
}


static inline uintptr_t
vm_page_unpack_ptr(uintptr_t p)
{
	if (p >= VM_PAGE_PACKED_FROM_ARRAY) {
		p &= ~VM_PAGE_PACKED_FROM_ARRAY;
		assert(p < (uintptr_t)vm_pages_count);
		return (uintptr_t)vm_page_get((uint32_t)p);
	}

	return VM_UNPACK_POINTER(p, VM_PAGE_PACKED_PTR);
}


























































extern
struct vm_speculative_age_q     vm_page_queue_speculative[];
extern void             vm_page_init_local_q(unsigned int num_cpus);
extern void             vm_page_create_canonical(ppnum_t pnum);
extern void             vm_page_create_retired(ppnum_t pn);
extern void             vm_pages_array_finalize(void);
extern vm_page_t        vm_page_alloc(
	vm_object_t             object,
	vm_object_offset_t      offset);
extern void             vm_page_reactivate_all_throttled(void);
extern void vm_pressure_response(void);
static inline void
vm_page_lock_queues(void)
{
	lck_mtx_lock(&vm_page_queue_lock);
}

static inline boolean_t
vm_page_trylock_queues(void)
{
	boolean_t ret;
	ret = lck_mtx_try_lock(&vm_page_queue_lock);
	return ret;
}

static inline void
vm_page_unlock_queues(void)
{
	lck_mtx_unlock(&vm_page_queue_lock);
}

static inline void
vm_page_lockspin_queues(void)
{
	lck_mtx_lock_spin(&vm_page_queue_lock);
}

static inline boolean_t
vm_page_trylockspin_queues(void)
{
	boolean_t ret;
	ret = lck_mtx_try_lock_spin(&vm_page_queue_lock);
	return ret;
}

extern void kdp_vm_page_sleep_find_owner(
	event64_t          wait_event,
	thread_waitinfo_t *waitinfo);
extern upl_size_t upl_get_size(
	upl_t                   upl);
extern kern_return_t    mach_vm_pressure_level_monitor(boolean_t wait_for_pressure, unsigned int *pressure_level);
extern kern_return_t    mach_vm_wire_level_monitor(int64_t requested_pages);
extern void vm_countdirtypages(void);
extern kern_return_t upl_transpose(
	upl_t   upl1,
	upl_t   upl2);
extern kern_return_t mach_vm_pressure_monitor(
	boolean_t       wait_for_pressure,
	unsigned int    nsecs_monitored,
	unsigned int    *pages_reclaimed_p,
	unsigned int    *pages_wanted_p);
extern kern_return_t
vm_set_buffer_cleanup_callout(
	boolean_t       (*func)(int));
extern int hibernate_flush_memory(void);
extern void hibernate_reset_stats(void);
extern void hibernate_create_paddr_map(void);
extern void vm_set_restrictions(unsigned int num_cpus);
extern kern_return_t vm_pageout_compress_page(void **, char *, vm_page_t);
extern kern_return_t vm_pageout_anonymous_pages(void);
extern void vm_pageout_disconnect_all_pages(void);
extern int vm_toggle_task_selfdonate_pages(task_t);
extern void vm_task_set_selfdonate_pages(task_t, bool);
__BEGIN_DECLS



extern void vm_pageout_garbage_collect(void *, wait_result_t);
extern void vm_object_set_pmap_cache_attr(
	vm_object_t             object,
	upl_page_info_array_t   user_page_list,
	unsigned int            num_pages,
	boolean_t               batch_pmap_op);
extern kern_return_t vm_object_iopl_request(
	vm_object_t             object,
	vm_object_offset_t      offset,
	upl_size_t              size,
	upl_t                  *upl_ptr,
	upl_page_info_array_t   user_page_list,
	unsigned int           *page_list_count,
	upl_control_flags_t     cntrl_flags,
	vm_tag_t                tag);
extern kern_return_t vm_map_enter_upl(
	vm_map_t                map,
	upl_t                   upl,
	vm_map_offset_t         *dst_addr);
extern kern_return_t vm_map_remove_upl(
	vm_map_t                map,
	upl_t                   upl);
extern kern_return_t vm_map_enter_upl_range(
	vm_map_t                map,
	upl_t                   upl,
	vm_object_offset_t             offset,
	vm_size_t               size,
	vm_prot_t               prot,
	vm_map_offset_t         *dst_addr);
extern kern_return_t vm_map_remove_upl_range(
	vm_map_t                map,
	upl_t                   upl,
	vm_object_offset_t             offset,
	vm_size_t               size);
extern struct vm_page_delayed_work*
vm_page_delayed_work_get_ctx(void);
extern void
vm_page_delayed_work_finish_ctx(struct vm_page_delayed_work* dwp);
extern void vm_pageout_throttle_up(vm_page_t page);
extern kern_return_t vm_paging_map_object(
	vm_page_t               page,
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_prot_t               protection,
	boolean_t               can_unlock_object,
	vm_map_size_t           *size,          
	vm_map_offset_t         *address,       
	boolean_t               *need_unmap);
extern void vm_paging_unmap_object(
	vm_object_t             object,
	vm_map_offset_t         start,
	vm_map_offset_t         end);
decl_simple_lock_data(extern, vm_paging_lock);
extern void vm_pageout_steal_laundry(
	vm_page_t page,
	boolean_t queues_locked);
__BEGIN_DECLS


extern void memoryshot(unsigned int event, unsigned int control);
extern void update_vm_info(void);
extern void upl_set_iodone(upl_t, void *);
extern void upl_set_iodone_error(upl_t, int);
extern void upl_callout_iodone(upl_t);
extern ppnum_t upl_get_highest_page(
	upl_t                   upl);
extern upl_t upl_associated_upl(upl_t upl);
extern void upl_set_associated_upl(upl_t upl, upl_t associated_upl);
extern void upl_set_map_exclusive(upl_t upl);
extern void upl_clear_map_exclusive(upl_t upl);
extern upl_size_t upl_adjusted_size(
	upl_t upl,
	vm_map_offset_t page_mask);
extern vm_object_offset_t upl_adjusted_offset(
	upl_t upl,
	vm_map_offset_t page_mask);
extern vm_object_offset_t upl_get_data_offset(
	upl_t upl);
extern kern_return_t vm_map_create_upl(
	vm_map_t                map,
	vm_map_address_t        offset,
	upl_size_t              *upl_size,
	upl_t                   *upl,
	upl_page_info_array_t   page_list,
	unsigned int            *count,
	upl_control_flags_t     *flags,
	vm_tag_t            tag);
extern void               vm_page_free_list(
	vm_page_t   mem,
	boolean_t   prepare_object);
extern kern_return_t vm_page_alloc_list(
	vm_size_t   page_count,
	kma_flags_t flags,
	vm_page_t  *list);
extern kern_return_t    vm_pageout_wait(uint64_t deadline);
extern void vector_upl_set_iostate(upl_t, upl_t, upl_offset_t, upl_size_t);
extern void             vm_pageout(void);
__startup_func extern void             vm_config_init(void);
extern kern_return_t    vm_pageout_internal_start(void);
extern void             vm_pageout_object_terminate(
	vm_object_t     object);
extern void             vm_pageout_cluster(
	vm_page_t       m);
extern void             vm_pageout_initialize_page(
	vm_page_t       m);
extern void vector_upl_deallocate(upl_t);
extern void vector_upl_set_submap(upl_t, vm_map_t, vm_offset_t);
extern void vector_upl_get_submap(upl_t, vm_map_t*, vm_offset_t*);
extern void vector_upl_get_iostate(upl_t, upl_t, upl_offset_t*, upl_size_t*);
extern void vector_upl_get_iostate_byindex(upl_t, uint32_t, upl_offset_t*, upl_size_t*);
extern upl_t vector_upl_subupl_byindex(upl_t, uint32_t);
extern upl_t vector_upl_subupl_byoffset(upl_t, upl_offset_t*, upl_size_t*);
extern void vm_page_free_reserve(int pages);
extern void vm_swapout_thread(void);
static inline int
VMP_CS_FOR_OFFSET(
	vm_map_offset_t fault_phys_offset)
{
	assertf(fault_phys_offset < PAGE_SIZE &&
	    !(fault_phys_offset & FOURK_PAGE_MASK),
	    "offset 0x%llx\n", (uint64_t)fault_phys_offset);
	return 1 << (fault_phys_offset >> FOURK_PAGE_SHIFT);
}
static inline bool
VMP_CS_VALIDATED(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (fault_page_size == PAGE_SIZE) {
		return p->vmp_cs_validated == VMP_CS_ALL_TRUE;
	}
	return p->vmp_cs_validated & VMP_CS_FOR_OFFSET(fault_phys_offset);
}
static inline bool
VMP_CS_TAINTED(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (fault_page_size == PAGE_SIZE) {
		return p->vmp_cs_tainted != VMP_CS_ALL_FALSE;
	}
	return p->vmp_cs_tainted & VMP_CS_FOR_OFFSET(fault_phys_offset);
}
static inline bool
VMP_CS_NX(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (fault_page_size == PAGE_SIZE) {
		return p->vmp_cs_nx != VMP_CS_ALL_FALSE;
	}
	return p->vmp_cs_nx & VMP_CS_FOR_OFFSET(fault_phys_offset);
}
static inline void
VMP_CS_SET_VALIDATED(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset,
	boolean_t value)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (value) {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_validated = VMP_CS_ALL_TRUE;
		}
		p->vmp_cs_validated |= VMP_CS_FOR_OFFSET(fault_phys_offset);
	} else {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_validated = VMP_CS_ALL_FALSE;
		}
		p->vmp_cs_validated &= ~VMP_CS_FOR_OFFSET(fault_phys_offset);
	}
}
static inline void
VMP_CS_SET_TAINTED(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset,
	boolean_t value)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (value) {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_tainted = VMP_CS_ALL_TRUE;
		}
		p->vmp_cs_tainted |= VMP_CS_FOR_OFFSET(fault_phys_offset);
	} else {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_tainted = VMP_CS_ALL_FALSE;
		}
		p->vmp_cs_tainted &= ~VMP_CS_FOR_OFFSET(fault_phys_offset);
	}
}
static inline void
VMP_CS_SET_NX(
	vm_page_t p,
	vm_map_size_t fault_page_size,
	vm_map_offset_t fault_phys_offset,
	boolean_t value)
{
	assertf(fault_page_size <= PAGE_SIZE,
	    "fault_page_size 0x%llx fault_phys_offset 0x%llx\n",
	    (uint64_t)fault_page_size, (uint64_t)fault_phys_offset);
	if (value) {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_nx = VMP_CS_ALL_TRUE;
		}
		p->vmp_cs_nx |= VMP_CS_FOR_OFFSET(fault_phys_offset);
	} else {
		if (fault_page_size == PAGE_SIZE) {
			p->vmp_cs_nx = VMP_CS_ALL_FALSE;
		}
		p->vmp_cs_nx &= ~VMP_CS_FOR_OFFSET(fault_phys_offset);
	}
}


static __inline__ void
vm_page_enqueue_tail(
	vm_page_queue_t         que,
	vm_page_queue_entry_t   elt)
{
	vm_page_queue_entry_t   old_tail;

	old_tail = (vm_page_queue_entry_t)VM_PAGE_UNPACK_PTR(que->prev);
	elt->next = VM_PAGE_PACK_PTR(que);
	elt->prev = que->prev;
	que->prev = old_tail->next = VM_PAGE_PACK_PTR(elt);
}

static __inline__ void
vm_page_remque(
	vm_page_queue_entry_t elt)
{
	vm_page_queue_entry_t next;
	vm_page_queue_entry_t prev;
	vm_page_packed_t      next_pck = elt->next;
	vm_page_packed_t      prev_pck = elt->prev;

	next = (vm_page_queue_entry_t)VM_PAGE_UNPACK_PTR(next_pck);

	
	prev = (vm_page_queue_entry_t)VM_PAGE_UNPACK_PTR(prev_pck);

	next->prev = prev_pck;
	prev->next = next_pck;

	elt->next = 0;
	elt->prev = 0;
}



extern  void    vm_page_assign_special_state(vm_page_t mem, int mode);
extern  void    vm_page_update_special_state(vm_page_t mem);
extern  void    vm_page_add_to_specialq(vm_page_t mem, boolean_t first);
extern  void    vm_page_remove_from_specialq(vm_page_t mem);
extern void             vm_page_bootstrap(
	vm_offset_t     *startp,
	vm_offset_t     *endp);
extern vm_page_t        kdp_vm_page_lookup(
	vm_object_t             object,
	vm_object_offset_t      offset);
extern vm_page_t        vm_page_lookup(
	vm_object_t             object,
	vm_object_offset_t      offset);
extern vm_page_t        vm_page_create_fictitious(void);
extern vm_page_t        vm_page_create_guard(bool canwait);
extern vm_page_t        vm_page_create_private(ppnum_t base_page);
extern bool             vm_page_is_canonical(const struct vm_page *m) __pure2;
extern bool             vm_page_is_fictitious(const struct vm_page *m);
extern bool             vm_page_is_guard(const struct vm_page *m) __pure2;
extern bool             vm_page_is_private(const struct vm_page *m);
extern void             vm_page_make_private(vm_page_t m, ppnum_t base_page);
extern void             vm_page_reset_private(vm_page_t m);
extern bool             vm_pool_low(void);
extern vm_page_t        vm_page_grab(void);
extern vm_page_t        vm_page_grab_options(int flags);
extern vm_page_t        vm_page_grablo(void);
extern void             vm_page_release(
	vm_page_t       page,
	boolean_t       page_queues_locked);
extern boolean_t        vm_page_wait(
	int             interruptible );
extern void             vm_page_init(
	vm_page_t       page,
	ppnum_t         phys_page);
extern void             vm_page_free(
	vm_page_t       page);
extern void             vm_page_free_unlocked(
	vm_page_t       page,
	boolean_t       remove_from_hash);
extern vm_memory_class_t        vm_page_get_memory_class(
	vm_page_t               page);
extern void                     vm_page_steal_free_page(
	vm_page_t               page,
	vm_remove_reason_t      remove_reason);
extern void             vm_page_balance_inactive(
	int             max_to_move);
extern void             vm_page_activate(
	vm_page_t       page);
extern void             vm_page_deactivate(
	vm_page_t       page);
extern void             vm_page_deactivate_internal(
	vm_page_t       page,
	boolean_t       clear_hw_reference);
extern void             vm_page_enqueue_cleaned(vm_page_t page);
extern void             vm_page_lru(
	vm_page_t       page);
extern void             vm_page_speculate(
	vm_page_t       page,
	boolean_t       new);
extern void             vm_page_speculate_ageit(
	struct vm_speculative_age_q *aq);
extern void             vm_page_reactivate_local(uint32_t lid, boolean_t force, boolean_t nolocks);
extern void             vm_page_rename(
	vm_page_t               page,
	vm_object_t             new_object,
	vm_object_offset_t      new_offset);
extern void             vm_page_insert(
	vm_page_t               page,
	vm_object_t             object,
	vm_object_offset_t      offset);
extern void             vm_page_insert_wired(
	vm_page_t               page,
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_tag_t                tag);
extern void             vm_page_insert_internal(
	vm_page_t               page,
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_tag_t                tag,
	boolean_t               queues_lock_held,
	boolean_t               insert_in_hash,
	boolean_t               batch_pmap_op,
	boolean_t               delayed_accounting,
	uint64_t                *delayed_ledger_update);
extern void             vm_page_replace(
	vm_page_t               mem,
	vm_object_t             object,
	vm_object_offset_t      offset);
extern void             vm_page_remove(
	vm_page_t       page,
	boolean_t       remove_from_hash);
extern void             vm_page_zero_fill(
	vm_page_t       page);
extern void             vm_page_part_zero_fill(
	vm_page_t       m,
	vm_offset_t     m_pa,
	vm_size_t       len);
extern void             vm_page_copy(
	vm_page_t       src_page,
	vm_page_t       dest_page);
extern void             vm_page_part_copy(
	vm_page_t       src_m,
	vm_offset_t     src_pa,
	vm_page_t       dst_m,
	vm_offset_t     dst_pa,
	vm_size_t       len);
extern void             vm_page_wire(
	vm_page_t       page,
	vm_tag_t        tag,
	boolean_t       check_memorystatus);
extern void             vm_page_unwire(
	vm_page_t       page,
	boolean_t       queueit);
extern void             vm_set_page_size(void);
extern void             vm_page_validate_cs(
	vm_page_t       page,
	vm_map_size_t   fault_page_size,
	vm_map_offset_t fault_phys_offset);
extern void             vm_page_validate_cs_mapped(
	vm_page_t       page,
	vm_map_size_t   fault_page_size,
	vm_map_offset_t fault_phys_offset,
	const void      *kaddr);
extern void             vm_page_validate_cs_mapped_slow(
	vm_page_t       page,
	const void      *kaddr);
extern void             vm_page_validate_cs_mapped_chunk(
	vm_page_t       page,
	const void      *kaddr,
	vm_offset_t     chunk_offset,
	vm_size_t       chunk_size,
	boolean_t       *validated,
	unsigned        *tainted);
extern void             vm_page_free_prepare_queues(
	vm_page_t       page);
extern void             vm_page_free_prepare_object(
	vm_page_t       page,
	boolean_t       remove_from_hash);
extern wait_result_t    vm_page_sleep(
	vm_object_t        object,
	vm_page_t          m,
	wait_interrupt_t   interruptible,
	lck_sleep_action_t action);
extern void             vm_page_wakeup(
	vm_object_t        object,
	vm_page_t          m);
extern void             vm_page_wakeup_done(
	vm_object_t        object,
	vm_page_t          m);
extern void             page_worker_register_worker(
	event_t            event,
	page_worker_token_t *out_token);
extern boolean_t        vm_page_is_relocatable(
	vm_page_t            m,
	vm_relocate_reason_t reloc_reason);
extern kern_return_t    vm_page_relocate(
	vm_page_t            m1,
	int *                compressed_pages,
	vm_relocate_reason_t reason,
	vm_page_t*           new_page);
extern bool             vm_page_is_restricted(
	vm_page_t mem);
static inline void
vm_free_page_lock(void)
{
	lck_mtx_lock(&vm_page_queue_free_lock);
}

static inline void
vm_free_page_lock_spin(void)
{
	lck_mtx_lock_spin(&vm_page_queue_free_lock);
}

static inline void
vm_free_page_lock_convert(void)
{
	lck_mtx_convert_spin(&vm_page_queue_free_lock);
}

static inline void
vm_free_page_unlock(void)
{
	lck_mtx_unlock(&vm_page_queue_free_lock);
}












kern_return_t
pmap_enter_object_options_check(
	pmap_t           pmap,
	vm_map_address_t virtual_address,
	vm_map_offset_t  fault_phys_offset,
	vm_object_t      object,
	ppnum_t          pn,
	vm_prot_t        protection,
	vm_prot_t        fault_type,
	boolean_t        wired,
	unsigned int     options);
extern kern_return_t pmap_enter_options_check(
	pmap_t           pmap,
	vm_map_address_t virtual_address,
	vm_map_offset_t  fault_phys_offset,
	vm_page_t        page,
	vm_prot_t        protection,
	vm_prot_t        fault_type,
	boolean_t        wired,
	unsigned int     options);
extern kern_return_t pmap_enter_check(
	pmap_t           pmap,
	vm_map_address_t virtual_address,
	vm_page_t        page,
	vm_prot_t        protection,
	vm_prot_t        fault_type,
	boolean_t        wired);
void vm_page_do_delayed_work(vm_object_t object, vm_tag_t tag, struct vm_page_delayed_work *dwp, int dw_count);
extern vm_page_t vm_object_page_grab(vm_object_t);
extern void vm_page_queues_remove(vm_page_t mem, boolean_t remove_from_specialq);
extern void vm_page_remove_internal(vm_page_t page);
extern void vm_page_enqueue_inactive(vm_page_t mem, boolean_t first);
extern void vm_page_enqueue_active(vm_page_t mem, boolean_t first);
extern void vm_page_check_pageable_safe(vm_page_t page);
extern void vm_retire_boot_pages(void);
extern  void            vm_phantom_cache_init(void);
extern  void            vm_phantom_cache_add_ghost(vm_page_t);
extern  vm_ghost_t      vm_phantom_cache_lookup_ghost(vm_page_t, uint32_t);
extern  void            vm_phantom_cache_update(vm_page_t);
extern  boolean_t       vm_phantom_cache_check_pressure(void);
extern  void            vm_phantom_cache_restart_sample(void);
extern mach_port_name_t ipc_port_copyout_send(
	ipc_port_t      sright,
	ipc_space_t     space);
extern mach_port_name_t ipc_port_copyout_send_pinned(
	ipc_port_t      sright,
	ipc_space_t     space);
extern kern_return_t mach_port_deallocate_kernel(
	ipc_space_t             space,
	mach_port_name_t        name,
	natural_t               kotype);
extern task_t port_name_to_task_kernel(
	mach_port_name_t name);
extern task_t port_name_to_task_read(
	mach_port_name_t name);
extern task_t port_name_to_task_name(
	mach_port_name_t name);
extern void ipc_port_release_send(
	ipc_port_t      port);
extern ipc_space_t  get_task_ipcspace(
	task_t t);
extern kern_return_t vm_tests(void);
extern void consider_machine_adjust(void);
extern vm_map_offset_t get_map_min(vm_map_t);
extern vm_map_offset_t get_map_max(vm_map_t);
extern vm_map_size_t get_vmmap_size(vm_map_t);
extern int get_task_page_size(task_t);
extern int get_map_nentries(vm_map_t);
extern vm_map_offset_t vm_map_page_mask(vm_map_t);
extern bool vm_shared_region_is_reslide(struct task *task);
extern memory_object_t swapfile_pager_setup(struct vnode *vp);
extern memory_object_control_t swapfile_pager_control(memory_object_t mem_obj);
extern void vnode_setswapmount(struct vnode *);
extern int64_t vnode_getswappin_avail(struct vnode *);
extern kern_return_t vnode_pager_init(
	memory_object_t,
	memory_object_control_t,
	memory_object_cluster_size_t);
extern  kern_return_t ubc_cs_check_validation_bitmap(
	struct vnode *vp,
	memory_object_offset_t offset,
	int optype);
extern int  ubc_map(
	struct vnode *vp,
	int flags);
extern void ubc_unmap(
	struct vnode *vp);
extern void   device_pager_reference(memory_object_t);
extern void   device_pager_deallocate(memory_object_t);
extern kern_return_t   device_pager_init(memory_object_t,
    memory_object_control_t,
    memory_object_cluster_size_t);
extern  kern_return_t device_pager_terminate(memory_object_t);
extern  kern_return_t   device_pager_data_request(memory_object_t,
    memory_object_offset_t,
    memory_object_cluster_size_t,
    vm_prot_t,
    memory_object_fault_info_t);
extern kern_return_t device_pager_data_return(memory_object_t,
    memory_object_offset_t,
    memory_object_cluster_size_t,
    memory_object_offset_t *,
    int *,
    boolean_t,
    boolean_t,
    int);
extern kern_return_t device_pager_data_initialize(memory_object_t,
    memory_object_offset_t,
    memory_object_cluster_size_t);
extern kern_return_t device_pager_map(memory_object_t, vm_prot_t);
extern kern_return_t device_pager_last_unmap(memory_object_t);
extern kern_return_t pager_map_to_phys_contiguous(
	memory_object_control_t object,
	memory_object_offset_t  offset,
	addr64_t                base_vaddr,
	vm_size_t               size);
extern int macx_swapinfo(
	memory_object_size_t    *total_p,
	memory_object_size_t    *avail_p,
	vm_size_t               *pagesize_p,
	boolean_t               *encrypted_p);
extern int cs_allow_invalid(struct proc *p);
extern int cs_invalid_page(addr64_t vaddr, boolean_t *cs_killed);
extern boolean_t cs_validate_range(struct vnode *vp,
    memory_object_t pager,
    memory_object_offset_t offset,
    const void *data,
    vm_size_t size,
    unsigned *result);
extern void cs_validate_page(
	struct vnode *vp,
	memory_object_t pager,
	memory_object_offset_t offset,
	const void *data,
	int *validated_p,
	int *tainted_p,
	int *nx_p);
extern kern_return_t mach_memory_entry_purgable_control(
	ipc_port_t      entry_port,
	vm_purgable_t   control,
	int             *state);
extern kern_return_t revalidate_text_page(task_t, vm_map_offset_t);
int vm_toggle_entry_reuse(int, int*);
void             do_fastwake_warmup_all(void);
boolean_t vm_purgeable_object_purge_one_unlocked(int force_purge_below_group);
void vm_owned_objects_disown(task_t task);
void vm_object_wired_page_update_ledgers(
	vm_object_t object,
	int64_t wired_delta);
extern void             memory_object_mark_for_realtime(
	memory_object_control_t         control,
	bool                            for_realtime);
extern uint64_t vm_purge_filebacked_pagers(void);
__enum_decl(vm_object_destroy_reason_t, uint8_t, {
	VM_OBJECT_DESTROY_UNKNOWN_REASON = 0,
	VM_OBJECT_DESTROY_RECLAIM = 1,
	VM_OBJECT_DESTROY_UNMOUNT = 2,
	VM_OBJECT_DESTROY_FORCED_UNMOUNT = 3,
	VM_OBJECT_DESTROY_UNGRAFT = 4,
	VM_OBJECT_DESTROY_PAGER = 5,
	VM_OBJECT_DESTROY_MAX = 5,
});
_Static_assert(VM_OBJECT_DESTROY_MAX < 8, "Need to fit in `no_pager_reason`'s number of bits");
void vm_update_darkwake_mode(boolean_t);
__BEGIN_DECLS


extern kern_return_t
vnode_pager_get_object_vnode(
	memory_object_t mem_obj,
	uintptr_t * vnodeaddr,
	uint32_t * vid);
extern memory_object_t shared_region_pager_setup(
	vm_object_t             backing_object,
	vm_object_offset_t      backing_offset,
	struct vm_shared_region_slide_info *slide_info,
	uint64_t                jop_key);
extern uint64_t apple_protect_pager_purge_all(void);
extern uint64_t shared_region_pager_purge_all(void);
extern uint64_t dyld_pager_purge_all(void);
extern void vnode_pager_was_dirtied(
	struct vnode *,
	vm_object_offset_t,
	vm_object_offset_t);
extern uint32_t vnode_trim(struct vnode *, int64_t offset, unsigned long len);
extern vm_object_offset_t vnode_pager_get_filesize(
	struct vnode *);
extern uint32_t vnode_pager_isinuse(
	struct vnode *);
extern boolean_t vnode_pager_isSSD(
	struct vnode *);
extern void vnode_pager_throttle(
	void);
extern uint32_t vnode_pager_return_throttle_io_limit(
	struct vnode *,
	uint32_t     *);
extern kern_return_t vnode_pager_get_name(
	struct vnode    *vp,
	char            *pathname,
	vm_size_t       pathname_len,
	char            *filename,
	vm_size_t       filename_len,
	boolean_t       *truncated_path_p);
extern kern_return_t vnode_pager_get_mtime(
	struct vnode    *vp,
	struct timespec *mtime,
	struct timespec *cs_mtime);
extern kern_return_t vnode_pager_get_cs_blobs(
	struct vnode    *vp,
	void            **blobs);
extern kern_return_t vnode_pager_get_object_size(
	memory_object_t,
	memory_object_offset_t *);
extern void vnode_pager_dirtied(
	memory_object_t,
	vm_object_offset_t,
	vm_object_offset_t);
extern kern_return_t vnode_pager_get_isinuse(
	memory_object_t,
	uint32_t *);
extern kern_return_t vnode_pager_get_isSSD(
	memory_object_t,
	boolean_t *);
extern kern_return_t vnode_pager_get_throttle_io_limit(
	memory_object_t,
	uint32_t *);
extern kern_return_t vnode_pager_get_object_name(
	memory_object_t mem_obj,
	char            *pathname,
	vm_size_t       pathname_len,
	char            *filename,
	vm_size_t       filename_len,
	boolean_t       *truncated_path_p);
extern kern_return_t vnode_pager_get_object_mtime(
	memory_object_t mem_obj,
	struct timespec *mtime,
	struct timespec *cs_mtime);
extern kern_return_t vnode_pager_data_request(
	memory_object_t,
	memory_object_offset_t,
	memory_object_cluster_size_t,
	vm_prot_t,
	memory_object_fault_info_t);
extern kern_return_t vnode_pager_data_return(
	memory_object_t,
	memory_object_offset_t,
	memory_object_cluster_size_t,
	memory_object_offset_t *,
	int *,
	boolean_t,
	boolean_t,
	int);
extern kern_return_t vnode_pager_data_initialize(
	memory_object_t,
	memory_object_offset_t,
	memory_object_cluster_size_t);
extern void vnode_pager_reference(
	memory_object_t         mem_obj);
extern kern_return_t vnode_pager_map(
	memory_object_t         mem_obj,
	vm_prot_t               prot);
extern kern_return_t vnode_pager_last_unmap(
	memory_object_t         mem_obj);
extern kern_return_t vnode_pager_terminate(
	memory_object_t);
extern struct vnode *vnode_pager_lookup_vnode(
	memory_object_t);
extern bool memory_object_is_vnode_pager(memory_object_t mem_obj);
extern struct vm_object *find_vnode_object(struct vm_map_entry *entry);
extern boolean_t is_device_pager_ops(const struct memory_object_pager_ops *pager_ops);
extern void log_stack_execution_failure(addr64_t vaddr, vm_prot_t prot);
extern void log_unnest_badness(
	vm_map_t map,
	vm_map_offset_t start_unnest,
	vm_map_offset_t end_unnest,
	boolean_t is_nested_map,
	vm_map_offset_t lowest_unnestable_addr);
extern vm_object_t vm_named_entry_to_vm_object(
	vm_named_entry_t        named_entry);
extern void vm_named_entry_associate_vm_object(
	vm_named_entry_t        named_entry,
	vm_object_t             object,
	vm_object_offset_t      offset,
	vm_object_size_t        size,
	vm_prot_t               prot);
extern int macx_backing_store_compaction(int flags);
extern unsigned int mach_vm_ctl_page_free_wanted(void);
extern kern_return_t compressor_memory_object_create(
	memory_object_size_t,
	memory_object_t *);
u_int32_t vnode_trim_list(struct vnode *vp, struct trim_list *tl, boolean_t route_only);
extern void vm_start_ecc_thread(void);
extern void vm_ecc_lock(void);
extern void vm_ecc_unlock(void);
void vm_purgeable_nonvolatile_owner_update(task_t       owner,
    int          delta);
void vm_purgeable_volatile_owner_update(task_t          owner,
    int             delta);
decl_lck_mtx_data(extern, vm_purgeable_queue_lock);
kern_return_t vm_purgeable_token_add(purgeable_q_t queue);
void vm_purgeable_token_delete_first(purgeable_q_t queue);
void vm_purgeable_token_delete_last(purgeable_q_t queue);
void vm_purgeable_q_advance_all(void);
void vm_purgeable_object_purge_all(void);
void vm_purgeable_object_add(vm_object_t object, purgeable_q_t queue, int group);
purgeable_q_t vm_purgeable_object_remove(vm_object_t object);
void vm_purgeable_nonvolatile_enqueue(vm_object_t object, task_t task);
void vm_purgeable_nonvolatile_dequeue(vm_object_t object);
void vm_purgeable_accounting(vm_object_t        object,
    vm_purgable_t      old_state);
void vm_object_owner_compressed_update(vm_object_t      object,
    int              delta);
boolean_t vm_purgeable_object_purge_one(int force_purge_below_group, int flags);
void vm_purgeable_stats(vm_purgeable_info_t info, task_t target_task);
uint64_t vm_purgeable_purge_task_owned(task_t task);
__BEGIN_DECLS
#pragma GCC visibility push(hidden)






static inline
kern_return_t
vm_sanitize_get_kr(kern_return_t kr)
{
	if (kr == VM_ERR_RETURN_NOW) {
		return KERN_SUCCESS;
	}
	return kr;
}


__enum_closed_decl(vm_sanitize_caller_id_t, uint32_t, {
	VM_SANITIZE_CALLER_ID_NONE,

	
	VM_SANITIZE_CALLER_ID_MACH_MAKE_MEMORY_ENTRY,
	VM_SANITIZE_CALLER_ID_MACH_MEMORY_ENTRY_PAGE_OP,
	VM_SANITIZE_CALLER_ID_MACH_MEMORY_ENTRY_RANGE_OP,
	VM_SANITIZE_CALLER_ID_MACH_MEMORY_ENTRY_MAP_SIZE,
	VM_SANITIZE_CALLER_ID_MACH_MEMORY_OBJECT_MEMORY_ENTRY,

	
	VM_SANITIZE_CALLER_ID_VM_ALLOCATE_FIXED,
	VM_SANITIZE_CALLER_ID_VM_ALLOCATE_ANYWHERE,
	VM_SANITIZE_CALLER_ID_VM_DEALLOCATE,
	VM_SANITIZE_CALLER_ID_MUNMAP,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_REMAP,
	VM_SANITIZE_CALLER_ID_MMAP,
	VM_SANITIZE_CALLER_ID_MREMAP_ENCRYPTED,
	VM_SANITIZE_CALLER_ID_MAP_WITH_LINKING_NP,
	VM_SANITIZE_CALLER_ID_ENTER_MEM_OBJ,
	VM_SANITIZE_CALLER_ID_ENTER_MEM_OBJ_CTL,

	
	VM_SANITIZE_CALLER_ID_VM_WIRE_USER,
	VM_SANITIZE_CALLER_ID_VM_UNWIRE_USER,
	VM_SANITIZE_CALLER_ID_VM_MAP_WIRE,
	VM_SANITIZE_CALLER_ID_VM_MAP_UNWIRE,
	VM_SANITIZE_CALLER_ID_VSLOCK,
	VM_SANITIZE_CALLER_ID_VSUNLOCK,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_COPY_OVERWRITE,
	VM_SANITIZE_CALLER_ID_VM_MAP_COPYIN,
	VM_SANITIZE_CALLER_ID_VM_MAP_READ_USER,
	VM_SANITIZE_CALLER_ID_VM_MAP_WRITE_USER,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_INHERIT,
	VM_SANITIZE_CALLER_ID_MINHERIT,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_PROTECT,
	VM_SANITIZE_CALLER_ID_MPROTECT,
	VM_SANITIZE_CALLER_ID_USERACC,

	
	VM_SANITIZE_CALLER_ID_VM_BEHAVIOR_SET,
	VM_SANITIZE_CALLER_ID_MADVISE,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_MSYNC,
	VM_SANITIZE_CALLER_ID_MSYNC,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_MACHINE_ATTRIBUTE,

	
	VM_SANITIZE_CALLER_ID_VM_MAP_PAGE_RANGE_INFO,
	VM_SANITIZE_CALLER_ID_VM_MAP_PAGE_RANGE_QUERY,
	VM_SANITIZE_CALLER_ID_MINCORE,

	
	VM_SANITIZE_CALLER_ID_MACH_VM_DEFERRED_RECLAMATION_BUFFER_INIT,
	VM_SANITIZE_CALLER_ID_MACH_VM_RANGE_CREATE,
	VM_SANITIZE_CALLER_ID_SHARED_REGION_MAP_AND_SLIDE_2_NP,

	
	VM_SANITIZE_CALLER_ID_TEST
});
__options_closed_decl(vm_sanitize_flags_t, uint32_t, {
	VM_SANITIZE_FLAGS_NONE                     = 0x00000000,
	VM_SANITIZE_FLAGS_CHECK_ALIGNED_START      = 0x00000001,
	VM_SANITIZE_FLAGS_SIZE_ZERO_SUCCEEDS       = 0x00000002,
	VM_SANITIZE_FLAGS_SIZE_ZERO_FAILS          = 0x00000004,
	VM_SANITIZE_FLAGS_SIZE_ZERO_FALLTHROUGH    = 0x00000008,
	VM_SANITIZE_FLAGS_GET_UNALIGNED_VALUES     = 0x00000010,
	VM_SANITIZE_FLAGS_REALIGN_START            = 0x00000020,
	VM_SANITIZE_FLAGS_CHECK_USER_MEM_MAP_FLAGS = 0x00000040,
	VM_SANITIZE_FLAGS_CANONICALIZE             = 0x00000080,
	VM_SANITIZE_FLAGS_CHECK_ALIGNED_SIZE       = 0x00000100,
	VM_SANITIZE_FLAGS_CHECK_ADDR_RANGE         = 0x00000200,
});
VM_SANITIZE_DECL_CALLER(MACH_MEMORY_ENTRY_PAGE_OP);
VM_SANITIZE_DECL_CALLER(MACH_MEMORY_ENTRY_RANGE_OP);
VM_SANITIZE_DECL_CALLER(MACH_MEMORY_ENTRY_MAP_SIZE);
VM_SANITIZE_DECL_CALLER(MACH_MEMORY_OBJECT_MEMORY_ENTRY);
VM_SANITIZE_DECL_CALLER(VM_ALLOCATE_FIXED);
VM_SANITIZE_DECL_CALLER(VM_ALLOCATE_ANYWHERE);
VM_SANITIZE_DECL_CALLER(VM_DEALLOCATE);
VM_SANITIZE_DECL_CALLER(MUNMAP);
VM_SANITIZE_DECL_CALLER(VM_MAP_REMAP);
VM_SANITIZE_DECL_CALLER(MMAP);
VM_SANITIZE_DECL_CALLER(MREMAP_ENCRYPTED);
VM_SANITIZE_DECL_CALLER(MAP_WITH_LINKING_NP);
VM_SANITIZE_DECL_CALLER(ENTER_MEM_OBJ);
VM_SANITIZE_DECL_CALLER(ENTER_MEM_OBJ_CTL);
VM_SANITIZE_DECL_CALLER(VM_WIRE_USER);
VM_SANITIZE_DECL_CALLER(VM_UNWIRE_USER);
VM_SANITIZE_DECL_CALLER(VM_MAP_WIRE);
VM_SANITIZE_DECL_CALLER(VM_MAP_UNWIRE);
VM_SANITIZE_DECL_CALLER(VSLOCK);
VM_SANITIZE_DECL_CALLER(VSUNLOCK);
VM_SANITIZE_DECL_CALLER(VM_MAP_COPY_OVERWRITE);
VM_SANITIZE_DECL_CALLER(VM_MAP_COPYIN);
VM_SANITIZE_DECL_CALLER(VM_MAP_READ_USER);
VM_SANITIZE_DECL_CALLER(VM_MAP_WRITE_USER);
VM_SANITIZE_DECL_CALLER(MACH_VM_INHERIT);
VM_SANITIZE_DECL_CALLER(VM_INHERIT);
VM_SANITIZE_DECL_CALLER(VM32_INHERIT);
VM_SANITIZE_DECL_CALLER(VM_MAP_INHERIT);
VM_SANITIZE_DECL_CALLER(MINHERIT);
VM_SANITIZE_DECL_CALLER(MACH_VM_PROTECT);
VM_SANITIZE_DECL_CALLER(VM_PROTECT);
VM_SANITIZE_DECL_CALLER(VM32_PROTECT);
VM_SANITIZE_DECL_CALLER(VM_MAP_PROTECT);
VM_SANITIZE_DECL_CALLER(MPROTECT);
VM_SANITIZE_DECL_CALLER(USERACC);
VM_SANITIZE_DECL_CALLER(VM_BEHAVIOR_SET);
VM_SANITIZE_DECL_CALLER(MADVISE);
VM_SANITIZE_DECL_CALLER(VM_MAP_MSYNC);
VM_SANITIZE_DECL_CALLER(MSYNC);
VM_SANITIZE_DECL_CALLER(VM_MAP_MACHINE_ATTRIBUTE);
VM_SANITIZE_DECL_CALLER(VM_MAP_PAGE_RANGE_INFO);
VM_SANITIZE_DECL_CALLER(VM_MAP_PAGE_RANGE_QUERY);
VM_SANITIZE_DECL_CALLER(MINCORE);
VM_SANITIZE_DECL_CALLER(MACH_VM_DEFERRED_RECLAMATION_BUFFER_INIT);
VM_SANITIZE_DECL_CALLER(MACH_VM_RANGE_CREATE);
VM_SANITIZE_DECL_CALLER(SHARED_REGION_MAP_AND_SLIDE_2_NP);
VM_SANITIZE_DECL_CALLER(TEST);
__attribute__((always_inline, warn_unused_result))
vm_addr_struct_t vm_sanitize_wrap_addr(vm_address_t val);
__attribute__((always_inline, warn_unused_result))
vm_size_struct_t vm_sanitize_wrap_size(vm_size_t val);
__attribute__((always_inline, warn_unused_result))
vm32_size_struct_t vm32_sanitize_wrap_size(vm32_size_t val);
__attribute__((always_inline, warn_unused_result))
vm_prot_ut vm_sanitize_wrap_prot(vm_prot_t val);
__attribute__((always_inline, warn_unused_result))
static inline vm_prot_ut *
vm_sanitize_wrap_prot_ref(vm_prot_t *val)
{
	return (vm_prot_ut *)val;
}


__attribute__((always_inline, warn_unused_result))
vm_inherit_ut vm_sanitize_wrap_inherit(vm_inherit_t val);
__attribute__((always_inline, warn_unused_result))
vm_behavior_ut vm_sanitize_wrap_behavior(vm_behavior_t val);
__attribute__((always_inline, warn_unused_result))
vm_addr_struct_t vm_sanitize_expand_addr_to_64(vm32_address_ut val);
__attribute__((always_inline, warn_unused_result))
vm_size_struct_t vm_sanitize_expand_size_to_64(vm32_size_ut val);
__attribute__((always_inline, warn_unused_result))
vm32_address_ut vm_sanitize_trunc_addr_to_32(vm_addr_struct_t val);
__attribute__((always_inline, warn_unused_result))
vm32_size_ut vm_sanitize_trunc_size_to_32(vm_size_struct_t val);
__attribute__((always_inline, warn_unused_result, overloadable))
bool vm_sanitize_add_overflow(
	vm32_address_ut         addr_u,
	vm32_size_ut            size_u,
	vm32_address_ut        *addr_out_u);
__attribute__((always_inline, warn_unused_result, overloadable))
bool vm_sanitize_add_overflow(
	vm_addr_struct_t        addr_u,
	vm_size_struct_t        size_u,
	vm_addr_struct_t       *addr_out_u);
__attribute__((always_inline, warn_unused_result, overloadable))
bool vm_sanitize_add_overflow(
	vm_size_struct_t        size1_u,
	vm_size_struct_t        size2_u,
	vm_size_struct_t       *size_out_u);
__attribute__((always_inline, warn_unused_result))
vm_addr_struct_t vm_sanitize_compute_ut_end(
	vm_addr_struct_t        addr_u,
	vm_size_struct_t        size_u);
__attribute__((always_inline, warn_unused_result))
vm_size_struct_t vm_sanitize_compute_ut_size(
	vm_addr_struct_t        addr_u,
	vm_addr_struct_t        end_u);
__attribute__((always_inline, warn_unused_result))
mach_vm_address_t vm_sanitize_addr(
	vm_map_t                map,
	vm_addr_struct_t        addr_u);
__attribute__((always_inline, warn_unused_result))
mach_vm_offset_t vm_sanitize_offset_in_page(
	vm_map_offset_t         mask,
	vm_addr_struct_t        addr_u);
__attribute__((always_inline, warn_unused_result, overloadable))
static inline mach_vm_offset_t
vm_sanitize_offset_in_page(
	vm_map_t                map,
	vm_addr_struct_t        addr_u)
{
	return vm_sanitize_offset_in_page(vm_map_page_mask(map), addr_u);
}


__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_offset(
	vm_addr_struct_t        offset_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_address_t        addr,
	vm_map_address_t        end,
	vm_map_offset_t        *offset);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_mask(
	vm_addr_struct_t        mask_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_offset_t        *mask);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_object_size(
	vm_size_struct_t        size_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_sanitize_flags_t     flags,
	vm_object_offset_t     *size)
__vm_sanitize_require_size_zero_flag(flags);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_size(
	vm_addr_struct_t        offset_u,
	vm_size_struct_t        size_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_sanitize_flags_t     flags,
	mach_vm_size_t         *size)
__vm_sanitize_require_size_zero_flag(flags);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_addr_size(
	vm_addr_struct_t        addr_u,
	vm_size_struct_t        size_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	mach_vm_offset_t        mask,
	vm_map_t                map_or_null,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *addr,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags);
__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_addr_size(
	vm_addr_struct_t        addr_u,
	vm_size_struct_t        size_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	mach_vm_offset_t        mask,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *addr,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags)
{
	return vm_sanitize_addr_size(addr_u, size_u, vm_sanitize_caller, mask,
	           VM_MAP_NULL, flags, addr, end, size);
}



__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_addr_size(
	vm_addr_struct_t        addr_u,
	vm_size_struct_t        size_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *addr,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags)
{
	mach_vm_offset_t mask = vm_map_page_mask(map);

	return vm_sanitize_addr_size(addr_u, size_u, vm_sanitize_caller, mask,
	           map, flags, addr, end, size);
}


__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_addr_end(
	vm_addr_struct_t        addr_u,
	vm_addr_struct_t        end_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	mach_vm_offset_t        mask,
	vm_map_t                map_or_null,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *start,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags);
__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_addr_end(
	vm_addr_struct_t        addr_u,
	vm_addr_struct_t        end_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	mach_vm_offset_t        mask,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *start,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags)
{
	return vm_sanitize_addr_end(addr_u, end_u, vm_sanitize_caller, mask,
	           VM_MAP_NULL, flags, start, end, size);
}


__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_addr_end(
	vm_addr_struct_t        addr_u,
	vm_addr_struct_t        end_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_sanitize_flags_t     flags,
	vm_map_offset_t        *start,
	vm_map_offset_t        *end,
	vm_map_size_t          *size)
__vm_sanitize_require_size_zero_flag(flags)
{
	mach_vm_offset_t mask = vm_map_page_mask(map);

	return vm_sanitize_addr_end(addr_u, end_u, vm_sanitize_caller, mask,
	           map, flags, start, end, size);
}


__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_prot(
	vm_prot_ut              prot_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_prot_t               extra_mask,
	vm_prot_t              *prot);
__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_prot(
	vm_prot_ut              prot_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_prot_t              *prot)
{
	return vm_sanitize_prot(prot_u, vm_sanitize_caller, map, VM_PROT_NONE, prot);
}


__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_cur_and_max_prots(
	vm_prot_ut              cur_prot_u,
	vm_prot_ut              max_prot_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_prot_t               extra_mask,
	vm_prot_t              *cur_prot,
	vm_prot_t              *max_prot);
__attribute__((always_inline, warn_unused_result, overloadable))
static inline kern_return_t
vm_sanitize_cur_and_max_prots(
	vm_prot_ut              cur_prot_u,
	vm_prot_ut              max_prot_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_map_t                map,
	vm_prot_t              *cur_prot,
	vm_prot_t              *max_prot)
{
	return vm_sanitize_cur_and_max_prots(cur_prot_u, max_prot_u, vm_sanitize_caller, map,
	           VM_PROT_NONE, cur_prot, max_prot);
}


__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_memory_entry_perm(
	vm_prot_ut              perm_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_sanitize_flags_t     flags,
	vm_prot_t               extra_mask,
	vm_prot_t              *perm);
__attribute__((always_inline, warn_unused_result))
vm_prot_t vm_sanitize_prot_bsd(
	vm_prot_ut              prot_u,
	vm_sanitize_caller_t    vm_sanitize_caller);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_inherit(
	vm_inherit_ut           inherit_u,
	vm_sanitize_caller_t    vm_sanitize_caller,
	vm_inherit_t           *inherit);
__attribute__((always_inline, warn_unused_result))
kern_return_t vm_sanitize_behavior(
	vm_behavior_ut           behavior_u,
	vm_sanitize_caller_t    vm_sanitize_caller __unused,
	vm_behavior_t           *behavior);
#pragma mark Methods


__enum_closed_decl(vm_sanitize_method_t, uint64_t, {
	VM_SANITIZE_METHOD_MACH_MAKE_MEMORY_ENTRY = 1,
	VM_SANITIZE_METHOD_MACH_MEMORY_ENTRY_PAGE_OP,
	VM_SANITIZE_METHOD_MACH_MEMORY_ENTRY_RANGE_OP,
	VM_SANITIZE_METHOD_MACH_MEMORY_ENTRY_MAP_SIZE,
	VM_SANITIZE_METHOD_MACH_MEMORY_OBJECT_MEMORY_ENTRY,
	VM_SANITIZE_METHOD_VM_ALLOCATE_FIXED,
	VM_SANITIZE_METHOD_VM_ALLOCATE_ANYWHERE,
	VM_SANITIZE_METHOD_VM_DEALLOCATE,
	VM_SANITIZE_METHOD_MUNMAP,
	VM_SANITIZE_METHOD_VM_MAP_REMAP,
	VM_SANITIZE_METHOD_MMAP,
	VM_SANITIZE_METHOD_MAP_WITH_LINKING_NP,
	VM_SANITIZE_METHOD_ENTER_MEM_OBJ,
	VM_SANITIZE_METHOD_ENTER_MEM_OBJ_CTL,
	VM_SANITIZE_METHOD_MREMAP_ENCRYPTED,
	VM_SANITIZE_METHOD_VM_WIRE_USER,
	VM_SANITIZE_METHOD_VM_UNWIRE_USER,
	VM_SANITIZE_METHOD_VM_MAP_WIRE,
	VM_SANITIZE_METHOD_VM_MAP_UNWIRE,
	VM_SANITIZE_METHOD_VSLOCK,
	VM_SANITIZE_METHOD_VSUNLOCK,
	VM_SANITIZE_METHOD_VM_MAP_COPY_OVERWRITE,
	VM_SANITIZE_METHOD_VM_MAP_COPYIN,
	VM_SANITIZE_METHOD_VM_MAP_READ_USER,
	VM_SANITIZE_METHOD_VM_MAP_WRITE_USER,
	VM_SANITIZE_METHOD_MACH_VM_INHERIT,
	VM_SANITIZE_METHOD_VM_INHERIT,
	VM_SANITIZE_METHOD_VM32_INHERIT,
	VM_SANITIZE_METHOD_VM_MAP_INHERIT,
	VM_SANITIZE_METHOD_MINHERIT,
	VM_SANITIZE_METHOD_MACH_VM_PROTECT,
	VM_SANITIZE_METHOD_VM_PROTECT,
	VM_SANITIZE_METHOD_VM32_PROTECT,
	VM_SANITIZE_METHOD_VM_MAP_PROTECT,
	VM_SANITIZE_METHOD_MPROTECT,
	VM_SANITIZE_METHOD_USERACC,
	VM_SANITIZE_METHOD_VM_MAP_MSYNC,
	VM_SANITIZE_METHOD_MSYNC,
	VM_SANITIZE_METHOD_VM_MAP_MACHINE_ATTRIBUTE,
	VM_SANITIZE_METHOD_MINCORE,
	VM_SANITIZE_METHOD_VM_MAP_PAGE_RANGE_INFO,
	VM_SANITIZE_METHOD_VM_MAP_PAGE_RANGE_QUERY,
	VM_SANITIZE_METHOD_VM_BEHAVIOR_SET,
	VM_SANITIZE_METHOD_MADVISE,
	VM_SANITIZE_METHOD_MACH_VM_DEFERRED_RECLAMATION_BUFFER_INIT,
	VM_SANITIZE_METHOD_MACH_VM_RANGE_CREATE,
	VM_SANITIZE_METHOD_SHARED_REGION_MAP_AND_SLIDE_2_NP,
	VM_SANITIZE_METHOD_TEST,
});
#pragma mark Checkers


__enum_closed_decl(vm_sanitize_checker_t, uint64_t, {
	VM_SANITIZE_CHECKER_ADDR_SIZE = 1,  
	VM_SANITIZE_CHECKER_PROT_CUR_MAX,   
});
__enum_closed_decl(vm_sanitize_checker_count_t, uint64_t, {
	VM_SANITIZE_CHECKER_COUNT_1 = 1,
	VM_SANITIZE_CHECKER_COUNT_2,
	VM_SANITIZE_CHECKER_COUNT_3,
	VM_SANITIZE_CHECKER_COUNT_4,
	VM_SANITIZE_CHECKER_COUNT_5,
});
#pragma mark Telemetry API


void vm_sanitize_send_telemetry(
	vm_sanitize_method_t                method,
	vm_sanitize_checker_t               checker,
	vm_sanitize_checker_count_t         checker_count,
	enum vm_sanitize_subsys_error_codes ktriage_code,
	uint64_t                            arg1,
	uint64_t                            arg2,
	uint64_t                            arg3,
	uint64_t                            arg4,
	uint64_t                            future_ret,
	uint64_t                            past_ret);
__BEGIN_DECLS

extern kern_return_t vm_shared_region_slide_page(
	vm_shared_region_slide_info_t si,
	vm_offset_t                   vaddr,
	mach_vm_offset_t              uservaddr,
	uint32_t                      pageIndex,
	uint64_t                      jop_key);
extern kern_return_t vm_shared_region_enter(
	struct _vm_map          *map,
	struct task             *task,
	boolean_t               is_64bit,
	void                    *fsroot,
	cpu_type_t              cpu,
	cpu_subtype_t           cpu_subtype,
	boolean_t               reslide,
	boolean_t               is_driverkit,
	uint32_t                rsr_version);
extern void vm_shared_region_remove(
	struct task             *task,
	struct vm_shared_region *sr);
extern vm_map_t vm_shared_region_vm_map(
	struct vm_shared_region *shared_region);
extern vm_shared_region_t vm_shared_region_lookup(
	void                    *root_dir,
	cpu_type_t              cpu,
	cpu_subtype_t           cpu_subtype,
	boolean_t               is_64bit,
	int                     target_page_shift,
	boolean_t               reslide,
	boolean_t               is_driverkit,
	uint32_t                rsr_version);
extern kern_return_t vm_shared_region_start_address(
	struct vm_shared_region *shared_region,
	mach_vm_offset_t        *start_address,
	task_t                  task);
extern void vm_shared_region_undo_mappings(
	vm_map_t sr_map,
	mach_vm_offset_t sr_base_address,
	struct _sr_file_mappings *srf_mappings,
	struct _sr_file_mappings *srf_mappings_count,
	unsigned int mappings_count);
__attribute__((noinline))
extern kern_return_t vm_shared_region_map_file(
	struct vm_shared_region *shared_region,
	int                     sr_mappings_count,
	struct _sr_file_mappings *sr_mappings);
extern void *vm_shared_region_root_dir(
	struct vm_shared_region *shared_region);
extern kern_return_t vm_commpage_enter(
	struct _vm_map          *map,
	struct task             *task,
	boolean_t               is64bit);
int vm_shared_region_slide(uint32_t,
    mach_vm_offset_t,
    mach_vm_size_t,
    mach_vm_offset_t,
    mach_vm_size_t,
    mach_vm_offset_t,
    memory_object_control_t,
    vm_prot_t);
extern void vm_shared_region_pivot(void);
extern void vm_shared_region_reference(vm_shared_region_t sr);
extern vm_shared_region_t vm_shared_region_get(
	struct task             *task);
extern vm_shared_region_t vm_shared_region_trim_and_get(
	struct task             *task);
extern void vm_shared_region_deallocate(
	struct vm_shared_region *shared_region);
extern void vm_shared_region_set(
	struct task             *task,
	struct vm_shared_region *new_shared_region);
extern kern_return_t vm_shared_region_sliding_valid(uint32_t slide);
extern void vm_commpage_init(void);
extern void vm_commpage_text_init(void);
extern void vm_shared_region_reslide_stale(boolean_t driverkit);
extern struct vnode * upl_lookup_vnode(upl_t upl);
extern upl_t vector_upl_create(vm_offset_t, uint32_t);
extern upl_size_t vector_upl_get_size(const upl_t);
extern boolean_t vector_upl_is_valid(upl_t);
extern boolean_t vector_upl_set_subupl(upl_t, upl_t, u_int32_t);
extern void vector_upl_set_pagelist(upl_t);
uint32_t vector_upl_max_upls(const upl_t upl);
extern kern_return_t    memory_object_pages_resident(
	memory_object_control_t         control,
	boolean_t                       *               has_pages_resident);
extern kern_return_t    memory_object_signed(
	memory_object_control_t         control,
	boolean_t                       is_signed);
extern boolean_t        memory_object_is_signed(
	memory_object_control_t control);
extern void             memory_object_mark_used(
	memory_object_control_t         control);
extern void             memory_object_mark_unused(
	memory_object_control_t         control,
	boolean_t                       rage);
extern void             memory_object_mark_io_tracking(
	memory_object_control_t         control);
extern void             memory_object_mark_trusted(
	memory_object_control_t         control);
extern memory_object_t vnode_pager_setup(
	struct vnode *, memory_object_t);
extern void vnode_pager_deallocate(
	memory_object_t);
extern void vnode_pager_vrele(
	struct vnode *vp);
extern kern_return_t memory_object_create_named(
	memory_object_t pager,
	memory_object_offset_t  size,
	memory_object_control_t         *control);
extern pager_return_t   vnode_pagein(
	struct vnode *, upl_t,
	upl_offset_t, vm_object_offset_t,
	upl_size_t, int, int *);
extern pager_return_t   vnode_pageout(
	struct vnode *, upl_t,
	upl_offset_t, vm_object_offset_t,
	upl_size_t, int, int *);
__BEGIN_DECLS

extern kern_return_t vm_upl_map
(
	vm_map_t target_task,
	upl_t upl,
	vm_address_t *address
);
extern kern_return_t vm_upl_unmap
(
	vm_map_t target_task,
	upl_t upl
);
extern kern_return_t vm_upl_map_range
(
	vm_map_t target_task,
	upl_t upl,
	vm_offset_t offset,
	vm_size_t size,
	vm_prot_t prot,
	vm_address_t *address
);
extern kern_return_t vm_upl_unmap_range
(
	vm_map_t target_task,
	upl_t upl,
	vm_offset_t offset,
	vm_size_t size
);
extern kern_return_t vm_map_get_upl(
	vm_map_t                target_map,
	vm_map_offset_t         map_offset,
	upl_size_t              *size,
	upl_t                   *upl,
	upl_page_info_array_t   page_info,
	unsigned int            *page_infoCnt,
	upl_control_flags_t     *flags,
	vm_tag_t                tag,
	int                     force_data_sync);
void
WKdm_decompress_4k(const WK_word* src_buf,
    WK_word* dest_buf,
    WK_word* scratch,
    unsigned int bytes);
int
WKdm_compress_4k(const WK_word* src_buf,
    WK_word* dest_buf,
    WK_word* scratch,
    unsigned int limit);
void
WKdm_decompress_16k(WK_word* src_buf,
    WK_word* dest_buf,
    WK_word* scratch,
    unsigned int bytes);
int
WKdm_compress_16k(WK_word* src_buf,
    WK_word* dest_buf,
    WK_word* scratch,
    unsigned int limit);
#error  Wrong architecture - this file is meant for x86_64



#pragma pack(8)         
typedef 
#pragma pack()
extern lowglo lowGlo;
void mach_bridge_register_regwrite_timestamp_callback(mach_bridge_regwrite_timestamp_func_t func);
static inline boolean_t
is_thread_state32(const arm_unified_thread_state_t *its)
{
	return its->ash.flavor == ARM_THREAD_STATE32;
}

static inline boolean_t
is_thread_state64(const arm_unified_thread_state_t *its)
{
	return its->ash.flavor == ARM_THREAD_STATE64;
}

static inline arm_thread_state32_t*
thread_state32(arm_unified_thread_state_t *its)
{
	return &its->ts_32;
}

static inline arm_thread_state64_t*
thread_state64(arm_unified_thread_state_t *its)
{
	return &its->ts_64;
}

static inline const arm_thread_state32_t*
const_thread_state32(const arm_unified_thread_state_t *its)
{
	return &its->ts_32;
}

static inline const arm_thread_state64_t*
const_thread_state64(const arm_unified_thread_state_t *its)
{
	return &its->ts_64;
}






typedef struct arm_saved_state32 arm_saved_state32_t;
extern void ml_panic_on_invalid_old_cpsr(const arm_saved_state_t *) __attribute__((noreturn));
extern void ml_panic_on_invalid_new_cpsr(const arm_saved_state_t *, uint32_t) __attribute__((noreturn));
static inline boolean_t
is_saved_state32(const arm_saved_state_t *iss)
{
	return iss->ash.flavor == ARM_SAVED_STATE32;
}

static inline boolean_t
is_saved_state64(const arm_saved_state_t *iss)
{
	return iss->ash.flavor == ARM_SAVED_STATE64;
}

static inline arm_saved_state32_t*
saved_state32(arm_saved_state_t *iss)
{
	return &iss->ss_32;
}

static inline const arm_saved_state32_t*
const_saved_state32(const arm_saved_state_t *iss)
{
	return &iss->ss_32;
}

static inline arm_saved_state64_t*
saved_state64(arm_saved_state_t *iss)
{
	return &iss->ss_64;
}

static inline const arm_saved_state64_t*
const_saved_state64(const arm_saved_state_t *iss)
{
	return &iss->ss_64;
}

static inline register_t
get_saved_state_pc(const arm_saved_state_t *iss)
{
	return (register_t)(is_saved_state32(iss) ? const_saved_state32(iss)->pc : const_saved_state64(iss)->pc);
}

static inline void
add_saved_state_pc(arm_saved_state_t *iss, int diff)
{
	if (is_saved_state32(iss)) {
		uint64_t pc = saved_state32(iss)->pc + (uint32_t)diff;
		saved_state32(iss)->pc = CAST_ASSERT_SAFE(uint32_t, pc);
	} else {
	}
}

static inline void
add_user_saved_state_pc(arm_saved_state_t *iss, int diff)
{
	if (is_saved_state32(iss)) {
		uint64_t pc = saved_state32(iss)->pc + (uint32_t)diff;
		saved_state32(iss)->pc = CAST_ASSERT_SAFE(uint32_t, pc);
	} else {
	}
}

static inline void
set_saved_state_pc(arm_saved_state_t *iss, register_t pc)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->pc = CAST_ASSERT_SAFE(uint32_t, pc);
	} else {
	}
}

static inline void
set_user_saved_state_pc(arm_saved_state_t *iss, register_t pc)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->pc = CAST_ASSERT_SAFE(uint32_t, pc);
	} else {
	}
}

static inline register_t
get_saved_state_sp(const arm_saved_state_t *iss)
{
	return (register_t)(is_saved_state32(iss) ? const_saved_state32(iss)->sp : const_saved_state64(iss)->sp);
}

static inline void
set_saved_state_sp(arm_saved_state_t *iss, register_t sp)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->sp = CAST_ASSERT_SAFE(uint32_t, sp);
	} else {
		saved_state64(iss)->sp = (uint64_t)sp;
	}
}

static inline register_t
get_saved_state_lr(const arm_saved_state_t *iss)
{
	return (register_t)(is_saved_state32(iss) ? const_saved_state32(iss)->lr : const_saved_state64(iss)->lr);
}

static inline void
set_saved_state_lr(arm_saved_state_t *iss, register_t lr)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->lr = CAST_ASSERT_SAFE(uint32_t, lr);
	} else {
	}
}

static inline void
set_user_saved_state_lr(arm_saved_state_t *iss, register_t lr)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->lr = CAST_ASSERT_SAFE(uint32_t, lr);
	} else {
	}
}

static inline register_t
get_saved_state_fp(const arm_saved_state_t *iss)
{
	return (register_t)(is_saved_state32(iss) ? const_saved_state32(iss)->r[7] : const_saved_state64(iss)->fp);
}

static inline void
set_saved_state_fp(arm_saved_state_t *iss, register_t fp)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->r[7] = CAST_ASSERT_SAFE(uint32_t, fp);
	} else {
		saved_state64(iss)->fp = (uint64_t)fp;
	}
}

static inline int
check_saved_state_reglimit(const arm_saved_state_t *iss, unsigned reg)
{
	return is_saved_state32(iss) ? (reg < ARM_SAVED_STATE32_COUNT) : (reg < ARM_SAVED_STATE64_COUNT);
}

static inline register_t
get_saved_state_reg(const arm_saved_state_t *iss, unsigned reg)
{
	if (!check_saved_state_reglimit(iss, reg)) {
		return 0;
	}

	return (register_t)(is_saved_state32(iss) ? (const_saved_state32(iss)->r[reg]) : (const_saved_state64(iss)->x[reg]));
}

static inline void
set_saved_state_reg(arm_saved_state_t *iss, unsigned reg, register_t value)
{
	if (!check_saved_state_reglimit(iss, reg)) {
		return;
	}

	if (is_saved_state32(iss)) {
		saved_state32(iss)->r[reg] = CAST_ASSERT_SAFE(uint32_t, value);
	} else {
		saved_state64(iss)->x[reg] = (uint64_t)value;
	}
}

static inline void
set_user_saved_state_reg(arm_saved_state_t *iss, unsigned reg, register_t value)
{
	if (!check_saved_state_reglimit(iss, reg)) {
		return;
	}

	if (is_saved_state32(iss)) {
		saved_state32(iss)->r[reg] = CAST_ASSERT_SAFE(uint32_t, value);
	} else {
		saved_state64(iss)->x[reg] = (uint64_t)value;
	}
}


static inline uint32_t
get_saved_state_cpsr(const arm_saved_state_t *iss)
{
	return is_saved_state32(iss) ? const_saved_state32(iss)->cpsr : const_saved_state64(iss)->cpsr;
}

static inline void
mask_saved_state_cpsr(arm_saved_state_t *iss, uint32_t set_bits, uint32_t clear_bits)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->cpsr |= set_bits;
		saved_state32(iss)->cpsr &= ~clear_bits;
	} else {
	}
}

static inline void
mask_user_saved_state_cpsr(arm_saved_state_t *iss, uint32_t set_bits, uint32_t clear_bits)
{
	if (is_saved_state32(iss)) {
		uint32_t new_cpsr = saved_state32(iss)->cpsr;
		new_cpsr |= set_bits;
		new_cpsr &= ~clear_bits;
		if (!PSR_IS_USER(new_cpsr)) {
			ml_panic_on_invalid_new_cpsr(iss, new_cpsr);
		}
		saved_state32(iss)->cpsr = new_cpsr;
	} else {
	}
}


static inline void
set_user_saved_state_cpsr(arm_saved_state_t *iss, uint32_t cpsr)
{
	if (is_saved_state32(iss)) {
		if (!PSR_IS_USER(cpsr)) {
			ml_panic_on_invalid_new_cpsr(iss, cpsr);
		}
		saved_state32(iss)->cpsr = cpsr;
	} else {
	}
}

static inline register_t
get_saved_state_far(const arm_saved_state_t *iss)
{
	return (register_t)(is_saved_state32(iss) ? const_saved_state32(iss)->far : const_saved_state64(iss)->far);
}

static inline void
set_saved_state_far(arm_saved_state_t *iss, register_t far)
{
	if (is_saved_state32(iss)) {
		saved_state32(iss)->far = CAST_ASSERT_SAFE(uint32_t, far);
	} else {
		saved_state64(iss)->far = (uint64_t)far;
	}
}

static inline uint64_t
get_saved_state_esr(const arm_saved_state_t *iss)
{
	return is_saved_state32(iss) ? const_saved_state32(iss)->esr : const_saved_state64(iss)->esr;
}

static inline void
set_saved_state_esr(arm_saved_state_t *iss, uint64_t esr)
{
	if (is_saved_state32(iss)) {
		assert(esr < (uint64_t) (uint32_t) -1);
		saved_state32(iss)->esr = (uint32_t) esr;
	} else {
		saved_state64(iss)->esr = esr;
	}
}

extern void panic_unimplemented(void);
static inline int
get_saved_state_svc_number(const arm_saved_state_t *iss)
{
	return is_saved_state32(iss) ? (int)const_saved_state32(iss)->r[12] : (int)const_saved_state64(iss)->x[ARM64_SYSCALL_CODE_REG_NUM]; 
}

typedef _STRUCT_ARM_LEGACY_DEBUG_STATE arm_legacy_debug_state_t;
static inline boolean_t
is_neon_saved_state32(const arm_neon_saved_state_t *state)
{
	return state->nsh.flavor == ARM_NEON_SAVED_STATE32;
}

static inline boolean_t
is_neon_saved_state64(const arm_neon_saved_state_t *state)
{
	return state->nsh.flavor == ARM_NEON_SAVED_STATE64;
}

static inline arm_neon_saved_state32_t *
neon_state32(arm_neon_saved_state_t *state)
{
	return &state->ns_32;
}

static inline arm_neon_saved_state64_t *
neon_state64(arm_neon_saved_state_t *state)
{
	return &state->ns_64;
}






typedef struct arm_context arm_context_t;
extern void saved_state_to_thread_state64(const arm_saved_state_t*, arm_thread_state64_t*);
extern void thread_state64_to_saved_state(const arm_thread_state64_t*, arm_saved_state_t*);
extern void saved_state_to_thread_state32(const arm_saved_state_t*, arm_thread_state32_t*);
extern void thread_state32_to_saved_state(const arm_thread_state32_t*, arm_saved_state_t*);
extern int getclock(int, struct timespec *);
#pragma once



kern_return_t buffer_stage_initialize(struct kdp_output_stage *stage, size_t buffer_size);
kern_return_t lz4_stage_initialize(struct kdp_output_stage *stage);
void lz4_stage_monitor_availability(void);
kern_return_t zlib_stage_initialize(struct kdp_output_stage *stage);
kern_return_t aea_stage_initialize(struct kdp_output_stage *stage, const void *recipient_public_key, size_t recipient_public_key_size);
void aea_stage_monitor_availability(void);
bool aea_stage_is_available(void);
kern_return_t disk_stage_initialize(struct kdp_output_stage *stage);
kern_return_t disk_stage_write(struct kdp_output_stage *stage, uint64_t offset, uint64_t length, const void *data);
kern_return_t disk_stage_read(struct kdp_output_stage *stage, uint64_t offset, uint64_t length, void *data);
kern_return_t net_stage_initialize(struct kdp_output_stage *stage);
kern_return_t progress_notify_stage_initialize(struct kdp_output_stage *stage);
kern_return_t memory_backing_aware_buffer_stage_initialize(struct kdp_output_stage *stage);
kern_return_t shmem_stage_initialize(struct kdp_output_stage *stage);
void kdp_jtag_coredump_init(void);
void vmx_resume(boolean_t is_wake_from_hibernate);
void vmx_suspend(void);
boolean_t vmx_hv_support(void);
extern int __vmxoff(void);
extern int __vmxon(addr64_t v);
void *vmx_pcalloc(void);
addr64_t vmx_paddr(void *);
void vmx_pfree(void *);
_Static_assert(PTE_PER_PVE == 2, "PTE_PER_PVE is not 2");
extern void flush_mmu_tlb_region(vm_offset_t va, unsigned length);
extern uint64_t get_mmu_control(void);
extern uint64_t get_aux_control(void);
extern void set_aux_control(uint64_t);
extern void set_mmu_ttb(uint64_t);
extern void set_mmu_ttb_alternate(uint64_t);
extern uint64_t get_tcr(void);
extern void set_tcr(uint64_t);
extern uint64_t pmap_get_arm64_prot(pmap_t, vm_offset_t);
extern pmap_paddr_t get_mmu_ttb(void);
extern pmap_paddr_t mmu_kvtop(vm_offset_t va);
extern pmap_paddr_t mmu_kvtop_wpreflight(vm_offset_t va);
extern pmap_paddr_t mmu_uvtop(vm_offset_t va);
extern void pmap_clear_user_ttb(void);
extern void pmap_bootstrap(vm_offset_t);
extern vm_map_address_t pmap_ptov(pmap_t, ppnum_t);
extern pmap_paddr_t pmap_find_pa(pmap_t map, addr64_t va);
extern pmap_paddr_t pmap_find_pa_nofault(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys_nofault(pmap_t map, addr64_t va);
extern void pmap_switch_user(thread_t th, vm_map_t map);
extern void pmap_set_pmap(pmap_t pmap, thread_t thread);
extern  void pmap_gc(void);
extern pmap_paddr_t kvtophys(vm_offset_t va);
extern pmap_paddr_t kvtophys_nofail(vm_offset_t va);
extern vm_map_address_t phystokv(pmap_paddr_t pa);
extern vm_map_address_t phystokv_range(pmap_paddr_t pa, vm_size_t *max_len);
extern vm_map_address_t pmap_map(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot, unsigned int flags);
extern vm_map_address_t pmap_map_high_window_bd( vm_offset_t pa, vm_size_t len, vm_prot_t prot);
extern kern_return_t pmap_map_block(pmap_t pmap, addr64_t va, ppnum_t pa, uint32_t size, vm_prot_t prot, int attr, unsigned int flags);
extern kern_return_t pmap_map_block_addr(pmap_t pmap, addr64_t va, pmap_paddr_t pa, uint32_t size, vm_prot_t prot, int attr, unsigned int flags);
extern void pmap_map_globals(void);
extern vm_map_address_t pmap_map_bd_with_options(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot, int32_t options);
extern vm_map_address_t pmap_map_bd(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot);
extern void pmap_init_pte_page(pmap_t, pt_entry_t *, vm_offset_t, unsigned int ttlevel, boolean_t alloc_ptd);
extern boolean_t pmap_valid_address(pmap_paddr_t addr);
extern void pmap_disable_NX(pmap_t pmap);
extern void pmap_set_nested(pmap_t pmap);
extern void pmap_create_commpages(vm_map_address_t *kernel_data_addr, vm_map_address_t *kernel_text_addr,
    vm_map_address_t *kernel_ro_data_addr, vm_map_address_t *user_text_addr);
extern void pmap_insert_commpage(pmap_t pmap);
extern vm_offset_t pmap_cpu_windows_copy_addr(int cpu_num, unsigned int index);
extern unsigned int pmap_map_cpu_windows_copy(ppnum_t pn, vm_prot_t prot, unsigned int wimg_bits);
extern void pmap_unmap_cpu_windows_copy(unsigned int index);
extern vm_offset_t pmap_ro_zone_align(vm_offset_t);
extern void pmap_ro_zone_memcpy(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    vm_offset_t new_data, vm_size_t new_data_size);
extern uint64_t pmap_ro_zone_atomic_op(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    uint32_t op, uint64_t value);
extern void pmap_ro_zone_bzero(zone_id_t zid, vm_offset_t va, vm_offset_t offset, vm_size_t size);
extern boolean_t pmap_valid_page(ppnum_t pn);
extern boolean_t pmap_bootloader_page(ppnum_t pn);
extern boolean_t pmap_is_empty(pmap_t pmap, vm_map_offset_t start, vm_map_offset_t end);
extern vm_map_offset_t pmap_max_offset(boolean_t is64, unsigned int option);
extern vm_map_offset_t pmap_max_64bit_offset(unsigned int option);
extern vm_map_offset_t pmap_max_32bit_offset(unsigned int option);
boolean_t pmap_virtual_region(unsigned int region_select, vm_map_offset_t *startp, vm_map_size_t *size);
boolean_t pmap_enforces_execute_only(pmap_t pmap);
void pmap_abandon_measurement(void);
extern void pmap_cpu_data_init(void);
extern pmap_cpu_data_t *pmap_get_cpu_data(void);
extern pmap_cpu_data_t *pmap_get_remote_cpu_data(unsigned int cpu);
static inline bool
_pmap_pending_preemption_real(void)
{
	return !!(*((volatile ast_t*)ast_pending()) & AST_URGENT);
}



extern void pmap_nop(pmap_t);
extern void CleanPoC_DcacheRegion_Force_nopreempt_nohid_nobarrier(vm_offset_t va, size_t length);
extern bool pmap_is_page_free(pmap_paddr_t paddr);
static inline bool
pa_valid(pmap_paddr_t pa)
{
	return (pa >= vm_first_phys) && (pa < vm_last_phys);
}




static inline unsigned int
pa_index(pmap_paddr_t pa)
{
	return (unsigned int)atop(pa - vm_first_phys);
}


static inline pmap_paddr_t
pai_to_pa(unsigned int pai)
{
	return ptoa((pmap_paddr_t)pai) + vm_first_phys;
}


extern uintptr_t *pv_head_table;
static inline uintptr_t
pai_to_pvh(unsigned int pai)
{
	return pv_head_table[pai];
}
































static inline void
pvh_assert_locked(__assert_only unsigned int index)
{
	assertf(os_atomic_load(&pv_head_table[index], relaxed) & PVH_LOCK_FLAGS,
	    "%s: PVH %p (=%p) for pai 0x%x not locked or in sleep mode", __func__,
	    &pv_head_table[index], (void*)(os_atomic_load(&pv_head_table[index], relaxed)), index);
}


static inline uint32_t*
pvh_lock_word(unsigned int index)
{
	return (uint32_t*)(&pv_head_table[index]) + PVH_LOCK_WORD;
}




static inline locked_pvh_t __attribute__((warn_unused_result))
pvh_lock(unsigned int index)
{
	extern unsigned int not_in_kdp;
	const bool was_preemptible = preemption_enabled();
	assert(was_preemptible || (startup_phase < STARTUP_SUB_EARLY_BOOT) ||
	    PMAP_IS_HIBERNATING() || !not_in_kdp);

	bool (^check_preemption)(void) = ^bool (void) {
		return was_preemptible && pmap_pending_preemption();
	};

	hw_lock_status_t ret;
	locked_pvh_t locked_pvh = {.pvh = 0, .pai = index};
	do {
		ret = hw_lock_bit_to_b(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET,
		    &hw_lock_bit_policy, check_preemption, &pmap_lck_grp);

		if (ret == HW_LOCK_ACQUIRED) {
			locked_pvh.pvh = os_atomic_load(&pv_head_table[index], relaxed);
			if (__improbable(locked_pvh.pvh & PVH_FLAG_SLEEP)) {
				wait_result_t wres;
				wres = assert_wait(&pv_head_table[index], THREAD_UNINT);
				hw_unlock_bit(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET);
				assertf(wres == THREAD_WAITING, "%s: unexpected wait result %d", __func__, wres);
				thread_block(THREAD_CONTINUE_NULL);
				ret = HW_LOCK_CONTENDED;
			}
		}
	} while (ret != HW_LOCK_ACQUIRED);

	return locked_pvh;
}


static inline locked_pvh_t __attribute__((warn_unused_result))
pvh_lock_nopreempt(unsigned int index)
{
	if (__improbable(preemption_enabled())) {
		return pvh_lock(index);
	}
	hw_lock_bit(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET, &pmap_lck_grp);
	const locked_pvh_t locked_pvh = {.pvh = os_atomic_load(&pv_head_table[index], relaxed), .pai = index};

	if (__improbable(locked_pvh.pvh & PVH_FLAG_SLEEP)) {
		panic("%s invoked on sleep-mode PVH %p for pai 0x%x", __func__, &pv_head_table[index], index);
	}

	return locked_pvh;
}


static inline locked_pvh_t __attribute__((warn_unused_result))
pvh_try_lock(unsigned int index)
{
	locked_pvh_t locked_pvh = {.pvh = 0, .pai = index};
	bool locked = hw_lock_bit_try(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET, &pmap_lck_grp);

	if (locked) {
		locked_pvh.pvh = os_atomic_load(&pv_head_table[index], relaxed);
		assert(locked_pvh.pvh != 0);
		if (__improbable(locked_pvh.pvh & PVH_FLAG_SLEEP)) {
			hw_unlock_bit(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET);
			locked_pvh.pvh = 0;
		}
	}

	return locked_pvh;
}


static inline bool
pvh_try_lock_success(const locked_pvh_t *locked_pvh)
{
	assert(locked_pvh != NULL);
	return locked_pvh->pvh != 0;
}


static inline void
pvh_lock_enter_sleep_mode(locked_pvh_t *locked_pvh)
{
	assert(locked_pvh != NULL);
	assert(locked_pvh->pvh != 0);
	unsigned int index = locked_pvh->pai;
	pvh_assert_locked(index);
	const uintptr_t old_pvh = os_atomic_load(&pv_head_table[index], relaxed);
	if (!(old_pvh & PVH_FLAG_SLEEP)) {
		assert(old_pvh & PVH_FLAG_LOCK);
		os_atomic_store(&pv_head_table[index], old_pvh | PVH_FLAG_SLEEP, relaxed);
		
		locked_pvh->pri_token = thread_priority_floor_start();
		hw_unlock_bit(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET);
	}

	
	assert(preemption_enabled() || PMAP_IS_HIBERNATING());
}


static inline bool
pvh_test_type(uintptr_t pvh, uintptr_t type)
{
	return (pvh & PVH_TYPE_MASK) == type;
}


static inline void
pvh_unlock(locked_pvh_t *locked_pvh)
{
	assert(locked_pvh != NULL);
	assert(locked_pvh->pvh != 0);
	unsigned int index = locked_pvh->pai;
	pvh_assert_locked(index);
	const uintptr_t old_pvh = os_atomic_load(&pv_head_table[index], relaxed);
	bool pri_floor_end = false;

	if (__improbable(old_pvh & PVH_FLAG_SLEEP)) {
		pri_floor_end = true;
		const bool was_preemptible = preemption_enabled();
		bool (^check_preemption)(void) = ^bool (void) {
			return was_preemptible && pmap_pending_preemption();
		};

		hw_lock_status_t ret;
		do {
			ret = hw_lock_bit_to_b(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET,
			    &hw_lock_bit_policy, check_preemption, &pmap_lck_grp);
		} while (ret != HW_LOCK_ACQUIRED);

		os_atomic_store(&pv_head_table[index],
		    (locked_pvh->pvh & ~PVH_FLAG_SLEEP) | PVH_FLAG_LOCK, relaxed);
		thread_wakeup(&pv_head_table[index]);
	} else if ((old_pvh & ~PVH_LOCK_FLAGS) != (locked_pvh->pvh & ~PVH_LOCK_FLAGS)) {
		os_atomic_store(&pv_head_table[index],
		    (locked_pvh->pvh & ~PVH_FLAG_SLEEP) | PVH_FLAG_LOCK, relaxed);
	}
	hw_unlock_bit(pvh_lock_word(index), PVH_LOCK_BIT_OFFSET);

	if (__improbable(pri_floor_end)) {
		thread_priority_floor_end(&locked_pvh->pri_token);
	}

	locked_pvh->pvh = 0;
}


static inline pt_entry_t*
pvh_ptep(uintptr_t pvh)
{
	assert(pvh_test_type(pvh, PVH_TYPE_PTEP));
	return (pt_entry_t *)((pvh & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline pv_entry_t*
pvh_pve_list(uintptr_t pvh)
{
	assert(pvh_test_type(pvh, PVH_TYPE_PVEP));
	return (pv_entry_t *)((pvh & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline uintptr_t
pvh_get_flags(uintptr_t pvh)
{
	return pvh & PVH_MUTABLE_FLAGS;
}


static inline void
pvh_set_flags(locked_pvh_t *locked_pvh, uintptr_t flags)
{
	locked_pvh->pvh = (locked_pvh->pvh & ~PVH_MUTABLE_FLAGS) | (flags & PVH_MUTABLE_FLAGS);
}


static inline void
pvh_update_head(locked_pvh_t *locked_pvh, void *pvep, unsigned int type)
{
	assert(!((uintptr_t)pvep & PVH_TYPE_MASK));
	const uintptr_t pvh_flags = locked_pvh->pvh & PVH_HIGH_FLAGS;
	locked_pvh->pvh = ((uintptr_t)pvep & ~PVH_HIGH_FLAGS) | type | pvh_flags;
}


static inline bool
pvh_ptep_is_iommu(const pt_entry_t *ptep)
{
	return (uintptr_t)ptep & PVH_FLAG_IOMMU;
}


static inline const pt_entry_t*
pvh_strip_ptep(const pt_entry_t *ptep)
{
	const uintptr_t pte_va = (uintptr_t)ptep;
	return (const pt_entry_t*)((pte_va & ~PVH_FLAG_IOMMU) | PVH_FLAG_IOMMU_TABLE);
}






static inline void
pve_set_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] | PVE_PTEP_ALTACCT);
}


static inline void
pve_set_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] | PVE_PTEP_INTERNAL);
}


static inline void
pve_clr_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_ALTACCT);
}


static inline void
pve_clr_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_INTERNAL);
}


static inline bool
pve_get_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (uintptr_t)pvep->pve_ptep[idx] & PVE_PTEP_ALTACCT;
}


static inline bool
pve_get_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (uintptr_t)pvep->pve_ptep[idx] & PVE_PTEP_INTERNAL;
}


static inline pv_entry_t *
pve_next(pv_entry_t *pvep)
{
	return pvep->pve_next;
}


static inline pv_entry_t **
pve_next_ptr(pv_entry_t *pvep)
{
	return &pvep->pve_next;
}


static inline pt_entry_t *
pve_get_ptep(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_FLAGS);
}


static inline void
pve_set_ptep(pv_entry_t *pvep, unsigned idx, pt_entry_t *ptep_new)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = ptep_new;
}


static inline void
pve_init(pv_entry_t *pvep)
{
	pvep->pve_next = PV_ENTRY_NULL;
	for (int i = 0; i < PTE_PER_PVE; i++) {
		pvep->pve_ptep[i] = PT_ENTRY_NULL;
	}
}


static inline int
pve_find_ptep_index(pv_entry_t *pvep, pt_entry_t *ptep)
{
	for (unsigned int i = 0; i < PTE_PER_PVE; i++) {
		if (pve_get_ptep(pvep, i) == ptep) {
			return (int)i;
		}
	}

	return -1;
}


static inline bool
pve_is_empty(pv_entry_t *pvep)
{
	for (unsigned int i = 0; i < PTE_PER_PVE; i++) {
		if (pve_get_ptep(pvep, i) != PT_ENTRY_NULL) {
			return false;
		}
	}

	return true;
}


static inline void
pve_add(locked_pvh_t *locked_pvh, pv_entry_t *pvep)
{
	assert(pvh_test_type(locked_pvh->pvh, PVH_TYPE_PVEP));

	pvep->pve_next = pvh_pve_list(locked_pvh->pvh);
	pvh_update_head(locked_pvh, pvep, PVH_TYPE_PVEP);
}


static inline void
pve_remove(locked_pvh_t *locked_pvh, pv_entry_t **pvepp, pv_entry_t *pvep)
{
	assert(pvh_test_type(locked_pvh->pvh, PVH_TYPE_PVEP));

	if (pvepp == NULL) {
		assertf(pvh_pve_list(locked_pvh->pvh) == pvep, "%s: pvh %p != pvep %p",
		    __func__, (void*)locked_pvh->pvh, pvep);
		if (pve_next(pvep) == PV_ENTRY_NULL) {
			
			pvh_update_head(locked_pvh, PV_ENTRY_NULL, PVH_TYPE_NULL);
		} else {
			
			pvh_update_head(locked_pvh, pve_next(pvep), PVH_TYPE_PVEP);
		}
	} else {
		
		*pvepp = pve_next(pvep);
	}
}




typedef 


typedef uint64_t iommu_instance_t;
_Static_assert(sizeof(pmap_sptm_percpu_data_t) <= PMAP_SPTM_PCPU_ALIGN,
    "sizeof(pmap_sptm_percpu_data_t) is larger than PMAP_SPTM_PCPU_ALIGN");
PERCPU_DECL(pmap_sptm_percpu_data_t, pmap_sptm_percpu);
static inline pt_desc_t*
pvh_ptd(uintptr_t pvh)
{
	return (pt_desc_t *)((pvh & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline pt_desc_t *
ptep_get_ptd(const pt_entry_t *ptep)
{
	assert(ptep != NULL);

	const vm_offset_t pt_base_va = (vm_offset_t)ptep;
	uintptr_t pvh = pai_to_pvh(pa_index(kvtophys(pt_base_va)));

	if (__improbable(!pvh_test_type(pvh, PVH_TYPE_PTDP))) {
		panic("%s: invalid PV head 0x%llx for PTE %p", __func__, (uint64_t)pvh, ptep);
	}

	return pvh_ptd(pvh);
}


static inline struct pmap *
ptep_get_pmap(const pt_entry_t *ptep)
{
	return ptep_get_ptd(ptep)->pmap;
}



static inline pt_desc_t *
tte_get_ptd(const tt_entry_t tte)
{
	const vm_offset_t pt_base_va = (vm_offset_t)(tte & ~((tt_entry_t)PAGE_MASK));
	uintptr_t pvh = pai_to_pvh(pa_index(pt_base_va));

	if (__improbable(!pvh_test_type(pvh, PVH_TYPE_PTDP))) {
		panic("%s: invalid PV head 0x%llx for TTE 0x%llx", __func__, (uint64_t)pvh, (uint64_t)tte);
	}

	return pvh_ptd(pvh);
}


static inline ptd_info_t *
ptd_get_info(pt_desc_t *ptd)
{
	assert(ptd != NULL);
	return ptd->ptd_info;
}


static inline ptd_info_t *
ptep_get_info(const pt_entry_t *ptep)
{
	return ptd_get_info(ptep_get_ptd(ptep));
}


static inline vm_map_address_t
ptd_get_va(const pt_desc_t *ptdp, const pt_entry_t *ptep)
{
	const pt_attr_t * const pt_attr = pmap_get_pt_attr(ptdp->pmap);

	vm_map_address_t va = ptdp->va;

	const uint64_t pmap_page_shift = pt_attr_leaf_shift(pmap_get_pt_attr(ptdp->pmap));
	const vm_offset_t ptep_page = (vm_offset_t)ptep >> pmap_page_shift;

	
	const unsigned int ttep_index = ptep_page & ((1U << (PAGE_SHIFT - pmap_page_shift)) - 1);
	va += ttep_index * pt_attr_twig_size(pt_attr);

	
	const vm_offset_t ptep_index = ((vm_offset_t)ptep & pt_attr_leaf_offmask(pt_attr)) / sizeof(*ptep);
	va += (ptep_index << pt_attr_leaf_shift(pt_attr));

	return va;
}


static inline vm_map_address_t
ptep_get_va(const pt_entry_t *ptep)
{
	return ptd_get_va(ptep_get_ptd(ptep), ptep);
}




typedef uint16_t pp_attr_t;
static inline void
ppattr_set_bits(unsigned int pai, pp_attr_t bits)
{
	volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	os_atomic_or(ppattr, bits, relaxed);
}


static inline void
ppattr_clear_bits(unsigned int pai, pp_attr_t bits)
{
	volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	os_atomic_andnot(ppattr, bits, relaxed);
}


static inline void
ppattr_modify_bits(unsigned int pai, pp_attr_t bits_to_clear, pp_attr_t bits_to_set)
{
	assert((bits_to_set & bits_to_clear) == 0);
	pp_attr_t prev_ppattr, new_ppattr;
	os_atomic_rmw_loop(&pp_attr_table[pai], prev_ppattr, new_ppattr, relaxed, {
		new_ppattr = (prev_ppattr & ~bits_to_clear) | bits_to_set;
	});
}


static inline bool
ppattr_test_bits(unsigned int pai, pp_attr_t bits)
{
	const volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	return (*ppattr & bits) == bits;
}


static inline void
ppattr_pa_set_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	if (pa_valid(pa)) {
		ppattr_set_bits(pa_index(pa), bits);
	}
}


static inline void
ppattr_pa_clear_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	if (pa_valid(pa)) {
		ppattr_clear_bits(pa_index(pa), bits);
	}
}


static inline bool
ppattr_pa_test_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	return pa_valid(pa) ? ppattr_test_bits(pa_index(pa), bits) : false;
}


static inline void
ppattr_pa_set_modify(pmap_paddr_t pa)
{
	ppattr_pa_set_bits(pa, PP_ATTR_MODIFIED);
}


static inline void
ppattr_pa_clear_modify(pmap_paddr_t pa)
{
	ppattr_pa_clear_bits(pa, PP_ATTR_MODIFIED);
}


static inline void
ppattr_pa_set_reference(pmap_paddr_t pa)
{
	ppattr_pa_set_bits(pa, PP_ATTR_REFERENCED);
}


static inline void
ppattr_pa_clear_reference(pmap_paddr_t pa)
{
	ppattr_pa_clear_bits(pa, PP_ATTR_REFERENCED);
}


static inline void
ppattr_set_internal(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_INTERNAL);
}


static inline void
ppattr_clear_internal(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_INTERNAL);
}


static inline bool
ppattr_test_internal(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_INTERNAL);
}


static inline void
ppattr_set_reusable(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_REUSABLE);
}


static inline void
ppattr_clear_reusable(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_REUSABLE);
}


static inline bool
ppattr_test_reusable(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_REUSABLE);
}


static inline void
ppattr_set_altacct(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_ALTACCT);
}


static inline void
ppattr_clear_altacct(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_ALTACCT);
}


static inline bool
ppattr_is_altacct(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_ALTACCT);
}


static inline bool
ppattr_is_internal(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_INTERNAL);
}


static inline bool
ppattr_pve_is_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	return (pvep == PV_ENTRY_NULL) ? ppattr_is_altacct(pai) : pve_get_altacct(pvep, idx);
}


static inline bool
ppattr_pve_is_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	return (pvep == PV_ENTRY_NULL) ? ppattr_is_internal(pai) : pve_get_internal(pvep, idx);
}


static inline void
ppattr_pve_set_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_set_altacct(pai);
	} else {
		pve_set_altacct(pvep, idx);
	}
}


static inline void
ppattr_pve_set_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_set_internal(pai);
	} else {
		pve_set_internal(pvep, idx);
	}
}


static inline void
ppattr_pve_clr_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_clear_altacct(pai);
	} else {
		pve_clr_altacct(pvep, idx);
	}
}


static inline void
ppattr_pve_clr_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_clear_internal(pai);
	} else {
		pve_clr_internal(pvep, idx);
	}
}


static inline void
ppattr_set_reffault(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_REFFAULT);
}


static inline void
ppattr_clear_reffault(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_REFFAULT);
}


static inline bool
ppattr_test_reffault(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_REFFAULT);
}


static inline void
ppattr_set_modfault(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_MODFAULT);
}


static inline void
ppattr_clear_modfault(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_MODFAULT);
}


static inline bool
ppattr_test_modfault(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_MODFAULT);
}




static inline void
pmap_retype_epoch_enter(void)
{
	mp_disable_preemption();
	pmap_retype_epoch_t *retype_epoch = &PERCPU_GET(pmap_sptm_percpu)->retype_epoch;
	assert(!preemption_enabled());

	
	assert(retype_epoch->local_seq == 0);
	retype_epoch->local_seq = ++retype_epoch->next_seq;
	
	assert(retype_epoch->local_seq != 0);

	
	os_atomic_thread_fence(seq_cst);
}


static inline void
pmap_retype_epoch_exit(void)
{
	pmap_retype_epoch_t *retype_epoch = &PERCPU_GET(pmap_sptm_percpu)->retype_epoch;
	assert(!preemption_enabled());
	assert(retype_epoch->local_seq == retype_epoch->next_seq);

	
	os_atomic_store(&retype_epoch->local_seq, 0, release);
	mp_enable_preemption();
}


static inline bool
pmap_in_epoch(void)
{
	return !preemption_enabled() && (PERCPU_GET(pmap_sptm_percpu)->retype_epoch.local_seq != 0);
}


static inline void
pmap_retype_epoch_prepare_drain(void)
{
	mp_disable_preemption();
	pmap_retype_epoch_t *retype_epoch = &PERCPU_GET(pmap_sptm_percpu)->retype_epoch;
	assert(retype_epoch->flags == 0);
	unsigned int i = 0;
	uint8_t flags = PMAP_RETYPE_EPOCH_PREPARED;

	
	percpu_foreach(pmap_pcpu, pmap_sptm_percpu) {
		const uint64_t remote_epoch =
		    os_atomic_load(&pmap_pcpu->retype_epoch.local_seq, relaxed);
		retype_epoch->remote_seq[i] = remote_epoch;

		
		if (remote_epoch != 0) {
			flags |= PMAP_RETYPE_EPOCH_DRAIN_REQUIRED;
		}
		++i;
	}
	retype_epoch->flags = flags;

	
	os_atomic_thread_fence(acquire);
}


static inline void
pmap_retype_epoch_drain(void)
{
	assert(!preemption_enabled());
	pmap_retype_epoch_t *retype_epoch = &PERCPU_GET(pmap_sptm_percpu)->retype_epoch;
	const uint8_t flags = retype_epoch->flags;
	assert(flags & PMAP_RETYPE_EPOCH_PREPARED);
	retype_epoch->flags = 0;
	if (!(flags & PMAP_RETYPE_EPOCH_DRAIN_REQUIRED)) {
		mp_enable_preemption();
		return;
	}
	unsigned int i = 0;
	percpu_foreach(pmap_pcpu, pmap_sptm_percpu) {
		if (retype_epoch->remote_seq[i] != 0) {
			assert((pmap_pcpu->retype_epoch.local_seq == 0) ||
			    (pmap_pcpu->retype_epoch.local_seq >= retype_epoch->remote_seq[i]));
			
			while ((os_atomic_load_exclusive(&pmap_pcpu->retype_epoch.local_seq, relaxed) ==
			    retype_epoch->remote_seq[i])) {
				__builtin_arm_wfe();
			}
			
			os_atomic_clear_exclusive();
		}
		++i;
	}
	mp_enable_preemption();
	
	os_atomic_thread_fence(acquire);
}


static inline bool
pmap_type_requires_retype_on_unmap(sptm_frame_type_t frame_type)
{
	return (frame_type == XNU_USER_EXEC) || (frame_type == XNU_USER_DEBUG) ||
	       (frame_type == XNU_USER_JIT) || (frame_type == XNU_ROZONE) ||
	       (frame_type == XNU_KERNEL_RESTRICTED);
}



static inline bool
pmap_prepare_unmapped_page_for_retype(pmap_paddr_t pa)
{
	pvh_assert_locked(pa_index(pa));
	const sptm_frame_type_t frame_type = sptm_get_frame_type(pa);
	if (__improbable(pmap_type_requires_retype_on_unmap(frame_type))) {
		pmap_retype_epoch_prepare_drain();
		return true;
	}
	return false;
}


static inline bool
pmap_retype_unmapped_page(pmap_paddr_t pa)
{
	pvh_assert_locked(pa_index(pa));
	const sptm_frame_type_t frame_type = sptm_get_frame_type(pa);
	if (__improbable(pmap_type_requires_retype_on_unmap(frame_type))) {
		sptm_retype_params_t retype_params = {.raw = SPTM_RETYPE_PARAMS_NULL};
		pmap_retype_epoch_drain();
		sptm_retype(pa & ~PAGE_MASK, frame_type, XNU_DEFAULT, retype_params);
		return true;
	}
	return false;
}

static inline boolean_t
pmap_is_preemptible(void)
{
	return preemption_enabled() || (startup_phase < STARTUP_SUB_EARLY_BOOT) || PMAP_IS_HIBERNATING();
}


static inline void
pmap_verify_preemptible(void)
{
	assert(pmap_is_preemptible());
}














extern pmap_paddr_t sptm_cpu_iommu_scratch_start;
extern void pmap_data_bootstrap(void);
extern void pmap_enqueue_pages(vm_page_t);
extern kern_return_t pmap_page_alloc(pmap_paddr_t *, unsigned);
extern void pmap_page_free(pmap_paddr_t);
OS_ENUM(pmap_lock_mode, uint8_t,
    PMAP_LOCK_SHARED,
    PMAP_LOCK_EXCLUSIVE,
    PMAP_LOCK_HELD);
extern pv_alloc_return_t pv_alloc(
	pmap_t, pmap_lock_mode_t, unsigned int, pv_entry_t **, locked_pvh_t *, volatile uint16_t *);
extern void pv_free(pv_entry_t *);
extern void pv_list_free(pv_entry_t *, pv_entry_t *, unsigned int);
extern void pmap_compute_pv_targets(void);
extern pv_alloc_return_t pmap_enter_pv(
	pmap_t, pt_entry_t *, unsigned int, pmap_lock_mode_t, locked_pvh_t *, pv_entry_t **, int *);
extern pv_remove_return_t pmap_remove_pv(pmap_t, pt_entry_t *, locked_pvh_t *, bool *, bool *);
extern void ptd_bootstrap(pt_desc_t *, unsigned int);
extern pt_desc_t *ptd_alloc_unlinked(unsigned int);
extern pt_desc_t *ptd_alloc(pmap_t, unsigned int);
extern void ptd_deallocate(pt_desc_t *);
extern void ptd_info_init(
	pt_desc_t *, pmap_t, vm_map_address_t, unsigned int, pt_entry_t *);
extern kern_return_t pmap_ledger_credit(pmap_t, int, ledger_amount_t);
extern kern_return_t pmap_ledger_debit(pmap_t, int, ledger_amount_t);
extern void validate_pmap_internal(const volatile struct pmap *, const char *);
extern void validate_pmap_mutable_internal(const volatile struct pmap *, const char *);
extern pmap_io_range_t* pmap_find_io_attr(pmap_paddr_t);
extern void pmap_cpu_data_init_internal(unsigned int);
extern void pmap_remove_range_options(
	pmap_t, vm_map_address_t, vm_map_address_t, int);
extern void pmap_tte_deallocate(
	pmap_t, vm_offset_t, tt_entry_t *, unsigned int);
extern pmap_t current_pmap(void);
extern void pmap_tt_ledger_credit(pmap_t, vm_size_t);
extern void pmap_tt_ledger_debit(pmap_t, vm_size_t);
extern void write_pte(pt_entry_t *, pt_entry_t);
extern void qsort(void *a, size_t n, size_t es, cmpfunc_t cmp);
extern void     commpage_set_timestamp(uint64_t tbr, uint64_t secs, uint64_t frac, uint64_t scale, uint64_t tick_per_sec);
extern  void    commpage_set_memory_pressure( unsigned int pressure );
extern  void    commpage_update_active_cpus(void);
extern  void    commpage_set_spin_count(unsigned int  count);
extern  void    commpage_update_timebase(void);
extern  void    commpage_update_mach_approximate_time(uint64_t);
extern  void    commpage_update_kdebug_state(void);
extern  void    commpage_update_atm_diagnostic_config(uint32_t);
extern  void    commpage_update_mach_continuous_time(uint64_t sleeptime);
extern  void    commpage_update_mach_continuous_time_hw_offset(uint64_t offset);
extern  void    commpage_update_multiuser_config(uint32_t);
extern  void    commpage_update_boottime(uint64_t boottime_usec);
extern  void    commpage_set_remotetime_params(double rate, uint64_t base_local_ts, uint64_t base_remote_ts);
extern  void    commpage_update_dof(boolean_t enabled);
extern  void    commpage_update_dyld_flags(uint64_t value);
extern uint32_t commpage_is_in_pfz64(addr64_t addr);
extern  void    commpage_update_apt_active(bool active);
_Static_assert(PTE_PER_PVE == 2, "PTE_PER_PVE is not 2");
extern void flush_mmu_tlb_region(vm_offset_t va, unsigned length);
extern uint64_t get_mmu_control(void);
extern uint64_t get_aux_control(void);
extern void set_aux_control(uint64_t);
extern void set_mmu_ttb(uint64_t);
extern void set_mmu_ttb_alternate(uint64_t);
extern uint64_t get_tcr(void);
extern void set_tcr(uint64_t);
extern uint64_t pmap_get_arm64_prot(pmap_t, vm_offset_t);
extern pmap_paddr_t get_mmu_ttb(void);
extern pmap_paddr_t mmu_kvtop(vm_offset_t va);
extern pmap_paddr_t mmu_kvtop_wpreflight(vm_offset_t va);
extern pmap_paddr_t mmu_uvtop(vm_offset_t va);
extern int copysafe(vm_map_address_t from, vm_map_address_t to, uint32_t cnt, int type, uint32_t *bytes_copied);
extern void pmap_clear_user_ttb(void);
extern void pmap_bootstrap(vm_offset_t);
extern vm_map_address_t pmap_ptov(pmap_t, ppnum_t);
extern pmap_paddr_t pmap_find_pa(pmap_t map, addr64_t va);
extern pmap_paddr_t pmap_find_pa_nofault(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys(pmap_t map, addr64_t va);
extern ppnum_t pmap_find_phys_nofault(pmap_t map, addr64_t va);
extern void pmap_switch_user(thread_t th, vm_map_t map);
extern void pmap_set_pmap(pmap_t pmap, thread_t thread);
extern  void pmap_gc(void);
extern pmap_paddr_t kvtophys(vm_offset_t va);
extern pmap_paddr_t kvtophys_nofail(vm_offset_t va);
extern vm_map_address_t phystokv(pmap_paddr_t pa);
extern vm_map_address_t phystokv_range(pmap_paddr_t pa, vm_size_t *max_len);
extern vm_map_address_t pmap_map(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot, unsigned int flags);
extern vm_map_address_t pmap_map_high_window_bd( vm_offset_t pa, vm_size_t len, vm_prot_t prot);
extern kern_return_t pmap_map_block(pmap_t pmap, addr64_t va, ppnum_t pa, uint32_t size, vm_prot_t prot, int attr, unsigned int flags);
extern kern_return_t pmap_map_block_addr(pmap_t pmap, addr64_t va, pmap_paddr_t pa, uint32_t size, vm_prot_t prot, int attr, unsigned int flags);
extern void pmap_map_globals(void);
extern vm_map_address_t pmap_map_bd_with_options(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot, int32_t options);
extern vm_map_address_t pmap_map_bd(vm_map_address_t va, vm_offset_t sa, vm_offset_t ea, vm_prot_t prot);
extern void pmap_init_pte_page(pmap_t, pt_entry_t *, vm_offset_t, unsigned int ttlevel, boolean_t alloc_ptd);
extern boolean_t pmap_valid_address(pmap_paddr_t addr);
extern void pmap_disable_NX(pmap_t pmap);
extern void pmap_set_nested(pmap_t pmap);
extern void pmap_create_commpages(vm_map_address_t *kernel_data_addr, vm_map_address_t *kernel_text_addr,
    vm_map_address_t *kernel_ro_data_addr, vm_map_address_t *user_text_addr);
extern void pmap_insert_commpage(pmap_t pmap);
extern vm_offset_t pmap_cpu_windows_copy_addr(int cpu_num, unsigned int index);
extern unsigned int pmap_map_cpu_windows_copy(ppnum_t pn, vm_prot_t prot, unsigned int wimg_bits);
extern void pmap_unmap_cpu_windows_copy(unsigned int index);
static inline vm_offset_t
pmap_ro_zone_align(vm_offset_t value)
{
	return value;
}

extern void pmap_ro_zone_memcpy(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    vm_offset_t new_data, vm_size_t new_data_size);
extern uint64_t pmap_ro_zone_atomic_op(zone_id_t zid, vm_offset_t va, vm_offset_t offset,
    uint32_t op, uint64_t value);
extern void pmap_ro_zone_bzero(zone_id_t zid, vm_offset_t va, vm_offset_t offset, vm_size_t size);
extern boolean_t pmap_valid_page(ppnum_t pn);
extern boolean_t pmap_bootloader_page(ppnum_t pn);
extern boolean_t pmap_is_empty(pmap_t pmap, vm_map_offset_t start, vm_map_offset_t end);
extern vm_map_offset_t pmap_max_offset(boolean_t is64, unsigned int option);
extern vm_map_offset_t pmap_max_64bit_offset(unsigned int option);
extern vm_map_offset_t pmap_max_32bit_offset(unsigned int option);
boolean_t pmap_virtual_region(unsigned int region_select, vm_map_offset_t *startp, vm_map_size_t *size);
boolean_t pmap_enforces_execute_only(pmap_t pmap);
void pmap_pin_kernel_pages(vm_offset_t kva, size_t nbytes);
void pmap_unpin_kernel_pages(vm_offset_t kva, size_t nbytes);
void pmap_abandon_measurement(void);
extern void pmap_cpu_data_init(void);
extern pmap_cpu_data_t *pmap_get_cpu_data(void);
extern pmap_cpu_data_t *pmap_get_remote_cpu_data(unsigned int cpu);
static inline bool
_pmap_pending_preemption_real(void)
{
	return !!(*((volatile ast_t*)ast_pending()) & AST_URGENT);
}




extern void pmap_lockdown_ppl(void);
extern void pmap_nop(pmap_t);
extern void CleanPoC_DcacheRegion_Force_nopreempt_nohid(vm_offset_t va, size_t length);
static inline bool
pa_valid(pmap_paddr_t pa)
{
	return (pa >= vm_first_phys) && (pa < vm_last_phys);
}



static inline unsigned int
pa_index(pmap_paddr_t pa)
{
	return (unsigned int)atop(pa - vm_first_phys);
}


extern pv_entry_t **pv_head_table;
static inline pv_entry_t **
pai_to_pvh(unsigned int pai)
{
	return &pv_head_table[pai];
}










































static inline void
pvh_assert_locked(__assert_only unsigned int index)
{
	assert((vm_offset_t)(pv_head_table[index]) & PVH_FLAG_LOCK);
}



static inline void
pvh_lock(unsigned int index)
{
	pmap_lock_bit((uint32_t*)(&pv_head_table[index]) + PVH_LOCK_WORD,
	    PVH_LOCK_BIT - (PVH_LOCK_WORD * 32));
}


static inline void
pvh_unlock(unsigned int index)
{
	pvh_assert_locked(index);

	pmap_unlock_bit((uint32_t*)(&pv_head_table[index]) + PVH_LOCK_WORD,
	    PVH_LOCK_BIT - (PVH_LOCK_WORD * 32));
}


static inline bool
pvh_test_type(pv_entry_t **pvh, vm_offset_t type)
{
	return ((*(vm_offset_t *)pvh) & PVH_TYPE_MASK) == type;
}


static inline pt_entry_t*
pvh_ptep(pv_entry_t **pvh)
{
	return (pt_entry_t *)(((*(vm_offset_t *)pvh) & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline pv_entry_t*
pvh_pve_list(pv_entry_t **pvh)
{
	return (pv_entry_t *)(((*(vm_offset_t *)pvh) & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline vm_offset_t
pvh_get_flags(pv_entry_t **pvh)
{
	return (*(vm_offset_t *)pvh) & PVH_HIGH_FLAGS;
}


static inline void
pvh_set_flags(pv_entry_t **pvh, vm_offset_t flags)
{
	os_atomic_store((vm_offset_t *)pvh, ((*(vm_offset_t *)pvh) & ~PVH_HIGH_FLAGS) | flags, relaxed);
}


static inline void
pvh_update_head(pv_entry_t **pvh, void *pvep, unsigned int type)
{
	assert((*(vm_offset_t *)pvh) & PVH_FLAG_LOCK);
	os_atomic_store((vm_offset_t *)pvh, (vm_offset_t)pvep | type | PVH_FLAG_LOCK, relaxed);
}


static inline void
pvh_update_head_unlocked(pv_entry_t **pvh, void *pvep, unsigned int type)
{
	assert(!((*(vm_offset_t *)pvh) & PVH_FLAG_LOCK));
	*(vm_offset_t *)pvh = ((vm_offset_t)pvep | type) & ~PVH_FLAG_LOCK;
}


static inline bool
pvh_ptep_is_iommu(const pt_entry_t *ptep)
{
	return (vm_offset_t)ptep & PVH_FLAG_IOMMU;
}


static inline const pt_entry_t*
pvh_strip_ptep(const pt_entry_t *ptep)
{
	const vm_offset_t pte_va = (vm_offset_t)ptep;
	return (const pt_entry_t*)((pte_va & ~PVH_FLAG_IOMMU) | PVH_FLAG_IOMMU_TABLE);
}






static inline void
pve_set_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] | PVE_PTEP_ALTACCT);
}

static inline void
pve_set_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] | PVE_PTEP_INTERNAL);
}


static inline void
pve_clr_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_ALTACCT);
}

static inline void
pve_clr_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_INTERNAL);
}


static inline bool
pve_get_altacct(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (uintptr_t)pvep->pve_ptep[idx] & PVE_PTEP_ALTACCT;
}

static inline bool
pve_get_internal(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (uintptr_t)pvep->pve_ptep[idx] & PVE_PTEP_INTERNAL;
}


static inline pv_entry_t *
pve_next(pv_entry_t *pvep)
{
	return pvep->pve_next;
}


static inline pv_entry_t **
pve_next_ptr(pv_entry_t *pvep)
{
	return &pvep->pve_next;
}


static inline pt_entry_t *
pve_get_ptep(pv_entry_t *pvep, unsigned idx)
{
	assert(idx < PTE_PER_PVE);
	return (pt_entry_t *)((uintptr_t)pvep->pve_ptep[idx] & ~PVE_PTEP_FLAGS);
}


static inline void
pve_set_ptep(pv_entry_t *pvep, unsigned idx, pt_entry_t *ptep_new)
{
	assert(idx < PTE_PER_PVE);
	pvep->pve_ptep[idx] = ptep_new;
}


static inline void
pve_init(pv_entry_t *pvep)
{
	pvep->pve_next = PV_ENTRY_NULL;
	for (int i = 0; i < PTE_PER_PVE; i++) {
		pvep->pve_ptep[i] = PT_ENTRY_NULL;
	}
}


static inline int
pve_find_ptep_index(pv_entry_t *pvep, pt_entry_t *ptep)
{
	for (unsigned int i = 0; i < PTE_PER_PVE; i++) {
		if (pve_get_ptep(pvep, i) == ptep) {
			return (int)i;
		}
	}

	return -1;
}


static inline bool
pve_is_empty(pv_entry_t *pvep)
{
	for (unsigned int i = 0; i < PTE_PER_PVE; i++) {
		if (pve_get_ptep(pvep, i) != PT_ENTRY_NULL) {
			return false;
		}
	}

	return true;
}


static inline void
pve_add(pv_entry_t **pvh, pv_entry_t *pvep)
{
	assert(pvh_test_type(pvh, PVH_TYPE_PVEP));

	pvep->pve_next = pvh_pve_list(pvh);
	pvh_update_head(pvh, pvep, PVH_TYPE_PVEP);
}


static inline void
pve_remove(pv_entry_t **pvh, pv_entry_t **pvepp, pv_entry_t *pvep)
{
	assert(pvh_test_type(pvh, PVH_TYPE_PVEP));

	if (pvepp == pvh) {
		if (pve_next(pvep) == PV_ENTRY_NULL) {
			
			pvh_update_head(pvh, PV_ENTRY_NULL, PVH_TYPE_NULL);
		} else {
			
			pvh_update_head(pvh, pve_next(pvep), PVH_TYPE_PVEP);
		}
	} else {
		
		*pvepp = pve_next(pvep);
	}
}








typedef 


typedef 


static inline pt_desc_t*
pvh_ptd(pv_entry_t **pvh)
{
	return (pt_desc_t *)(((*(vm_offset_t *)pvh) & PVH_LIST_MASK) | PVH_HIGH_FLAGS);
}


static inline pt_desc_t *
ptep_get_ptd(const pt_entry_t *ptep)
{
	assert(ptep != NULL);

	const vm_offset_t pt_base_va = (vm_offset_t)ptep;
	pv_entry_t **pvh = pai_to_pvh(pa_index(ml_static_vtop(pt_base_va)));

	if (__improbable(!pvh_test_type(pvh, PVH_TYPE_PTDP))) {
		panic("%s: invalid PV head 0x%llx for PTE %p", __func__, (uint64_t)(*pvh), ptep);
	}

	return pvh_ptd(pvh);
}


static inline struct pmap *
ptep_get_pmap(const pt_entry_t *ptep)
{
	return ptep_get_ptd(ptep)->pmap;
}



static inline pt_desc_t *
tte_get_ptd(const tt_entry_t tte)
{
	const vm_offset_t pt_base_va = (vm_offset_t)(tte & ~((tt_entry_t)PAGE_MASK));
	pv_entry_t **pvh = pai_to_pvh(pa_index(pt_base_va));

	if (__improbable(!pvh_test_type(pvh, PVH_TYPE_PTDP))) {
		panic("%s: invalid PV head 0x%llx for TTE 0x%llx", __func__, (uint64_t)(*pvh), (uint64_t)tte);
	}

	return pvh_ptd(pvh);
}


static inline unsigned
ptd_get_index(__unused const pt_desc_t *ptd, __unused const tt_entry_t *ttep)
{
}


static inline ptd_info_t *
ptd_get_info(pt_desc_t *ptd, const tt_entry_t *ttep)
{
	assert((ptd != NULL) && (ptd->ptd_info[0].refcnt < PT_DESC_IOMMU_GRANTED_REFCOUNT));

	return &ptd->ptd_info[ptd_get_index(ptd, ttep)];
}


static inline ptd_info_t *
ptep_get_info(const pt_entry_t *ptep)
{
	return ptd_get_info(ptep_get_ptd(ptep), ptep);
}


static inline vm_map_address_t
ptd_get_va(const pt_desc_t *ptdp, const pt_entry_t *ptep)
{
	const pt_attr_t * const pt_attr = pmap_get_pt_attr(ptdp->pmap);

	vm_map_address_t va = ptdp->va[ptd_get_index(ptdp, ptep)];
	vm_offset_t ptep_index = ((vm_offset_t)ptep & pt_attr_leaf_offmask(pt_attr)) / sizeof(*ptep);

	va += (ptep_index << pt_attr_leaf_shift(pt_attr));

	return va;
}


static inline vm_map_address_t
ptep_get_va(const pt_entry_t *ptep)
{
	return ptd_get_va(ptep_get_ptd(ptep), ptep);
}




typedef uint16_t pp_attr_t;
static inline void
ppattr_set_bits(unsigned int pai, pp_attr_t bits)
{
	volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	os_atomic_or(ppattr, bits, acq_rel);
}


static inline void
ppattr_clear_bits(unsigned int pai, pp_attr_t bits)
{
	volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	os_atomic_andnot(ppattr, bits, acq_rel);
}


static inline bool
ppattr_test_bits(unsigned int pai, pp_attr_t bits)
{
	const volatile pp_attr_t *ppattr = &pp_attr_table[pai];
	return (*ppattr & bits) == bits;
}


static inline void
ppattr_pa_set_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	if (pa_valid(pa)) {
		ppattr_set_bits(pa_index(pa), bits);
	}
}


static inline void
ppattr_pa_clear_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	if (pa_valid(pa)) {
		ppattr_clear_bits(pa_index(pa), bits);
	}
}


static inline bool
ppattr_pa_test_bits(pmap_paddr_t pa, pp_attr_t bits)
{
	return pa_valid(pa) ? ppattr_test_bits(pa_index(pa), bits) : false;
}


static inline void
ppattr_pa_set_modify(pmap_paddr_t pa)
{
	ppattr_pa_set_bits(pa, PP_ATTR_MODIFIED);
}


static inline void
ppattr_pa_clear_modify(pmap_paddr_t pa)
{
	ppattr_pa_clear_bits(pa, PP_ATTR_MODIFIED);
}


static inline void
ppattr_pa_set_reference(pmap_paddr_t pa)
{
	ppattr_pa_set_bits(pa, PP_ATTR_REFERENCED);
}


static inline void
ppattr_pa_clear_reference(pmap_paddr_t pa)
{
	ppattr_pa_clear_bits(pa, PP_ATTR_REFERENCED);
}



static inline void
ppattr_set_internal(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_INTERNAL);
}


static inline void
ppattr_clear_internal(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_INTERNAL);
}


static inline bool
ppattr_test_internal(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_INTERNAL);
}


static inline void
ppattr_set_reusable(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_REUSABLE);
}


static inline void
ppattr_clear_reusable(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_REUSABLE);
}


static inline bool
ppattr_test_reusable(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_REUSABLE);
}


static inline void
ppattr_set_altacct(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_ALTACCT);
}


static inline void
ppattr_clear_altacct(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_ALTACCT);
}


static inline bool
ppattr_is_altacct(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_ALTACCT);
}

static inline bool
ppattr_is_internal(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_INTERNAL);
}


static inline bool
ppattr_pve_is_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	return (pvep == PV_ENTRY_NULL) ? ppattr_is_altacct(pai) : pve_get_altacct(pvep, idx);
}

static inline bool
ppattr_pve_is_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	return (pvep == PV_ENTRY_NULL) ? ppattr_is_internal(pai) : pve_get_internal(pvep, idx);
}


static inline void
ppattr_pve_set_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_set_altacct(pai);
	} else {
		pve_set_altacct(pvep, idx);
	}
}

static inline void
ppattr_pve_set_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_set_internal(pai);
	} else {
		pve_set_internal(pvep, idx);
	}
}


static inline void
ppattr_pve_clr_altacct(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_clear_altacct(pai);
	} else {
		pve_clr_altacct(pvep, idx);
	}
}

static inline void
ppattr_pve_clr_internal(unsigned int pai, pv_entry_t *pvep, unsigned idx)
{
	if (pvep == PV_ENTRY_NULL) {
		ppattr_clear_internal(pai);
	} else {
		pve_clr_internal(pvep, idx);
	}
}


static inline void
ppattr_set_reffault(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_REFFAULT);
}


static inline void
ppattr_clear_reffault(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_REFFAULT);
}


static inline bool
ppattr_test_reffault(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_REFFAULT);
}


static inline void
ppattr_set_modfault(unsigned int pai)
{
	ppattr_set_bits(pai, PP_ATTR_MODFAULT);
}


static inline void
ppattr_clear_modfault(unsigned int pai)
{
	ppattr_clear_bits(pai, PP_ATTR_MODFAULT);
}


static inline bool
ppattr_test_modfault(unsigned int pai)
{
	return ppattr_test_bits(pai, PP_ATTR_MODFAULT);
}

static inline boolean_t
pmap_is_preemptible(void)
{
	return preemption_enabled() || (startup_phase < STARTUP_SUB_EARLY_BOOT);
}


static inline void
pmap_verify_preemptible(void)
{
	assert(pmap_is_preemptible());
}










extern unsigned int inuse_pmap_pages_count;
extern void pmap_data_bootstrap(void);
extern void pmap_enqueue_pages(vm_page_t);
extern kern_return_t pmap_pages_alloc_zeroed(pmap_paddr_t *, unsigned, unsigned);
extern void pmap_pages_free(pmap_paddr_t, unsigned);
OS_ENUM(pmap_lock_mode, uint8_t,
    PMAP_LOCK_SHARED,
    PMAP_LOCK_EXCLUSIVE);
extern pv_alloc_return_t pv_alloc(
	pmap_t, unsigned int, pmap_lock_mode_t, unsigned int, pv_entry_t **);
extern void pv_free(pv_entry_t *);
extern void pv_list_free(pv_entry_t *, pv_entry_t *, int);
extern void pmap_compute_pv_targets(void);
extern pv_alloc_return_t pmap_enter_pv(
	pmap_t, pt_entry_t *, int, unsigned int, pmap_lock_mode_t, pv_entry_t **, int *new_pve_ptep_idx);
extern void pmap_remove_pv(pmap_t, pt_entry_t *, int, bool, bool *, bool *);
extern void ptd_bootstrap(pt_desc_t *, unsigned int);
extern pt_desc_t *ptd_alloc_unlinked(void);
extern pt_desc_t *ptd_alloc(pmap_t);
extern void ptd_deallocate(pt_desc_t *);
extern void ptd_info_init(
	pt_desc_t *, pmap_t, vm_map_address_t, unsigned int, pt_entry_t *);
extern kern_return_t pmap_ledger_credit(pmap_t, int, ledger_amount_t);
extern kern_return_t pmap_ledger_debit(pmap_t, int, ledger_amount_t);
extern void validate_pmap_internal(const volatile struct pmap *, const char *);
extern void validate_pmap_mutable_internal(const volatile struct pmap *, const char *);
extern pmap_io_range_t* pmap_find_io_attr(pmap_paddr_t);
extern pmap_io_filter_entry_t *pmap_find_io_filter_entry(pmap_paddr_t, uint64_t, const pmap_io_range_t **);
extern void pmap_cpu_data_init_internal(unsigned int);
extern void pmap_flush_noncoherent_page(pmap_paddr_t paddr);
extern int pmap_remove_range_options(
	pmap_t, vm_map_address_t, pt_entry_t *, pt_entry_t *, vm_map_address_t *, bool *, int);
extern void pmap_tte_deallocate(
	pmap_t, vm_offset_t, vm_offset_t, bool, tt_entry_t *, unsigned int);
extern pmap_t current_pmap(void);
extern void pmap_tt_ledger_credit(pmap_t, vm_size_t);
extern void pmap_tt_ledger_debit(pmap_t, vm_size_t);
extern void write_pte(pt_entry_t *, pt_entry_t);
extern void qsort(void *a, size_t n, size_t es, cmpfunc_t cmp);
extern int __getpid(void);
extern int __kill(int pid, int signum, int posix);
extern int __exit(int) __attribute__((noreturn));
char *mach_error_string_int(mach_error_t, boolean_t *);
__BEGIN_DECLS
extern void mig_init(void *);
extern void mach_init_ports(void);
int _mach_snprintf(char *buffer, int length, const char *fmt, ...) __printflike(3, 4);
int _mach_vsnprintf(char *buffer, int length, const char *fmt, va_list ap) __printflike(3, 0);
void *memcpy(void *dst0, const void *src0, size_t length);
void *memset(void *dst0, int c0, size_t length);
void bzero(void *dst0, size_t length);
__BEGIN_DECLS



API_UNAVAILABLE(macos) API_AVAILABLE(ios(13.0), tvos(13.0), watchos(6.0), bridgeos(4.0))
extern
size_t os_proc_available_memory(void);
extern void _thread_set_tsd_base(void *tsd_base);
__BEGIN_DECLS

int _getprivatesystemidentifier(uuid_t uuid, const struct timespec *timeout) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int _register_gethostuuid_callback(int (*)(uuid_t)) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
void __libkernel_init(_libkernel_functions_t fns, const char *envp[],
    const char *apple[], const struct ProgramVars *vars);
kern_return_t __libkernel_platform_init(_libkernel_string_functions_t fns);
kern_return_t __libkernel_voucher_init(_libkernel_voucher_functions_t fns);
void __libkernel_init_late(_libkernel_late_init_config_t config);
__BEGIN_DECLS



int     proc_listpidspath(uint32_t      type,
    uint32_t      typeinfo,
    const char    *path,
    uint32_t      pathflags,
    void          *buffer,
    int           buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_listpids(uint32_t type, uint32_t typeinfo, void *buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_listallpids(void * buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_1);
int proc_listpgrppids(pid_t pgrpid, void * buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_1);
int proc_listchildpids(pid_t ppid, void * buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_1);
int proc_pidinfo(int pid, int flavor, uint64_t arg, void *buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_pidfdinfo(int pid, int fd, int flavor, void * buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_pidfileportinfo(int pid, uint32_t fileport, int flavor, void *buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
int proc_name(int pid, void * buffer, uint32_t buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_regionfilename(int pid, uint64_t address, void * buffer, uint32_t buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_kmsgbuf(void * buffer, uint32_t buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_pidpath(int pid, void * buffer, uint32_t  buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_pidpath_audittoken(audit_token_t *audittoken, void * buffer, uint32_t  buffersize) API_AVAILABLE(macos(10.16), ios(14.0), watchos(7.0), tvos(14.0));
int proc_libversion(int *major, int * minor) __OSX_AVAILABLE_STARTING(__MAC_10_5, __IPHONE_2_0);
int proc_pid_rusage(int pid, int flavor, rusage_info_t *buffer) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int proc_setpcontrol(const int control) __OSX_AVAILABLE_STARTING(__MAC_10_6, __IPHONE_3_2);
int proc_setpcontrol(const int control);
int proc_track_dirty(pid_t pid, uint32_t flags);
int proc_set_dirty(pid_t pid, bool dirty);
int proc_get_dirty(pid_t pid, uint32_t *flags);
int proc_clear_dirty(pid_t pid, uint32_t flags);
int proc_terminate(pid_t pid, int *sig);
int proc_terminate_all_rsr(int sig);
int proc_signal_with_audittoken(audit_token_t *audittoken, int sig);
int proc_terminate_with_audittoken(audit_token_t *audittoken, int *sig);
int proc_signal_delegate(audit_token_t instigator, audit_token_t target, int sig);
int proc_terminate_delegate(audit_token_t instigator, audit_token_t target, int *sig);
int proc_set_no_smt(void) __API_AVAILABLE(macos(10.16));
int proc_setthread_no_smt(void) __API_AVAILABLE(macos(10.16));
int proc_set_csm(uint32_t flags) __API_AVAILABLE(macos(10.16));
int proc_setthread_csm(uint32_t flags) __API_AVAILABLE(macos(10.16));
int proc_udata_info(int pid, int flavor, void *buffer, int buffersize);
__BEGIN_DECLS



int proc_setcpu_percentage(pid_t pid, int action, int percentage) __OSX_AVAILABLE_STARTING(__MAC_10_12_2, __IPHONE_5_0);
int proc_clear_cpulimits(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_12_2, __IPHONE_5_0);
int proc_setthread_cpupercent(uint8_t percentage, uint32_t ms_refill) __OSX_AVAILABLE_STARTING(__MAC_10_10, __IPHONE_5_0);
int proc_donate_importance_boost(void);
int proc_importance_assertion_begin_with_msg(mach_msg_header_t  *msg,
    mach_msg_trailer_t *trailer,
    uint64_t *assertion_token) __OSX_AVAILABLE_BUT_DEPRECATED(__MAC_10_8, __MAC_10_10, __IPHONE_6_0, __IPHONE_8_0);
int proc_importance_assertion_complete(uint64_t assertion_handle);
int proc_denap_assertion_begin_with_msg(mach_msg_header_t  *msg,
    uint64_t *assertion_token);
int proc_denap_assertion_complete(uint64_t assertion_handle);
int proc_set_cpumon_defaults(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_8, __IPHONE_6_0);
int proc_set_cpumon_params(pid_t pid, int percentage, int interval) __OSX_AVAILABLE_STARTING(__MAC_10_8, __IPHONE_6_0);
int proc_set_cpumon_params_fatal(pid_t pid, int percentage, int interval) __OSX_AVAILABLE_STARTING(__MAC_10_10, __IPHONE_8_0);
int proc_get_cpumon_params(pid_t pid, int *percentage, int *interval) __OSX_AVAILABLE_STARTING(__MAC_10_8, __IPHONE_6_0);
int proc_resume_cpumon(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0);
int proc_disable_cpumon(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_8, __IPHONE_6_0);
int proc_set_wakemon_defaults(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int proc_set_wakemon_params(pid_t pid, int rate_hz, int flags) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int proc_get_wakemon_params(pid_t pid, int *rate_hz, int *flags) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int proc_disable_wakemon(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
int proc_reset_footprint_interval(pid_t pid) __OSX_AVAILABLE_STARTING(__MAC_10_14, __IPHONE_12_0);
int proc_trace_log(pid_t pid, uint64_t uniqueid) __OSX_AVAILABLE_STARTING(__MAC_10_10, __IPHONE_8_0);
int proc_pidoriginatorinfo(int flavor, void *buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_10, __IPHONE_8_0);
int proc_listcoalitions(int flavor, int coaltype, void *buffer, int buffersize) __OSX_AVAILABLE_STARTING(__MAC_10_11, __IPHONE_8_3);
int proc_current_thread_schedinfo(void *buffer, size_t buffersize);
int proc_set_dyld_all_image_info(void *buffer, int buffersize);
__BEGIN_DECLS


int proc_list_uptrs(pid_t pid, uint64_t *buffer, uint32_t buffersize);
int proc_list_dynkqueueids(int pid, kqueue_id_t *buf, uint32_t bufsz);
int proc_piddynkqueueinfo(int pid, int flavor, kqueue_id_t kq_id, void *buffer,
    int buffersize);
__BEGIN_DECLS


int     posix_spawn(pid_t * __restrict, const char * __restrict,
    const posix_spawn_file_actions_t *,
    const posix_spawnattr_t * __restrict,
    char *const __argv[__restrict],
    char *const __envp[__restrict]) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnp(pid_t * __restrict, const char * __restrict,
    const posix_spawn_file_actions_t *,
    const posix_spawnattr_t * __restrict,
    char *const __argv[__restrict],
    char *const __envp[__restrict]) __API_AVAILABLE(macos(10.5), ios(2.0));
int     posix_spawn_file_actions_addclose(posix_spawn_file_actions_t *, int) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawn_file_actions_adddup2(posix_spawn_file_actions_t *, int,
    int) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawn_file_actions_addopen(
	posix_spawn_file_actions_t * __restrict, int,
	const char * __restrict, int, mode_t) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawn_file_actions_destroy(posix_spawn_file_actions_t *) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawn_file_actions_init(posix_spawn_file_actions_t *) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_destroy(posix_spawnattr_t *) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_getsigdefault(const posix_spawnattr_t * __restrict,
    sigset_t * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_getflags(const posix_spawnattr_t * __restrict,
    short * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_getpgroup(const posix_spawnattr_t * __restrict,
    pid_t * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_getsigmask(const posix_spawnattr_t * __restrict,
    sigset_t * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_init(posix_spawnattr_t *) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_setsigdefault(posix_spawnattr_t * __restrict,
    const sigset_t * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_setflags(posix_spawnattr_t *, short) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_setpgroup(posix_spawnattr_t *, pid_t) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
int     posix_spawnattr_setsigmask(posix_spawnattr_t * __restrict,
    const sigset_t * __restrict) __API_AVAILABLE(macos(10.5), ios(2.0)) __SPI_AVAILABLE(watchos(2.0), tvos(9.0), bridgeos(1.0));
__BEGIN_DECLS

int     posix_spawnattr_getpcontrol_np(const posix_spawnattr_t * __restrict, int * __restrict) __API_AVAILABLE(macos(10.6), ios(3.2));
int     posix_spawnattr_setpcontrol_np(posix_spawnattr_t *, const int) __API_AVAILABLE(macos(10.6), ios(3.2));
int     posix_spawnattr_getprocesstype_np(const posix_spawnattr_t * __restrict, int * __restrict) __API_AVAILABLE(macos(10.8), ios(6.0));
int     posix_spawnattr_setprocesstype_np(posix_spawnattr_t *, const int) __API_AVAILABLE(macos(10.8), ios(6.0));
int     posix_spawnattr_setcpumonitor(posix_spawnattr_t * __restrict, uint64_t, uint64_t) __API_AVAILABLE(macos(10.8), ios(6.0));
int     posix_spawnattr_getcpumonitor(posix_spawnattr_t * __restrict, uint64_t *, uint64_t *) __API_AVAILABLE(macos(10.8), ios(6.0));
int     posix_spawnattr_setcpumonitor_default(posix_spawnattr_t * __restrict) __API_AVAILABLE(macos(10.9), ios(6.0));
int     posix_spawnattr_setjetsam_ext(posix_spawnattr_t * __restrict attr,
    short flags, int priority, int memlimit_active, int memlimit_inactive) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_jetsam_ttr_np(const posix_spawnattr_t * __restrict attr, uint32_t count, uint32_t *ttrs_millis) __OSX_AVAILABLE_STARTING(__MAC_10_15, __IPHONE_13_0);
int     posix_spawnattr_set_threadlimit_ext(posix_spawnattr_t * __restrict attr,
    int thread_limit)  __API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0));
int     posix_spawnattr_set_portlimits_ext(posix_spawnattr_t * __restrict attr,
    uint32_t port_soft_limit, uint32_t port_hard_limit)  __API_AVAILABLE(macos(12.0), ios(15.0), tvos(15.0), watchos(8.0));
int     posix_spawnattr_set_filedesclimit_ext(posix_spawnattr_t * __restrict attr,
    uint32_t filedesc_soft_limit, uint32_t filedesc_hard_limit)  __API_AVAILABLE(macos(12.0), ios(15.0), tvos(15.0), watchos(8.0));
int     posix_spawnattr_set_kqworklooplimit_ext(posix_spawnattr_t * __restrict attr,
    uint32_t kqwl_soft_limit, uint32_t kqwl_hard_limit) __API_AVAILABLE(macos(14.3), ios(17.4), tvos(17.4), watchos(10.4));
int     posix_spawnattr_set_importancewatch_port_np(posix_spawnattr_t * __restrict attr,
    int count, mach_port_t portarray[])  __API_AVAILABLE(macos(10.9), ios(6.0));
int     posix_spawnattr_set_registered_ports_np(posix_spawnattr_t * __restrict attr, mach_port_t portarray[], uint32_t count) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int
posix_spawnattr_set_ptrauth_task_port_np(posix_spawnattr_t * __restrict attr,
    mach_port_t port) __API_AVAILABLE(macos(10.16), ios(14.0), tvos(14.0), watchos(7.0));
int     posix_spawnattr_getmacpolicyinfo_np(const posix_spawnattr_t * __restrict, const char *, void **, size_t *) __API_AVAILABLE(macos(10.9), ios(7.0));
int     posix_spawnattr_setmacpolicyinfo_np(posix_spawnattr_t * __restrict, const char *, void *, size_t) __API_AVAILABLE(macos(10.9), ios(7.0));
int     posix_spawnattr_setcoalition_np(const posix_spawnattr_t * __restrict, uint64_t, int, int) __API_AVAILABLE(macos(10.10), ios(8.0));
int     posix_spawnattr_set_qos_clamp_np(const posix_spawnattr_t * __restrict, uint64_t) __API_AVAILABLE(macos(10.10), ios(8.0));
int     posix_spawnattr_get_qos_clamp_np(const posix_spawnattr_t * __restrict, uint64_t * __restrict) __API_AVAILABLE(macos(10.10), ios(8.0));
int     posix_spawnattr_set_darwin_role_np(const posix_spawnattr_t * __restrict, uint64_t) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_get_darwin_role_np(const posix_spawnattr_t * __restrict, uint64_t * __restrict) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_persona_np(const posix_spawnattr_t * __restrict, uid_t, uint32_t) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_persona_uid_np(const posix_spawnattr_t * __restrict, uid_t) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_persona_gid_np(const posix_spawnattr_t * __restrict, gid_t) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_persona_groups_np(const posix_spawnattr_t * __restrict, int, gid_t * __restrict, uid_t) __API_AVAILABLE(macos(10.11), ios(9.0));
int     posix_spawnattr_set_max_addr_np(const posix_spawnattr_t * __restrict attr, uint64_t max_addr) __API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0));
int     posix_spawnattr_set_uid_np(const posix_spawnattr_t * __restrict, uid_t) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int     posix_spawnattr_set_gid_np(const posix_spawnattr_t * __restrict, gid_t) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int     posix_spawnattr_set_groups_np(const posix_spawnattr_t * __restrict, int, gid_t * __restrict, uid_t) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int     posix_spawnattr_set_login_np(const posix_spawnattr_t * __restrict, const char * __restrict) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int     posix_spawnattr_set_subsystem_root_path_np(posix_spawnattr_t *attr, char *path) __API_AVAILABLE(macos(11.0), ios(14.0), tvos(14.0), watchos(7.0));
int     posix_spawnattr_set_platform_np(posix_spawnattr_t *attr, int platform, uint32_t flags) __API_AVAILABLE(macos(11.0), ios(14.0), tvos(14.0), watchos(7.0));
int     posix_spawnattr_disable_ptr_auth_a_keys_np(posix_spawnattr_t *attr, uint32_t flags) __API_AVAILABLE(macos(11.0), ios(14.0), tvos(14.0), watchos(7.0));
int     posix_spawnattr_set_alt_rosetta_np(posix_spawnattr_t *attr, uint32_t flags) __API_AVAILABLE(macos(12.0), ios(15.0), tvos(15.0), watchos(8.0));
int     posix_spawn_file_actions_add_fileportdup2_np(posix_spawn_file_actions_t * __restrict, mach_port_t, int) __API_AVAILABLE(macos(10.15), ios(13.0), tvos(13.0), watchos(6.0));
int     posix_spawnattr_set_crash_count_np(posix_spawnattr_t * __restrict attr, uint32_t crash_count, uint32_t timeout) __SPI_AVAILABLE(macos(13.1), ios(16.2), tvos(16.2), watchos(9.2));
int     posix_spawnattr_set_crash_behavior_np(posix_spawnattr_t *attr, uint32_t flags) __API_AVAILABLE(macos(13.0), ios(16.0), tvos(16.0), watchos(9.0));
int     posix_spawnattr_set_crash_behavior_deadline_np(posix_spawnattr_t *attr, uint64_t deadline, uint32_t flags) __API_AVAILABLE(macos(13.0), ios(16.0), tvos(16.0), watchos(9.0));
int     posix_spawnattr_set_launch_type_np(posix_spawnattr_t *attr, uint8_t launch_type) __SPI_AVAILABLE(macos(13.0), ios(16.0), tvos(16.0), watchos(9.0));
int     posix_spawnattr_set_use_sec_transition_shims_np(posix_spawnattr_t *attr, uint32_t flags) __SPI_AVAILABLE(macos(15.2), ios(18.2), tvos(18.2), watchos(11.2));
int     posix_spawnattr_setdataless_iopolicy_np(posix_spawnattr_t * __restrict attr, const int policy) __SPI_AVAILABLE(macos(13.3), ios(16.4), tvos(16.4), watchos(9.4));
int     posix_spawnattr_set_conclave_id_np(const posix_spawnattr_t *attr, const char *conclave_id) __SPI_AVAILABLE(macos(14.0), ios(17.0), tvos(17.0), watchos(10.0));
void    *memmove(void *, const void *, size_t);
void    *memset(void *, int, size_t);
void    *memchr(const void *, int, size_t);
int      memcmp(const void *, const void *, size_t);
int      strcmp(const char *, const char *);
int      strncmp(const char *, const char *, size_t);
char    *strcpy(char *, const char *);
size_t   strlen(const char *);
size_t   strnlen(const char *, size_t);
size_t   strlcpy(char *, const char *, size_t);
size_t   strlcat(char *, const char *, size_t);
char    *strsep(char **, const char *);
void     bcopy(const void *, void *, size_t);
void     bzero(void *, size_t);
void     __bzero(void *, size_t);
char    *index(const char *, int);
char    *strchr(const char *, int);
void    *_libkernel_memmove(void *, const void *, size_t);
void    *_libkernel_memset(void *, int, size_t);
int      _libkernel_strcmp(const char *, const char *);
char    *_libkernel_strcpy(char *, const char *);
size_t   _libkernel_strnlen(const char *, size_t);
size_t   _libkernel_strlen(const char *);
size_t   _libkernel_strlcpy(char *, const char *, size_t);
void     _libkernel_bzero(void *, size_t);
char    *_libkernel_strchr(const char *, int);
__BEGIN_DECLS

extern void                     panic_init(mach_port_t);
extern void                     panic(const char *, ...);
extern void                     slot_name(cpu_type_t,
    cpu_subtype_t,
    char **,
    char **);
extern void                     mig_reply_setup(mach_msg_header_t *,
    mach_msg_header_t *);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern void                     mach_msg_destroy(mach_msg_header_t *);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_receive(mach_msg_header_t *);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_send(mach_msg_header_t *);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_server_once(boolean_t (*)
    (mach_msg_header_t *,
    mach_msg_header_t *),
    mach_msg_size_t,
    mach_port_t,
    mach_msg_options_t);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_server(boolean_t (*)
    (mach_msg_header_t *,
    mach_msg_header_t *),
    mach_msg_size_t,
    mach_port_t,
    mach_msg_options_t);
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
extern mach_msg_return_t        mach_msg_server_importance(boolean_t (*)
    (mach_msg_header_t *,
    mach_msg_header_t *),
    mach_msg_size_t,
    mach_port_t,
    mach_msg_options_t);
extern kern_return_t    clock_get_res(mach_port_t,
    clock_res_t *);
extern kern_return_t    clock_set_res(mach_port_t,
    clock_res_t);
extern kern_return_t    clock_sleep(mach_port_t,
    int,
    mach_timespec_t,
    mach_timespec_t *);
extern boolean_t voucher_mach_msg_set(mach_msg_header_t *msg);
extern void voucher_mach_msg_clear(mach_msg_header_t *msg);
extern voucher_mach_msg_state_t voucher_mach_msg_adopt(mach_msg_header_t *msg);
extern void voucher_mach_msg_revert(voucher_mach_msg_state_t state);
extern mach_msg_size_t voucher_mach_msg_fill_aux(mach_msg_aux_header_t *aux_hdr, mach_msg_size_t sz);
extern boolean_t voucher_mach_msg_fill_aux_supported(void);
__BEGIN_DECLS
char            *mach_error_string(

	mach_error_t error_value
	);
void            mach_error(

	const char      *str,
	mach_error_t    error_value
	);
char            *mach_error_type(

	mach_error_t    error_value
	);
__BEGIN_DECLS
extern mach_port_t mach_host_self(void);
extern mach_port_t mach_thread_self(void);
__API_AVAILABLE(macos(11.3), ios(14.5), tvos(14.5), watchos(7.3))
extern boolean_t mach_task_is_self(task_name_t task);
extern kern_return_t host_page_size(host_t, vm_size_t *);
OS_EXPORT OS_WARN_RESULT
mach_right_recv_t
mach_right_recv_construct(mach_right_flags_t flags,
    mach_right_send_t *_Nullable sr, uintptr_t ctx);
OS_EXPORT
void
mach_right_recv_destruct(mach_right_recv_t r, mach_right_send_t *_Nullable s,
    uintptr_t ctx);
OS_EXPORT OS_WARN_RESULT
mach_right_send_t
mach_right_send_create(mach_right_recv_t r);
OS_EXPORT OS_WARN_RESULT
mach_right_send_t
mach_right_send_retain(mach_right_send_t s);
OS_EXPORT
void
mach_right_send_release(mach_right_send_t s);
OS_EXPORT OS_WARN_RESULT
mach_right_send_once_t
mach_right_send_once_create(mach_right_recv_t r);
OS_EXPORT
void
mach_right_send_once_consume(mach_right_send_once_t so);
__BEGIN_DECLS


extern kern_return_t mach_sync_ipc_link_monitoring_start(mach_port_t* port);
extern kern_return_t mach_sync_ipc_link_monitoring_stop(mach_port_t port, boolean_t* in_effect);
extern kern_return_t thread_destruct_special_reply_port(mach_port_name_t port, thread_destruct_special_reply_port_rights_t rights);
extern mach_port_t mig_get_special_reply_port(void);
extern void mig_dealloc_special_reply_port(mach_port_t migport);
__BEGIN_DECLS


const char *mach_host_special_port_description(int offset);
const char *mach_task_special_port_description(int offset);
const char *mach_thread_special_port_description(int offset);
int mach_host_special_port_for_id(const char *id);
int mach_task_special_port_for_id(const char *id);
int mach_thread_special_port_for_id(const char *id);
__BEGIN_DECLS
extern void port_obj_init(int);
extern  vm_size_t       vm_kernel_page_size     __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
extern  vm_size_t       vm_kernel_page_mask     __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
extern  int             vm_kernel_page_shift    __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0);
__BEGIN_DECLS



typedef union {
	os_atomic(uint64_t) fcp_atomic_pos;
	uint64_t fcp_pos;
	struct {
		uint16_t fcp_next_entry_offs;
		uint16_t fcp_private_offs;
		uint8_t  fcp_refcnt;
		uint8_t  fcp_qos;
		uint8_t  fcp_stream;
		uint8_t  fcp_flag_full : 1;
		uint8_t  fcp_flag_io : 1;
		uint8_t  fcp_quarantined : 1;
		uint8_t  _fcp_flag_unused : 5;
	};
} firehose_chunk_pos_u;
OS_ASSUME_NONNULL_BEGIN

__BEGIN_DECLS


OS_OPTIONS(firehose_activity_flags, unsigned long,
    firehose_activity_flags_default             = 0x0000,

    firehose_activity_flags_info_mode           = 0x0001,
    firehose_activity_flags_debug_mode          = 0x0002,
    firehose_activity_flags_stream_live_mode    = 0x0004,

    firehose_activity_flags_precise_timestamp   = 0x0080,
    );
OS_ENUM(firehose_stream, uint8_t,
    firehose_stream_persist                     = 0,
    firehose_stream_special                     = 1,
    firehose_stream_memory                      = 2,
    firehose_stream_metadata                    = 3,
    firehose_stream_signpost                    = 4,
    firehose_stream_memory_wifi                 = 5,
    firehose_stream_memory_baseband             = 6,

    _firehose_stream_max,
    _firehose_stream_disabled = (uint8_t)-1,
    );
OS_ENUM(firehose_tracepoint_namespace, uint8_t,
    firehose_tracepoint_namespace_activity      = 0x02,
    firehose_tracepoint_namespace_trace         = 0x03,
    firehose_tracepoint_namespace_log           = 0x04,
    firehose_tracepoint_namespace_metadata      = 0x05,
    firehose_tracepoint_namespace_signpost      = 0x06,
    firehose_tracepoint_namespace_loss          = 0x07,
    );
OS_ENUM(firehose_tracepoint_code, uint32_t,
    firehose_tracepoint_code_invalid              = 0x00,
    firehose_tracepoint_code_load                 = 0x01,
    firehose_tracepoint_code_unload               = 0x02,
    firehose_tracepoint_code_load_memory          = 0x08,
    firehose_tracepoint_code_load_filesystem_ftab = 0x10,
    firehose_tracepoint_code_load_exclavekit_dsc  = 0x20,
    );
OS_OPTIONS(firehose_tracepoint_flags, uint16_t,
    _firehose_tracepoint_flags_base_has_current_aid         = 0x0001,
    _firehose_tracepoint_flags_pc_style_none                = 0x0000 << 1,
        _firehose_tracepoint_flags_pc_style_main_exe            = 0x0001 << 1,
        _firehose_tracepoint_flags_pc_style_shared_cache        = 0x0002 << 1,
        _firehose_tracepoint_flags_pc_style_main_plugin         = 0x0003 << 1,
        _firehose_tracepoint_flags_pc_style_absolute            = 0x0004 << 1,
        _firehose_tracepoint_flags_pc_style_uuid_relative       = 0x0005 << 1,
        _firehose_tracepoint_flags_pc_style_large_shared_cache  = 0x0006 << 1,
        _firehose_tracepoint_flags_pc_style__unused7            = 0x0007 << 1,
        _firehose_tracepoint_flags_base_has_unique_pid          = 0x0010,
        _firehose_tracepoint_flags_base_has_large_offset        = 0x0020,
    );
OS_ENUM(_firehose_tracepoint_type_activity, firehose_tracepoint_type_t,
    _firehose_tracepoint_type_activity_create               = 0x01,
    _firehose_tracepoint_type_activity_swap                 = 0x02,
    _firehose_tracepoint_type_activity_useraction           = 0x03,
    );
OS_OPTIONS(_firehose_tracepoint_flags_activity, uint16_t,
    _firehose_tracepoint_flags_activity_user_interface      = 0x0100,
    _firehose_tracepoint_flags_activity_has_other_aid       = 0x0200,
    );
OS_ENUM(_firehose_tracepoint_type_trace, firehose_tracepoint_type_t,
    _firehose_tracepoint_type_trace_default                 = 0x00,
    _firehose_tracepoint_type_trace_info                    = 0x01,
    _firehose_tracepoint_type_trace_debug                   = 0x02,
    _firehose_tracepoint_type_trace_error                   = 0x10,
    _firehose_tracepoint_type_trace_fault                   = 0x11,
    );
OS_ENUM(_firehose_tracepoint_type_log, firehose_tracepoint_type_t,
    _firehose_tracepoint_type_log_default                   = 0x00,
    _firehose_tracepoint_type_log_info                      = 0x01,
    _firehose_tracepoint_type_log_debug                     = 0x02,
    _firehose_tracepoint_type_log_error                     = 0x10,
    _firehose_tracepoint_type_log_fault                     = 0x11,
    );
OS_OPTIONS(_firehose_tracepoint_flags_log, uint16_t,
    _firehose_tracepoint_flags_log_has_private_data         = 0x0100,
    _firehose_tracepoint_flags_log_has_subsystem            = 0x0200,
    _firehose_tracepoint_flags_log_has_rules                = 0x0400,
    _firehose_tracepoint_flags_log_has_oversize             = 0x0800,
    _firehose_tracepoint_flags_log_has_context_data         = 0x1000,
    );
OS_ENUM(_firehose_tracepoint_type_metadata, firehose_tracepoint_type_t,
    _firehose_tracepoint_type_metadata_dyld                 = 0x01,
    _firehose_tracepoint_type_metadata_subsystem            = 0x02,
    _firehose_tracepoint_type_metadata_kext                 = 0x03,
    _firehose_tracepoint_type_metadata_coprocessor          = 0x04,
    _firehose_tracepoint_type_metadata_exclave              = 0x05,
    );
OS_ENUM(_firehose_tracepoint_type_signpost, firehose_tracepoint_type_t,
    _firehose_tracepoint_type_signpost_event                = 0x00,
    _firehose_tracepoint_type_signpost_interval_begin       = 0x01,
    _firehose_tracepoint_type_signpost_interval_end         = 0x02,

    _firehose_tracepoint_type_signpost_scope_mask           = 0xc0,
    _firehose_tracepoint_type_signpost_scope_thread         = 0x40,
    _firehose_tracepoint_type_signpost_scope_process        = 0x80,
    _firehose_tracepoint_type_signpost_scope_system         = 0xc0,
    );
OS_OPTIONS(_firehose_tracepoint_flags_signpost, uint16_t,
    
    _firehose_tracepoint_flags_signpost_has_private_data    = 0x0100,
    _firehose_tracepoint_flags_signpost_has_subsystem       = 0x0200,
    _firehose_tracepoint_flags_signpost_has_rules           = 0x0400,
    _firehose_tracepoint_flags_signpost_has_oversize        = 0x0800,
    _firehose_tracepoint_flags_signpost_has_context_data    = 0x1000,

    
    _firehose_tracepoint_flags_signpost_has_name            = 0x8000,
    );
OS_ASSUME_NONNULL_BEGIN


typedef union {
	struct {
		firehose_tracepoint_namespace_t _namespace;
		firehose_tracepoint_type_t _type;
		firehose_tracepoint_flags_t _flags;
		uint32_t _code;
	} ftid;
	firehose_tracepoint_id_t ftid_value;
	os_atomic(firehose_tracepoint_id_t) ftid_atomic_value;
} firehose_tracepoint_id_u;
STAILQ_HEAD(kxld_array_head, kxld_array_pool);
kern_return_t kxld_array_init(KXLDArray *array, size_t itemsize, u_int nitems)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_array_copy(KXLDArray *array, const KXLDArray *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_array_reset(KXLDArray *array)
__attribute__((visibility("hidden")));
void kxld_array_clear(KXLDArray *array)
__attribute__((visibility("hidden")));
void kxld_array_deinit(KXLDArray *array)
__attribute__((visibility("hidden")));
void *kxld_array_get_item(const KXLDArray *array, u_int idx)
__attribute__((pure, nonnull, visibility("hidden")));
void *kxld_array_get_slot(const KXLDArray *array, u_int idx)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_array_get_index(const KXLDArray *array, const void *item,
    u_int *idx)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_array_resize(KXLDArray *array, u_int nitems)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_array_remove(KXLDArray *array, u_int idx)
__attribute__((nonnull, visibility("hidden")));
const char * kxld_demangle(const char *str, char **buffer, size_t *length)
__attribute__((nonnull(1), visibility("hidden")));
kern_return_t kxld_dict_init(KXLDDict *dict, kxld_dict_hash hash,
    kxld_dict_cmp cmp, u_int num_entries)
__attribute__((nonnull, visibility("hidden")));
void kxld_dict_iterator_init(KXLDDictIterator *iter, const KXLDDict *dict)
__attribute__((nonnull, visibility("hidden")));
void kxld_dict_clear(KXLDDict *dict)
__attribute__((nonnull, visibility("hidden")));
void kxld_dict_deinit(KXLDDict *dict)
__attribute__((nonnull, visibility("hidden")));
u_int kxld_dict_get_num_entries(const KXLDDict *dict)
__attribute__((pure, nonnull, visibility("hidden")));
void * kxld_dict_find(const KXLDDict *dict, const void *key)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_dict_insert(KXLDDict *dict, const void *key, void *value)
__attribute__((nonnull, visibility("hidden")));
void kxld_dict_remove(KXLDDict *dict, const void *key, void **value)
__attribute__((nonnull(1, 2), visibility("hidden")));
void kxld_dict_iterator_get_next(KXLDDictIterator *iter, const void **key,
    void **value)
__attribute__((nonnull, visibility("hidden")));
void kxld_dict_iterator_reset(KXLDDictIterator *iter)
__attribute__((nonnull, visibility("hidden")));
u_int kxld_dict_string_hash(const KXLDDict *dict, const void *key)
__attribute__((pure, nonnull, visibility("hidden")));
u_int kxld_dict_uint32_hash(const KXLDDict *dict, const void *key)
__attribute__((pure, nonnull, visibility("hidden")));
u_int kxld_dict_kxldaddr_hash(const KXLDDict *dict, const void *key)
__attribute__((pure, nonnull, visibility("hidden")));
u_int kxld_dict_string_cmp(const void *key1, const void *key2)
__attribute__((pure, visibility("hidden")));
u_int kxld_dict_uint32_cmp(const void *key1, const void *key2)
__attribute__((pure, visibility("hidden")));
u_int kxld_dict_kxldaddr_cmp(const void *key1, const void *key2)
__attribute__((pure, visibility("hidden")));
size_t kxld_kext_sizeof(void)
__attribute__((const, visibility("hidden")));
kern_return_t kxld_kext_init(KXLDKext *kext, struct kxld_object *kext_object,
    struct kxld_object *interface_object)
__attribute__((nonnull(1, 2), visibility("hidden")));
void kxld_kext_clear(KXLDKext *kext)
__attribute__((nonnull, visibility("hidden")));
void kxld_kext_deinit(KXLDKext *kext)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_kext_export_symbols(const KXLDKext *kext,
    struct kxld_dict *defined_symbols_by_name,
    struct kxld_dict *obsolete_symbols_by_name,
    struct kxld_dict *defined_cxx_symbols_by_value)
__attribute__((nonnull(1), visibility("hidden")));
void kxld_kext_get_vmsize_for_seg_by_name(const KXLDKext *kext,
    const char *segname,
    u_long *vmsize)
__attribute__((nonnull, visibility("hidden")));
void kxld_kext_get_vmsize(const KXLDKext *kext,
    u_long *header_size, u_long *vmsize)
__attribute__((nonnull, visibility("hidden")));
void kxld_kext_set_linked_object_size(KXLDKext *kext, u_long vmsize)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_kext_export_linked_object(const KXLDKext *kext,
    void *linked_object,
    kxld_addr_t *kmod_info)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_kext_export_vtables(KXLDKext *kext,
    const struct kxld_dict *defined_cxx_symbols,
    const struct kxld_dict *defined_symbols,
    struct kxld_dict *vtables)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_kext_relocate(KXLDKext *kext,
    kxld_addr_t link_address,
    struct kxld_dict *patched_vtables,
    const struct kxld_dict *defined_symbols,
    const struct kxld_dict *obsolete_symbols,
    const struct kxld_dict *defined_cxx_symbols)
__attribute__((nonnull(1, 3, 4), visibility("hidden")));
size_t kxld_object_sizeof(void)
__attribute__((const, visibility("hidden")));
kern_return_t kxld_object_init_from_macho(KXLDObject *object,
    u_char *file, u_long size, const char *name,
    struct kxld_array *section_order,
    cpu_type_t cputype, cpu_subtype_t cpusubtype, KXLDFlags flags)
__attribute__((nonnull(1, 2, 4), visibility("hidden")));
void kxld_object_clear(KXLDObject *object)
__attribute__((nonnull, visibility("hidden")));
void kxld_object_deinit(KXLDObject *object)
__attribute__((nonnull, visibility("hidden")));
const u_char * kxld_object_get_file(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
const char * kxld_object_get_name(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_is_32_bit(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_is_final_image(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_is_kernel(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_is_linked(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_target_supports_strict_patching(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_object_target_supports_common_symbols(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_relocator * kxld_object_get_relocator(
	const KXLDObject * object)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_reloc * kxld_object_get_reloc_at_symbol(
	const KXLDObject *object, const struct kxld_sym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_sym * kxld_object_get_symbol_of_reloc(
	const KXLDObject *object, const struct kxld_reloc *reloc,
	const struct kxld_sect *sect)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_sect * kxld_object_get_section_by_index(
	const KXLDObject *object, u_int sectnum)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_array * kxld_object_get_extrelocs(
	const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
const struct kxld_symtab * kxld_object_get_symtab(const KXLDObject *object)
__attribute__((pure, nonnull, visibility("hidden")));
void kxld_object_get_vmsize(const KXLDObject *object, u_long *header_size,
    u_long *vmsize)
__attribute__((nonnull, visibility("hidden")));
void kxld_object_set_linked_object_size(KXLDObject *object, u_long vmsize)
__attribute__((nonnull, visibility("hidden")));
void kxld_object_get_vmsize_for_seg_by_name(const KXLDObject *object,
    const char *segname,
    u_long *vmsize)
__attribute__((nonnull, visibility("hidden")));
splitKextLinkInfo * kxld_object_get_link_info(KXLDObject *object)
__attribute__((nonnull, visibility("hidden")));
void kxld_object_set_link_info(KXLDObject *object,
    splitKextLinkInfo *link_info)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_export_linked_object(const KXLDObject *object,
    void *linked_object
    )
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_index_symbols_by_name(KXLDObject *object)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_index_cxx_symbols_by_value(KXLDObject *object)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_relocate(KXLDObject *object, kxld_addr_t link_address)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_resolve_symbol(KXLDObject *object,
    const struct kxld_sym *sym, kxld_addr_t addr)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_patch_symbol(KXLDObject *object,
    const struct kxld_sym *sym)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_add_symbol(KXLDObject *object, char *name,
    kxld_addr_t link_addr, const struct kxld_sym **sym_out)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_object_process_relocations(KXLDObject *object,
    const struct kxld_dict *patched_vtables)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_relocator_init(KXLDRelocator *relocator, u_char *file,
    const struct kxld_symtab *symtab, const struct kxld_array *sectarray,
    cpu_type_t cputype, cpu_subtype_t cpusubtype, boolean_t swap)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_reloc_create_macho(struct kxld_array *relocarray,
    const KXLDRelocator *relocator, const struct relocation_info *srcs,
    u_int nsrcs) __attribute__((nonnull, visibility("hidden")));
void kxld_relocator_clear(KXLDRelocator *relocator)
__attribute__((nonnull, visibility("hidden")));
boolean_t kxld_relocator_has_pair(const KXLDRelocator *relocator, u_int r_type)
__attribute__((pure, nonnull, visibility("hidden")));
u_int kxld_relocator_get_pair_type(const KXLDRelocator *relocator,
    u_int last_r_type)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_relocator_has_got(const KXLDRelocator *relocator, u_int r_type)
__attribute__((pure, nonnull, visibility("hidden")));
kxld_addr_t kxld_relocator_get_pointer_at_addr(const KXLDRelocator *relocator,
    const u_char *data, u_long offset)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_reloc_get_reloc_index_by_offset(const struct kxld_array *relocs,
    kxld_size_t offset, u_int *idx)
__attribute__((nonnull, visibility("hidden")));
KXLDReloc * kxld_reloc_get_reloc_by_offset(const struct kxld_array *relocs,
    kxld_addr_t offset)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_reloc_update_symindex(KXLDReloc *reloc, u_int symindex)
__attribute__((nonnull, visibility("hidden")));
void kxld_relocator_set_vtables(KXLDRelocator *relocator,
    const struct kxld_dict *vtables)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_relocator_process_sect_reloc(KXLDRelocator *relocator,
    const KXLDReloc *reloc, const struct kxld_sect *sect)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_relocator_process_table_reloc(KXLDRelocator *relocator,
    const KXLDReloc *reloc,
    const struct kxld_seg *seg,
    kxld_addr_t link_addr)
__attribute__((nonnull, visibility("hidden")));
void kxld_sect_clear(KXLDSect *sect)
__attribute__((nonnull, visibility("hidden")));
void kxld_sect_deinit(KXLDSect *sect)
__attribute__((nonnull, visibility("hidden")));
u_int kxld_sect_get_num_relocs(const KXLDSect *sect)
__attribute__((pure, nonnull, visibility("hidden")));
kxld_addr_t kxld_sect_align_address(const KXLDSect *sect, kxld_addr_t address)
__attribute__((pure, nonnull, visibility("hidden")));
u_long kxld_sect_get_macho_header_size(boolean_t is_32_bit)
__attribute__((const, visibility("hidden")));
u_long kxld_sect_get_macho_data_size(const KXLDSect *sect)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_sect_export_macho_to_file_buffer(const KXLDSect *sect, u_char *buf,
    u_long *header_offset, u_long header_size, u_long *data_offset,
    u_long data_size, boolean_t is_32_bit)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sect_export_macho_to_vm(const KXLDSect *sect, u_char *buf,
    u_long *header_offset,
    u_long header_size,
    kxld_addr_t link_addr,
    u_long data_size,
    boolean_t is_32_bit)
__attribute__((nonnull, visibility("hidden")));
void kxld_sect_relocate(KXLDSect *sect, kxld_addr_t link_addr)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sect_process_relocs(KXLDSect *sect,
    struct kxld_relocator *relocator)
__attribute__((nonnull, visibility("hidden")));
void kxld_seg_clear(KXLDSeg *seg)
__attribute__((nonnull, visibility("hidden")));
void kxld_seg_deinit(KXLDSeg *seg)
__attribute__((nonnull, visibility("hidden")));
kxld_size_t kxld_seg_get_vmsize(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
u_long kxld_seg_get_macho_header_size(const KXLDSeg *seg, boolean_t is_32_bit)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t
kxld_seg_export_macho_to_file_buffer(const KXLDSeg *seg, u_char *buf,
    u_long *header_offset, u_long header_size,
    u_long *data_offset, u_long data_size,
    boolean_t is_32_bit)
__attribute__((nonnull, visibility("hidden")));
kern_return_t
kxld_seg_export_macho_to_vm(const KXLDSeg *seg,
    u_char *buf,
    u_long *header_offset,
    u_long header_size,
    u_long data_size,
    kxld_addr_t file_link_addr,
    boolean_t is_32_bit)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_seg_add_section(KXLDSeg *seg, struct kxld_sect *sect)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_seg_finish_init(KXLDSeg *seg)
__attribute__((nonnull, visibility("hidden")));
void kxld_seg_set_vm_protections(KXLDSeg *seg, boolean_t strict_protections)
__attribute__((nonnull, visibility("hidden")));
void kxld_seg_relocate(KXLDSeg *seg, kxld_addr_t link_addr)
__attribute__((nonnull, visibility("hidden")));
void kxld_seg_populate_linkedit(KXLDSeg *seg, const struct kxld_symtab *symtab,
    boolean_t is_32_bit
    , uint32_t splitinfolc_size
    )
__attribute__((nonnull, visibility("hidden")));
boolean_t kxld_seg_is_split_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_text_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_text_exec_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_data_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_data_const_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_linkedit_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_seg_is_llvm_cov_seg(const KXLDSeg *seg)
__attribute__((pure, nonnull, visibility("hidden")));
void kxld_splitinfolc_init_from_macho(KXLDsplitinfolc *splitinfolc, struct linkedit_data_command *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_splitinfolc_clear(KXLDsplitinfolc *splitinfolc)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_splitinfolc_get_macho_header_size(void)
__attribute__((pure, visibility("hidden")));
kern_return_t
kxld_splitinfolc_export_macho(const KXLDsplitinfolc *splitinfolc,
    splitKextLinkInfo *linked_object,
    u_long *header_offset,
    u_long header_size,
    u_long *data_offset,
    u_long size)
__attribute__((pure, nonnull, visibility("hidden")));
void kxld_srcversion_init_from_macho(KXLDsrcversion *srcversion, struct source_version_command *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_srcversion_clear(KXLDsrcversion *srcversion)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_srcversion_get_macho_header_size(void)
__attribute__((pure, visibility("hidden")));
kern_return_t
kxld_srcversion_export_macho(const KXLDsrcversion *srcversion, u_char *buf,
    u_long *header_offset, u_long header_size)
__attribute__((pure, nonnull, visibility("hidden")));
void kxld_sym_init_absolute(KXLDSym *sym, char *name, kxld_addr_t link_addr)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_deinit(KXLDSym *sym)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_destroy(KXLDSym *sym)
__attribute__((nonnull, visibility("hidden")));
boolean_t kxld_sym_is_absolute(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_section(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_defined(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_defined_locally(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_external(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_exported(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_undefined(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_indirect(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_replaced(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_common(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_unresolved(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_obsolete(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_stab(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_weak(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_cxx(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_pure_virtual(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_vtable(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_class_vtable(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_metaclass_vtable(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_padslot(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_metaclass(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_is_super_metaclass_pointer(const KXLDSym *sym)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_name_is_pure_virtual(const char *name)
__attribute__((pure, nonnull, visibility("hidden")));
boolean_t kxld_sym_name_is_padslot(const char *name)
__attribute__((pure, nonnull, visibility("hidden")));
u_int kxld_sym_get_section_offset(const KXLDSym *sym,
    const struct kxld_sect *sect)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_class_name_from_metaclass(const KXLDSym *sym,
    char class_name[], u_long class_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_class_name_from_super_metaclass_pointer(
	const KXLDSym *sym, char class_name[], u_long class_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_class_name_from_vtable(const KXLDSym *sym,
    char class_name[], u_long class_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_class_name_from_vtable_name(const char *vtable_name,
    char class_name[], u_long class_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_vtable_name_from_class_name(const char *class_name,
    char vtable_name[], u_long vtable_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_meta_vtable_name_from_class_name(const char *class_name,
    char meta_vtable_name[], u_long meta_vtable_name_len)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_get_final_sym_name_from_class_name(const char *class_name,
    char final_sym_name[], u_long final_sym_name_len)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_sym_get_function_prefix_from_class_name(const char *class_name,
    char function_prefix[], u_long function_prefix_len)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_relocate(KXLDSym *sym, const struct kxld_sect *sect)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_sym_resolve(KXLDSym *sym, const kxld_addr_t addr)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_delete(KXLDSym *sym)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_patch(KXLDSym *sym)
__attribute__((nonnull, visibility("hidden")));
void kxld_sym_mark_private(KXLDSym *sym)
__attribute__((nonnull, visibility("hidden")));
size_t kxld_symtab_sizeof(void)
__attribute__((const, visibility("hidden")));
void kxld_symtab_iterator_init(KXLDSymtabIterator *iter,
    const KXLDSymtab *symtab, KXLDSymPredicateTest test, boolean_t negate)
__attribute__((nonnull, visibility("hidden")));
void kxld_symtab_clear(KXLDSymtab *symtab)
__attribute__((nonnull, visibility("hidden")));
void kxld_symtab_deinit(KXLDSymtab *symtab)
__attribute__((nonnull, visibility("hidden")));
u_int kxld_symtab_get_num_symbols(const KXLDSymtab *symtab)
__attribute__((pure, nonnull, visibility("hidden")));
KXLDSym * kxld_symtab_get_symbol_by_index(const KXLDSymtab *symtab, u_int idx)
__attribute__((pure, nonnull, visibility("hidden")));
KXLDSym * kxld_symtab_get_symbol_by_name(const KXLDSymtab *symtab,
    const char *name)
__attribute__((pure, nonnull, visibility("hidden")));
KXLDSym * kxld_symtab_get_locally_defined_symbol_by_name(
	const KXLDSymtab *symtab, const char *name)
__attribute__((pure, nonnull, visibility("hidden")));
KXLDSym * kxld_symtab_get_cxx_symbol_by_value(const KXLDSymtab *symtab,
    kxld_addr_t value)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_symtab_get_sym_index(const KXLDSymtab *symtab,
    const KXLDSym * sym, u_int *idx)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_symtab_get_macho_header_size(void)
__attribute__((pure, visibility("hidden")));
u_long kxld_symtab_get_macho_data_size(const KXLDSymtab *symtab,
    boolean_t is_32_bit)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t
kxld_symtab_export_macho(const KXLDSymtab *symtab, u_char *buf,
    u_long *header_offset, u_long header_size,
    u_long *data_offset, u_long data_size,
    boolean_t is_32_bit)
__attribute__((nonnull, visibility("hidden")));
u_int kxld_symtab_iterator_get_num_remaining(const KXLDSymtabIterator *iter)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_symtab_index_symbols_by_name(KXLDSymtab *symtab)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_symtab_index_cxx_symbols_by_value(KXLDSymtab *symtab)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_symtab_relocate(KXLDSymtab *symtab,
    const struct kxld_array *sectarray)
__attribute__((nonnull, visibility("hidden")));
kern_return_t kxld_symtab_add_symbol(KXLDSymtab *symtab, char *name,
    kxld_addr_t link_addr, KXLDSym **symout)
__attribute__((nonnull, visibility("hidden")));
KXLDSym * kxld_symtab_iterator_get_next(KXLDSymtabIterator *iter)
__attribute__((nonnull, visibility("hidden")));
void kxld_symtab_iterator_reset(KXLDSymtabIterator *iter)
__attribute__((nonnull, visibility("hidden")));
void kxld_set_logging_callback(KXLDLoggingCallback logging_callback)
__attribute__((visibility("hidden")));
void kxld_set_logging_callback_data(const char * name, void *user_data)
__attribute__((visibility("hidden")));
void kxld_log(KXLDLogSubsystem subsystem, KXLDLogLevel level,
    const char *format, ...)
__attribute__((visibility("hidden"), format(printf, 3, 4)));
void * kxld_calloc(size_t size)
__attribute__((malloc, visibility("hidden")));
void * kxld_alloc(size_t size)
__attribute__((malloc, visibility("hidden")));
void * kxld_page_alloc(size_t size)
__attribute__((malloc, visibility("hidden")));
void * kxld_page_alloc_untracked(size_t size)
__attribute__((malloc, visibility("hidden")));
void kxld_free(void *ptr, size_t size)
__attribute__((visibility("hidden")));
void kxld_page_free(void *ptr, size_t size)
__attribute__((visibility("hidden")));
void kxld_page_free_untracked(void *ptr, size_t size)
__attribute__((visibility("hidden")));
kern_return_t validate_and_swap_macho_32(u_char *file, u_long size
    ) __attribute__((visibility("hidden")));
kern_return_t validate_and_swap_macho_64(u_char *file, u_long size
    ) __attribute__((visibility("hidden")));
kxld_addr_t kxld_align_address(kxld_addr_t address, u_int align)
__attribute__((const, visibility("hidden")));
boolean_t kxld_is_32_bit(cpu_type_t)
__attribute__((const, visibility("hidden")));
void kxld_print_memory_report(void)
__attribute__((visibility("hidden")));
kxld_size_t kxld_get_effective_page_size(void);
kxld_addr_t kxld_round_page_cross_safe(kxld_addr_t addr);
void kxld_uuid_init_from_macho(KXLDuuid *uuid, struct uuid_command *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_uuid_clear(KXLDuuid *uuid)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_uuid_get_macho_header_size(void)
__attribute__((pure, visibility("hidden")));
kern_return_t
kxld_uuid_export_macho(const KXLDuuid *uuid, u_char *buf,
    u_long *header_offset, u_long header_size)
__attribute__((pure, nonnull, visibility("hidden")));
void kxld_versionmin_init_from_macho(KXLDversionmin *versionmin, struct version_min_command *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_versionmin_init_from_build_cmd(KXLDversionmin *versionmin, struct build_version_command *src)
__attribute__((nonnull, visibility("hidden")));
void kxld_versionmin_clear(KXLDversionmin *versionmin)
__attribute__((nonnull, visibility("hidden")));
u_long kxld_versionmin_get_macho_header_size(const KXLDversionmin *versionmin)
__attribute__((pure, visibility("hidden")));
kern_return_t
kxld_versionmin_export_macho(const KXLDversionmin *versionmin, u_char *buf,
    u_long *header_offset, u_long header_size)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_vtable_init(KXLDVTable *vtable,
    const struct kxld_sym *vtable_sym, const struct kxld_object *object,
    const struct kxld_dict *defined_cxx_symbols)
__attribute__((nonnull, visibility("hidden")));
void kxld_vtable_clear(KXLDVTable *vtable)
__attribute__((visibility("hidden")));
void kxld_vtable_deinit(KXLDVTable *vtable)
__attribute__((visibility("hidden")));
KXLDVTableEntry * kxld_vtable_get_entry_for_offset(const KXLDVTable *vtable,
    u_long offset, boolean_t is_32_bit)
__attribute__((pure, nonnull, visibility("hidden")));
kern_return_t kxld_vtable_patch(KXLDVTable *vtable, const KXLDVTable *super_vtable,
    struct kxld_object *object)
__attribute__((nonnull, visibility("hidden")));
BLOCK_EXPORT void *_Block_copy(const void *aBlock);
BLOCK_EXPORT void _Block_release(const void *aBlock);
BLOCK_EXPORT void _Block_object_assign(void *, const void *, const int);
BLOCK_EXPORT void _Block_object_dispose(const void *, const int);
static inline __typeof__(void (*)(void *, ...))
_Block_get_invoke_fn(struct Block_layout *block)
{
	return (void (*)(void *, ...))_Block_get_function_pointer(block->invoke);
}

static inline void
_Block_set_invoke_fn(struct Block_layout *block, void (*fn)(void *, ...))
{
	_Block_set_function_pointer(block->invoke, fn);
}

static inline void *
_Block_get_descriptor(struct Block_layout *aBlock)
{
}

static inline __typeof__(void (*)(void *, const void *))
_Block_get_copy_fn(struct Block_descriptor_2 *desc)
{
	return (void (*)(void *, const void *))_Block_get_function_pointer(desc->copy);
}

static inline void
_Block_set_copy_fn(struct Block_descriptor_2 *desc,
    void (*fn)(void *, const void *))
{
	_Block_set_function_pointer(desc->copy, fn);
}


static inline __typeof__(void (*)(const void *))
_Block_get_dispose_fn(struct Block_descriptor_2 *desc)
{
	return (void (*)(const void *))_Block_get_function_pointer(desc->dispose);
}

static inline void
_Block_set_dispose_fn(struct Block_descriptor_2 *desc,
    void (*fn)(const void *))
{
	_Block_set_function_pointer(desc->dispose, fn);
}

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-align"

static inline __typeof__(void (*)(void *, const void *))
_Block_get_copy_function(struct Block_layout *aBlock)
{
	if (!(aBlock->flags & BLOCK_HAS_COPY_DISPOSE)) {
		return NULL;
	}


	void *desc = _Block_get_descriptor(aBlock);
	struct Block_descriptor_2 *bd2 =
	    (struct Block_descriptor_2 *)((unsigned char *)desc +
	    sizeof(struct Block_descriptor_1));
	return _Block_get_copy_fn(bd2);
}

static inline __typeof__(void (*)(const void *))
_Block_get_dispose_function(struct Block_layout *aBlock)
{
	if (!(aBlock->flags & BLOCK_HAS_COPY_DISPOSE)) {
		return NULL;
	}


	void *desc = _Block_get_descriptor(aBlock);
	struct Block_descriptor_2 *bd2 =
	    (struct Block_descriptor_2 *)((unsigned char *)desc +
	    sizeof(struct Block_descriptor_1));
	return _Block_get_dispose_fn(bd2);
}

#pragma clang diagnostic pop





BLOCK_EXPORT size_t Block_size(void *aBlock);
BLOCK_EXPORT bool _Block_has_signature(void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT bool _Block_use_stret(void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT const char * _Block_signature(void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT const char * _Block_layout(void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT const char * _Block_extended_layout(void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_8, __IPHONE_7_0);
BLOCK_EXPORT bool _Block_tryRetain(const void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT bool _Block_isDeallocating(const void *aBlock)
__OSX_AVAILABLE_STARTING(__MAC_10_7, __IPHONE_4_3);
BLOCK_EXPORT void * _NSConcreteMallocBlock[32]
__OSX_AVAILABLE_STARTING(__MAC_10_6, __IPHONE_3_2);
BLOCK_EXPORT void * _NSConcreteAutoBlock[32]
__OSX_AVAILABLE_STARTING(__MAC_10_6, __IPHONE_3_2);
BLOCK_EXPORT void * _NSConcreteFinalizingBlock[32]
__OSX_AVAILABLE_STARTING(__MAC_10_6, __IPHONE_3_2);
BLOCK_EXPORT void * _NSConcreteWeakBlockVariable[32]
__OSX_AVAILABLE_STARTING(__MAC_10_6, __IPHONE_3_2);
__BEGIN_DECLS

uint32_t        crc32(uint32_t crc, const void *bufp, size_t len);
vm_offset_t getlastaddr(kernel_mach_header_t *header);
vm_offset_t getlastkerneladdr(void);
kernel_segment_command_t *firstseg(void);
kernel_segment_command_t *firstsegfromheader(kernel_mach_header_t *header);
kernel_segment_command_t *nextsegfromheader(
	kernel_mach_header_t    *header,
	kernel_segment_command_t        *seg);
kernel_segment_command_t *getsegbyname(const char *seg_name);
kernel_segment_command_t *getsegbynamefromheader(
	kernel_mach_header_t    *header,
	const char              *seg_name);
void *getsegdatafromheader(kernel_mach_header_t *, const char *, unsigned long *);
kernel_section_t *getsectbyname(const char *seg_name, const char *sect_name);
kernel_section_t *getsectbynamefromheader(
	kernel_mach_header_t    *header,
	const char              *seg_name,
	const char              *sect_name);
kernel_section_t *getsectbynamefromseg(
	kernel_segment_command_t        *sgp,
	const char                      *segname,
	const char                      *sectname);
uint32_t getsectoffsetfromheader(
	kernel_mach_header_t *mhp,
	const char *segname,
	const char *sectname);
void *getsectdatafromheader(kernel_mach_header_t *, const char *, const char *, unsigned long *);
kernel_section_t *firstsect(kernel_segment_command_t *sgp);
kernel_section_t *nextsect(kernel_segment_command_t *sgp, kernel_section_t *sp);
void *getcommandfromheader(kernel_mach_header_t *, uint32_t);
void *getuuidfromheader(kernel_mach_header_t *, unsigned long *);
bool kernel_text_contains(vm_offset_t);
__BEGIN_DECLS



typedef 


subs_entry_t kext_identifier_prefix_subs[] = {
	{ "com.apple.driver.", '>' },
	{ "com.apple.iokit.", '|' },
	{ "com.apple.security.", '$' },
	{ "com.apple.", '@' },

	{ (char *)NULL, '\0' }
};
subs_entry_t kext_identifier_substring_subs[] = {
	{ "AppleUSB", 'U' },
	{ "Apple", 'A' },
	{ "Family", 'F' },
	{ "Storage", 'S' },
	{ "Controller", 'C' },
	{ "Bluetooth", 'B' },
	{ "Intel", 'I' },

	{ (char *)NULL, '\0' }
};
kern_return_t kxld_create_context(
	KXLDContext **context,
	KXLDAllocateCallback allocate_callback,
	KXLDLoggingCallback log_callback,
	KXLDFlags flags,
	cpu_type_t cputype,
	cpu_subtype_t cpusubtype,
	vm_size_t pagesize)
__attribute__((nonnull(1), visibility("default")));
void kxld_destroy_context(
	KXLDContext *context)
__attribute__((nonnull, visibility("default")));
kern_return_t kxld_link_file(
	KXLDContext *context,
	u_char *file,
	u_long size,
	const char *name,
	void *callback_data,
	KXLDDependency *dependencies,
	u_int num_dependencies,
	u_char **linked_object,
	kxld_addr_t *kmod_info_kern)
__attribute__((nonnull(1, 2, 4, 6, 8, 9), visibility("default")));
kern_return_t kxld_link_split_file(
	KXLDContext *context,
	splitKextLinkInfo *link_info,
	const char *name,
	void *callback_data,
	KXLDDependency *dependencies,
	u_int num_dependencies,
	kxld_addr_t *kmod_info_kern)
__attribute__((nonnull(1, 2, 3, 5, 7), visibility("default")));
boolean_t kxld_validate_copyright_string(const char *str)
__attribute__((pure, nonnull, visibility("default")));
__BEGIN_DECLS
u_int8_t *
compress_lzss(u_int8_t * dst, u_int32_t dstlen,
    u_int8_t * src, u_int32_t srclen);
int
decompress_lzss(u_int8_t * dst, u_int32_t dstlen,
    u_int8_t * src, u_int32_t srclen);
u_int32_t
mkext_adler32(u_int8_t * src, int32_t length);
extern Boolean OSCompareAndSwap64(
	UInt64            oldValue,
	UInt64            newValue,
	volatile UInt64 * address);
extern SInt64 OSAddAtomic64(
	SInt64            theAmount,
	volatile SInt64 * address);
inline static SInt64
OSIncrementAtomic64(volatile SInt64 * address)
{
	return OSAddAtomic64(1LL, address);
}


inline static SInt64
OSDecrementAtomic64(volatile SInt64 * address)
{
	return OSAddAtomic64(-1LL, address);
}


extern long OSAddAtomicLong(
	long            theAmount,
	volatile long * address);
inline static long
OSIncrementAtomicLong(volatile long * address)
{
	return OSAddAtomicLong(1L, address);
}


inline static long
OSDecrementAtomicLong(volatile long * address)
{
	return OSAddAtomicLong(-1L, address);
}


extern Boolean OSCompareAndSwap8(
	UInt8            oldValue,
	UInt8            newValue,
	volatile UInt8 * address);
extern Boolean OSCompareAndSwap16(
	UInt16            oldValue,
	UInt16            newValue,
	volatile UInt16 * address);
extern Boolean OSCompareAndSwap(
	UInt32            oldValue,
	UInt32            newValue,
	volatile UInt32 * address);
extern Boolean OSCompareAndSwapPtr(
	void            * oldValue,
	void            * newValue,
	void * volatile * address);
extern SInt32 OSAddAtomic(
	SInt32            amount,
	volatile SInt32 * address);
extern SInt16 OSAddAtomic16(
	SInt32            amount,
	volatile SInt16 * address);
extern SInt8 OSAddAtomic8(
	SInt32           amount,
	volatile SInt8 * address);
extern SInt32 OSIncrementAtomic(volatile SInt32 * address);
extern SInt16 OSIncrementAtomic16(volatile SInt16 * address);
extern SInt8 OSIncrementAtomic8(volatile SInt8 * address);
extern SInt32 OSDecrementAtomic(volatile SInt32 * address);
extern SInt16 OSDecrementAtomic16(volatile SInt16 * address);
extern SInt8 OSDecrementAtomic8(volatile SInt8 * address);
extern UInt32 OSBitAndAtomic(
	UInt32            mask,
	volatile UInt32 * address);
extern UInt16 OSBitAndAtomic16(
	UInt32            mask,
	volatile UInt16 * address);
extern UInt8 OSBitAndAtomic8(
	UInt32           mask,
	volatile UInt8 * address);
extern UInt32 OSBitOrAtomic(
	UInt32            mask,
	volatile UInt32 * address);
extern UInt16 OSBitOrAtomic16(
	UInt32            mask,
	volatile UInt16 * address);
extern UInt8 OSBitOrAtomic8(
	UInt32           mask,
	volatile UInt8 * address);
extern UInt32 OSBitXorAtomic(
	UInt32            mask,
	volatile UInt32 * address);
extern UInt16 OSBitXorAtomic16(
	UInt32            mask,
	volatile UInt16 * address);
extern UInt8 OSBitXorAtomic8(
	UInt32           mask,
	volatile UInt8 * address);
extern Boolean OSTestAndSet(
	UInt32           bit,
	volatile UInt8 * startAddress);
extern Boolean OSTestAndClear(
	UInt32           bit,
	volatile UInt8 * startAddress);
extern Boolean OSSpinLockTry(volatile OSSpinLock * lock);
extern void OSSpinLockUnlock(volatile OSSpinLock * lock);
extern void OSSynchronizeIO(void);
__BEGIN_DECLS


extern void OSReportWithBacktrace(const char *str, ...) __printflike(1, 2);
extern unsigned OSBacktrace(void **bt, unsigned maxAddrs);
extern void OSPrintBacktrace(void);
vm_offset_t OSKernelStackRemaining( void );
OSReturn OSKextLoadKextWithIdentifier(const char * kextIdentifier);
OSReturn OSKextRetainKextWithLoadTag(OSKextLoadTag loadTag);
OSReturn OSKextReleaseKextWithLoadTag(OSKextLoadTag loadTag);
OSReturn OSKextRequestResource(
	const char                    * kextIdentifier,
	const char                    * resourceName,
	OSKextRequestResourceCallback   callback,
	void                          * context,
	OSKextRequestTag              * requestTagOut);
OSReturn OSKextCancelRequest(
	OSKextRequestTag    requestTag,
	void             ** contextOut);
int
OSKextGrabPgoData(uuid_t uuid,
    uint64_t *pSize,
    char *pBuffer,
    uint64_t bufferSize,
    int wait_for_unload,
    int metadata);
void
OSKextResetPgoCountersLock(void);
void
OSKextResetPgoCountersUnlock(void);
void
OSKextResetPgoCounters(void);
OSKextVersion OSKextParseVersionString(const char * versionString);
Boolean OSKextVersionGetString(
	OSKextVersion   aVersion,
	char          * buffer,
	uint32_t        bufferSize);
void kext_weak_symbol_referenced(void) __abortlike;
vm_map_t kext_get_vm_map(kmod_info_t * info);
void OSKextRegisterKextsWithDTrace(void);
void kext_dump_panic_lists(int (*printf_func)(const char *fmt, ...));
uint32_t OSKextGetLoadTagForBundleIdentifier(
	const char * kextIdentifier);
OSReturn OSKextUnloadKextWithLoadTag(uint32_t loadTag);
void OSKextLoadedKextSummariesUpdated(void);
extern const vm_allocation_site_t * OSKextGetAllocationSiteForCaller(uintptr_t address);
extern uint32_t                     OSKextGetKmodIDForSite(const vm_allocation_site_t * site,
    char * name, vm_size_t namelen);
extern void                         OSKextFreeSite(vm_allocation_site_t * site);
extern kern_return_t                OSKextSetReceiptQueried(void);
extern void *OSKextKextForAddress(const void *addr);
extern kern_return_t OSKextGetLoadedKextSummaryForAddress(
	const void              * addr,
	OSKextLoadedKextSummary * summary);
__BEGIN_DECLS




ptrauth_generic_signature_t
ptrauth_utils_sign_blob_generic(const void * ptr, size_t len_bytes, uint64_t data, int flags);
void
ptrauth_utils_auth_blob_generic(const void * ptr, size_t len_bytes, uint64_t data, int flags, ptrauth_generic_signature_t signature);
__attribute__((noreturn))
extern void __stack_chk_fail(void);
__BEGIN_DECLS





int     kernel_sysctlbyname(const char *, void *, size_t *, void *, size_t);
#  ifndef const 
#    define const       
#  endif




#  ifdef MAXSEG_64K
#    define MAX_MEM_LEVEL 8
#  else
#    define MAX_MEM_LEVEL 9
#  endif


#  define MAX_WBITS   15 





#  ifdef STDC
#    define OF(args)  args
#  else
#    define OF(args)  ()
#  endif





#  define ZEXTERN extern
#  define ZEXPORT
#  define ZEXPORTVA

#  define FAR

typedef unsigned int   uInt;
ZEXTERN const char * ZEXPORT zlibVersion OF((void));
ZEXTERN int ZEXPORT deflate OF((z_streamp strm, int flush));
ZEXTERN int ZEXPORT deflateEnd OF((z_streamp strm));
ZEXTERN int ZEXPORT inflate OF((z_streamp strm, int flush));
ZEXTERN int ZEXPORT inflateEnd OF((z_streamp strm));
ZEXTERN int ZEXPORT deflateSetDictionary OF((z_streamp strm,
    const Bytef *dictionary,
    uInt  dictLength));
ZEXTERN int ZEXPORT deflateCopy OF((z_streamp dest,
    z_streamp source));
ZEXTERN int ZEXPORT deflateResetWithIO(z_streamp strm, z_input_func zinput, z_output_func zoutput);
ZEXTERN int ZEXPORT deflateReset OF((z_streamp strm));
ZEXTERN int ZEXPORT deflateParams OF((z_streamp strm,
    int level,
    int strategy));
ZEXTERN int ZEXPORT deflateTune OF((z_streamp strm,
    int good_length,
    int max_lazy,
    int nice_length,
    int max_chain));
ZEXTERN uLong ZEXPORT deflateBound OF((z_streamp strm,
    uLong sourceLen));
ZEXTERN int ZEXPORT deflatePrime OF((z_streamp strm,
    int bits,
    int value));
ZEXTERN int ZEXPORT deflateSetHeader OF((z_streamp strm,
    gz_headerp head));
ZEXTERN int ZEXPORT inflateSetDictionary OF((z_streamp strm,
    const Bytef *dictionary,
    uInt  dictLength));
ZEXTERN int ZEXPORT inflateSync OF((z_streamp strm));
ZEXTERN int ZEXPORT inflateCopy OF((z_streamp dest,
    z_streamp source));
ZEXTERN int ZEXPORT inflateReset OF((z_streamp strm));
ZEXTERN int ZEXPORT inflatePrime OF((z_streamp strm,
    int bits,
    int value));
ZEXTERN int ZEXPORT inflateGetHeader OF((z_streamp strm,
    gz_headerp head));
ZEXTERN int ZEXPORT inflateBack OF((z_streamp strm,
    in_func in, void FAR *in_desc,
    out_func out, void FAR *out_desc));
ZEXTERN int ZEXPORT inflateBackEnd OF((z_streamp strm));
ZEXTERN uLong ZEXPORT zlibCompileFlags OF((void));
ZEXTERN int ZEXPORT compress OF((Bytef * dest, uLongf *destLen,
    const Bytef *source, uLong sourceLen));
ZEXTERN int ZEXPORT compress2 OF((Bytef * dest, uLongf *destLen,
    const Bytef *source, uLong sourceLen,
    int level));
ZEXTERN uLong ZEXPORT compressBound OF((uLong sourceLen));
ZEXTERN int ZEXPORT uncompress OF((Bytef * dest, uLongf *destLen,
    const Bytef *source, uLong sourceLen));
ZEXTERN uLong zlib_deflate_memory_size(int wbits, int memlevel);
ZEXTERN uLong ZEXPORT adler32 OF((uLong adler, const Bytef *buf, uInt len));
ZEXTERN uLong ZEXPORT adler32_combine OF((uLong adler1, uLong adler2,
    z_off_t len2));
ZEXTERN uLong ZEXPORT z_crc32   OF((uLong crc, const Bytef *buf, uInt len));
ZEXTERN uLong ZEXPORT z_crc32_combine OF((uLong crc1, uLong crc2, z_off_t len2));
ZEXTERN int ZEXPORT deflateInit_ OF((z_streamp strm, int level,
    const char *version, int stream_size));
ZEXTERN int ZEXPORT inflateInit_ OF((z_streamp strm,
    const char *version, int stream_size));
ZEXTERN int ZEXPORT deflateInit2_ OF((z_streamp strm, int  level, int  method,
    int windowBits, int memLevel,
    int strategy, const char *version,
    int stream_size));
ZEXTERN int ZEXPORT inflateInit2_ OF((z_streamp strm, int  windowBits,
    const char *version, int stream_size));
ZEXTERN int ZEXPORT inflateBackInit_ OF((z_streamp strm, int windowBits,
    unsigned char FAR *window,
    const char *version,
    int stream_size));
ZEXTERN const char   * ZEXPORT zError           OF((int));
ZEXTERN int            ZEXPORT inflateSyncPoint OF((z_streamp z));
ZEXTERN const uLongf * ZEXPORT get_crc_table    OF((void));
namespace os {
template <class T> using remove_volatile_t = typename std::remove_volatile<T>::type;

template <class T>
inline volatile std::atomic<remove_volatile_t<T> > *
cast_to_atomic_pointer(T *v)
{
	return reinterpret_cast<volatile std::atomic<remove_volatile_t<T> > *>(v);
}

template <class T>
inline volatile std::atomic<remove_volatile_t<T> > *
cast_to_atomic_pointer(std::atomic<T> *v)
{
	return reinterpret_cast<volatile std::atomic<remove_volatile_t<T> > *>(v);
}

template <class T>
inline volatile std::atomic<remove_volatile_t<T> > *
cast_to_atomic_pointer(volatile std::atomic<T> *v)
{
	return reinterpret_cast<volatile std::atomic<remove_volatile_t<T> > *>(v);
}

template <class T>
inline remove_volatile_t<T> *
cast_to_nonatomic_pointer(T *v)
{
	return const_cast<remove_volatile_t<T> *>(v);
}

template <class T>
inline remove_volatile_t<T> *
cast_to_nonatomic_pointer(std::atomic<T> *v)
{
	return reinterpret_cast<remove_volatile_t<T> *>(v);
}

template <class T>
inline remove_volatile_t<T> *
cast_to_nonatomic_pointer(volatile std::atomic<T> *v)
{
	auto _v = const_cast<std::atomic<T> *>(v);
	return reinterpret_cast<remove_volatile_t<T> *>(_v);
}
};
__BEGIN_DECLS


__WATCHOS_AVAILABLE(3.0) __OSX_AVAILABLE(10.12) __IOS_AVAILABLE(10.0) __TVOS_AVAILABLE(10.0)
void __firehose_buffer_push_to_logd(firehose_buffer_t fb, bool for_io);
__WATCHOS_AVAILABLE(3.0) __OSX_AVAILABLE(10.12) __IOS_AVAILABLE(10.0) __TVOS_AVAILABLE(10.0)
void __firehose_allocate(vm_offset_t *addr, vm_size_t size);
__WATCHOS_AVAILABLE(3.0) __OSX_AVAILABLE(10.12) __IOS_AVAILABLE(10.0) __TVOS_AVAILABLE(10.0)
extern void __firehose_critical_region_enter(void);
__WATCHOS_AVAILABLE(3.0) __OSX_AVAILABLE(10.12) __IOS_AVAILABLE(10.0) __TVOS_AVAILABLE(10.0)
extern void __firehose_critical_region_leave(void);
extern void oslogwakeup(void);
OS_ALWAYS_INLINE
static inline void
    _os_log_verify_format_str(__unused const char *msg, ...)
__osloglike(1, 2);
OS_ALWAYS_INLINE
static inline void
_os_log_verify_format_str(__unused const char *msg, ...)                                       
{
}





OS_EXPORT
struct os_log_s _os_log_default;
OS_ENUM(os_log_type, uint8_t,
    OS_LOG_TYPE_DEFAULT = 0x00,
    OS_LOG_TYPE_INFO    = 0x01,
    OS_LOG_TYPE_DEBUG   = 0x02,
    OS_LOG_TYPE_ERROR   = 0x10,
    OS_LOG_TYPE_FAULT   = 0x11);
OS_EXPORT OS_NOTHROW OS_WARN_RESULT OS_OBJECT_RETURNS_RETAINED
os_log_t
os_log_create(const char *subsystem, const char *category);
OS_EXPORT OS_NOTHROW OS_WARN_RESULT
bool
os_log_info_enabled(os_log_t log);
OS_EXPORT OS_NOTHROW OS_WARN_RESULT
bool
os_log_debug_enabled(os_log_t log);
bool
os_log_coprocessor(void *buff, uint64_t buff_len, os_log_type_t type,
    const char *uuid, uint64_t timestamp, uint32_t offset, bool stream_log);
void
os_log_coprocessor_register(const char *uuid, const char *file_path, bool copy);
void
os_log_coprocessor_register_with_type(const char *uuid, const char *file_path, os_log_coproc_reg_t register_type);
OS_EXPORT OS_NOTHROW
void
_os_log_internal(void *dso, os_log_t log, os_log_type_t type, const char *message, ...)
__osloglike(4, 5);
OS_EXPORT OS_NOTHROW
int
_os_log_internal_driverKit(void *dso, os_log_t log, os_log_type_t type, const char *message, ...)
__osloglike(4, 5);
OS_EXPORT OS_NOTHROW
void
_os_log_at_time(void *dso, os_log_t log, os_log_type_t type, uint64_t ts, const char *message, ...)
__osloglike(5, 6);
void os_log_context_init(os_log_context_t, logmem_t *, uint8_t *, size_t);
void os_log_context_free(os_log_context_t);
bool os_log_context_encode(os_log_context_t, const char *, va_list, uintptr_t, size_t, uint16_t);
#pragma mark - buffer support structures, enums

OS_ENUM(os_log_fmt_hdr_flags, uint8_t,
    OSLF_HDR_FLAG_HAS_PRIVATE    = 0x01,
    OSLF_HDR_FLAG_HAS_NON_SCALAR = 0x02,
    );
OS_ENUM(os_log_fmt_cmd_type, uint8_t,
    OSLF_CMD_TYPE_SCALAR      = 0, 
    OSLF_CMD_TYPE_COUNT       = 1, 
    OSLF_CMD_TYPE_STRING      = 2, 
    OSLF_CMD_TYPE_POINTER     = 3, 
    OSLF_CMD_TYPE_OBJECT      = 4, 
    OSLF_CMD_TYPE_WIDE_STRING = 5, 
    OSLF_CMD_TYPE_ERRNO       = 6, 
    OSLF_CMD_TYPE_MASK        = 7, 
    );
OS_ENUM(os_log_fmt_cmd_flags, uint8_t,
    OSLF_CMD_FLAG_PRIVATE    = 0x1,
    OSLF_CMD_FLAG_PUBLIC     = 0x2,
    OSLF_CMD_FLAG_SENSITIVE  = 0x4 | OSLF_CMD_FLAG_PRIVATE,
    );
bool os_log_subsystem_id_valid(uint16_t);
bool logmem_ready(const logmem_t *);
void *logmem_alloc(logmem_t *, size_t *);
void *logmem_alloc_locked(logmem_t *, size_t *);
void logmem_free(logmem_t *, void *, size_t);
void logmem_free_locked(logmem_t *, void *, size_t);
size_t logmem_max_size(const logmem_t *);
bool logmem_empty(const logmem_t *);
size_t logmem_required_size(size_t, size_t);
__BEGIN_DECLS


void
os_log_with_args(os_log_t oslog, os_log_type_t type, const char *format, va_list args, void *ret_addr)
__osloglike(3, 0);
bool os_log_encoded_metadata(firehose_tracepoint_id_u, uint64_t, const void *, size_t);
bool os_log_encoded_signpost(firehose_stream_t, firehose_tracepoint_id_u, uint64_t, const void *, size_t, size_t);
bool os_log_encoded_log(firehose_stream_t, firehose_tracepoint_id_u, uint64_t, const void *, size_t, size_t);
bool log_queue_log(log_payload_t, const void *, bool);
__BEGIN_DECLS


__OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0)
OS_EXPORT
void*
os_retain(void *object);
__OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0)
OS_EXPORT
void
os_release(void *object);
__BEGIN_DECLS




OS_ENUM(os_reason_libsystem_code, uint64_t,
    OS_REASON_LIBSYSTEM_CODE_WORKLOOP_OWNERSHIP_LEAK = 1,
    OS_REASON_LIBSYSTEM_CODE_FAULT = 2, 
    OS_REASON_LIBSYSTEM_CODE_SECINIT_INITIALIZER = 3,
    OS_REASON_LIBSYSTEM_CODE_PTHREAD_CORRUPTION = 4,
    OS_REASON_LIBSYSTEM_CODE_OS_LOG_FAULT = 5, 
    );
# if DEVELOPMENT || DEBUG
#  define OS_REFCNT_DEBUG 1
# else
#  define OS_REFCNT_DEBUG 0
# endif


__BEGIN_DECLS


static void os_ref_init_count(struct os_refcnt *, struct os_refgrp *, os_ref_count_t count)
os_error_if(count == 0, "Reference count must be non-zero initialized");
static void os_ref_retain(struct os_refcnt *);
static os_ref_count_t os_ref_release(struct os_refcnt *) OS_WARN_RESULT;
static os_ref_count_t os_ref_release_relaxed(struct os_refcnt *) OS_WARN_RESULT;
static void os_ref_release_live(struct os_refcnt *);
static bool os_ref_retain_try(struct os_refcnt *) OS_WARN_RESULT;
static void os_ref_retain_locked(struct os_refcnt *);
static os_ref_count_t os_ref_release_locked(struct os_refcnt *) OS_WARN_RESULT;
static os_ref_count_t os_ref_get_count(struct os_refcnt *rc);
extern void os_pcpu_ref_init(os_pcpu_ref_t *ref, struct os_refgrp *grp);
extern os_ref_count_t os_pcpu_ref_kill(os_pcpu_ref_t ref, struct os_refgrp *grp) __result_use_check;
extern void os_pcpu_ref_wait_for_death(os_pcpu_ref_t ref);
extern void os_pcpu_ref_destroy(os_pcpu_ref_t *ref, struct os_refgrp *grp);
extern os_ref_count_t os_pcpu_ref_count(os_pcpu_ref_t ref);
extern void os_pcpu_ref_retain(os_pcpu_ref_t ref, struct os_refgrp *grp);
extern bool os_pcpu_ref_retain_try(os_pcpu_ref_t ref, struct os_refgrp *grp) __result_use_check;
extern void os_pcpu_ref_release_live(os_pcpu_ref_t ref, struct os_refgrp *grp);
extern os_ref_count_t os_pcpu_ref_release(os_pcpu_ref_t ref, struct os_refgrp *grp) __result_use_check;
#pragma GCC visibility push(hidden)



static void os_ref_init_count_raw(os_ref_atomic_t *, struct os_refgrp *, os_ref_count_t count)
os_error_if(count == 0, "Reference count must be non-zero initialized");
static void os_ref_retain_floor(struct os_refcnt *, os_ref_count_t f)
os_error_if(!__builtin_constant_p(f) || f == 0, "refcount floor must be >= 1");
static void os_ref_retain_raw(os_ref_atomic_t *, struct os_refgrp *);
static void os_ref_retain_floor_raw(os_ref_atomic_t *, os_ref_count_t f, struct os_refgrp *)
os_error_if(!__builtin_constant_p(f) || f == 0, "refcount floor must be >= 1");
static os_ref_count_t os_ref_release_raw(os_ref_atomic_t *, struct os_refgrp *) OS_WARN_RESULT;
static os_ref_count_t os_ref_release_raw_relaxed(os_ref_atomic_t *, struct os_refgrp *) OS_WARN_RESULT;
static void os_ref_release_live_raw(os_ref_atomic_t *, struct os_refgrp *);
static bool os_ref_retain_try_raw(os_ref_atomic_t *, struct os_refgrp *) OS_WARN_RESULT;
static bool os_ref_retain_floor_try_raw(os_ref_atomic_t *, os_ref_count_t f, struct os_refgrp *) OS_WARN_RESULT
    os_error_if(!__builtin_constant_p(f) || f == 0, "refcount floor must be >= 1");
static void os_ref_retain_locked_raw(os_ref_atomic_t *, struct os_refgrp *);
static void os_ref_retain_floor_locked_raw(os_ref_atomic_t *, os_ref_count_t f, struct os_refgrp *)
os_error_if(!__builtin_constant_p(f) || f == 0, "refcount floor must be >= 1");
static os_ref_count_t os_ref_release_locked_raw(os_ref_atomic_t *, struct os_refgrp *) OS_WARN_RESULT;
static os_ref_count_t os_ref_get_count_raw(os_ref_atomic_t *rc);
void os_ref_init_count_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp,
    os_ref_count_t init_count, uint32_t init_bits)
os_error_if(init_count == 0, "Reference count must be non-zero initialized")
os_error_if(b > 26, "Bitwise reference count limited to 26 bits")
os_error_if(init_bits >= (1U << b), "Bits out of range");
static uint32_t os_ref_get_raw_mask(os_ref_atomic_t *rc);
static uint32_t os_ref_get_bits_mask(os_ref_atomic_t *rc, uint32_t b);
static os_ref_count_t os_ref_get_count_mask(os_ref_atomic_t *rc, uint32_t b);
static void
os_ref_retain_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp);
static void
os_ref_retain_acquire_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp);
static uint32_t
os_ref_retain_try_mask(os_ref_atomic_t *, uint32_t b, uint32_t reject_mask,
    struct os_refgrp *grp) OS_WARN_RESULT;
static bool
os_ref_retain_try_acquire_mask(os_ref_atomic_t *, uint32_t b, uint32_t reject_mask,
    struct os_refgrp *grp) OS_WARN_RESULT;
static uint32_t
os_ref_release_raw_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp) OS_WARN_RESULT;
static uint32_t
os_ref_release_raw_relaxed_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp) OS_WARN_RESULT;
static os_ref_count_t
os_ref_release_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp) OS_WARN_RESULT;
static os_ref_count_t
os_ref_release_relaxed_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp) OS_WARN_RESULT;
static uint32_t
os_ref_release_live_raw_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp);
static void
os_ref_release_live_mask(os_ref_atomic_t *rc, uint32_t b, struct os_refgrp *grp);
# define OS_REF_ATOMIC_INITIALIZER 0

__BEGIN_DECLS


void os_ref_init_count_external(os_ref_atomic_t *, struct os_refgrp *, os_ref_count_t);
void os_ref_retain_external(os_ref_atomic_t *, struct os_refgrp *);
void os_ref_retain_locked_external(os_ref_atomic_t *, struct os_refgrp *);
os_ref_count_t os_ref_release_external(os_ref_atomic_t *, struct os_refgrp *,
    memory_order release_order, memory_order dealloc_order);
os_ref_count_t os_ref_release_relaxed_external(os_ref_atomic_t *, struct os_refgrp *);
os_ref_count_t os_ref_release_barrier_external(os_ref_atomic_t *, struct os_refgrp *);
os_ref_count_t os_ref_release_locked_external(os_ref_atomic_t *, struct os_refgrp *);
bool os_ref_retain_try_external(os_ref_atomic_t *, struct os_refgrp *);
void os_ref_init_count_internal(os_ref_atomic_t *, struct os_refgrp *, os_ref_count_t);
void os_ref_retain_internal(os_ref_atomic_t *, struct os_refgrp *);
void os_ref_retain_floor_internal(os_ref_atomic_t *, os_ref_count_t, struct os_refgrp *);
os_ref_count_t os_ref_release_relaxed_internal(os_ref_atomic_t *, struct os_refgrp *);
os_ref_count_t os_ref_release_barrier_internal(os_ref_atomic_t *, struct os_refgrp *);
os_ref_count_t os_ref_release_internal(os_ref_atomic_t *, struct os_refgrp *,
    memory_order release_order, memory_order dealloc_order);
bool os_ref_retain_try_internal(os_ref_atomic_t *, struct os_refgrp *);
bool os_ref_retain_floor_try_internal(os_ref_atomic_t *, os_ref_count_t, struct os_refgrp *);
void os_ref_retain_locked_internal(os_ref_atomic_t *, struct os_refgrp *);
void os_ref_retain_floor_locked_internal(os_ref_atomic_t *, os_ref_count_t, struct os_refgrp *);
os_ref_count_t os_ref_release_locked_internal(os_ref_atomic_t *, struct os_refgrp *);
static inline void
os_ref_init_count(struct os_refcnt *rc, struct os_refgrp * __unused grp, os_ref_count_t count)
{
	os_ref_init_count_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL), count);
}

static inline void
os_ref_retain(struct os_refcnt *rc)
{
	os_ref_retain_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL));
}

static inline os_ref_count_t
os_ref_release_locked(struct os_refcnt *rc)
{
	return os_ref_release_locked_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL));
}

static inline void
os_ref_retain_locked(struct os_refcnt *rc)
{
	os_ref_retain_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL));
}

static inline bool
os_ref_retain_try(struct os_refcnt *rc)
{
	return os_ref_retain_try_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL));
}

__deprecated_msg("inefficient codegen, prefer os_ref_release / os_ref_release_relaxed")
static inline os_ref_count_t OS_WARN_RESULT
os_ref_release_explicit(struct os_refcnt *rc, memory_order release_order, memory_order dealloc_order)
{
	return os_ref_release_internal(&rc->ref_count, os_ref_if_debug(rc->ref_group, NULL),
	           release_order, dealloc_order);
}


void os_ref_panic_live(void *rc) __abortlike;
void os_ref_panic_last(void *rc) __abortlike;
static inline os_ref_count_t OS_WARN_RESULT
os_ref_release(struct os_refcnt *rc)
{
	return os_ref_release_barrier_internal(&rc->ref_count,
	           os_ref_if_debug(rc->ref_group, NULL));
}

static inline void
os_ref_release_last(struct os_refcnt *rc)
{
	if (__improbable(os_ref_release(rc) != 0)) {
		os_ref_panic_last(rc);
	}
}

static inline os_ref_count_t OS_WARN_RESULT
os_ref_release_relaxed(struct os_refcnt *rc)
{
	return os_ref_release_relaxed_internal(&rc->ref_count,
	           os_ref_if_debug(rc->ref_group, NULL));
}

static inline void
os_ref_release_live(struct os_refcnt *rc)
{
	if (__improbable(os_ref_release(rc) == 0)) {
		os_ref_panic_live(rc);
	}
}

static inline os_ref_count_t
os_ref_get_count_internal(os_ref_atomic_t *rc)
{
	return atomic_load_explicit(rc, memory_order_relaxed);
}

static inline os_ref_count_t
os_ref_get_count(struct os_refcnt *rc)
{
	return os_ref_get_count_internal(&rc->ref_count);
}


#pragma GCC visibility push(hidden)



static inline void
os_ref_init_count_raw(os_ref_atomic_t *rc, struct os_refgrp *grp, os_ref_count_t count)
{
	os_ref_init_count_internal(rc, grp, count);
}

static inline void
os_ref_retain_floor(struct os_refcnt *rc, os_ref_count_t f)
{
	os_ref_retain_floor_internal(&rc->ref_count, f, os_ref_if_debug(rc->ref_group, NULL));
}

static inline void
os_ref_retain_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	os_ref_retain_internal(rc, grp);
}

static inline void
os_ref_retain_floor_raw(os_ref_atomic_t *rc, os_ref_count_t f, struct os_refgrp *grp)
{
	os_ref_retain_floor_internal(rc, f, grp);
}

static inline os_ref_count_t
os_ref_release_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	return os_ref_release_barrier_internal(rc, grp);
}

static inline os_ref_count_t
os_ref_release_raw_relaxed(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	return os_ref_release_relaxed_internal(rc, grp);
}

static inline void
os_ref_release_live_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	if (__improbable(os_ref_release_barrier_internal(rc, grp) == 0)) {
		os_ref_panic_live(rc);
	}
}

static inline bool
os_ref_retain_try_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	return os_ref_retain_try_internal(rc, grp);
}

static inline bool
os_ref_retain_floor_try_raw(os_ref_atomic_t *rc, os_ref_count_t f,
    struct os_refgrp *grp)
{
	return os_ref_retain_floor_try_internal(rc, f, grp);
}

static inline void
os_ref_retain_locked_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	os_ref_retain_locked_internal(rc, grp);
}

static inline void
os_ref_retain_floor_locked_raw(os_ref_atomic_t *rc, os_ref_count_t f,
    struct os_refgrp *grp)
{
	os_ref_retain_floor_locked_internal(rc, f, grp);
}

static inline os_ref_count_t
os_ref_release_locked_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	return os_ref_release_locked_internal(rc, grp);
}

static inline void
os_ref_release_live_locked_raw(os_ref_atomic_t *rc, struct os_refgrp *grp)
{
	if (__improbable(os_ref_release_locked_internal(rc, grp) == 0)) {
		os_ref_panic_live(rc);
	}
}

static inline os_ref_count_t
os_ref_get_count_raw(os_ref_atomic_t *rc)
{
	return os_ref_get_count_internal(rc);
}


extern void
os_ref_log_fini(struct os_refgrp *grp);
extern void
os_ref_log_init(struct os_refgrp *grp);
extern void
os_ref_retain_mask_internal(os_ref_atomic_t *rc, uint32_t n, struct os_refgrp *grp);
extern void
os_ref_retain_acquire_mask_internal(os_ref_atomic_t *rc, uint32_t n, struct os_refgrp *grp);
extern uint32_t
os_ref_retain_try_mask_internal(os_ref_atomic_t *, uint32_t n,
    uint32_t reject_mask, struct os_refgrp *grp) OS_WARN_RESULT;
extern bool
os_ref_retain_try_acquire_mask_internal(os_ref_atomic_t *, uint32_t n,
    uint32_t reject_mask, struct os_refgrp *grp) OS_WARN_RESULT;
extern uint32_t
os_ref_release_barrier_mask_internal(os_ref_atomic_t *rc, uint32_t n, struct os_refgrp *grp);
extern uint32_t
os_ref_release_relaxed_mask_internal(os_ref_atomic_t *rc, uint32_t n, struct os_refgrp *grp);
__BEGIN_DECLS



__WATCHOS_AVAILABLE(9.0) __OSX_AVAILABLE(13.0) __IOS_AVAILABLE(16.0) __TVOS_AVAILABLE(16.0)

OS_EXPORT OS_NOTHROW
void
record_system_event(uint8_t type, uint8_t subsystem, const char *event, const char *format, ...) __printflike(4, 5);
__WATCHOS_AVAILABLE(9.0) __OSX_AVAILABLE(13.0) __IOS_AVAILABLE(16.0) __TVOS_AVAILABLE(16.0)
OS_EXPORT OS_NOTHROW
void
record_system_event_no_varargs(uint8_t type, uint8_t subsystem, const char *event, const char *payload);
OS_ALWAYS_INLINE
static inline void
_os_trace_verify_printf(const char *msg, ...)
__attribute__((format(printf, 1, 2)))
{
#pragma unused(msg)
}






#pragma mark - Other defines












typedef void (^os_trace_payload_t)(xpc_object_t xdict);
#pragma mark - function declarations








__OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0)
OS_EXPORT OS_NOTHROW OS_WARN_RESULT
bool
os_trace_info_enabled(void);
__OSX_AVAILABLE(10.10) __IOS_AVAILABLE(8.0) __WATCHOS_AVAILABLE(1.0) __TVOS_AVAILABLE(9.0)
OS_EXPORT OS_NOTHROW OS_WARN_RESULT
bool
os_trace_debug_enabled(void);
__OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0)
OS_EXPORT OS_NOTHROW
size_t
_os_trace_encode(uint8_t *buf, size_t buf_size, const char *format, ...);
__OSX_AVAILABLE_STARTING(__MAC_10_12, __IPHONE_10_0)
OS_EXPORT OS_NOTHROW
void
_os_trace_internal(void *dso, uint8_t type, const char *format, const uint8_t *buf, size_t buf_size, os_trace_payload_t payload);
__OSX_AVAILABLE(10.10) __IOS_AVAILABLE(8.0) __WATCHOS_AVAILABLE(1.0) __TVOS_AVAILABLE(9.0)
OS_EXPORT OS_NOTHROW
void
_os_trace_with_buffer(void *dso, const char *message, uint8_t type, const void *buffer, size_t buffer_size, os_trace_payload_t payload);
__BEGIN_DECLS

OS_ALWAYS_INLINE
inline uint32_t
_os_trace_offset(const void *dso, const void *addr, _firehose_tracepoint_flags_activity_t flags __unused)
{
	assert((uintptr_t)addr >= (uintptr_t)dso);
	return (uint32_t) ((uintptr_t)addr - (uintptr_t)dso);
}

bool
_os_trace_addr_in_text_segment(const void *dso, const void *addr);
int  _tr_tally        OF((deflate_state *s, unsigned dist, unsigned lc));
void _tr_flush_block  OF((deflate_state *s, charf *buf, ulg stored_len,
                          int eof));
void _tr_align        OF((deflate_state *s));
void _tr_stored_block OF((deflate_state *s, charf *buf, ulg stored_len,
                          int eof));
void inflate_fast OF((z_streamp strm, unsigned start));
extern int inflate_table OF((codetype type, unsigned short FAR *lens,
                             unsigned codes, code FAR * FAR *table,
                             unsigned FAR *bits, unsigned short FAR *work));
#  define OS_CODE  0x03  

#  define F_OPEN(name, mode) fopen((name), (mode))

         

#  ifdef MSDOS
     
#    define NO_vsnprintf
#  endif
#  ifdef __TURBOC__
#    define NO_vsnprintf
#  endif
#  ifdef WIN32
     
#    if !defined(vsnprintf) && !defined(NO_vsnprintf)
#      define vsnprintf _vsnprintf
#    endif
#  endif
#  ifdef __SASC
#    define NO_vsnprintf
#  endif




voidpf zcalloc OF((voidpf opaque, unsigned items, unsigned size));
void   zcfree  OF((voidpf opaque, voidpf ptr));
OS_EXPORT OS_NONNULL1
void
amfi_interface_register(const amfi_t *mfi);
OS_EXPORT OS_NONNULL1
void apple_encrypted_archive_interface_register(const apple_encrypted_archive_t *aea);
void apple_encrypted_archive_interface_set_registration_callback(registration_callback_t callback);
class OSArray : public OSCollection
{
	friend class OSSet;
	friend class OSSerialize;

	OSDeclareDefaultStructors(OSArray);

	typedef OSTaggedPtr<const OSMetaClassBase> ArrayPtrType;
	typedef OSTaggedSharedPtr<const OSMetaClassBase, OSCollection> ArraySharedPtrType;



	virtual unsigned int iteratorSize() const APPLE_KEXT_OVERRIDE;
	virtual bool initIterator(void * iterator) const APPLE_KEXT_OVERRIDE;
	virtual bool getNextObjectForIterator(void * iterator, OSObject ** ret) const APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<OSArray> withCapacity(unsigned int capacity);



	static OSPtr<OSArray> withObjects(
		const OSObject * objects[],
		unsigned int     count,
		unsigned int     capacity = 0);



	static OSPtr<OSArray> withArray(
		const OSArray * array,
		unsigned int    capacity = 0);



	virtual bool initWithCapacity(unsigned int capacity);



	virtual bool initWithObjects(
		const OSObject * objects[],
		unsigned int     count,
		unsigned int     capacity = 0);


	virtual bool initWithArray(
		const OSArray * anArray,
		unsigned int    capacity = 0);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCount() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacity() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacityIncrement() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int setCapacityIncrement(unsigned increment) APPLE_KEXT_OVERRIDE;



	virtual unsigned int ensureCapacity(unsigned int newCapacity) APPLE_KEXT_OVERRIDE;



	virtual void flushCollection() APPLE_KEXT_OVERRIDE;



	virtual bool setObject(const OSMetaClassBase * anObject);

	bool setObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool setObject(
		unsigned int            index,
		const OSMetaClassBase * anObject);

	bool setObject(
		unsigned int index,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool merge(const OSArray * otherArray);



	virtual void replaceObject(
		unsigned int            index,
		const OSMetaClassBase * anObject);

	void replaceObject(
		unsigned int            index,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual void removeObject(unsigned int index);



	virtual bool isEqualTo(const OSArray * anArray) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual OSObject * getObject(unsigned int index) const;



	virtual OSObject * getLastObject() const;



	virtual unsigned int getNextIndexOfObject(
		const OSMetaClassBase * anObject,
		unsigned int            index) const;


	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;



	virtual unsigned setOptions(
		unsigned   options,
		unsigned   mask,
		void     * context = NULL) APPLE_KEXT_OVERRIDE;



	OSPtr<OSCollection> copyCollection(OSDictionary * cycleDict = NULL) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSArray, 0);
	OSMetaClassDeclareReservedUnused(OSArray, 1);
	OSMetaClassDeclareReservedUnused(OSArray, 2);
	OSMetaClassDeclareReservedUnused(OSArray, 3);
	OSMetaClassDeclareReservedUnused(OSArray, 4);
	OSMetaClassDeclareReservedUnused(OSArray, 5);
	OSMetaClassDeclareReservedUnused(OSArray, 6);
	OSMetaClassDeclareReservedUnused(OSArray, 7);
};
class OSBoolean : public OSObject
{
	OSDeclareDefaultStructors(OSBoolean);
	friend class OSSerialize;

protected:
	bool value;


	virtual void taggedRelease(
		const void * tag,
		const int    when) const APPLE_KEXT_OVERRIDE;

public:
	static void initialize();


	static OSPtr<OSBoolean> withBoolean(bool value) __returns_nonnull_osptr;


	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual void taggedRetain(const void * tag) const APPLE_KEXT_OVERRIDE;



	virtual bool isTrue() const;



	virtual bool isFalse() const;



	virtual bool getValue() const;



	virtual bool isEqualTo(const OSBoolean * aBoolean) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSBoolean, 0);
	OSMetaClassDeclareReservedUnused(OSBoolean, 1);
	OSMetaClassDeclareReservedUnused(OSBoolean, 2);
	OSMetaClassDeclareReservedUnused(OSBoolean, 3);
	OSMetaClassDeclareReservedUnused(OSBoolean, 4);
	OSMetaClassDeclareReservedUnused(OSBoolean, 5);
	OSMetaClassDeclareReservedUnused(OSBoolean, 6);
	OSMetaClassDeclareReservedUnused(OSBoolean, 7);
};
#pragma clang diagnostic pop






class OSCollection : public OSObject
{
	friend class OSCollectionIterator;

	OSDeclareAbstractStructors(OSCollection);

	struct ExpansionData { };

protected:

	unsigned int updateStamp;

protected:


	unsigned int fOptions;

protected:




	virtual unsigned int iteratorSize() const = 0;



	virtual bool initIterator(void * iterationContext) const = 0;



	virtual bool getNextObjectForIterator(
		void      * iterationContext,
		OSObject ** nextObject) const = 0;



	virtual bool init() APPLE_KEXT_OVERRIDE;

public:


	typedef enum {
		kImmutable  = 0x00000001,
		kSort       = 0x00000002,
		kMASK       = (unsigned) - 1
	} _OSCollectionFlags;




	void haveUpdated();



	virtual unsigned int getCount() const = 0;



	virtual unsigned int getCapacity() const = 0;



	virtual unsigned int getCapacityIncrement() const = 0;



	virtual unsigned int setCapacityIncrement(unsigned increment) = 0;



	virtual unsigned int ensureCapacity(unsigned int newCapacity) = 0;



	virtual void flushCollection() = 0;



	virtual unsigned setOptions(
		unsigned   options,
		unsigned   mask,
		void     * context = NULL);


	virtual OSPtr<OSCollection> copyCollection(OSDictionary * cycleDict = NULL);


	bool iterateObjects(void * refcon, bool (*callback)(void * refcon, OSObject * object));


	OSMetaClassDeclareReservedUsedX86(OSCollection, 0);
	OSMetaClassDeclareReservedUsedX86(OSCollection, 1);
	OSMetaClassDeclareReservedUnused(OSCollection, 2);
	OSMetaClassDeclareReservedUnused(OSCollection, 3);
	OSMetaClassDeclareReservedUnused(OSCollection, 4);
	OSMetaClassDeclareReservedUnused(OSCollection, 5);
	OSMetaClassDeclareReservedUnused(OSCollection, 6);
	OSMetaClassDeclareReservedUnused(OSCollection, 7);
};
__enum_closed_decl(OSCollectionIteratorStorageType, uint8_t, {
	OSCollectionIteratorStorageUnallocated = 0,
	OSCollectionIteratorStorageInvalid = 1,
	OSCollectionIteratorStorageInline = 2,
	OSCollectionIteratorStoragePointer = 3, 
});
class OSCollectionIterator : public OSIterator
{
	OSDeclareDefaultStructors(OSCollectionIterator);

protected:

	OSPtr<const OSCollection> collection;
	union {
		struct {
			uint8_t inlineStorage[4];
			uint8_t __padding[4];
		};
		void * collIterator;
	};
	unsigned int         initialUpdateStamp;
	bool                 valid;

private:
	
	bool   isSubclassed();

	
	bool   initializeIteratorStorage();

	
	void * getIteratorStorage();

	
	void   freeIteratorStorage();

	
	OSCollectionIteratorStorageType getStorageType();

	
	void setStorageType(OSCollectionIteratorStorageType storageType);

public:

	static OSPtr<OSCollectionIterator> withCollection(const OSCollection * inColl);



	virtual bool initWithCollection(const OSCollection * inColl);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual void reset() APPLE_KEXT_OVERRIDE;



	virtual bool isValid() APPLE_KEXT_OVERRIDE;



	virtual OSObject * getNextObject() APPLE_KEXT_OVERRIDE;
};
void OSPrintMemory(void);
class OSData : public OSObject
{
	friend class OSSerialize;

	OSDeclareDefaultStructors(OSData);



public:
	typedef void (*DeallocFunction)(void * ptr, unsigned int length);
protected:
	struct ExpansionData {
		DeallocFunction deallocFunction;
		bool            disableSerialization;
	};


	ExpansionData * reserved;

public:


	static OSPtr<OSData> withCapacity(unsigned int capacity __xnu_data_size);



	static OSPtr<OSData> withBytes(
		const void   * bytes,
		unsigned int   numBytes __xnu_data_size);



	template <typename T>
	static OSPtr<OSData>
	withValue(const T& value)
	{
		validateValueType<T, kValueCopy>();
		return withBytes(&value, sizeof(T));
	}



	static OSPtr<OSData> withBytesNoCopy(
		void         * bytes,
		unsigned int   numBytes);



	template <typename T>
	static OSPtr<OSData>
	withValueNoCopy(T& value)
	{
		validateValueType<T, kValueNoCopy>();
		return withBytesNoCopy(&value, sizeof(T));
	}




	static OSPtr<OSData> withData(const OSData * inData);



	static OSPtr<OSData> withData(
		const OSData * inData,
		unsigned int   start,
		unsigned int   numBytes);



	virtual bool initWithCapacity(unsigned int capacity __xnu_data_size);



	virtual bool initWithBytes(
		const void   * bytes,
		unsigned int   numBytes __xnu_data_size);



	template <typename T>
	bool
	initWithValue(const T& value)
	{
		validateValueType<T, kValueCopy>();
		return initWithBytes(&value, sizeof(T));
	}



	virtual bool initWithBytesNoCopy(
		void         * bytes,
		unsigned int   numBytes);



	template <typename T>
	bool
	initWithValueNoCopy(T& value)
	{
		validateValueType<T, kValueNoCopy>();
		return initWithBytesNoCopy(&value, sizeof(T));
	}




	virtual bool initWithData(const OSData * inData);



	virtual bool initWithData(
		const OSData * inData,
		unsigned int   start,
		unsigned int   numBytes);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getLength() const;



	virtual unsigned int getCapacity() const;



	virtual unsigned int getCapacityIncrement() const;



	virtual unsigned int setCapacityIncrement(unsigned increment);





	virtual unsigned int ensureCapacity(unsigned int newCapacity);


	bool clipForCopyout();


	virtual bool appendBytes(
		const void   * bytes,
		unsigned int   numBytes __xnu_data_size);



	template <typename T>
	bool
	appendValue(const T& value)
	{
		validateValueType<T, kValueCopy>();
		return appendBytes(&value, sizeof(T));
	}



	virtual bool appendBytes(const OSData * aDataObj);



	__xnu_returns_data_pointer
	virtual const void * getBytesNoCopy() const;



	__xnu_returns_data_pointer
	virtual const void * getBytesNoCopy(
		unsigned int start,
		unsigned int numBytes) const;



	virtual bool isEqualTo(const OSData * aDataObj) const;



	virtual bool isEqualTo(
		const void   * bytes,
		unsigned int   numBytes) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool isEqualTo(const OSString * aString) const;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;



	virtual bool appendByte(
		unsigned char byte,
		unsigned int  numBytes);


	void setSerializable(bool serializable);


public:
	virtual void setDeallocFunction(DeallocFunction func);
	bool isSerializable(void);

private:
	enum ValueAcquisition {
		kValueCopy,
		kValueNoCopy
	};


private:
	OSMetaClassDeclareReservedUsedX86(OSData, 0);
	OSMetaClassDeclareReservedUnused(OSData, 1);
	OSMetaClassDeclareReservedUnused(OSData, 2);
	OSMetaClassDeclareReservedUnused(OSData, 3);
	OSMetaClassDeclareReservedUnused(OSData, 4);
	OSMetaClassDeclareReservedUnused(OSData, 5);
	OSMetaClassDeclareReservedUnused(OSData, 6);
	OSMetaClassDeclareReservedUnused(OSData, 7);
};
class OSDictionary : public OSCollection
{
	friend class OSSerialize;

	OSDeclareDefaultStructors(OSDictionary);



	virtual unsigned int iteratorSize() const APPLE_KEXT_OVERRIDE;
	virtual bool initIterator(void * iterator) const APPLE_KEXT_OVERRIDE;
	virtual bool getNextObjectForIterator(void * iterator, OSObject ** ret) const APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<OSDictionary> withCapacity(unsigned int capacity);



	static OSPtr<OSDictionary> withObjects(
		const OSObject * objects[],
		const OSSymbol * keys[],
		unsigned int     count,
		unsigned int     capacity = 0);


	static OSPtr<OSDictionary> withObjects(
		const OSObject * objects[],
		const OSString * keys[],
		unsigned int     count,
		unsigned int     capacity = 0);



	static OSPtr<OSDictionary> withDictionary(
		const OSDictionary * dict,
		unsigned int         capacity = 0);



	virtual bool initWithCapacity(unsigned int capacity);



	virtual bool initWithObjects(
		const OSObject * objects[],
		const OSSymbol * keys[],
		unsigned int     count,
		unsigned int     capacity = 0);



	virtual bool initWithObjects(
		const OSObject * objects[],
		const OSString * keys[],
		unsigned int     count,
		unsigned int     capacity = 0);



	virtual bool initWithDictionary(
		const OSDictionary * dict,
		unsigned int         capacity = 0);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCount() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacity() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacityIncrement() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int setCapacityIncrement(unsigned increment) APPLE_KEXT_OVERRIDE;



	virtual unsigned int ensureCapacity(unsigned int newCapacity) APPLE_KEXT_OVERRIDE;



	virtual void flushCollection() APPLE_KEXT_OVERRIDE;



	virtual bool setObject(
		const OSSymbol        * aKey,
		const OSMetaClassBase * anObject);

	bool setObject(
		OSSharedPtr<const OSSymbol> const& aKey,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool setObject(
		const OSString        * aKey,
		const OSMetaClassBase * anObject);

	bool setObject(
		const OSString        * aKey,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool setObject(
		const char            * aKey,
		const OSMetaClassBase * anObject);

	bool setObject(
		const char            * aKey,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual void removeObject(const OSSymbol * aKey);



	virtual void removeObject(const OSString * aKey);



	virtual void removeObject(const char * aKey);



	virtual bool merge(const OSDictionary * aDictionary);



	virtual OSObject * getObject(const OSSymbol * aKey) const;



	virtual OSObject * getObject(const OSString * aKey) const;



	virtual OSObject * getObject(const char * aKey) const;



	virtual bool isEqualTo(
		const OSDictionary * aDictionary,
		const OSCollection * keys) const;



	virtual bool isEqualTo(const OSDictionary * aDictionary) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;



	virtual unsigned setOptions(
		unsigned   options,
		unsigned   mask,
		void     * context = NULL) APPLE_KEXT_OVERRIDE;



	OSPtr<OSCollection> copyCollection(OSDictionary * cycleDict = NULL) APPLE_KEXT_OVERRIDE;

	bool setObject(const OSSymbol *aKey, const OSMetaClassBase *anObject, bool onlyAdd);
	void sortBySymbol(void);
	OSPtr<OSArray> copyKeys(void);



	bool iterateObjects(void * refcon, bool (*callback)(void * refcon, const OSSymbol * key, OSObject * object));


	OSMetaClassDeclareReservedUnused(OSDictionary, 0);
	OSMetaClassDeclareReservedUnused(OSDictionary, 1);
	OSMetaClassDeclareReservedUnused(OSDictionary, 2);
	OSMetaClassDeclareReservedUnused(OSDictionary, 3);
	OSMetaClassDeclareReservedUnused(OSDictionary, 4);
	OSMetaClassDeclareReservedUnused(OSDictionary, 5);
	OSMetaClassDeclareReservedUnused(OSDictionary, 6);
	OSMetaClassDeclareReservedUnused(OSDictionary, 7);
};
class OSIterator : public OSObject
{
	OSDeclareAbstractStructors(OSIterator);

public:

	virtual void reset() = 0;



	virtual bool isValid() = 0;



	virtual OSObject *getNextObject() = 0;

	OSMetaClassDeclareReservedUnused(OSIterator, 0);
	OSMetaClassDeclareReservedUnused(OSIterator, 1);
	OSMetaClassDeclareReservedUnused(OSIterator, 2);
	OSMetaClassDeclareReservedUnused(OSIterator, 3);
};
extern "C" {

}





extern "C" {
void osdata_kmem_free(void * ptr, unsigned int length);
void osdata_phys_free(void * ptr, unsigned int length);
void osdata_vm_deallocate(void * ptr, unsigned int length);
void osdata_kext_free(void * ptr, unsigned int length);
void kxld_log_callback(
	KXLDLogSubsystem    subsystem,
	KXLDLogLevel        level,
	const char        * format,
	va_list             argList,
	void              * user_data);
};
extern "C" {
void OSKextLog(
	OSKext         * aKext,
	OSKextLogSpec    msgLogSpec,
	const char     * format, ...) __printflike(3, 4);

void OSKextVLog(
	OSKext         * aKext,
	OSKextLogSpec    msgLogSpec,
	const char     * format,
	va_list          srcArgList) __printflike(3, 0);;

void OSKextRemoveKextBootstrap(void);

kern_return_t OSRuntimeInitializeCPP(
	OSKext * kext);
kern_return_t OSRuntimeFinalizeCPP(
	OSKext * kext);
void OSRuntimeUnloadCPPForSegment(
	kernel_segment_command_t * segment);
void
OSRuntimeSignStructors(
	kernel_mach_header_t * header);
void
OSRuntimeSignStructorsInFileset(
	kernel_mach_header_t * fileset_header);

kern_return_t is_io_catalog_send_data(
	mach_port_t              masterPort,
	uint32_t                 flag,
	io_buf_ptr_t             inData,
	mach_msg_type_number_t   inDataCount,
	kern_return_t          * result);

void kmod_dump_log(vm_offset_t*, unsigned int, boolean_t);
void *OSKextKextForAddress(const void *addr);

kern_return_t OSKextGetLoadedKextSummaryForAddress(
	const void              * addr,
	OSKextLoadedKextSummary * summary);

};
class OSKextSavedMutableSegment : public OSObject {
	OSDeclareDefaultStructors(OSKextSavedMutableSegment);
public:
	static OSPtr<OSKextSavedMutableSegment> withSegment(kernel_segment_command_t *seg);
	OSReturn restoreContents(kernel_segment_command_t *seg);
	vm_offset_t getVMAddr() const;
	vm_size_t getVMSize() const;
	virtual void free(void) APPLE_KEXT_OVERRIDE;
private:
	bool initWithSegment(kernel_segment_command_t *seg);
	kernel_segment_command_t *savedSegment;
	vm_offset_t vmaddr;
	vm_size_t   vmsize;
	void      * data;
};
class OSDextStatistics : public OSObject {
	OSDeclareDefaultStructors(OSDextStatistics);
public:
	static OSPtr<OSDextStatistics> create();
	virtual bool init() APPLE_KEXT_OVERRIDE;
	virtual void free() APPLE_KEXT_OVERRIDE;

	OSDextCrashPolicy recordCrash();
	size_t getCrashCount();

private:
	OSPtr<OSArray> crashes;
	IOLock * lock;
};
__enum_closed_decl(OSKextInitResult, uint8_t, {
	kOSKextInitFailure = 0,
	kOSKextInitialized,
	kOSKextAlreadyExist,
});
class OSKext : public OSObject
{
	OSDeclareDefaultStructors(OSKext);

	friend class IOCatalogue;
	friend class KLDBootstrap;
	friend class OSMetaClass;

	friend int OSKextGrabPgoData(uuid_t uuid,
	    uint64_t *pSize,
	    char *pBuffer,
	    uint64_t bufferSize,
	    int wait_for_unload,
	    int metadata);

	friend void OSKextVLog(
		OSKext         * aKext,
		OSKextLogSpec    msgLogSpec,
		const char     * format,
		va_list          srcArgList) __printflike(3, 0);

	friend void OSKextRemoveKextBootstrap(void);
	friend OSReturn OSKextUnloadKextWithLoadTag(uint32_t);

	friend kern_return_t kext_request(
		host_priv_t                             hostPriv,
		 uint32_t                 clientLogSpec,
		 vm_offset_t              requestIn,
		 mach_msg_type_number_t   requestLengthIn,
		 vm_offset_t            * responseOut,
		 mach_msg_type_number_t * responseLengthOut,
		 vm_offset_t            * logDataOut,
		 mach_msg_type_number_t * logDataLengthOut,
		 kern_return_t          * op_result);

	friend kxld_addr_t kern_allocate(
		u_long              size,
		KXLDAllocateFlags * flags,
		void              * user_data);

	friend void kxld_log_shim(
		KXLDLogSubsystem    subsystem,
		KXLDLogLevel        level,
		const char        * format,
		va_list             argList,
		void              * user_data);

	friend void _OSKextConsiderUnloads(
		__unused thread_call_param_t p0,
		__unused thread_call_param_t p1);

	friend kern_return_t OSRuntimeInitializeCPP(
		OSKext * kext);
	friend kern_return_t OSRuntimeFinalizeCPP(
		OSKext * kext);
	friend void OSRuntimeUnloadCPPForSegment(
		kernel_segment_command_t * segment);

	friend kern_return_t is_io_catalog_send_data(
		mach_port_t              masterPort,
		uint32_t                 flag,
		io_buf_ptr_t             inData,
		mach_msg_type_number_t   inDataCount,
		kern_return_t          * result);

	friend void kmod_panic_dump(vm_offset_t*, unsigned int);
	friend void kmod_dump_log(vm_offset_t*, unsigned int, boolean_t);
	friend void kext_dump_panic_lists(int (*printf_func)(const char * fmt, ...));
	friend void *OSKextKextForAddress(const void *addr);

	friend kern_return_t OSKextGetLoadedKextSummaryForAddress(
		const void              * addr,
		OSKextLoadedKextSummary * summary);


private:


	OSPtr<OSDictionary>  infoDict;

	OSPtr<const OSSymbol>    bundleID;
	OSPtr<OSString>    path;           
	OSPtr<OSString>    executableRelPath;
	OSPtr<OSString>    userExecutableRelPath;

	OSKextVersion    version;        
	OSKextVersion    compatibleVersion;


	OSKextLoadTag    loadTag;        
	                                 
	kmod_info_t    * kmod_info;      

	OSPtr<OSArray>     dependencies;   
	


	OSPtr<OSData>       linkedExecutable;
	OSPtr<OSSet>        metaClasses;       


	OSPtr<OSData>       interfaceUUID;
	OSPtr<OSData>       driverKitUUID;

	struct {
		unsigned int loggingEnabled:1;

		unsigned int hasAllDependencies:1;
		unsigned int hasBleedthrough:1;

		unsigned int interface:1;
		unsigned int kernelComponent:1;
		unsigned int prelinked:1;
		unsigned int builtin:1;
		unsigned int loaded:1;
		unsigned int dtraceInitialized:1;
		unsigned int starting:1;
		unsigned int started:1;
		unsigned int stopping:1;
		unsigned int unloading:1;
		unsigned int resetSegmentsFromVnode:1;

		unsigned int requireExplicitLoad:1;
		unsigned int autounloadEnabled:1;
		unsigned int delayAutounload:1; 

		unsigned int CPPInitialized:1;
		unsigned int jettisonLinkeditSeg:1;
		unsigned int resetSegmentsFromImmutableCopy:1;
		unsigned int unloadUnsupported:1;
		unsigned int dextToReplace:1;

		
		unsigned int unslidMachO:1;
	} flags;

	uint32_t matchingRefCount;
	kc_kind_t kc_type;

	struct list_head pendingPgoHead;
	uuid_t instance_uuid;
	OSKextAccount * account;
	uint32_t builtinKmodIdx;
	OSPtr<OSArray> savedMutableSegments;
	OSPtr<OSDextStatistics> dextStatistics;
	OSPtr<OSData> dextUniqueID;
	uint32_t dextLaunchedCount;



public:
	static void           initialize(void);
	static OSPtr<OSDictionary> copyKexts(void);
	static OSReturn       removeKextBootstrap(void);
	static void           willShutdown(void);
	static void           willUserspaceReboot(void);
	static void           resetAfterUserspaceReboot(void);
	static  void reportOSMetaClassInstances(
		const char     * kextIdentifier,
		OSKextLogSpec    msgLogSpec);
	static void OSKextLogDriverKitInfoLoad(OSKext *kext);
	static bool iokitDaemonAvailable(void);

private:

	static bool setLoadEnabled(bool flag);
	static bool setUnloadEnabled(bool flag);
	static bool setAutounloadsEnabled(bool flag);
	static bool setKernelRequestsEnabled(bool flag);


	static bool getLoadEnabled(void);
	static bool getUnloadEnabled(void);
	static bool getAutounloadEnabled(void);
	static bool getKernelRequestsEnabled(void);


	static OSData *parseDextUniqueID(
		OSDictionary * anInfoDict,
		const char *dextIDCS);
	static void setDextUniqueIDInPersonalities(
		OSDictionary * anInfoDict,
		OSData * dextUniqueID);

	static OSPtr<OSKext> withBooterData(
		OSString * deviceTreeName,
		OSData   * booterData);
	virtual bool initWithBooterData(
		OSString * deviceTreeName,
		OSData   * booterData);

	static OSPtr<OSKext> withPrelinkedInfoDict(
		OSDictionary * infoDict,
		bool doCoalesedSlides, kc_kind_t type);
	virtual bool initWithPrelinkedInfoDict(
		OSDictionary * infoDict,
		bool doCoalesedSlides, kc_kind_t type);
	static OSSharedPtr<OSKext> withCodelessInfo(
		OSDictionary * infoDict, OSKextInitResult *result);

	virtual OSKextInitResult initWithCodelessInfo(
		OSDictionary * infoDict);

	static void setAllVMAttributes(void);

	virtual bool setInfoDictionaryAndPath(
		OSDictionary * aDictionary,
		OSString     * aPath);
	virtual bool setExecutable(
		OSData       * anExecutable,
		OSData       * externalData        = NULL,
		bool           externalDataIsMkext = false);
	virtual OSKextInitResult registerIdentifier(void);

	virtual void free(void) APPLE_KEXT_OVERRIDE;

	static OSReturn removeKext(
		OSKext * aKext,
		bool     terminateServicesAndRemovePersonalitiesFlag = false);

	virtual bool isInExcludeList(void);
	virtual bool isLoadable(void);

	static OSKext * allocAndInitFakeKext(
		kmod_info_t *kmod_info);




	virtual bool resolveDependencies(
		OSArray * loopStack = NULL); 
	virtual bool addBleedthroughDependencies(OSArray * anArray);
	virtual bool flushDependencies(bool forceFlag = false); 
	virtual uint32_t  getNumDependencies(void);
	virtual OSArray * getDependencies(void);


	static OSReturn loadFromMkext(
		OSKextLogSpec   clientLogSpec,
		char          * mkextBuffer,
		uint32_t        mkextBufferLength,
		char         ** logInfoOut,
		uint32_t      * logInfoLengthOut);
	static OSReturn handleRequest(
		host_priv_t     hostPriv,
		OSKextLogSpec   clientLogSpec,
		char          * requestBuffer,
		uint32_t        requestLength,
		char         ** responseOut,
		uint32_t      * responseLengthOut,
		char         ** logInfoOut,
		uint32_t      * logInfoLengthOut);
	static OSReturn loadCodelessKext(
		OSString      * kextIdentifier,
		OSDictionary  * requestDict);
	static OSReturn serializeLogInfo(
		OSArray   * logInfoArray,
		char     ** logInfoOut,
		uint32_t  * logInfoLengthOut);


	static bool addKextsFromKextCollection(kernel_mach_header_t *mh,
	    OSDictionary *infoDict, const char *text_seg_name,
	    OSData **kcUUID, kc_kind_t type);

	static bool addKextsFromKextCollection(kernel_mach_header_t *mh,
	    OSDictionary *infoDict, const char *text_seg_name,
	    OSSharedPtr<OSData> &kcUUID, kc_kind_t type);

	static bool registerDeferredKextCollection(kernel_mach_header_t *mh,
	    OSSharedPtr<OSObject> &parsedXML, kc_kind_t type);
	static OSSharedPtr<OSObject> consumeDeferredKextCollection(kc_kind_t type);

	virtual OSReturn load(
		OSKextExcludeLevel   startOpt         = kOSKextExcludeNone,
		OSKextExcludeLevel   startMatchingOpt = kOSKextExcludeAll,
		OSArray            * personalityNames = NULL);
	virtual OSReturn unload(void);
	static OSReturn queueKextNotification(
		const char * notificationName,
		OSString   * kextIdentifier,
		OSData     * dextUniqueIdentifier);

	static void recordIdentifierRequest(
		OSString * kextIdentifier);

	virtual OSReturn slidePrelinkedExecutable(bool doCoalesedSlides);
	virtual OSReturn loadExecutable(void);
	virtual void     jettisonLinkeditSegment(void);
	virtual void     jettisonDATASegmentPadding(void);
	static  void     considerDestroyingLinkContext(void);
	virtual OSData * getExecutable(void);
	virtual void     setLinkedExecutable(OSData * anExecutable);

	friend  void OSKextRegisterKextsWithDTrace(void);
	static  void registerKextsWithDTrace(void);
	virtual void registerWithDTrace(void);
	virtual void unregisterWithDTrace(void);

	virtual OSReturn start(bool startDependenciesFlag = true);
	virtual OSReturn stop(void);
	virtual OSReturn setVMAttributes(bool protect, bool wire);
	virtual boolean_t segmentShouldBeWired(kernel_segment_command_t *seg);
	virtual OSReturn validateKextMapping(bool startFlag);
	virtual boolean_t verifySegmentMapping(kernel_segment_command_t *seg);

	static OSPtr<OSArray> copyAllKextPersonalities(
		bool filterSafeBootFlag = false);

	static  void  setPrelinkedPersonalities(OSArray * personalitiesArray);

	static  void  sendAllKextPersonalitiesToCatalog(
		bool startMatching = false);
	virtual OSReturn  sendPersonalitiesToCatalog(
		bool      startMatching    = false,
		OSArray * personalityNames = NULL);

	static bool canUnloadKextWithIdentifier(
		OSString * kextIdentifier,
		bool       checkClassesFlag = true);

	static OSReturn autounloadKext(OSKext * aKext);


	static OSReturn pingIOKitDaemon(void);
	static bool driverkitEnabled(void);


	static  OSPtr<OSDictionary> copyLoadedKextInfo(
		OSArray * kextIdentifiers = NULL,
		OSArray * keys = NULL);
	static  OSPtr<OSDictionary> copyLoadedKextInfoByUUID(
		OSArray * kextIdentifiers = NULL,
		OSArray * keys = NULL);
	static  OSPtr<OSDictionary> copyKextCollectionInfo(
		OSDictionary *requestDict,
		OSArray  *infoKeys = NULL);
	static OSPtr<OSData> copyKextUUIDForAddress(OSNumber *address = NULL);
	static OSPtr<OSArray> copyDextsInfo(
		OSArray * kextIdentifiers = NULL,
		OSArray * keys = NULL);
	virtual OSPtr<OSDictionary> copyInfo(OSArray * keys = NULL);


	static OSKextLogSpec setUserSpaceLogFilter(
		OSKextLogSpec  userLogSpec,
		bool           captureFlag = false);
	static OSPtr<OSArray> clearUserSpaceLogFilter(void);
	static OSKextLogSpec getUserSpaceLogFilter(void);


	virtual OSReturn addClass(
		OSMetaClass * aClass,
		uint32_t     numClasses);
	virtual OSReturn removeClass(
		OSMetaClass * aClass);
	virtual bool    hasOSMetaClassInstances(void);
	virtual OSSet * getMetaClasses(void);

	virtual void reportOSMetaClassInstances(
		OSKextLogSpec msgLogSpec);


	static OSReturn loadFileSetKexts(OSDictionary * requestDict);

	static OSReturn loadKCFileSet(const char *filepath, kc_kind_t type);


	static void jettisonFileSetLinkeditSegment(kernel_mach_header_t *mh);
	static OSReturn validateKCFileSetUUID(
		OSDictionary         *infoDict,
		kc_kind_t            type);

	static OSReturn validateKCUUIDfromPrelinkInfo(
		uuid_t               *loaded_kcuuid,
		kc_kind_t             type,
		OSDictionary         *infoDict,
		const char           *uuid_key);

	static OSReturn dispatchResource(OSDictionary * requestDict);

	static OSReturn setMissingAuxKCBundles(OSDictionary * requestDict);

	static OSReturn setAuxKCBundleAvailable(OSString *kextIdentifier,
	    OSDictionary *requestDict);

	static OSReturn dequeueCallbackForRequestTag(
		OSKextRequestTag    requestTag,
		LIBKERN_RETURNS_RETAINED OSDictionary     ** callbackRecordOut);
	static OSReturn dequeueCallbackForRequestTag(
		OSNumber     *    requestTagNum,
		LIBKERN_RETURNS_RETAINED OSDictionary ** callbackRecordOut);

	static OSReturn dequeueCallbackForRequestTag(
		OSKextRequestTag    requestTag,
		OSSharedPtr<OSDictionary> &callbackRecordOut);
	static OSReturn dequeueCallbackForRequestTag(
		OSNumber     *    requestTagNum,
		OSSharedPtr<OSDictionary> &callbackRecordOut);

	static void invokeRequestCallback(
		OSDictionary * callbackRecord,
		OSReturn         requestResult);
	virtual void invokeOrCancelRequestCallbacks(
		OSReturn callbackResult,
		bool     invokeFlag = true);
	virtual uint32_t countRequestCallbacks(void);
	OSReturn resetMutableSegments(void);
	virtual OSData * getDextUniqueID(void);

	static bool upgradeDext(
		OSKext * olddext,
		OSKext * newdext);
	static bool removeDext(OSKext * dext);
	static void replaceDextInternal(
		OSKext * olddext,
		OSKext * newdext);

public:
	enum {
		kPrintKextsLock    = 0x01,
		kPrintKextsUnslide = 0x02,
		kPrintKextsTerse   = 0x04
	};
	static void printKextsInBacktrace(
		vm_offset_t   * addr,
		unsigned int    cnt,
		int          (* printf_func)(const char *fmt, ...),
		uint32_t        flags);
	static void foreachKextInBacktrace(
		vm_offset_t   * addr,
		uint32_t        cnt,
		uint32_t        flags,
		void         (^ handler)(OSKextLoadedKextSummary *summary, uint32_t index));
	bool isDriverKit(void);
	bool isInFileset(void);
private:
	static OSKextLoadedKextSummary *summaryForAddress(const uintptr_t addr);
	static kern_return_t summaryForAddressExt(
		const void              * addr,
		OSKextLoadedKextSummary * summary);
	static void *kextForAddress(const void *addr);
	static boolean_t summaryIsInBacktrace(
		OSKextLoadedKextSummary * summary,
		vm_offset_t             * addr,
		unsigned int              cnt);
	static void printSummary(
		OSKextLoadedKextSummary * summary,
		int                    (* printf_func)(const char *fmt, ...),
		uint32_t                  flags);

	static int saveLoadedKextPanicListTyped(
		const char * prefix,
		int          invertFlag,
		int          libsFlag,
		char       * paniclist,
		uint32_t     list_size);
	static void saveLoadedKextPanicList(void);
	void savePanicString(bool isLoading);
	static void printKextPanicLists(int (*printf_func)(const char *fmt, ...));


	static void updateLoadedKextSummaries(void);
	void updateLoadedKextSummary(OSKextLoadedKextSummary *summary);
	void updateActiveAccount(OSKextActiveAccount *accountp);
	static void removeDaemonExitRequests(void);

public:


	virtual void               setCPPInitialized(bool initialized = true);

public:
	
	static OSPtr<OSKext> lookupKextWithIdentifier(const char * kextIdentifier);
	static OSPtr<OSKext> lookupKextWithIdentifier(OSString * kextIdentifier);
	static OSPtr<OSKext> lookupKextWithLoadTag(OSKextLoadTag aTag);
	static OSPtr<OSKext> lookupKextWithAddress(vm_address_t address);
	static OSPtr<OSKext> lookupKextWithUUID(uuid_t uuid);
	static OSPtr<OSKext> lookupDextWithIdentifier(OSString * dextIdentifier, OSData *dextUniqueIdentifier);

	kernel_section_t *lookupSection(const char *segname, const char*secname);

	static bool isKextWithIdentifierLoaded(const char * kextIdentifier);

	static OSReturn loadKextWithIdentifier(
		const char       * kextIdentifier,
		Boolean            allowDeferFlag      = true,
		Boolean            delayAutounloadFlag = false,
		OSKextExcludeLevel startOpt            = kOSKextExcludeNone,
		OSKextExcludeLevel startMatchingOpt    = kOSKextExcludeAll,
		OSArray          * personalityNames    = NULL);

	static OSReturn loadKextWithIdentifier(
		OSString         * kextIdentifier,
		LIBKERN_RETURNS_RETAINED_ON_ZERO OSObject        ** kextRef,
		Boolean            allowDeferFlag      = true,
		Boolean            delayAutounloadFlag = false,
		OSKextExcludeLevel startOpt            = kOSKextExcludeNone,
		OSKextExcludeLevel startMatchingOpt    = kOSKextExcludeAll,
		OSArray          * personalityNames    = NULL);

	static OSReturn loadKextWithIdentifier(
		OSString         *    kextIdentifier,
		OSSharedPtr<OSObject> &kextRef,
		Boolean                allowDeferFlag      = true,
		Boolean                delayAutounloadFlag = false,
		OSKextExcludeLevel     startOpt            = kOSKextExcludeNone,
		OSKextExcludeLevel     startMatchingOpt    = kOSKextExcludeAll,
		OSArray              * personalityNames    = NULL);

	static OSReturn loadKextFromKC(OSKext *theKext, OSDictionary *requestDict);

	static void dropMatchingReferences(
		OSSet * kexts);

	bool hasDependency(const OSSymbol * depID);

	static OSReturn removeKextWithIdentifier(
		const char * kextIdentifier,
		bool         terminateServicesAndRemovePersonalitiesFlag = false);
	static OSReturn removeKextWithLoadTag(
		OSKextLoadTag loadTag,
		bool          terminateServicesAndRemovePersonalitiesFlag = false);
	static OSReturn requestDaemonLaunch(
		OSString        * kextIdentifier,
		OSString        * serverName,
		OSNumber        * serverTag,
		OSBoolean       * reslide,
		class IOUserServerCheckInToken * checkInToken,
			OSData *serverDUI);
	static OSReturn notifyDextUpgrade(
		OSString        * kextIdentifier,
		OSData          * dextUniqueIdentifier);
	static OSReturn requestResource(
		const char                    * kextIdentifier,
		const char                    * resourceName,
		OSKextRequestResourceCallback   callback,
		void                          * context,
		OSKextRequestTag              * requestTagOut);
	static OSReturn cancelRequest(
		OSKextRequestTag    requestTag,
		void             ** contextOut);

	static void     considerUnloads(Boolean rescheduleOnlyFlag = false);
	static void     flushNonloadedKexts(Boolean flushPrelinkedKexts);
	static void     setIOKitDaemonActive(bool active = true);
	static void     setDeferredLoadSucceeded(Boolean succeeded = true);
	static void     considerRebuildOfPrelinkedKernel(void);
	static void     createExcludeListFromBooterData(
		OSDictionary * theDictionary,
		OSCollectionIterator * theIterator);
	static void     createExcludeListFromPrelinkInfo(OSArray * theInfoArray);
	static boolean_t updateExcludeList(OSDictionary * infoDict);

	static bool     pendingIOKitDaemonRequests(void);

	virtual bool    setAutounloadEnabled(bool flag);

	virtual const OSObject   * getBundleExecutable(void);
	virtual const OSSymbol   * getIdentifier(void);
	virtual const char       * getIdentifierCString(void);
	virtual OSKextVersion      getVersion(void);
	virtual OSKextVersion      getCompatibleVersion(void);
	virtual bool               isLibrary(void);
	virtual bool               isCompatibleWithVersion(OSKextVersion aVersion);
	virtual OSObject         * getPropertyForHostArch(const char * key);

	virtual OSKextLoadTag      getLoadTag(void);
	virtual void               getSizeInfo(uint32_t *loadSize, uint32_t *wiredSize);
	virtual OSPtr<OSData>          copyUUID(void);
	OSPtr<OSData>                  copyTextUUID(void);
	OSPtr<OSData>                  copyMachoUUID(const kernel_mach_header_t * header);
	OSPtr<OSDextStatistics>        copyDextStatistics();
	virtual OSPtr<OSArray>         copyPersonalitiesArray(void);
	static bool                copyUserExecutablePath(const OSSymbol * bundleID, char * pathResult, size_t pathSize);
	virtual void               setDriverKitUUID(LIBKERN_CONSUMED OSData *uuid);
	static  bool               incrementDextLaunchCount(OSKext *dext, OSData *dextUniqueIDToMatch);
	static  bool               decrementDextLaunchCount(OSString *bundleID);


	virtual void               removePersonalitiesFromCatalog(void);

	virtual void               updatePersonalitiesInCatalog(OSArray *upgradedPersonalities);


	static void uniquePersonalityProperties(OSDictionary * personalityDict);
	static void uniquePersonalityProperties(OSDictionary * personalityDict, bool defaultAddKernelBundleIdentifier);

	static bool                iokitDaemonActive(void);

	virtual bool               declaresExecutable(void); 
	virtual bool               isInterface(void);
	virtual bool               isKernel(void);
	virtual bool               isKernelComponent(void);
	virtual bool               isExecutable(void);
	virtual bool               isSpecialKernelBinary(void);
	virtual bool               isLoadableInSafeBoot(void);
	virtual bool               isPrelinked(void);
	virtual bool               isLoaded(void);
	virtual bool               isStarted(void);
	virtual bool               isCPPInitialized(void);

	const char *
	getKCTypeString(void)
	{
		switch (kc_type) {
		case KCKindPrimary:
			return kKCTypePrimary;
		case KCKindPageable:
			return kKCTypeSystem;
		case KCKindAuxiliary:
			return kKCTypeAuxiliary;
		case KCKindNone:
			return kKCTypeCodeless;
		default:
			return "??";
		}
	}
};
extern "C" void OSKextResetAfterUserspaceReboot(void);
class OSMetaClass : public OSMetaClassBase
{
	friend class OSKext;

private:

	static void * operator new(size_t size);


	struct ExpansionData *reserved;


	const OSMetaClass *superClassLink;


	const OSSymbol *className;


	unsigned int classSize;


	mutable unsigned int instanceCount;


	OSMetaClass();



	static void logError(OSReturn result);

public:


	static const OSMetaClass * getMetaClassWithName(const OSSymbol * name);



	static const OSMetaClass * copyMetaClassWithName(const OSSymbol * name);

	void releaseMetaClass() const;


protected:

	virtual void retain() const;



	virtual void release() const;



	virtual void release(int freeWhen) const;



	virtual void taggedRetain(const void * tag = NULL) const;



	virtual void taggedRelease(const void * tag = NULL) const;



	virtual void taggedRelease(
		const void * tag,
		const int    freeWhen) const;



	virtual int getRetainCount() const;



	virtual const OSMetaClass * getMetaClass() const;



	OSMetaClass(const char * className,
	    const OSMetaClass  * superclass,
	    unsigned int         classSize);


	OSMetaClass(const char * className,
	    const OSMetaClass  * superclass,
	    unsigned int         classSize,
	    zone_t             * zone,
	    const char         * zone_name,
	    zone_create_flags_t  zflags);


	virtual
	~OSMetaClass();



	void
	operator delete(void *, size_t)
	{
	}

public:
	static const OSMetaClass * const metaClass;


	static void * preModLoad(const char * kextID);



	static bool checkModLoad(void * loadHandle);



	static OSReturn postModLoad(void * loadHandle);


	static bool modHasInstance(const char * kextID);



	static void reportModInstances(const char * kextID);



	static void considerUnloads();

	static bool removeClasses(OSCollection * metaClasses);


	static OSObject * allocClassWithName(const OSSymbol * name);



	static OSObject * allocClassWithName(const OSString * name);



	static OSObject * allocClassWithName(const char * name);



	static OSMetaClassBase * checkMetaCastWithName(
		const OSSymbol        * className,
		const OSMetaClassBase * object);


	static OSMetaClassBase * checkMetaCastWithName(
		const OSString        * className,
		const OSMetaClassBase * object);


	static OSMetaClassBase * checkMetaCastWithName(
		const char            * className,
		const OSMetaClassBase * object);



	void instanceConstructed() const;



	void instanceDestructed() const;



	OSMetaClassBase * checkMetaCast(const OSMetaClassBase * object) const;



	unsigned int getInstanceCount() const;



	const OSMetaClass * getSuperClass() const;


	const OSSymbol * getKmodName() const;



	const char * getClassName() const;
	const OSSymbol * getClassNameSymbol() const;



	unsigned int getClassSize() const;



	virtual OSObject * alloc() const = 0;

	OSKext * getKext() const;
	void addInstance(const OSObject * instance, bool super = false) const;
	void removeInstance(const OSObject * instance, bool super = false) const;
	void applyToInstances(OSMetaClassInstanceApplierFunction applier,
	    void * context) const;
	static void applyToInstancesOfClassName(
		const OSSymbol * name,
		OSMetaClassInstanceApplierFunction  applier,
		void * context);
private:
	static void applyToInstances(OSOrderedSet * set,
	    OSMetaClassInstanceApplierFunction  applier,
	    void * context);
public:






































































	void reservedCalled(int ind) const;
















	static void printInstanceCounts();
	static void serializeClassDictionary(OSDictionary * dict);

private:

	static OSDictionary * getClassDictionary();
	virtual bool serialize(OSSerialize * serializer) const;


	OSMetaClassDeclareReservedUnused(OSMetaClass, 0);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 1);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 2);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 3);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 4);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 5);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 6);
	OSMetaClassDeclareReservedUnused(OSMetaClass, 7);
};
class OSNumber : public OSObject
{
	friend class OSSerialize;

	OSDeclareDefaultStructors(OSNumber);


public:


	static OSPtr<OSNumber> withNumber(
		unsigned long long value,
		unsigned int       numberOfBits);


	static OSPtr<OSNumber> withDouble(
		double             value);

	static OSPtr<OSNumber> withFloat(
		float              value);


	static OSPtr<OSNumber> withNumber(
		const char   * valueString,
		unsigned int   numberOfBits);



	virtual bool init(
		unsigned long long value,
		unsigned int       numberOfBits);



	virtual bool init(
		const char   * valueString,
		unsigned int   numberOfBits);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int numberOfBits() const;



	virtual unsigned int numberOfBytes() const;







	virtual unsigned char unsigned8BitValue() const;



	virtual unsigned short unsigned16BitValue() const;



	virtual unsigned int unsigned32BitValue() const;



	virtual unsigned long long unsigned64BitValue() const;


	double doubleValue() const;

	float floatValue() const;





	virtual void addValue(signed long long value);



	virtual void setValue(unsigned long long value);




	virtual bool isEqualTo(const OSNumber * aNumber) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;


	OSMetaClassDeclareReservedUnused(OSNumber, 0);
	OSMetaClassDeclareReservedUnused(OSNumber, 1);
	OSMetaClassDeclareReservedUnused(OSNumber, 2);
	OSMetaClassDeclareReservedUnused(OSNumber, 3);
	OSMetaClassDeclareReservedUnused(OSNumber, 4);
	OSMetaClassDeclareReservedUnused(OSNumber, 5);
	OSMetaClassDeclareReservedUnused(OSNumber, 6);
	OSMetaClassDeclareReservedUnused(OSNumber, 7);
};
class OSObject : public OSMetaClassBase
{
	OSDeclareAbstractStructorsWithDispatchAndNoOperators(OSObject);


private:

	mutable int retainCount;

protected:




	virtual void release(int freeWhen) const APPLE_KEXT_OVERRIDE;


	virtual void taggedRelease(const void * tag, const int freeWhen) const APPLE_KEXT_OVERRIDE;



	virtual bool init();



	virtual void free();



	static void operator delete(void * mem, size_t size);






	static void * operator new(size_t size);

public:


	virtual int getRetainCount() const APPLE_KEXT_OVERRIDE;



	virtual void retain() const APPLE_KEXT_OVERRIDE;



	virtual void release() const APPLE_KEXT_OVERRIDE;



	virtual void taggedRetain(const void * tag = NULL) const APPLE_KEXT_OVERRIDE;



	virtual void taggedRelease(const void * tag = NULL) const APPLE_KEXT_OVERRIDE;




	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;


	bool taggedTryRetain(const void *tag) const;

	bool iterateObjects(void * refcon, bool (*callback)(void * refcon, OSObject * object));



	OSMetaClassDeclareReservedUnused(OSObject, 0);
	OSMetaClassDeclareReservedUnused(OSObject, 1);
	OSMetaClassDeclareReservedUnused(OSObject, 2);
	OSMetaClassDeclareReservedUnused(OSObject, 3);
	OSMetaClassDeclareReservedUnused(OSObject, 4);
	OSMetaClassDeclareReservedUnused(OSObject, 5);
	OSMetaClassDeclareReservedUnused(OSObject, 6);
	OSMetaClassDeclareReservedUnused(OSObject, 7);
	OSMetaClassDeclareReservedUnused(OSObject, 8);
	OSMetaClassDeclareReservedUnused(OSObject, 9);
	OSMetaClassDeclareReservedUnused(OSObject, 10);
	OSMetaClassDeclareReservedUnused(OSObject, 11);
	OSMetaClassDeclareReservedUnused(OSObject, 12);
	OSMetaClassDeclareReservedUnused(OSObject, 13);
	OSMetaClassDeclareReservedUnused(OSObject, 14);
	OSMetaClassDeclareReservedUnused(OSObject, 15);
};
class OSOrderedSet : public OSCollection
{
	OSDeclareDefaultStructors(OSOrderedSet);

public:

	typedef SInt32 (*OSOrderFunction)(const OSMetaClassBase * obj1,
	    const OSMetaClassBase * obj2,
	    void * context);

	typedef int32_t (^OSOrderBlock)(const OSMetaClassBase * obj1,
	    const OSMetaClassBase * obj2);

protected:
	struct _Element * array;
	OSOrderFunction   ordering;
	void            * orderingRef;
	unsigned int      count;
	unsigned int      capacity;
	unsigned int      capacityIncrement;

	struct ExpansionData { };


	ExpansionData *reserved;

protected:

	virtual unsigned int iteratorSize() const APPLE_KEXT_OVERRIDE;
	virtual bool initIterator(void *iterator) const APPLE_KEXT_OVERRIDE;
	virtual bool getNextObjectForIterator(void *iterator, OSObject **ret) const APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<OSOrderedSet> withCapacity(
		unsigned int      capacity,
		OSOrderFunction   orderFunc = NULL,
		void            * orderingContext = NULL);

	static OSPtr<OSOrderedSet> withCapacity(
		unsigned int      capacity,
		OSOrderBlock      orderBlock);



	virtual bool initWithCapacity(
		unsigned int      capacity,
		OSOrderFunction   orderFunc = NULL,
		void            * orderingContext = NULL);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCount() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacity() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacityIncrement() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int setCapacityIncrement(unsigned increment) APPLE_KEXT_OVERRIDE;



	virtual unsigned int ensureCapacity(unsigned int newCapacity) APPLE_KEXT_OVERRIDE;



	virtual void flushCollection() APPLE_KEXT_OVERRIDE;



	virtual bool setObject(const OSMetaClassBase * anObject);

	bool setObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool setFirstObject(const OSMetaClassBase * anObject);

	bool setFirstObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool setLastObject(const OSMetaClassBase * anObject);

	bool setLastObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual void removeObject(const OSMetaClassBase * anObject);

	void removeObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool containsObject(const OSMetaClassBase * anObject) const;



	virtual bool member(const OSMetaClassBase * anObject) const;



	virtual OSObject * getFirstObject() const;



	virtual OSObject * getLastObject() const;



	virtual SInt32 orderObject(const OSMetaClassBase * anObject);



	virtual bool setObject(
		unsigned int            index,
		const OSMetaClassBase * anObject);

	bool setObject(
		unsigned int            index,
		OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual OSObject * getObject(unsigned int index) const;



	virtual void * getOrderingRef();



	virtual bool isEqualTo(const OSOrderedSet * anOrderedSet) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual unsigned setOptions(
		unsigned   options,
		unsigned   mask,
		void     * context = NULL) APPLE_KEXT_OVERRIDE;



	OSPtr<OSCollection> copyCollection(OSDictionary * cycleDict = NULL) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSOrderedSet, 0);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 1);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 2);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 3);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 4);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 5);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 6);
	OSMetaClassDeclareReservedUnused(OSOrderedSet, 7);
};
OSObjectPtr
OSUnserializeBinary(const void *buffer, size_t bufferSize);
class OSSerialize : public OSObject
{
	OSDeclareDefaultStructors(OSSerialize);
	friend class OSBoolean;

private:
	char         * data;           
	unsigned int   length;         
	unsigned int   capacity;       
	unsigned int   capacityIncrement;

	OSPtr<OSArray> tags;                    

public:
	typedef const OSMetaClassBase * (*Editor)(void                  * reference,
	    OSSerialize           * s,
	    OSCollection          * container,
	    const OSSymbol        * name,
	    const OSMetaClassBase * value);

	bool     binary;
	bool     endCollection;
	Editor   editor;
	void   * editRef;
	OSPtr<OSData> indexData;

	bool binarySerialize(const OSMetaClassBase *o);
	bool binarySerializeInternal(const OSMetaClassBase *o);
	bool addBinary(const void * data, size_t size);
	bool addBinaryObject(const OSMetaClassBase * o, uint32_t key, const void * _bits, uint32_t size,
	    uint32_t * startCollection);
	void endBinaryCollection(uint32_t startCollection);

public:


	static OSPtr<OSSerialize> withCapacity(unsigned int capacity);

	static OSPtr<OSSerialize> binaryWithCapacity(unsigned int inCapacity, Editor editor = NULL, void * reference = NULL);
	void setIndexed(bool index);


	virtual char * text() const;



	virtual void clearText();




	virtual bool previouslySerialized(const OSMetaClassBase * object);



	virtual bool addXMLStartTag(
		const OSMetaClassBase * object,
		const char            * tagString);



	virtual bool addXMLEndTag(const char * tagString);



	virtual bool addChar(const char aChar);



	virtual bool addString(const char * cString);



	virtual bool initWithCapacity(unsigned int inCapacity);
	virtual unsigned int getLength() const;
	virtual unsigned int getCapacity() const;
	virtual unsigned int getCapacityIncrement() const;
	virtual unsigned int setCapacityIncrement(unsigned increment);
	virtual unsigned int ensureCapacity(unsigned int newCapacity);
	virtual void free() APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSSerialize, 0);
	OSMetaClassDeclareReservedUnused(OSSerialize, 1);
	OSMetaClassDeclareReservedUnused(OSSerialize, 2);
	OSMetaClassDeclareReservedUnused(OSSerialize, 3);
	OSMetaClassDeclareReservedUnused(OSSerialize, 4);
	OSMetaClassDeclareReservedUnused(OSSerialize, 5);
	OSMetaClassDeclareReservedUnused(OSSerialize, 6);
	OSMetaClassDeclareReservedUnused(OSSerialize, 7);
};
class OSSerializer : public OSObject
{
	OSDeclareDefaultStructors(OSSerializer);

	void * target;
	void * ref;
	OSSerializerCallback callback;

public:

	static OSPtr<OSSerializer> forTarget(
		void * target,
		OSSerializerCallback callback,
		void * ref = NULL);


	virtual void free( void ) APPLE_KEXT_OVERRIDE;

	static bool callbackToBlock(void * target, void * ref,
	    OSSerialize * serializer);

	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;
};
class OSSet : public OSCollection
{
	friend class OSSerialize;

	OSDeclareDefaultStructors(OSSet);



	virtual unsigned int iteratorSize() const APPLE_KEXT_OVERRIDE;
	virtual bool initIterator(void * iterator) const APPLE_KEXT_OVERRIDE;
	virtual bool getNextObjectForIterator(void * iterator, OSObject ** ret) const APPLE_KEXT_OVERRIDE;

public:



	static OSPtr<OSSet> withCapacity(unsigned int capacity);



	static OSPtr<OSSet> withObjects(
		const OSObject * objects[],
		unsigned int     count,
		unsigned int     capacity = 0);



	static OSPtr<OSSet> withArray(
		const OSArray * array,
		unsigned int    capacity = 0);



	static OSPtr<OSSet> withSet(const OSSet * set,
	    unsigned int capacity = 0);



	virtual bool initWithCapacity(unsigned int capacity);



	virtual bool initWithObjects(
		const OSObject * objects[],
		unsigned int     count,
		unsigned int     capacity = 0);



	virtual bool initWithArray(
		const OSArray * array,
		unsigned int capacity = 0);



	virtual bool initWithSet(const OSSet *set,
	    unsigned int capacity = 0);



	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCount() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacity() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int getCapacityIncrement() const APPLE_KEXT_OVERRIDE;



	virtual unsigned int setCapacityIncrement(unsigned increment) APPLE_KEXT_OVERRIDE;



	virtual unsigned int ensureCapacity(unsigned int newCapacity) APPLE_KEXT_OVERRIDE;



	virtual void flushCollection() APPLE_KEXT_OVERRIDE;



	virtual bool setObject(const OSMetaClassBase * anObject);

	bool setObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool merge(const OSArray * array);



	virtual bool merge(const OSSet * set);



	virtual void removeObject(const OSMetaClassBase * anObject);

	void removeObject(OSSharedPtr<const OSMetaClassBase> const& anObject);



	virtual bool containsObject(const OSMetaClassBase * anObject) const;



	virtual bool member(const OSMetaClassBase * anObject) const;



	virtual OSObject * getAnyObject() const;



	virtual bool isEqualTo(const OSSet * aSet) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;



	virtual unsigned setOptions(unsigned options, unsigned mask, void * context = NULL) APPLE_KEXT_OVERRIDE;



	OSPtr<OSCollection>  copyCollection(OSDictionary *cycleDict = NULL) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSSet, 0);
	OSMetaClassDeclareReservedUnused(OSSet, 1);
	OSMetaClassDeclareReservedUnused(OSSet, 2);
	OSMetaClassDeclareReservedUnused(OSSet, 3);
	OSMetaClassDeclareReservedUnused(OSSet, 4);
	OSMetaClassDeclareReservedUnused(OSSet, 5);
	OSMetaClassDeclareReservedUnused(OSSet, 6);
	OSMetaClassDeclareReservedUnused(OSSet, 7);
};
template <typename T>
class __attribute__((trivial_abi)) OSSharedPtr: public libkern::intrusive_shared_ptr<T, intrusive_osobject_retainer> {
	using libkern::intrusive_shared_ptr<T, intrusive_osobject_retainer>::intrusive_shared_ptr;
};
template <typename T, typename Tag>
class __attribute__((trivial_abi)) OSTaggedSharedPtr: public libkern::intrusive_shared_ptr<T, intrusive_tagged_osobject_retainer<Tag> > {
	using libkern::intrusive_shared_ptr<T, intrusive_tagged_osobject_retainer<Tag> >::intrusive_shared_ptr;
};
class OSString : public OSObject
{
	OSDeclareDefaultStructors(OSString);

	enum { kMaxStringLength  = 262142 };


public:


	static OSPtr<OSString> withString(const OSString * aString);



	static OSPtr<OSString> withCString(const char * cString);



	static OSPtr<OSString> withCStringNoCopy(const char * cString);


	static OSPtr<OSString> withCString(const char *cString, size_t length);


	virtual bool initWithString(const OSString * aString);



	virtual bool initWithCString(const char * cString);



	virtual bool initWithCStringNoCopy(const char * cString);

	bool initWithStringOfLength(const char *cString, size_t inlength);


	virtual void free() APPLE_KEXT_OVERRIDE;



	virtual unsigned int getLength() const;



	virtual char getChar(unsigned int index) const;



	virtual bool setChar(char aChar, unsigned int index);



	virtual const char * getCStringNoCopy() const;



	virtual bool isEqualTo(const OSString * aString) const;



	virtual bool isEqualTo(const char * cString) const;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;



	virtual bool isEqualTo(const OSData * aDataObject) const;



	virtual bool serialize(OSSerialize * serializer) const APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(OSString, 0);
	OSMetaClassDeclareReservedUnused(OSString, 1);
	OSMetaClassDeclareReservedUnused(OSString, 2);
	OSMetaClassDeclareReservedUnused(OSString, 3);
	OSMetaClassDeclareReservedUnused(OSString, 4);
	OSMetaClassDeclareReservedUnused(OSString, 5);
	OSMetaClassDeclareReservedUnused(OSString, 6);
	OSMetaClassDeclareReservedUnused(OSString, 7);
	OSMetaClassDeclareReservedUnused(OSString, 8);
	OSMetaClassDeclareReservedUnused(OSString, 9);
	OSMetaClassDeclareReservedUnused(OSString, 10);
	OSMetaClassDeclareReservedUnused(OSString, 11);
	OSMetaClassDeclareReservedUnused(OSString, 12);
	OSMetaClassDeclareReservedUnused(OSString, 13);
	OSMetaClassDeclareReservedUnused(OSString, 14);
	OSMetaClassDeclareReservedUnused(OSString, 15);
};
class OSSymbol final : public OSString
{
	friend class OSSymbolPool;

	OSDeclareDefaultStructors(OSSymbol);

private:
	struct smrq_slink hashlink;

	static void initialize();


	virtual bool initWithString(const OSString * aString) APPLE_KEXT_OVERRIDE;



	virtual bool initWithCString(const char * cString) APPLE_KEXT_OVERRIDE;



	virtual bool initWithCStringNoCopy(const char *cString) APPLE_KEXT_OVERRIDE;

protected:


	virtual void taggedRetain(
		const void * tag) const APPLE_KEXT_OVERRIDE;


	virtual void taggedRelease(
		const void * tag,
		const int    freeWhen) const APPLE_KEXT_OVERRIDE;




	virtual void free() APPLE_KEXT_OVERRIDE;

public:





	virtual void taggedRelease(const void * tag) const  APPLE_KEXT_OVERRIDE;



	static OSPtr<const OSSymbol> withString(const OSString * aString);



	static OSPtr<const OSSymbol> withCString(const char * cString);



	static OSPtr<const OSSymbol> withCStringNoCopy(const char * cString);


	static OSPtr<const OSSymbol> existingSymbolForString(const OSString *aString);


	static OSPtr<const OSSymbol> existingSymbolForCString(const char *aCString);


	virtual bool isEqualTo(const OSSymbol * aSymbol) const;



	virtual bool isEqualTo(const char * cString) const APPLE_KEXT_OVERRIDE;



	virtual bool isEqualTo(const OSMetaClassBase * anObject) const APPLE_KEXT_OVERRIDE;






	static void checkForPageUnload(
		void * startAddr,
		void * endAddr);

	static unsigned int bsearch(
		const void *  key,
		const void *  array,
		unsigned int  arrayCount,
		size_t        memberSize);

	inline void smr_free();

	inline uint32_t hash() const;


	OSMetaClassDeclareReservedUnused(OSSymbol, 0);
	OSMetaClassDeclareReservedUnused(OSSymbol, 1);
	OSMetaClassDeclareReservedUnused(OSSymbol, 2);
	OSMetaClassDeclareReservedUnused(OSSymbol, 3);
	OSMetaClassDeclareReservedUnused(OSSymbol, 4);
	OSMetaClassDeclareReservedUnused(OSSymbol, 5);
	OSMetaClassDeclareReservedUnused(OSSymbol, 6);
	OSMetaClassDeclareReservedUnused(OSSymbol, 7);
};
extern "C++" OSPtr<OSObject> OSUnserializeXML(
	const char  * buffer,
	OSString * * errorString = NULL);
extern "C++" OSPtr<OSObject> OSUnserializeXML(
	const char  * buffer,
	OSSharedPtr<OSString>& errorString);
extern "C++" OSPtr<OSObject> OSUnserializeXML(
	const char  * buffer,
	size_t        bufferSize,
	OSString *   *errorString = NULL);
extern "C++" OSPtr<OSObject> OSUnserializeXML(
	const char  * buffer,
	size_t        bufferSize,
	OSSharedPtr<OSString> &errorString);
extern "C++" OSPtr<OSObject>
OSUnserializeBinary(const char *buffer, size_t bufferSize, OSString * *errorString);
extern "C++" OSPtr<OSObject>
OSUnserializeBinary(const char *buffer, size_t bufferSize, OSSharedPtr<OSString>& errorString);
#line 61 "OSUnserializeXML.y"







typedef 



typedef 


static int              OSUnserializeerror(parser_state_t *state, const char *s);
static int              yylex(YYSTYPE *lvalp, parser_state_t *state);
static object_t         *newObject(parser_state_t *state);
static void             freeObject(parser_state_t *state, object_t *o);
static void             rememberObject(parser_state_t *state, int tag, OSObject *o);
static object_t         *retrieveObject(parser_state_t *state, int tag);
static void             cleanupObjects(parser_state_t *state);
static object_t         *buildDictionary(parser_state_t *state, object_t *o);
static object_t         *buildArray(parser_state_t *state, object_t *o);
static object_t         *buildSet(parser_state_t *state, object_t *o);
static object_t         *buildString(parser_state_t *state, object_t *o);
static object_t         *buildSymbol(parser_state_t *state, object_t *o);
static object_t         *buildData(parser_state_t *state, object_t *o);
static object_t         *buildNumber(parser_state_t *state, object_t *o);
static object_t         *buildBoolean(parser_state_t *state, object_t *o);
__BEGIN_DECLS
__END_DECLS

static inline void *
malloc_impl(size_t size)
{
	if (size == 0) {
		return NULL;
	}
	return kalloc_data(size,
	           Z_VM_TAG_BT(Z_WAITOK_ZERO, VM_KERN_MEMORY_LIBKERN));
}

static inline void
free_impl(void *addr)
{
	kfree_data_addr(addr);
}
static inline void
safe_free(void *addr, size_t size)
{
	kfree_data(addr, size);
}

static inline void *
realloc_impl(void *addr, size_t osize, size_t nsize)
{
	return krealloc_data(addr, osize, nsize,
	           Z_VM_TAG_BT(Z_WAITOK_ZERO, VM_KERN_MEMORY_LIBKERN));
}




# define YYDEBUG 0




# define YYTOKEN_TABLE 0








#line 283 "OSUnserializeXML.tab.c"






# ifdef __SIZE_TYPE__
#  define YYSIZE_T __SIZE_TYPE__
# elif defined size_t
#  define YYSIZE_T size_t
# elif !defined YYSIZE_T && (defined __STDC__ || defined __C99__FUNC__         || defined __cplusplus || defined _MSC_VER)
#  include <stddef.h> 
#  define YYSIZE_T size_t
# else
#  define YYSIZE_T unsigned int
# endif


# if defined YYENABLE_NLS && YYENABLE_NLS
#  if ENABLE_NLS
#   include <libintl.h> 
#   define YY_(msgid) dgettext ("bison-runtime", msgid)
#  endif
# endif
# ifndef YY_
#  define YY_(msgid) msgid
# endif




# define YYID(n) (n)
















static const yytype_uint8 yytranslate[] =
{
	0, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	15, 16, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 17, 2, 18, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 13, 2, 14, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 1, 2, 3, 4,
	5, 6, 7, 8, 9, 10, 11, 12
};
# define YYLLOC_DEFAULT(Current, Rhs, N)                                    do                                                                        if (YYID (N))                                                    	{                                                               	  (Current).first_line   = YYRHSLOC (Rhs, 1).first_line;        	  (Current).first_column = YYRHSLOC (Rhs, 1).first_column;      	  (Current).last_line    = YYRHSLOC (Rhs, N).last_line;         	  (Current).last_column  = YYRHSLOC (Rhs, N).last_column;       	}                                                                     else                                                              	{                                                               	  (Current).first_line   = (Current).last_line   =              	    YYRHSLOC (Rhs, 0).last_line;                                	  (Current).first_column = (Current).last_column =              	    YYRHSLOC (Rhs, 0).last_column;                              	}                                                                   while (YYID (0))




# if defined YYLTYPE_IS_TRIVIAL && YYLTYPE_IS_TRIVIAL
#  define YY_LOCATION_PRINT(File, Loc)                       fprintf (File, "%d.%d-%d.%d",                      	      (Loc).first_line, (Loc).first_column,     	      (Loc).last_line,  (Loc).last_column)
# else
#  define YY_LOCATION_PRINT(File, Loc) ((void) 0)
# endif









# define YYINITDEPTH 200



# define YYMAXDEPTH 10000





/*-----------------------------------------------.
 | Release the memory associated to this symbol.  |
 |   `-----------------------------------------------*/

/*ARGSUSED*/
{
	YYUSE(yyvaluep);

	if (!yymsg) {
		yymsg = "Deleting";
	}
	YY_SYMBOL_PRINT(yymsg, yytype, yyvaluep, yylocationp);

	switch (yytype) {
	default:
		break;
	}
}


/* Prevent warnings from -Wmissing-prototypes.  */







/*----------.
 | yyparse.  |
 |   `----------*/

{
	/* The look-ahead symbol.  */
	int yychar;

/* The semantic value of the look-ahead symbol.  */
	YYSTYPE yylval;

/* Number of syntax errors so far.  */
	int yynerrs;

	int yystate;
	int yyn;
	int yyresult;
	/* Number of tokens to shift before error messages enabled.  */
	int yyerrstatus;
	/* Look-ahead token as an internal (translated) token number.  */
	int yytoken = 0;

	/* Three stacks and their tools:
	 *  `yyss': related to states,
	 *  `yyvs': related to semantic values,
	 *  `yyls': related to locations.
	 *
	 *  Refer to the stacks thru separate pointers, to allow yyoverflow
	 *  to reallocate them elsewhere.  */

	/* The state stack.  */
	yytype_int16 yyssa[YYINITDEPTH];
	yytype_int16 *yyss = yyssa;
	yytype_int16 *yyssp;

	/* The semantic value stack.  */
	YYSTYPE yyvsa[YYINITDEPTH];
	YYSTYPE *yyvs = yyvsa;
	YYSTYPE *yyvsp;




	YYSIZE_T yystacksize = YYINITDEPTH;

	/* The variables used to return semantic value and location from the
	 *  action routines.  */
	YYSTYPE yyval;


	/* The number of symbols on the RHS of the reduced rule.
	 *  Keep to zero when no symbol should be popped.  */
	int yylen = 0;

	YYDPRINTF((stderr, "Starting parse\n"));

	yystate = 0;
	yyerrstatus = 0;
	yynerrs = 0;
	yychar = YYEMPTY;       /* Cause a token to be read.  */

	/* Initialize stack pointers.
	 *  Waste one element of value and location stack
	 *  so that they stay on the same level as the state stack.
	 *  The wasted elements are never initialized.  */

	yyssp = yyss;
	yyvsp = yyvs;

	goto yysetstate;

/*------------------------------------------------------------.
 | yynewstate -- Push a new state, which is found in yystate.  |
 |   `------------------------------------------------------------*/
yynewstate:
	/* In all cases, when you get here, the value and location stacks
	 *  have just been pushed.  So pushing a state here evens the stacks.  */
	yyssp++;

yysetstate:
	*yyssp = yystate;

	if (yyss + yystacksize - 1 <= yyssp) {
		/* Get the current used size of the three stacks, in elements.  */
		YYSIZE_T yysize = yyssp - yyss + 1;


		yyssp = yyss + yysize - 1;
		yyvsp = yyvs + yysize - 1;


		YYDPRINTF((stderr, "Stack size increased to %lu\n",
		    (unsigned long int) yystacksize));

		if (yyss + yystacksize - 1 <= yyssp) {
			YYABORT;
		}
	}

	YYDPRINTF((stderr, "Entering state %d\n", yystate));

	goto yybackup;

/*-----------.
 | yybackup.  |
 |   `-----------*/
yybackup:

	/* Do appropriate processing given the current state.  Read a
	 *  look-ahead token if we need one and don't already have one.  */

	/* First try to decide what to do without reference to look-ahead token.  */
	yyn = yypact[yystate];
	if (yyn == YYPACT_NINF) {
		goto yydefault;
	}

	/* Not known => get a look-ahead token if don't already have one.  */

	/* YYCHAR is either YYEMPTY or YYEOF or a valid look-ahead symbol.  */
	if (yychar == YYEMPTY) {
		YYDPRINTF((stderr, "Reading a token: "));
		yychar = YYLEX;
	}

	if (yychar <= YYEOF) {
		yychar = yytoken = YYEOF;
		YYDPRINTF((stderr, "Now at end of input.\n"));
	} else {
		yytoken = YYTRANSLATE(yychar);
		YY_SYMBOL_PRINT("Next token is", yytoken, &yylval, &yylloc);
	}

	/* If the proper action on seeing token YYTOKEN is to reduce or to
	 *  detect an error, take that action.  */
	yyn += yytoken;
	if (yyn < 0 || YYLAST < yyn || yycheck[yyn] != yytoken) {
		goto yydefault;
	}
	yyn = yytable[yyn];
	if (yyn <= 0) {
		if (yyn == 0 || yyn == YYTABLE_NINF) {
			goto yyerrlab;
		}
		yyn = -yyn;
		goto yyreduce;
	}

	if (yyn == YYFINAL) {
		YYACCEPT;
	}

	/* Count tokens shifted since error; after three, turn off error
	 *  status.  */
	if (yyerrstatus) {
		yyerrstatus--;
	}

	/* Shift the look-ahead token.  */
	YY_SYMBOL_PRINT("Shifting", yytoken, &yylval, &yylloc);

	/* Discard the shifted token unless it is eof.  */
	if (yychar != YYEOF) {
		yychar = YYEMPTY;
	}

	yystate = yyn;
	*++yyvsp = yylval;

	goto yynewstate;


/*-----------------------------------------------------------.
 | yydefault -- do the default action for the current state.  |
 |   `-----------------------------------------------------------*/
yydefault:
	yyn = yydefact[yystate];
	if (yyn == 0) {
		goto yyerrlab;
	}
	goto yyreduce;


/*-----------------------------.
 | yyreduce -- Do a reduction.  |
 |   `-----------------------------*/
yyreduce:
	/* yyn is the number of a rule to reduce with.  */
	yylen = yyr2[yyn];

	/* If YYLEN is nonzero, implement the default value of the action:
	 *  `$$ = $1'.
	 *
	 *  Otherwise, the following line sets YYVAL to garbage.
	 *  This behavior is undocumented and Bison
	 *  users should not rely upon it.  Assigning to YYVAL
	 *  unconditionally makes the parser a bit smaller, and it avoids a
	 *  GCC warning that YYVAL may be used uninitialized.  */
	yyval = yyvsp[1 - yylen];


	YY_REDUCE_PRINT(yyn);
	switch (yyn) {
	case 2:
#line 217 "OSUnserializeXML.y"
		{ yyerror("unexpected end of buffer");
		  YYERROR;
		  ;}
		break;

	case 3:
#line 220 "OSUnserializeXML.y"
		{ STATE->parsedObject = (yyvsp[(1) - (1)])->object;
		  (yyvsp[(1) - (1)])->object = 0;
		  freeObject(STATE, (yyvsp[(1) - (1)]));
		  YYACCEPT;
		  ;}
		break;

	case 4:
#line 225 "OSUnserializeXML.y"
		{ yyerror("syntax error");
		  YYERROR;
		  ;}
		break;

	case 5:
#line 230 "OSUnserializeXML.y"
		{ (yyval) = buildDictionary(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildDictionary");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 6:
#line 242 "OSUnserializeXML.y"
		{ (yyval) = buildArray(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildArray");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 7:
#line 254 "OSUnserializeXML.y"
		{ (yyval) = buildSet(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildSet");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 8:
#line 266 "OSUnserializeXML.y"
		{ (yyval) = buildString(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildString");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 9:
#line 278 "OSUnserializeXML.y"
		{ (yyval) = buildData(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildData");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 10:
#line 290 "OSUnserializeXML.y"
		{ (yyval) = buildNumber(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildNumber");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 11:
#line 302 "OSUnserializeXML.y"
		{ (yyval) = buildBoolean(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildBoolean");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 12:
#line 314 "OSUnserializeXML.y"
		{ (yyval) = retrieveObject(STATE, (yyvsp[(1) - (1)])->idref);
		  if ((yyval)) {
			  STATE->retrievedObjectCount++;
			  (yyval)->object->retain();
			  if (STATE->retrievedObjectCount > MAX_REFED_OBJECTS) {
				  yyerror("maximum object reference count");
				  YYERROR;
			  }
		  } else {
			  yyerror("forward reference detected");
			  YYERROR;
		  }
		  freeObject(STATE, (yyvsp[(1) - (1)]));

		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 13:
#line 338 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 14:
#line 341 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 17:
#line 348 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(2) - (2)]);
		  (yyval)->next = (yyvsp[(1) - (2)]);

		  object_t *o;
		  o = (yyval)->next;
		  while (o) {
			  if (o->key == (yyval)->key) {
				  yyerror("duplicate dictionary key");
				  YYERROR;
			  }
			  o = o->next;
		  }
		  ;}
		break;

	case 18:
#line 363 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->key = (OSSymbol *)(yyval)->object;
		  (yyval)->object = (yyvsp[(2) - (2)])->object;
		  (yyval)->next = NULL;
		  (yyvsp[(2) - (2)])->object = 0;
		  freeObject(STATE, (yyvsp[(2) - (2)]));
		  ;}
		break;

	case 19:
#line 372 "OSUnserializeXML.y"
		{ (yyval) = buildSymbol(STATE, (yyvsp[(1) - (1)]));

//				  STATE->parsedObjectCount++;
//				  if (STATE->parsedObjectCount > MAX_OBJECTS) {
//				    yyerror("maximum object count");
//				    YYERROR;
//				  }
		  ;}
		break;

	case 20:
#line 384 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 21:
#line 387 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 23:
#line 393 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 24:
#line 396 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 26:
#line 402 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (1)]);
		  (yyval)->next = NULL;
		  ;}
		break;

	case 27:
#line 405 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(2) - (2)]);
		  (yyval)->next = (yyvsp[(1) - (2)]);
		  ;}
		break;


/* Line 1267 of yacc.c.  */
#line 1772 "OSUnserializeXML.tab.c"
	default: break;
	}
	YY_SYMBOL_PRINT("-> $$ =", yyr1[yyn], &yyval, &yyloc);

	YYPOPSTACK(yylen);
	yylen = 0;
	YY_STACK_PRINT(yyss, yyssp);

	*++yyvsp = yyval;


	/* Now `shift' the result of the reduction.  Determine what state
	 *  that goes to, based on the state we popped back to and the rule
	 *  number reduced by.  */

	yyn = yyr1[yyn];

	yystate = yypgoto[yyn - YYNTOKENS] + *yyssp;
	if (0 <= yystate && yystate <= YYLAST && yycheck[yystate] == *yyssp) {
		yystate = yytable[yystate];
	} else {
		yystate = yydefgoto[yyn - YYNTOKENS];
	}

	goto yynewstate;


/*------------------------------------.
 | yyerrlab -- here on detecting error |
 |   `------------------------------------*/
yyerrlab:
	/* If not already recovering from an error, report this error.  */
	if (!yyerrstatus) {
		++yynerrs;
	}



	if (yyerrstatus == 3) {
		/* If just tried and failed to reuse look-ahead token after an
		 *  error, discard it.  */

		if (yychar <= YYEOF) {
			/* Return failure if at end of input.  */
			if (yychar == YYEOF) {
				YYABORT;
			}
		} else {
			yydestruct("Error: discarding",
			    yytoken, &yylval);
			yychar = YYEMPTY;
		}
	}

	/* Else will try to reuse look-ahead token after shifting the error
	 *  token.  */
	goto yyerrlab1;


/*---------------------------------------------------.
 | yyerrorlab -- error raised explicitly by YYERROR.  |
 |   `---------------------------------------------------*/
yyerrorlab:

	/* Pacify compilers like GCC when the user code never invokes
	 *  YYERROR and the label yyerrorlab therefore never appears in user
	 *  code.  */
	if (/*CONSTCOND*/ 0) {
		goto yyerrorlab;
	}

	/* Do not reclaim the symbols of the rule which action triggered
	 *  this YYERROR.  */
	YYPOPSTACK(yylen);
	yylen = 0;
	YY_STACK_PRINT(yyss, yyssp);
	yystate = *yyssp;
	goto yyerrlab1;


/*-------------------------------------------------------------.
 | yyerrlab1 -- common code for both syntax error and YYERROR.  |
 |   `-------------------------------------------------------------*/
yyerrlab1:
	yyerrstatus = 3; /* Each real token shifted decrements this.  */

	for (;;) {
		yyn = yypact[yystate];
		if (yyn != YYPACT_NINF) {
			yyn += YYTERROR;
			if (0 <= yyn && yyn <= YYLAST && yycheck[yyn] == YYTERROR) {
				yyn = yytable[yyn];
				if (0 < yyn) {
					break;
				}
			}
		}

		/* Pop the current state because it cannot handle the error token.  */
		if (yyssp == yyss) {
			YYABORT;
		}


		yydestruct("Error: popping",
		    yystos[yystate], yyvsp);
		YYPOPSTACK(1);
		yystate = *yyssp;
		YY_STACK_PRINT(yyss, yyssp);
	}

	if (yyn == YYFINAL) {
		YYACCEPT;
	}

	*++yyvsp = yylval;


	/* Shift the error token.  */
	YY_SYMBOL_PRINT("Shifting", yystos[yyn], yyvsp, yylsp);

	yystate = yyn;
	goto yynewstate;


/*-------------------------------------.
 | yyacceptlab -- YYACCEPT comes here.  |
 |   `-------------------------------------*/
yyacceptlab:
	yyresult = 0;
	goto yyreturn;

/*-----------------------------------.
 | yyabortlab -- YYABORT comes here.  |
 |   `-----------------------------------*/
yyabortlab:
	yyresult = 1;
	goto yyreturn;

/*-------------------------------------------------.
 | yyexhaustedlab -- memory exhaustion comes here.  |
 |   `-------------------------------------------------*/
yyexhaustedlab:
	yyerror(YY_("memory exhausted"));
	yyresult = 2;
	/* Fall through.  */

yyreturn:
	if (yychar != YYEOF && yychar != YYEMPTY) {
		yydestruct("Cleanup: discarding lookahead",
		    yytoken, &yylval);
	}
	/* Do not reclaim the symbols of the rule which action triggered
	 *  this YYABORT or YYACCEPT.  */
	YYPOPSTACK(yylen);
	YY_STACK_PRINT(yyss, yyssp);
	while (yyssp != yyss) {
		yydestruct("Cleanup: popping",
		    yystos[*yyssp], yyvsp);
		YYPOPSTACK(1);
	}
	if (yyss != yyssa) {
		YYSTACK_FREE(yyss);
	}
	/* Make sure YYID is used.  */
	return YYID(yyresult);
}


#line 427 "OSUnserializeXML.y"


int
OSUnserializeerror(parser_state_t * state, const char *s)  /* Called by yyparse on errors */
{
	if (state->errorString) {
		char tempString[128];
		snprintf(tempString, 128, "OSUnserializeXML: %s near line %d\n", s, state->lineNumber);
		*(state->errorString) = OSString::withCString(tempString);
	}

	return 0;
}




static int
getTag(parser_state_t *state,
    char tag[TAG_MAX_LENGTH],
    int *attributeCount,
    char attributes[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH],
    char values[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH] )
{
	int length = 0;
	int c = currentChar();
	int tagType = TAG_START;

	*attributeCount = 0;

	if (c != '<') {
		return TAG_BAD;
	}
	c = nextChar();         // skip '<'


	// <!TAG   declarations     >
	// <!--     comments      -->
	if (c == '!') {
		c = nextChar();
		bool isComment = (c == '-') && ((c = nextChar()) != 0) && (c == '-');
		if (!isComment && !isAlpha(c)) {
			return TAG_BAD;                      // <!1, <!-A, <!eos
		}
		while (c && (c = nextChar()) != 0) {
			if (c == '\n') {
				state->lineNumber++;
			}
			if (isComment) {
				if (c != '-') {
					continue;
				}
				c = nextChar();
				if (c != '-') {
					continue;
				}
				c = nextChar();
			}
			if (c == '>') {
				(void)nextChar();
				return TAG_IGNORE;
			}
			if (isComment) {
				break;
			}
		}
		return TAG_BAD;
	} else
	// <? Processing Instructions  ?>
	if (c == '?') {
		while ((c = nextChar()) != 0) {
			if (c == '\n') {
				state->lineNumber++;
			}
			if (c != '?') {
				continue;
			}
			c = nextChar();
			if (!c) {
				return TAG_IGNORE;
			}
			if (c == '>') {
				(void)nextChar();
				return TAG_IGNORE;
			}
		}
		return TAG_BAD;
	} else
	// </ end tag >
	if (c == '/') {
		c = nextChar();         // skip '/'
		tagType = TAG_END;
	}
	if (!isAlpha(c)) {
		return TAG_BAD;
	}

	/* find end of tag while copying it */
	while (isAlphaNumeric(c)) {
		tag[length++] = c;
		c = nextChar();
		if (length >= (TAG_MAX_LENGTH - 1)) {
			return TAG_BAD;
		}
	}

	tag[length] = 0;

//	printf("tag %s, type %d\n", tag, tagType);

	// look for attributes of the form attribute = "value" ...
	while ((c != '>') && (c != '/')) {
		while (isSpace(c)) {
			c = nextChar();
		}

		length = 0;
		while (isAlphaNumeric(c)) {
			attributes[*attributeCount][length++] = c;
			if (length >= (TAG_MAX_LENGTH - 1)) {
				return TAG_BAD;
			}
			c = nextChar();
		}
		attributes[*attributeCount][length] = 0;

		while (isSpace(c)) {
			c = nextChar();
		}

		if (c != '=') {
			return TAG_BAD;
		}
		c = nextChar();

		while (isSpace(c)) {
			c = nextChar();
		}

		if (c != '"') {
			return TAG_BAD;
		}
		c = nextChar();
		length = 0;
		while (c != '"') {
			values[*attributeCount][length++] = c;
			if (length >= (TAG_MAX_LENGTH - 1)) {
				return TAG_BAD;
			}
			c = nextChar();
			if (!c) {
				return TAG_BAD;
			}
		}
		values[*attributeCount][length] = 0;

		c = nextChar(); // skip closing quote

//		printf("	attribute '%s' = '%s', nextchar = '%c'\n",
//		       attributes[*attributeCount], values[*attributeCount], c);

		(*attributeCount)++;
		if (*attributeCount >= TAG_MAX_ATTRIBUTES) {
			return TAG_BAD;
		}
	}

	if (c == '/') {
		c = nextChar();         // skip '/'
		tagType = TAG_EMPTY;
	}
	if (c != '>') {
		return TAG_BAD;
	}
	c = nextChar();         // skip '>'

	return tagType;
}

static char *
getString(parser_state_t *state, int *alloc_lengthp)
{
	int c = currentChar();
	int start, length, i, j;
	char * tempString;

	start = state->parseBufferIndex;
	/* find end of string */

	while (c != 0) {
		if (c == '\n') {
			state->lineNumber++;
		}
		if (c == '<') {
			break;
		}
		c = nextChar();
	}

	if (c != '<') {
		return 0;
	}

	length = state->parseBufferIndex - start;

	/* copy to null terminated buffer */
	tempString = (char *)malloc(length + 1);
	if (tempString == NULL) {
		printf("OSUnserializeXML: can't alloc temp memory\n");
		goto error;
	}
	if (alloc_lengthp) {
		*alloc_lengthp = length + 1;
	}

	// copy out string in tempString
	// "&amp;" -> '&', "&lt;" -> '<', "&gt;" -> '>'

	i = j = 0;
	while (i < length) {
		c = state->parseBuffer[start + i++];
		if (c != '&') {
			tempString[j++] = c;
		} else {
			if ((i + 3) > length) {
				goto error;
			}
			c = state->parseBuffer[start + i++];
			if (c == 'l') {
				if (state->parseBuffer[start + i++] != 't') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '<';
				continue;
			}
			if (c == 'g') {
				if (state->parseBuffer[start + i++] != 't') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '>';
				continue;
			}
			if ((i + 3) > length) {
				goto error;
			}
			if (c == 'a') {
				if (state->parseBuffer[start + i++] != 'm') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != 'p') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '&';
				continue;
			}
			goto error;
		}
	}
	tempString[j] = 0;

//	printf("string %s\n", tempString);

	return tempString;

error:
	if (tempString) {
		safe_free(tempString, length + 1);
		if (alloc_lengthp) {
			*alloc_lengthp = 0;
		}
	}
	return 0;
}

static long long
getNumber(parser_state_t *state)
{
	unsigned long long n = 0;
	int base = 10;
	bool negate = false;
	int c = currentChar();

	if (c == '0') {
		c = nextChar();
		if (c == 'x') {
			base = 16;
			c = nextChar();
		}
	}
	if (base == 10) {
		if (c == '-') {
			negate = true;
			c = nextChar();
		}
		while (isDigit(c)) {
			n = (n * base + c - '0');
			c = nextChar();
		}
		if (negate) {
			n = (unsigned long long)((long long)n * (long long)-1);
		}
	} else {
		while (isHexDigit(c)) {
			if (isDigit(c)) {
				n = (n * base + c - '0');
			} else {
				n = (n * base + 0xa + c - 'a');
			}
			c = nextChar();
		}
	}
//	printf("number 0x%x\n", (unsigned long)n);
	return n;
}

// taken from CFXMLParsing/CFPropertyList.c

static const signed char __CFPLDataDecodeTable[128] = {
	/* 000 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 010 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 020 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 030 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* ' ' */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* '(' */ -1, -1, -1, 62, -1, -1, -1, 63,
	/* '0' */ 52, 53, 54, 55, 56, 57, 58, 59,
	/* '8' */ 60, 61, -1, -1, -1, 0, -1, -1,
	/* '@' */ -1, 0, 1, 2, 3, 4, 5, 6,
	/* 'H' */ 7, 8, 9, 10, 11, 12, 13, 14,
	/* 'P' */ 15, 16, 17, 18, 19, 20, 21, 22,
	/* 'X' */ 23, 24, 25, -1, -1, -1, -1, -1,
	/* '`' */ -1, 26, 27, 28, 29, 30, 31, 32,
	/* 'h' */ 33, 34, 35, 36, 37, 38, 39, 40,
	/* 'p' */ 41, 42, 43, 44, 45, 46, 47, 48,
	/* 'x' */ 49, 50, 51, -1, -1, -1, -1, -1
};
static void *
getCFEncodedData(parser_state_t *state, unsigned int *size)
{
	int numeq = 0, cntr = 0;
	unsigned int acc = 0;
	int tmpbufpos = 0;
	size_t tmpbuflen = DATA_ALLOC_SIZE;
	unsigned char *tmpbuf = (unsigned char *)malloc(tmpbuflen);

	int c = currentChar();
	*size = 0;

	while (c != '<') {
		c &= 0x7f;
		if (c == 0) {
			safe_free(tmpbuf, tmpbuflen);
			return 0;
		}
		if (c == '=') {
			numeq++;
		} else {
			numeq = 0;
		}
		if (c == '\n') {
			state->lineNumber++;
		}
		if (__CFPLDataDecodeTable[c] < 0) {
			c = nextChar();
			continue;
		}
		cntr++;
		acc <<= 6;
		acc += __CFPLDataDecodeTable[c];
		if (0 == (cntr & 0x3)) {
			if (tmpbuflen <= tmpbufpos + 2) {
				size_t oldsize = tmpbuflen;
				tmpbuflen *= 2;
				tmpbuf = (unsigned char *)realloc(tmpbuf, oldsize, tmpbuflen);
			}
			tmpbuf[tmpbufpos++] = (acc >> 16) & 0xff;
			if (numeq < 2) {
				tmpbuf[tmpbufpos++] = (acc >> 8) & 0xff;
			}
			if (numeq < 1) {
				tmpbuf[tmpbufpos++] = acc & 0xff;
			}
		}
		c = nextChar();
	}
	*size = tmpbufpos;
	if (*size == 0) {
		safe_free(tmpbuf, tmpbuflen);
		return 0;
	}
	return tmpbuf;
}

static void *
getHexData(parser_state_t *state, unsigned int *size)
{
	int c;
	unsigned char *d, *start;

	size_t buflen = DATA_ALLOC_SIZE; // initial buffer size
	start = d = (unsigned char *)malloc(buflen);
	c = currentChar();

	while (c != '<') {
		if (isSpace(c)) {
			while ((c = nextChar()) != 0 && isSpace(c)) {
			}
		}
		;
		if (c == '\n') {
			state->lineNumber++;
			c = nextChar();
			continue;
		}

		// get high nibble
		if (isDigit(c)) {
			*d = (c - '0') << 4;
		} else if (isAlphaDigit(c)) {
			*d =  (0xa + (c - 'a')) << 4;
		} else {
			goto error;
		}

		// get low nibble
		c = nextChar();
		if (isDigit(c)) {
			*d |= c - '0';
		} else if (isAlphaDigit(c)) {
			*d |= 0xa + (c - 'a');
		} else {
			goto error;
		}

		d++;
		size_t oldsize = d - start;
		if (oldsize >= buflen) {
			assert(oldsize == buflen);
			buflen *= 2;
			start = (unsigned char *)realloc(start, oldsize, buflen);
			d = start + oldsize;
		}
		c = nextChar();
	}

	*size = d - start;
	return start;

error:

	*size = 0;
	safe_free(start, buflen);
	return 0;
}

static int
yylex(YYSTYPE *lvalp, parser_state_t *state)
{
	int c, i;
	int tagType;
	char tag[TAG_MAX_LENGTH];
	int attributeCount;
	char attributes[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH];
	char values[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH];
	object_t *object;
	int alloc_length;
top:
	c = currentChar();

	/* skip white space  */
	if (isSpace(c)) {
		while ((c = nextChar()) != 0 && isSpace(c)) {
		}
	}
	;

	/* keep track of line number, don't return \n's */
	if (c == '\n') {
		STATE->lineNumber++;
		(void)nextChar();
		goto top;
	}

	// end of the buffer?
	if (!c) {
		return 0;
	}

	tagType = getTag(STATE, tag, &attributeCount, attributes, values);
	if (tagType == TAG_BAD) {
		return SYNTAX_ERROR;
	}
	if (tagType == TAG_IGNORE) {
		goto top;
	}

	// handle allocation and check for "ID" and "IDREF" tags up front
	*lvalp = object = newObject(STATE);
	object->idref = -1;
	for (i = 0; i < attributeCount; i++) {
		if (attributes[i][0] == 'I' && attributes[i][1] == 'D') {
			// check for idref's, note: we ignore the tag, for
			
			
			if (attributes[i][2] == 'R' && attributes[i][3] == 'E' &&
			    attributes[i][4] == 'F' && !attributes[i][5]) {
				if (tagType != TAG_EMPTY) {
					return SYNTAX_ERROR;
				}
				object->idref = strtol(values[i], NULL, 0);
				return IDREF;
			}
			
			if (!attributes[i][2]) {
				object->idref = strtol(values[i], NULL, 0);
			} else {
				return SYNTAX_ERROR;
			}
		}
	}

	switch (*tag) {
	case 'a':
		if (!strcmp(tag, "array")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return ARRAY;
			}
			return (tagType == TAG_START) ? '(' : ')';
		}
		break;
	case 'd':
		if (!strcmp(tag, "dict")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return DICTIONARY;
			}
			return (tagType == TAG_START) ? '{' : '}';
		}
		if (!strcmp(tag, "data")) {
			unsigned int size;
			if (tagType == TAG_EMPTY) {
				object->data = NULL;
				object->size = 0;
				return DATA;
			}

			bool isHexFormat = false;
			for (i = 0; i < attributeCount; i++) {
				if (!strcmp(attributes[i], "format") && !strcmp(values[i], "hex")) {
					isHexFormat = true;
					break;
				}
			}
			
			if (isHexFormat) {
				object->data = getHexData(STATE, &size);
			} else {
				object->data = getCFEncodedData(STATE, &size);
			}
			object->size = size;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END) || strcmp(tag, "data")) {
				return SYNTAX_ERROR;
			}
			return DATA;
		}
		break;
	case 'f':
		if (!strcmp(tag, "false")) {
			if (tagType == TAG_EMPTY) {
				object->number = 0;
				return BOOLEAN;
			}
		}
		break;
	case 'i':
		if (!strcmp(tag, "integer")) {
			object->size = 64;      
			for (i = 0; i < attributeCount; i++) {
				if (!strcmp(attributes[i], "size")) {
					object->size = strtoul(values[i], NULL, 0);
				}
			}
			if (tagType == TAG_EMPTY) {
				object->number = 0;
				return NUMBER;
			}
			object->number = getNumber(STATE);
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END) || strcmp(tag, "integer")) {
				return SYNTAX_ERROR;
			}
			return NUMBER;
		}
		break;
	case 'k':
		if (!strcmp(tag, "key")) {
			if (tagType == TAG_EMPTY) {
				return SYNTAX_ERROR;
			}
			object->string = getString(STATE, &alloc_length);
			if (!object->string) {
				return SYNTAX_ERROR;
			}
			object->string_alloc_length = alloc_length;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END)
			    || strcmp(tag, "key")) {
				return SYNTAX_ERROR;
			}
			return KEY;
		}
		break;
	case 'p':
		if (!strcmp(tag, "plist")) {
			freeObject(STATE, object);
			goto top;
		}
		break;
	case 's':
		if (!strcmp(tag, "string")) {
			if (tagType == TAG_EMPTY) {
				object->string = (char *)malloc(1);
				object->string_alloc_length = 1;
				object->string[0] = 0;
				return STRING;
			}
			object->string = getString(STATE, &alloc_length);
			if (!object->string) {
				return SYNTAX_ERROR;
			}
			object->string_alloc_length = alloc_length;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END)
			    || strcmp(tag, "string")) {
				return SYNTAX_ERROR;
			}
			return STRING;
		}
		if (!strcmp(tag, "set")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return SET;
			}
			if (tagType == TAG_START) {
				return '[';
			} else {
				return ']';
			}
		}
		break;
	case 't':
		if (!strcmp(tag, "true")) {
			if (tagType == TAG_EMPTY) {
				object->number = 1;
				return BOOLEAN;
			}
		}
		break;
	}

	return SYNTAX_ERROR;
}











object_t *
newObject(parser_state_t *state)
{
	object_t *o;

	if (state->freeObjects) {
		o = state->freeObjects;
		state->freeObjects = state->freeObjects->next;
	} else {
		o = malloc_type(object_t);

		o->free = state->objects;
		state->objects = o;
	}

	return o;
}

void
freeObject(parser_state_t * state, object_t *o)
{
	o->next = state->freeObjects;
	state->freeObjects = o;
}

void
cleanupObjects(parser_state_t *state)
{
	object_t *t, *o = state->objects;

	while (o) {
		if (o->object) {

			o->object->release();
		}
		if (o->data) {

			free(o->data);
		}
		if (o->key) {

			o->key->release();
		}
		if (o->string) {

			free(o->string);
		}

		t = o;
		o = o->free;
		free_type(object_t, t);

	}

}





static void
rememberObject(parser_state_t *state, int tag, OSObject *o)
{
	char key[16];
	snprintf(key, 16, "%u", tag);



	state->tags->setObject(key, o);
}

static object_t *
retrieveObject(parser_state_t *state, int tag)
{
	OSObject *ref;
	object_t *o;
	char key[16];
	snprintf(key, 16, "%u", tag);



	ref = state->tags->getObject(key);
	if (!ref) {
		return 0;
	}

	o = newObject(state);
	o->object = ref;
	return o;
}





object_t *
buildDictionary(parser_state_t *state, object_t * header)
{
	object_t *o, *t;
	int count = 0;
	OSDictionary *dict;

	
	o = header->elements;
	header->elements = 0;
	while (o) {
		count++;
		t = o;
		o = o->next;

		t->next = header->elements;
		header->elements = t;
	}

	dict = OSDictionary::withCapacity(count);
	if (header->idref >= 0) {
		rememberObject(state, header->idref, dict);
	}

	o = header->elements;
	while (o) {
		dict->setObject(o->key, o->object);

		o->key->release();
		o->object->release();
		o->key = 0;
		o->object = 0;

		t = o;
		o = o->next;
		freeObject(state, t);
	}
	o = header;
	o->object = dict;
	return o;
};
object_t *
buildArray(parser_state_t *state, object_t * header)
{
	object_t *o, *t;
	int count = 0;
	OSArray *array;

	
	o = header->elements;
	header->elements = 0;
	while (o) {
		count++;
		t = o;
		o = o->next;

		t->next = header->elements;
		header->elements = t;
	}

	array = OSArray::withCapacity(count);
	if (header->idref >= 0) {
		rememberObject(state, header->idref, array);
	}

	o = header->elements;
	while (o) {
		array->setObject(o->object);

		o->object->release();
		o->object = 0;

		t = o;
		o = o->next;
		freeObject(state, t);
	}
	o = header;
	o->object = array;
	return o;
};
object_t *
buildSet(parser_state_t *state, object_t *header)
{
	object_t *o = buildArray(state, header);

	OSArray *array = (OSArray *)o->object;
	OSSet *set = OSSet::withArray(array, array->getCapacity());

	
	if (header->idref >= 0) {
		rememberObject(state, header->idref, set);
	}

	array->release();
	o->object = set;
	return o;
};
object_t *
buildString(parser_state_t *state, object_t *o)
{
	OSString *string;

	string = OSString::withCString(o->string);
	if (o->idref >= 0) {
		rememberObject(state, o->idref, string);
	}

	free(o->string);
	o->string = 0;
	o->object = string;

	return o;
};
object_t *
buildSymbol(parser_state_t *state, object_t *o)
{
	OSSymbol *symbol;

	symbol = const_cast < OSSymbol * > (OSSymbol::withCString(o->string));
	if (o->idref >= 0) {
		rememberObject(state, o->idref, symbol);
	}

	safe_free(o->string, o->string_alloc_length);
	o->string = 0;
	o->object = symbol;

	return o;
};
object_t *
buildData(parser_state_t *state, object_t *o)
{
	OSData *data;

	if (o->size) {
		data = OSData::withBytes(o->data, o->size);
	} else {
		data = OSData::withCapacity(0);
	}
	if (o->idref >= 0) {
		rememberObject(state, o->idref, data);
	}

	if (o->size) {
		free(o->data);
	}
	o->data = 0;
	o->object = data;
	return o;
};
object_t *
buildNumber(parser_state_t *state, object_t *o)
{
	OSNumber *number = OSNumber::withNumber(o->number, o->size);

	if (o->idref >= 0) {
		rememberObject(state, o->idref, number);
	}

	o->object = number;
	return o;
};
object_t *
buildBoolean(parser_state_t *state __unused, object_t *o)
{
	o->object = ((o->number == 0) ? kOSBooleanFalse : kOSBooleanTrue);
	o->object->retain();
	return o;
};
OS_EXPORT OS_NONNULL1
void compression_interface_register(const compression_ki_t *ki);
void compression_interface_set_registration_callback(registration_callback_t callback);
extern void core_analytics_send_event(ca_event_t event);
extern void core_analytics_send_event_preemption_disabled(ca_event_t event);
extern ca_event_t core_analytics_allocate_event(size_t data_size, const char *format_str, zalloc_flags_t flags);
extern size_t core_analytics_event_size(const char *event_spec);
__BEGIN_DECLS

OS_EXPORT OS_NONNULL1
void core_analytics_hub_register(core_analytics_hub_functions_t *fns);
core_analytics_family_service_t *core_analytics_family_match(void);
int core_analytics_send_event_lazy(core_analytics_family_service_t *core_analytics_hub, const char *event_spec, const ca_event_t event);
size_t core_analytics_field_is_string(const char *field_spec);
OS_EXPORT OS_NONNULL1
void
coretrust_interface_register(const coretrust_t *ct);
aes_rval aes_encrypt_key128(const unsigned char *key, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt_key256(const unsigned char *key, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt(const unsigned char *in, unsigned char *out, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
    unsigned char *out_blk, aes_encrypt_ctx cx[1]);
aes_rval aes_decrypt_key(const unsigned char *key, int key_len, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_key128(const unsigned char *key, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_key256(const unsigned char *key, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt(const unsigned char *in, unsigned char *out, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
    unsigned char *out_blk, aes_decrypt_ctx cx[1]);
aes_rval aes_encrypt_key_gcm(const unsigned char *key, int key_len, ccgcm_ctx *ctx);
aes_rval aes_encrypt_key_with_iv_gcm(const unsigned char *key, int key_len, const unsigned char *in_iv, ccgcm_ctx *ctx);
aes_rval aes_encrypt_set_iv_gcm(const unsigned char *in_iv, unsigned int len, ccgcm_ctx *ctx);
aes_rval aes_encrypt_reset_gcm(ccgcm_ctx *ctx);
aes_rval aes_encrypt_inc_iv_gcm(unsigned char *out_iv, ccgcm_ctx *ctx);
aes_rval aes_encrypt_aad_gcm(const unsigned char *aad, unsigned int aad_bytes, ccgcm_ctx *ctx);
aes_rval aes_encrypt_gcm(const unsigned char *in_blk, unsigned int num_bytes, unsigned char *out_blk, ccgcm_ctx *ctx);
aes_rval aes_encrypt_finalize_gcm(unsigned char *tag, size_t tag_bytes, ccgcm_ctx *ctx);
size_t aes_encrypt_get_ctx_size_gcm(void);
aes_rval aes_decrypt_key_gcm(const unsigned char *key, int key_len, ccgcm_ctx *ctx);
aes_rval aes_decrypt_key_with_iv_gcm(const unsigned char *key, int key_len, const unsigned char *in_iv, ccgcm_ctx *ctx);
aes_rval aes_decrypt_set_iv_gcm(const unsigned char *in_iv, size_t len, ccgcm_ctx *ctx);
aes_rval aes_decrypt_reset_gcm(ccgcm_ctx *ctx);
aes_rval aes_decrypt_inc_iv_gcm(unsigned char *out_iv, ccgcm_ctx *ctx);
aes_rval aes_decrypt_aad_gcm(const unsigned char *aad, unsigned int aad_bytes, ccgcm_ctx *ctx);
aes_rval aes_decrypt_gcm(const unsigned char *in_blk, unsigned int num_bytes, unsigned char *out_blk, ccgcm_ctx *ctx);
aes_rval aes_decrypt_finalize_gcm(unsigned char *tag, size_t tag_bytes, ccgcm_ctx *ctx);
size_t aes_decrypt_get_ctx_size_gcm(void);
int xts_encrypt(const uint8_t *pt, unsigned long ptlen,
    uint8_t *ct,
    const uint8_t *tweak,                             
    symmetric_xts *xts);
int xts_decrypt(const uint8_t *ct, unsigned long ptlen,
    uint8_t *pt,
    const uint8_t *tweak,                             
    symmetric_xts *xts);
void xts_done(symmetric_xts *xts);
int     chacha20poly1305_init(chacha20poly1305_ctx *ctx, const uint8_t *key);
int chacha20poly1305_reset(chacha20poly1305_ctx *ctx);
int chacha20poly1305_setnonce(chacha20poly1305_ctx *ctx, const uint8_t *nonce);
int chacha20poly1305_incnonce(chacha20poly1305_ctx *ctx, uint8_t *nonce);
int     chacha20poly1305_aad(chacha20poly1305_ctx *ctx, size_t nbytes, const void *aad);
int     chacha20poly1305_encrypt(chacha20poly1305_ctx *ctx, size_t nbytes, const void *ptext, void *ctext);
int     chacha20poly1305_finalize(chacha20poly1305_ctx *ctx, uint8_t *tag);
int     chacha20poly1305_decrypt(chacha20poly1305_ctx *ctx, size_t nbytes, const void *ctext, void *ptext);
int     chacha20poly1305_verify(chacha20poly1305_ctx *ctx, const uint8_t *tag);
int des_ecb_key_sched(des_cblock *key, des_ecb_key_schedule *ks);
int des_ecb_encrypt(des_cblock * in, des_cblock *out, des_ecb_key_schedule *ks, int encrypt);
int des3_ecb_key_sched(des_cblock *key, des3_ecb_key_schedule *ks);
int des3_ecb_encrypt(des_cblock *block, des_cblock *, des3_ecb_key_schedule *ks, int encrypt);
int des_is_weak_key(des_cblock *key);
__BEGIN_DECLS



typedef 

extern void MD5Init(MD5_CTX *);
extern void MD5Update(MD5_CTX *, const void *, unsigned int);
extern void MD5Final(unsigned char [MD5_DIGEST_LENGTH], MD5_CTX *);
int cc_rand_generate(void *out, size_t outlen);
void crypto_random_generate(
	crypto_random_ctx_t ctx,
	void *random,
	size_t random_size);
void crypto_random_uniform(
	crypto_random_ctx_t ctx,
	uint64_t bound,
	uint64_t *random);
size_t crypto_random_kmem_ctx_size(void);
void crypto_random_kmem_init(
	crypto_random_ctx_t ctx);
int random_buf(void *buf, size_t buflen);
__enum_decl(crypto_digest_alg_t, unsigned int, {
	CRYPTO_DIGEST_ALG_NONE,
	CRYPTO_DIGEST_ALG_MD5,
	CRYPTO_DIGEST_ALG_SHA1,
	CRYPTO_DIGEST_ALG_SHA256,
	CRYPTO_DIGEST_ALG_SHA384,
	CRYPTO_DIGEST_ALG_SHA512
});
int rsa_verify_pkcs1v15(rsa_pub_ctx *pub, const uint8_t *oid,
    size_t digest_len, const uint8_t *digest,
    size_t sig_len, const uint8_t *sig,
    bool *valid);
extern void SHA1Update(SHA1_CTX *, const void *, size_t);
extern void SHA1Final(void *, SHA1_CTX *);
void SHA256_Init(SHA256_CTX *ctx);
void SHA256_Update(SHA256_CTX *ctx, const void *data, size_t len);
void SHA256_Final(void *digest, SHA256_CTX *ctx);
void SHA384_Init(SHA384_CTX *ctx);
void SHA384_Update(SHA384_CTX *ctx, const void *data, size_t len);
void SHA384_Final(void *digest, SHA384_CTX *ctx);
void SHA512_Init(SHA512_CTX *ctx);
void SHA512_Update(SHA512_CTX *ctx, const void *data, size_t len);
void SHA512_Final(void *digest, SHA512_CTX *ctx);
__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN
OS_ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma mark Definitions


#pragma mark KPI

OS_EXPORT OS_NONNULL1
void
image4_dlxk_link(const image4_dlxk_interface_t *dlxk);
OS_EXPORT OS_WARN_RESULT
const image4_dlxk_interface_t *_Nullable
image4_dlxk_get(image4_struct_version_t v);
OS_EXPORT OS_NONNULL1
void
img4_interface_register(const img4_interface_t *i4);
kern_return_t
IOKitBSDInit( void );
void
IOServicePublishResource( const char * property, boolean_t value );
boolean_t
IOServiceWaitForMatchingResource( const char * property, uint64_t timeout );
boolean_t
IOCatalogueMatchingDriversPresent( const char * property );
#pragma pack(push, 4)

#pragma pack(pop)
typedef struct IORPCMessage IORPCMessage;
#pragma pack(4)

#pragma pack()




typedef struct IORPC IORPC;
extern void IOBSDMountChange(struct mount *mp, uint32_t op);
extern void IOBSDLowSpaceUnlinkKernelCore(void);
extern boolean_t IOCurrentTaskHasEntitlement(const char * entitlement);
extern boolean_t IOTaskHasEntitlement(task_t task, const char *entitlement);
extern boolean_t IOVnodeHasEntitlement(struct vnode *vnode, int64_t off, const char *entitlement);
extern boolean_t IOVnodeGetBooleanEntitlement(
	struct vnode *vnode,
	int64_t off,
	const char *entitlement,
	bool *value);
extern char * IOCurrentTaskGetEntitlement(const char * entitlement);
extern char * IOTaskGetEntitlement(task_t task, const char * entitlement);
extern char *IOVnodeGetEntitlement(struct vnode *vnode, int64_t offset, const char *entitlement);
extern boolean_t IOCurrentTaskHasStringEntitlement(const char *entitlement, const char *value);
extern boolean_t IOTaskHasStringEntitlement(task_t task, const char *entitlement, const char *value);
class IOBufferMemoryDescriptor : public IOGeneralMemoryDescriptor
{
	OSDeclareDefaultStructorsWithDispatch(IOBufferMemoryDescriptor);

private:

	struct ExpansionData {
		IOMemoryMap *   map;
	};


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData * reserved;

protected:
	void *               _buffer;
	vm_size_t            _capacity;
	vm_offset_t          _alignment;
	IOOptionBits         _options;
private:
	uintptr_t            _internalReserved;
	unsigned             _internalFlags;
	APPLE_KEXT_WSHADOW_POP;

private:

public:
	virtual bool initWithPhysicalMask(
		task_t            inTask,
		IOOptionBits      options,
		mach_vm_size_t    capacity,
		mach_vm_address_t alignment,
		mach_vm_address_t physicalMask);


	bool initControlWithPhysicalMask(
		task_t            inTask,
		IOOptionBits      options,
		mach_vm_size_t    capacity,
		mach_vm_address_t alignment,
		mach_vm_address_t physicalMask);

	
	static OSPtr<IOBufferMemoryDescriptor> inTaskWithGuardPages(
		task_t            inTask,
		IOOptionBits      options,
		mach_vm_size_t    capacity);

	bool initWithGuardPages(
		task_t            inTask,
		IOOptionBits      options,
		mach_vm_size_t    capacity);

	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 0);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 1);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 2);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 3);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 4);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 5);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 6);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 7);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 8);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 9);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 10);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 11);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 12);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 13);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 14);
	OSMetaClassDeclareReservedUnused(IOBufferMemoryDescriptor, 15);

protected:
	virtual void free() APPLE_KEXT_OVERRIDE;

public:



	static OSPtr<IOBufferMemoryDescriptor> withCopy(
		task_t            inTask,
		IOOptionBits      options,
		vm_map_t          sourceMap,
		mach_vm_address_t source,
		mach_vm_size_t    size);

	static OSPtr<IOBufferMemoryDescriptor> withOptions(  IOOptionBits options,
	    vm_size_t    capacity,
	    vm_offset_t  alignment = 1);



	static OSPtr<IOBufferMemoryDescriptor> inTaskWithOptions(
		task_t       inTask,
		IOOptionBits options,
		vm_size_t    capacity,
		vm_offset_t  alignment = 1);



	static OSPtr<IOBufferMemoryDescriptor> inTaskWithOptions(
		task_t       inTask,
		IOOptionBits options,
		vm_size_t    capacity,
		vm_offset_t  alignment,
		uint32_t     kernTag,
		uint32_t     userTag);



	static OSPtr<IOBufferMemoryDescriptor> inTaskWithPhysicalMask(
		task_t            inTask,
		IOOptionBits      options,
		mach_vm_size_t    capacity,
		mach_vm_address_t physicalMask);


	static OSPtr<IOBufferMemoryDescriptor> withCapacity(
		vm_size_t    capacity __xnu_data_size,
		IODirection  withDirection,
		bool         withContiguousMemory = false);


	static OSPtr<IOBufferMemoryDescriptor> withBytes(
		const void * bytes,
		vm_size_t    withLength __xnu_data_size,
		IODirection  withDirection,
		bool         withContiguousMemory = false);


	virtual void setLength(vm_size_t length);


	virtual void setDirection(IODirection direction);


	virtual vm_size_t getCapacity() const;


	__xnu_returns_data_pointer
	virtual void *getBytesNoCopy();


	__xnu_returns_data_pointer
	virtual void *getBytesNoCopy(vm_size_t start, vm_size_t withLength);


	virtual bool appendBytes(const void *bytes, vm_size_t withLength __xnu_data_size);

};
class IOCatalogue : public OSObject
{
	OSDeclareDefaultStructors(IOCatalogue);

private:
	IORWLock *               lock;
	SInt32                   generation;
	OSPtr<OSDictionary> personalities;
	OSArray * arrayForPersonality(OSDictionary * dict);
	void addPersonality(OSDictionary * dict);

public:

	static void initialize( void );


	bool init( OSArray * initArray );


	void free( void ) APPLE_KEXT_OVERRIDE;


	OSPtr<OSOrderedSet>  findDrivers( IOService * service, SInt32 * generationCount );


	OSPtr<OSOrderedSet>  findDrivers( OSDictionary * matching, SInt32 * generationCount );


	bool addDrivers( OSArray * array, bool doNubMatching = true );


	bool removeDrivers( OSDictionary * matching, bool doNubMatching = true );

	bool removeDrivers(bool doNubMatching, bool (^shouldRemove)(OSDictionary *personality));


	bool exchangeDrivers(OSDictionary * matchingForRemove, OSArray * personalitiesToAdd, bool doNubMatching = true);



	SInt32 getGenerationCount( void ) const;


	bool isModuleLoaded( OSDictionary * driver, OSObject ** kextRef ) const;

	bool isModuleLoaded( OSDictionary * driver, OSSharedPtr<OSObject>& kextRef ) const;


	void moduleHasLoaded( const OSSymbol * name );


	void moduleHasLoaded( const char * name );


	bool personalityIsBoot(OSDictionary * match);


	IOReturn terminateDrivers( OSDictionary * matching );


	IOReturn terminateDriversForModule( OSString * moduleName, bool unload = true, bool asynchronous = false);


	IOReturn terminateDriversForModule( const char * moduleName, bool unload = true, bool asynchronous = false);
	IOReturn terminateDrivers(OSDictionary * matching, io_name_t className, bool asynchronous);

	IOReturn terminateDriversForUserspaceReboot();

	IOReturn resetAfterUserspaceReboot();


	bool startMatching( const OSSymbol * identifier );

	


	void reset(void);


	bool resetAndAddDrivers(OSArray * drivers, bool doNubMatching = true);


	virtual bool serialize(OSSerialize * s) const APPLE_KEXT_OVERRIDE;

	bool serializeData(IOOptionBits kind, OSSerialize * s) const;



private:


	IOReturn unloadModule( OSString * moduleName ) const;

	IOReturn _removeDrivers(OSDictionary * matching);
};
class IOCommandGate : public IOEventSource
{
	OSDeclareDefaultStructors(IOCommandGate);

public:

	typedef IOReturn (*Action)(OSObject *owner,
	    void *arg0, void *arg1,
	    void *arg2, void *arg3);

protected:


	struct ExpansionData { };


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;

public:

	static OSPtr<IOCommandGate> commandGate(OSObject *owner, Action action = NULL);


	virtual bool init(OSObject *owner, Action action = NULL);


	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void setWorkLoop(IOWorkLoop *inWorkLoop) APPLE_KEXT_OVERRIDE;


	virtual IOReturn runCommand(void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);


	virtual IOReturn runAction(Action action,
	    void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);



	virtual IOReturn attemptCommand(void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);


	virtual IOReturn attemptAction(Action action,
	    void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);


	virtual IOReturn commandSleep(void *event,
	    UInt32 interruptible = THREAD_ABORTSAFE);


	virtual void commandWakeup(void *event, bool oneThread = false);


	virtual void disable() APPLE_KEXT_OVERRIDE;


	virtual void enable() APPLE_KEXT_OVERRIDE;


	virtual IOReturn commandSleep(void *event,
	    AbsoluteTime deadline,
	    UInt32 interruptible);

private:
	OSMetaClassDeclareReservedUnused(IOCommandGate, 0);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 1);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 2);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 3);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 4);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 5);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 6);
	OSMetaClassDeclareReservedUnused(IOCommandGate, 7);
};
class IOCommandQueue : public IOEventSource
{
	OSDeclareDefaultStructors(IOCommandQueue);

protected:
	static const int kIOCQDefaultSize = 128;

	void *queue;
	IOLock *producerLock;
	semaphore_port_t producerSema;
	int producerIndex, consumerIndex;
	int size;

	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual bool checkForWork() APPLE_KEXT_OVERRIDE;

public:
	static OSPtr<IOCommandQueue> commandQueue(OSObject *inOwner,
	    IOCommandQueueAction inAction = NULL,
	    int inSize = kIOCQDefaultSize)
	APPLE_KEXT_DEPRECATED;
	virtual bool init(OSObject *inOwner,
	    IOCommandQueueAction inAction = NULL,
	    int inSize = kIOCQDefaultSize)
	APPLE_KEXT_DEPRECATED;

	virtual kern_return_t enqueueCommand(bool gotoSleep = true,
	    void *field0 = NULL, void *field1 = NULL,
	    void *field2 = NULL, void *field3 = NULL)
	APPLE_KEXT_DEPRECATED;







	virtual int performAndFlush(OSObject *target = NULL,
	    IOCommandQueueAction inAction = NULL)
	APPLE_KEXT_DEPRECATED;
};
class IOConditionLock : public OSObject
{
	OSDeclareDefaultStructors(IOConditionLock);

private:
	IOLock *            cond_interlock;     
	volatile int        condition;

	IOLock *            sleep_interlock;    
	unsigned char       interruptible;
	volatile bool       want_lock;
	volatile bool       waiting;

public:
	static OSPtr<IOConditionLock> withCondition(int condition, bool inIntr = true);
	virtual bool initWithCondition(int condition, bool inIntr = true);
	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual bool tryLock(); 
	virtual int  lock();    
	virtual void unlock();  

	virtual bool getInterruptible() const;
	virtual int  getCondition() const;
	virtual int  setCondition(int condition);

	virtual int  lockWhen(int condition);
	virtual void unlockWith(int condition); 
};
class IOCPU : public IOService
{
	OSDeclareAbstractStructors(IOCPU);

private:
	OSPtr<OSArray> _cpuGroup;
	UInt32                 _cpuNumber;
	UInt32                 _cpuState;

protected:
	IOService              *cpuNub;
	processor_t            machProcessor;
	ipi_handler_t          ipi_handler;

	struct ExpansionData { };
	ExpansionData *iocpu_reserved;

	virtual void           setCPUNumber(UInt32 cpuNumber);
	virtual void           setCPUState(UInt32 cpuState);

public:
	virtual bool           start(IOService *provider) APPLE_KEXT_OVERRIDE;
	virtual void           detach(IOService *provider) APPLE_KEXT_OVERRIDE;

	virtual OSObject       *getProperty(const OSSymbol *aKey) const APPLE_KEXT_OVERRIDE;
	virtual bool           setProperty(const OSSymbol *aKey, OSObject *anObject) APPLE_KEXT_OVERRIDE;
	virtual bool           serializeProperties(OSSerialize *serialize) const APPLE_KEXT_OVERRIDE;
	virtual IOReturn       setProperties(OSObject *properties) APPLE_KEXT_OVERRIDE;
	virtual void           initCPU(bool boot) = 0;
	virtual void           quiesceCPU(void) = 0;
	virtual kern_return_t  startCPU(vm_offset_t start_paddr,
	    vm_offset_t arg_paddr) = 0;
	virtual void           haltCPU(void) = 0;
	virtual void           signalCPU(IOCPU *target);
	virtual void           signalCPUDeferred(IOCPU * target);
	virtual void           signalCPUCancel(IOCPU * target);
	virtual void           enableCPUTimeBase(bool enable);

	virtual UInt32         getCPUNumber(void);
	virtual UInt32         getCPUState(void);
	virtual OSArray        *getCPUGroup(void);
	virtual UInt32         getCPUGroupSize(void);
	virtual processor_t    getMachProcessor(void);

	virtual const OSSymbol *getCPUName(void) = 0;

	OSMetaClassDeclareReservedUnused(IOCPU, 0);
	OSMetaClassDeclareReservedUnused(IOCPU, 1);
	OSMetaClassDeclareReservedUnused(IOCPU, 2);
	OSMetaClassDeclareReservedUnused(IOCPU, 3);
	OSMetaClassDeclareReservedUnused(IOCPU, 4);
	OSMetaClassDeclareReservedUnused(IOCPU, 5);
	OSMetaClassDeclareReservedUnused(IOCPU, 6);
	OSMetaClassDeclareReservedUnused(IOCPU, 7);
};
class IOCPUInterruptController : public IOInterruptController
{
	OSDeclareDefaultStructors(IOCPUInterruptController);

private:
	int   enabledCPUs;

protected:
	int   numCPUs;
	int   numSources;

	struct ExpansionData { };
	ExpansionData *iocpuic_reserved;

public:
	virtual IOReturn initCPUInterruptController(int sources);
	virtual void     registerCPUInterruptController(void);
	virtual void     enableCPUInterrupt(IOCPU *cpu);

	virtual void     setCPUInterruptProperties(IOService *service) APPLE_KEXT_OVERRIDE;
	virtual IOReturn registerInterrupt(IOService *nub, int source,
	    void *target,
	    IOInterruptHandler handler,
	    void *refCon) APPLE_KEXT_OVERRIDE;

	virtual IOReturn getInterruptType(IOService *nub, int source,
	    int *interruptType) APPLE_KEXT_OVERRIDE;

	virtual IOReturn enableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;
	virtual IOReturn disableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;
	virtual IOReturn causeInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;

	virtual IOReturn handleInterrupt(void *refCon, IOService *nub,
	    int source) APPLE_KEXT_OVERRIDE;

	virtual IOReturn initCPUInterruptController(int sources, int cpus);

	OSMetaClassDeclareReservedUnused(IOCPUInterruptController, 1);
	OSMetaClassDeclareReservedUnused(IOCPUInterruptController, 2);
	OSMetaClassDeclareReservedUnused(IOCPUInterruptController, 3);
	OSMetaClassDeclareReservedUnused(IOCPUInterruptController, 4);
	OSMetaClassDeclareReservedUnused(IOCPUInterruptController, 5);
};
class __attribute__((deprecated)) IODataQueue: public OSObject
{
	OSDeclareDefaultStructors(IODataQueue);

protected:
	IODataQueueMemory * dataQueue;

	void *              notifyMsg;

	virtual void free() APPLE_KEXT_OVERRIDE;


	virtual void sendDataAvailableNotification();

public:

	static OSPtr<IODataQueue> withCapacity(UInt32 size __xnu_data_size);


	static OSPtr<IODataQueue> withEntries(UInt32 numEntries, UInt32 entrySize __xnu_data_size);


	virtual Boolean initWithCapacity(UInt32 size);


	virtual Boolean initWithEntries(UInt32 numEntries, UInt32 entrySize);


	virtual Boolean enqueue(void *data, UInt32 dataSize);


	virtual void setNotificationPort(mach_port_t port);


	virtual OSPtr<IOMemoryDescriptor> getMemoryDescriptor();
};
class IODeviceMemory : public IOMemoryDescriptor
{
	OSDeclareDefaultStructors(IODeviceMemory);

public:



	struct InitElement {
		IOPhysicalAddress       start;
		IOPhysicalLength        length;
		IOOptionBits            tag;
	};



	static OSPtr<OSArray>             arrayFromList(
		InitElement             list[],
		IOItemCount             count );



	static OSPtr<IODeviceMemory>      withRange(
		IOPhysicalAddress       start,
		IOPhysicalLength        length );



	static OSPtr<IODeviceMemory>      withSubRange(
		IODeviceMemory *        of,
		IOPhysicalAddress       offset,
		IOPhysicalLength        length );
};
LIBKERN_RETURNS_NOT_RETAINED IORegistryEntry * IODeviceTreeAlloc( void * dtTop );
bool IODTMatchNubWithKeys( IORegistryEntry * nub,
    const char * keys );
bool IODTCompareNubName( const IORegistryEntry * regEntry,
    OSString * name,
    LIBKERN_RETURNS_RETAINED_ON_NONZERO OSString ** matchingName );
bool IODTCompareNubName( const IORegistryEntry * regEntry,
    OSString * name,
    OSSharedPtr<OSString>& matchingName );
LIBKERN_RETURNS_RETAINED OSCollectionIterator * IODTFindMatchingEntries( IORegistryEntry * from,
    IOOptionBits options, const char * keys );
void IODTSetResolving( IORegistryEntry *        regEntry,
    IODTCompareAddressCellFunc      compareFunc,
    IODTNVLocationFunc              locationFunc );
void IODTGetCellCounts( IORegistryEntry * regEntry,
    UInt32 * sizeCount, UInt32 * addressCount);
bool IODTResolveAddressCell( IORegistryEntry * regEntry,
    UInt32 cellsIn[],
    IOPhysicalAddress * phys, IOPhysicalLength * len );
LIBKERN_RETURNS_NOT_RETAINED OSArray *
IODTResolveAddressing( IORegistryEntry * regEntry,
    const char * addressPropertyName,
    IODeviceMemory * parent );
IOReturn IODTMakeNVDescriptor( IORegistryEntry * regEntry,
    IONVRAMDescriptor * hdr );
LIBKERN_RETURNS_NOT_RETAINED OSData *
IODTFindSlotName( IORegistryEntry * regEntry, UInt32 deviceNumber );
const OSSymbol * IODTInterruptControllerName(
	IORegistryEntry * regEntry );
bool IODTMapInterrupts( IORegistryEntry * regEntry );
IOReturn IODTGetInterruptOptions( IORegistryEntry * regEntry, int source, IOOptionBits * options );
IOReturn IONDRVLibrariesInitialize( IOService * provider );
class IODMACommand : public IOCommand
{
	OSDeclareDefaultStructorsWithDispatch(IODMACommand);

	friend class IODMAEventSource;

public:


	struct Segment32 {
		UInt32 fIOVMAddr, fLength;
	};


	struct Segment64 {
		UInt64 fIOVMAddr, fLength;
	};


	enum MappingOptions {
		kMapped       = kIODMAMapOptionMapped,
		kBypassed     = kIODMAMapOptionBypassed,
		kNonCoherent  = kIODMAMapOptionNonCoherent,
		kUnmapped     = kIODMAMapOptionUnmapped,
		kTypeMask     = kIODMAMapOptionTypeMask,

		kNoCacheStore = kIODMAMapOptionNoCacheStore, 
		kOnChip       = kIODMAMapOptionOnChip,  
		kIterateOnly  = kIODMAMapOptionIterateOnly
	};

	struct SegmentOptions {
		uint8_t  fStructSize;
		uint8_t  fNumAddressBits;
		uint64_t fMaxSegmentSize;
		uint64_t fMaxTransferSize;
		uint32_t fAlignment;
		uint32_t fAlignmentLength;
		uint32_t fAlignmentInternalSegments;
	};


	enum SynchronizeOptions {
		kForceDoubleBuffer = 0x01000000
	};


	typedef bool (*SegmentFunction)(IODMACommand *target,
	    Segment64 segment,
	    void     *segments,
	    UInt32    segmentIndex);




	static bool OutputHost32(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static bool OutputBig32(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static bool OutputLittle32(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static bool OutputHost64(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static bool OutputBig64(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static bool OutputLittle64(IODMACommand *target,
	    Segment64 seg, void *segs, UInt32 ind);




	static OSPtr<IODMACommand>
	withSpecification(SegmentFunction  outSegFunc,
	    UInt8            numAddressBits,
	    UInt64           maxSegmentSize,
	    MappingOptions   mappingOptions = kMapped,
	    UInt64           maxTransferSize = 0,
	    UInt32           alignment = 1,
	    IOMapper        *mapper = NULL,
	    void            *refCon = NULL);







	static inline IOReturn weakWithSpecification
	(IODMACommand   **newCommand,
	    SegmentFunction outSegFunc,
	    UInt8           numAddressBits,
	    UInt64          maxSegmentSize,
	    MappingOptions  mapType = kMapped,
	    UInt64          maxTransferSize = 0,
	    UInt32          alignment = 1,
	    IOMapper       *mapper = NULL,
	    void           *refCon = NULL) __attribute__((always_inline));

	static OSPtr<IODMACommand>
	withSpecification(SegmentFunction        outSegFunc,
	    const SegmentOptions * segmentOptions,
	    uint32_t               mappingOptions,
	    IOMapper             * mapper,
	    void                 * refCon);



	static OSPtr<IODMACommand> withRefCon(void * refCon);


	virtual OSPtr<IODMACommand> cloneCommand(void *refCon = NULL);


	virtual bool initWithSpecification( SegmentFunction  outSegFunc,
	    UInt8     numAddressBits,
	    UInt64    maxSegmentSize,
	    MappingOptions mappingOptions = kMapped,
	    UInt64    maxTransferSize = 0,
	    UInt32    alignment = 1,
	    IOMapper *mapper = NULL,
	    void     *refCon = NULL);


	virtual IOReturn setMemoryDescriptor(const IOMemoryDescriptor *mem,
	    bool autoPrepare = true);


	virtual IOReturn clearMemoryDescriptor(bool autoComplete = true);


	virtual const IOMemoryDescriptor * getMemoryDescriptor() const;


	IOMemoryDescriptor * getIOMemoryDescriptor() const;



	virtual IOReturn prepare(UInt64 offset = 0, UInt64 length = 0, bool flushCache = true, bool synchronize = true);



	virtual IOReturn complete(bool invalidateCache = true, bool synchronize = true);



	virtual IOReturn synchronize(IOOptionBits options);


	virtual IOReturn genIOVMSegments(UInt64 *offset,
	    void   *segments,
	    UInt32 *numSegments);

private:
	virtual UInt64 transfer( IOOptionBits transferOp, UInt64 offset, void * buffer, UInt64 length );

public:



	UInt64 writeBytes(UInt64 offset, const void *bytes, UInt64 length);



	UInt64 readBytes(UInt64 offset, void *bytes, UInt64 length);


	inline IOReturn
	gen32IOVMSegments(UInt64   *offset,
	    Segment32 *segments,
	    UInt32     *numSegments)
	{
		return genIOVMSegments(offset, segments, numSegments);
	}


	inline IOReturn
	gen64IOVMSegments(UInt64    *offset,
	    Segment64 *segments,
	    UInt32    *numSegments)
	{
		return genIOVMSegments(offset, segments, numSegments);
	}

	IOReturn
	genIOVMSegments(SegmentFunction segmentFunction,
	    UInt64 *offsetP,
	    void   *segmentsP,
	    UInt32 *numSegmentsP);

	virtual void free() APPLE_KEXT_OVERRIDE;

private:
	IOReturn setSpecification(SegmentFunction        outSegFunc,
	    const SegmentOptions * segmentOptions,
	    uint32_t               mappingOptions,
	    IOMapper             * mapper);

	typedef IOReturn (*InternalSegmentFunction)(
		void         *reference,
		IODMACommand *target,
		Segment64     segment,
		void         *segments,
		UInt32        segmentIndex);

	IOReturn genIOVMSegments(uint32_t op,
	    InternalSegmentFunction outSegFunc,
	    void   *reference,
	    UInt64 *offsetP,
	    void   *segmentsP,
	    UInt32 *numSegmentsP);

	static IOReturn clientOutputSegment(
		void *reference, IODMACommand *target,
		Segment64 segment, void *vSegList, UInt32 outSegIndex);

	static IOReturn segmentOp(
		void         *reference,
		IODMACommand *target,
		Segment64     segment,
		void         *segments,
		UInt32        segmentIndex);
	IOReturn walkAll(uint32_t op);

public:



	virtual IOReturn prepareWithSpecification(SegmentFunction   outSegFunc,
	    UInt8             numAddressBits,
	    UInt64            maxSegmentSize,
	    MappingOptions    mappingOptions = kMapped,
	    UInt64            maxTransferSize = 0,
	    UInt32            alignment = 1,
	    IOMapper          *mapper = NULL,
	    UInt64            offset = 0,
	    UInt64            length = 0,
	    bool              flushCache = true,
	    bool              synchronize = true);

	static IOReturn transferSegment(void         *reference,
	    IODMACommand *target,
	    Segment64     segment,
	    void         *segments,
	    UInt32        segmentIndex);



	virtual IOReturn getPreparedOffsetAndLength(UInt64 * offset, UInt64 * length);

	UInt8    getNumAddressBits(void);
	UInt32   getAlignment(void);
	uint32_t getAlignmentLength(void);
	uint32_t getAlignmentInternalSegments(void);




	virtual
	bool initWithRefCon(void * refCon = NULL);

	virtual
	bool initWithSpecification(SegmentFunction        outSegFunc,
	    const SegmentOptions * segmentOptions,
	    uint32_t               mappingOptions,
	    IOMapper             * mapper,
	    void                 * refCon);

	virtual
	IOReturn prepareWithSpecification(SegmentFunction        outSegFunc,
	    const SegmentOptions * segmentOptions,
	    uint32_t               mappingOptions,
	    IOMapper             * mapper,
	    uint64_t               offset,
	    uint64_t               length,
	    bool                   flushCache = true,
	    bool                   synchronize = true);

	virtual
	OSPtr<IOBufferMemoryDescriptor> createCopyBuffer(IODirection direction, UInt64 length);

private:
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 0);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 1);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 2);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 3);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 4);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 5);
	OSMetaClassDeclareReservedUsedX86(IODMACommand, 6);
	OSMetaClassDeclareReservedUnused(IODMACommand, 7);
	OSMetaClassDeclareReservedUnused(IODMACommand, 8);
	OSMetaClassDeclareReservedUnused(IODMACommand, 9);
	OSMetaClassDeclareReservedUnused(IODMACommand, 10);
	OSMetaClassDeclareReservedUnused(IODMACommand, 11);
	OSMetaClassDeclareReservedUnused(IODMACommand, 12);
	OSMetaClassDeclareReservedUnused(IODMACommand, 13);
	OSMetaClassDeclareReservedUnused(IODMACommand, 14);
	OSMetaClassDeclareReservedUnused(IODMACommand, 15);

public:

	void *fRefCon;

protected:


	UInt64  fMaxSegmentSize;


	UInt64  fMaxTransferSize;

	UInt32  fAlignMaskLength;
	UInt32  fAlignMaskInternalSegments;


	OSPtr<IOMapper> fMapper;


	OSPtr<IOMemoryDescriptor> fMemory;


	SegmentFunction fOutSeg;


	UInt32  fAlignMask;


	UInt32 fNumAddressBits;


	UInt32  fNumSegments;


	uint32_t  fMappingOptions;


	UInt32 fActive;


	struct IODMACommandInternal  *reserved;
};
IOReturn
IODMACommand::
weakWithSpecification(IODMACommand **newCommand,
    SegmentFunction  outSegFunc,
    UInt8     numAddressBits,
    UInt64    maxSegmentSize,
    MappingOptions mapType,
    UInt64    maxTransferSize,
    UInt32    alignment,
    IOMapper *mapper,
    void     *refCon)
{
	if (!newCommand) {
		return kIOReturnBadArgument;
	}

	IODMACommand *self = (IODMACommand *)
	    OSMetaClass::allocClassWithName("IODMACommand");
	if (!self) {
		return kIOReturnUnsupported;
	}

	IOReturn ret;
	bool inited = self->
	    initWithSpecification(outSegFunc,
	    numAddressBits, maxSegmentSize, mapType,
	    maxTransferSize, alignment, mapper, refCon);
	if (inited) {
		ret =  kIOReturnSuccess;
	} else {
		self->release();
		self = NULL;
		ret = kIOReturnError;
	}

	*newCommand = self;
	return ret;
};
class IODMAController : public IOService
{
	OSDeclareAbstractStructors(IODMAController);

	friend class IODMAEventSource;

private:
	IOService       *_provider;
	OSPtr<const OSSymbol> _dmaControllerName;

protected:
	virtual void registerDMAController(IOOptionBits options = 0);
	virtual IOReturn initDMAChannel(IOService *provider, IODMAEventSource *dmaES, UInt32 *dmaIndex, UInt32 reqIndex) = 0;
	virtual IOReturn startDMACommand(UInt32 dmaIndex, IODMACommand *dmaCommand, IODirection direction,
	    IOByteCount byteCount = 0, IOByteCount byteOffset = 0) = 0;
	virtual IOReturn stopDMACommand(UInt32 dmaIndex, bool flush = false, uint64_t timeout = UINT64_MAX) = 0;
	virtual void completeDMACommand(IODMAEventSource *dmaES, IODMACommand *dmaCommand);
	virtual void notifyDMACommand(IODMAEventSource *dmaES, IODMACommand *dmaCommand, IOReturn status, IOByteCount actualByteCount, AbsoluteTime timeStamp);
	virtual IOReturn queryDMACommand(UInt32 dmaIndex, IODMACommand **dmaCommand, IOByteCount *transferCount, bool waitForIdle = false) = 0;
	virtual IOByteCount getFIFODepth(UInt32 dmaIndex, IODirection direction) = 0;
	virtual IOReturn setFIFODepth(UInt32 dmaIndex, IOByteCount depth) = 0;
	virtual IOByteCount validFIFODepth(UInt32 dmaIndex, IOByteCount depth, IODirection direction) = 0;
	virtual IOReturn setFrameSize(UInt32 dmaIndex, UInt8 byteCount) = 0;
	virtual IOReturn setDMAConfig(UInt32 dmaIndex, IOService *provider, UInt32 reqIndex) = 0;
	virtual bool validDMAConfig(UInt32 dmaIndex, IOService *provider, UInt32 reqIndex) = 0;

public:
	static OSPtr<const OSSymbol> createControllerName(UInt32 phandle);
	static IODMAController *getController(IOService *provider, UInt32 dmaIndex);

	virtual bool start(IOService *provider) APPLE_KEXT_OVERRIDE;
};
class IODMAEventSource : public IOEventSource
{
	OSDeclareDefaultStructors(IODMAEventSource);

	friend class IODMAController;

public:
	typedef void (*Action)(OSObject *owner, IODMAEventSource *dmaES, IODMACommand *dmaCommand, IOReturn status, IOByteCount actualByteCount, AbsoluteTime timeStamp);

protected:
	virtual void completeDMACommand(IODMACommand *dmaCommand);
	virtual void notifyDMACommand(IODMACommand *dmaCommand, IOReturn status, IOByteCount actualByteCount, AbsoluteTime timeStamp);

public:
	static OSPtr<IODMAEventSource> dmaEventSource(OSObject *owner,
	    IOService *provider,
	    Action completion = NULL,
	    Action notification = NULL,
	    UInt32 dmaIndex = 0);

	virtual IOReturn startDMACommand(IODMACommand *dmaCommand, IODirection direction, IOByteCount byteCount = 0, IOByteCount byteOffset = 0);
	virtual IOReturn stopDMACommand(bool flush = false, uint64_t timeout = UINT64_MAX);

	virtual IOReturn queryDMACommand(IODMACommand **dmaCommand, IOByteCount *transferCount, bool waitForIdle = false);

	virtual IOByteCount getFIFODepth(IODirection direction = kIODirectionNone);
	virtual IOReturn setFIFODepth(IOByteCount depth);
	virtual IOByteCount validFIFODepth(IOByteCount depth, IODirection direction);

	virtual IOReturn setFrameSize(UInt8 byteCount);

	virtual IOReturn setDMAConfig(UInt32 dmaIndex);
	virtual bool validDMAConfig(UInt32 dmaIndex);

private:
	IOService       *dmaProvider;
	OSPtr<IODMAController> dmaController;
	UInt32          dmaIndex;
	queue_head_t    dmaCommandsCompleted;
	IOSimpleLock    *dmaCommandsCompletedLock;
	Action          dmaCompletionAction;
	Action          dmaNotificationAction;
	bool            dmaSynchBusy;

	virtual bool init(OSObject *owner,
	    IOService *provider,
	    Action completion = NULL,
	    Action notification = NULL,
	    UInt32 dmaIndex = 0);
	virtual bool checkForWork(void) APPLE_KEXT_OVERRIDE;
	virtual void free(void) APPLE_KEXT_OVERRIDE;
};
__BEGIN_DECLS
__END_DECLS


class IOEventSource : public OSObject
{
	OSDeclareAbstractStructors(IOEventSource);
	friend class IOWorkLoop;

public:

	typedef void (*Action)(OSObject *owner, ...);




protected:

	IOEventSource *eventChainNext;


	OSObject *owner;



	union { Action action; ActionBlock actionBlock; };


	bool enabled;

	enum{
		kPassive         = 0x0001,
		kActive          = 0x0002,
		kActionBlock     = 0x0004,
		kSubClass0       = 0x0008,
	};
	uint8_t  eventSourceReserved1[1];
	uint16_t flags;
	uint8_t eventSourceReserved2[4];



	IOWorkLoop *workLoop;


	void *refcon;


	struct ExpansionData {
	};


	ExpansionData *reserved;


	virtual bool init(OSObject *owner, IOEventSource::Action action = NULL);

	virtual void free( void ) APPLE_KEXT_OVERRIDE;


	virtual bool checkForWork();


	virtual void setWorkLoop(IOWorkLoop *workLoop);


	virtual void setNext(IOEventSource *next);


	virtual IOEventSource *getNext() const;


protected:

	void signalWorkAvailable();
	void openGate();
	void closeGate();
	bool tryCloseGate();
	int sleepGate(void *event, UInt32 type);
	int sleepGate(void *event, AbsoluteTime deadline, UInt32 type);
	void wakeupGate(void *event, bool oneThread);

public:

	virtual void setAction(IOEventSource::Action action);


	virtual IOEventSource::Action getAction() const;



	void setRefcon(void *refcon);

	void * getRefcon() const;


	virtual void enable();


	virtual void disable();


	virtual bool isEnabled() const;


	virtual IOWorkLoop *getWorkLoop() const;


	virtual bool onThread() const;

private:
	OSMetaClassDeclareReservedUnused(IOEventSource, 0);
	OSMetaClassDeclareReservedUnused(IOEventSource, 1);
	OSMetaClassDeclareReservedUnused(IOEventSource, 2);
	OSMetaClassDeclareReservedUnused(IOEventSource, 3);
	OSMetaClassDeclareReservedUnused(IOEventSource, 4);
	OSMetaClassDeclareReservedUnused(IOEventSource, 5);
	OSMetaClassDeclareReservedUnused(IOEventSource, 6);
	OSMetaClassDeclareReservedUnused(IOEventSource, 7);
};
__BEGIN_DECLS
__END_DECLS

class IOExtensiblePaniclog : public OSObject
{
	OSDeclareDefaultStructorsWithDispatch(IOExtensiblePaniclog);

private:
	ext_paniclog_handle_t *extPaniclogHandle;

	IOBufferMemoryDescriptor *iomd;

protected:
	bool init() APPLE_KEXT_OVERRIDE;

	void free(void) APPLE_KEXT_OVERRIDE;

public:

	
	static bool createWithUUID(uuid_t uuid, const char *data_id,
	    uint32_t max_len, ext_paniclog_create_options_t options, IOExtensiblePaniclog **out);

	
	int setActive();

	
	int setInactive();

	
	int insertData(void *addr, uint32_t len);

	
	int appendData(void *addr, uint32_t len);

	
	void *claimBuffer();

	
	int yieldBuffer(uint32_t used_len);

	
	int setUsedLen(uint32_t used_len);
};
class IOFilterInterruptEventSource : public IOInterruptEventSource
{
	OSDeclareDefaultStructors(IOFilterInterruptEventSource);

public:

	typedef bool (*Filter)(OSObject *owner, IOFilterInterruptEventSource *sender);




private:

	virtual bool init(OSObject *inOwner,
	    IOInterruptEventSource::Action inAction = NULL,
	    IOService *inProvider = NULL,
	    int inIntIndex = 0) APPLE_KEXT_OVERRIDE;

	static OSPtr<IOInterruptEventSource>
	interruptEventSource(OSObject *inOwner,
	    IOInterruptEventSource::Action inAction = NULL,
	    IOService *inProvider = NULL,
	    int inIntIndex = 0);

protected:


	union { Filter filterAction; FilterBlock filterActionBlock; };


	struct ExpansionData { };


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;

public:

	static OSPtr<IOFilterInterruptEventSource>
	filterInterruptEventSource(OSObject *owner,
	    IOInterruptEventSource::Action action,
	    Filter filter,
	    IOService *provider,
	    int intIndex = 0);


	enum{
		kFilterBlock = kSubClass0,
	};


	virtual bool init(OSObject *owner,
	    IOInterruptEventSource::Action action,
	    Filter filter,
	    IOService *provider,
	    int intIndex = 0);

	virtual void free( void ) APPLE_KEXT_OVERRIDE;


	virtual void signalInterrupt();


	virtual Filter getFilterAction() const;



	virtual void normalInterruptOccurred(void *self, IOService *prov, int ind) APPLE_KEXT_OVERRIDE;


	virtual void disableInterruptOccurred(void *self, IOService *prov, int ind) APPLE_KEXT_OVERRIDE;

private:
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 0);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 1);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 2);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 3);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 4);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 5);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 6);
	OSMetaClassDeclareReservedUnused(IOFilterInterruptEventSource, 7);
};
class IOGuardPageMemoryDescriptor : public IOGeneralMemoryDescriptor
{
	OSDeclareDefaultStructors(IOGuardPageMemoryDescriptor);

protected:
	virtual void free() APPLE_KEXT_OVERRIDE;

	vm_offset_t    _buffer;
	vm_size_t      _size;

public:

	
	static OSPtr<IOGuardPageMemoryDescriptor> withSize(vm_size_t size);

	virtual bool initWithSize(vm_size_t size);

private:
	virtual IOReturn doMap(vm_map_t           addressMap,
	    IOVirtualAddress * atAddress,
	    IOOptionBits       options,
	    IOByteCount        sourceOffset = 0,
	    IOByteCount        length = 0 ) APPLE_KEXT_OVERRIDE;
};
__BEGIN_DECLS


extern int kdb_printf(const char *format, ...) __printflike(1, 2);
void
vm_compressor_do_warmup(void);
hibernate_page_list_t *
hibernate_page_list_allocate(boolean_t log);
kern_return_t
hibernate_alloc_page_lists(
	hibernate_page_list_t ** page_list_ret,
	hibernate_page_list_t ** page_list_wired_ret,
	hibernate_page_list_t ** page_list_pal_ret);
kern_return_t
hibernate_setup(IOHibernateImageHeader * header,
    boolean_t vmflush,
    hibernate_page_list_t * page_list,
    hibernate_page_list_t * page_list_wired,
    hibernate_page_list_t * page_list_pal);
kern_return_t
hibernate_teardown(hibernate_page_list_t * page_list,
    hibernate_page_list_t * page_list_wired,
    hibernate_page_list_t * page_list_pal);
kern_return_t
hibernate_pin_swap(boolean_t begin);
kern_return_t
hibernate_processor_setup(IOHibernateImageHeader * header);
void
hibernate_gobble_pages(uint32_t gobble_count, uint32_t free_page_time);
void
hibernate_free_gobble_pages(void);
void
hibernate_vm_lock_queues(void);
void
hibernate_vm_unlock_queues(void);
void
hibernate_vm_lock(void);
void
hibernate_vm_unlock(void);
void
hibernate_vm_lock_end(void);
boolean_t
hibernate_vm_locks_are_safe(void);
void
hibernate_page_list_setall(hibernate_page_list_t * page_list,
    hibernate_page_list_t * page_list_wired,
    hibernate_page_list_t * page_list_pal,
    boolean_t preflight,
    boolean_t discard_all,
    uint32_t * pagesOut);
void
hibernate_page_list_setall_machine(hibernate_page_list_t * page_list,
    hibernate_page_list_t * page_list_wired,
    boolean_t preflight,
    uint32_t * pagesOut);
void
hibernate_page_list_set_volatile( hibernate_page_list_t * page_list,
    hibernate_page_list_t * page_list_wired,
    uint32_t * pagesOut);
void
hibernate_page_list_discard(hibernate_page_list_t * page_list);
int
hibernate_should_abort(void);
void
hibernate_set_page_state(hibernate_page_list_t * page_list, hibernate_page_list_t * page_list_wired,
    vm_offset_t ppnum, vm_offset_t count, uint32_t kind);
void
hibernate_page_bitset(hibernate_page_list_t * list, boolean_t set, uint32_t page);
boolean_t
hibernate_page_bittst(hibernate_page_list_t * list, uint32_t page);
hibernate_bitmap_t *
hibernate_page_bitmap_pin(hibernate_page_list_t * list, uint32_t * page);
uint32_t
hibernate_page_bitmap_count(hibernate_bitmap_t * bitmap, uint32_t set, uint32_t page);
uintptr_t
hibernate_restore_phys_page(uint64_t src, uint64_t dst, uint32_t len, uint32_t procFlags);
void
hibernate_scratch_init(hibernate_scratch_t * scratch, hibernate_page_list_t * map, uint32_t * nextFree);
void
hibernate_scratch_start_read(hibernate_scratch_t * scratch);
void
hibernate_scratch_write(hibernate_scratch_t * scratch, const void * buffer, size_t size);
void
hibernate_scratch_read(hibernate_scratch_t * scratch, void * buffer, size_t size);
void
hibernate_machine_init(void);
uint32_t
hibernate_write_image(void);
ppnum_t
hibernate_page_list_grab(hibernate_page_list_t * list, uint32_t * pNextFree);
void
hibernate_reserve_restore_pages(uint64_t headerPhys, IOHibernateImageHeader *header, hibernate_page_list_t * map);
long
hibernate_machine_entrypoint(uint32_t p1, uint32_t p2, uint32_t p3, uint32_t p4);
long
hibernate_kernel_entrypoint(uint32_t p1, uint32_t p2, uint32_t p3, uint32_t p4);
void
hibernate_newruntime_map(void * map, vm_size_t map_size,
    uint32_t system_table_offset);
void
hibernate_rebuild_vm_structs(void);
class IOInterleavedMemoryDescriptor : public IOMemoryDescriptor
{
	OSDeclareDefaultStructors(IOInterleavedMemoryDescriptor);

protected:

	IOByteCount           _descriptorCapacity;
	UInt32                _descriptorCount;
	IOMemoryDescriptor ** _descriptors;
	IOByteCount         * _descriptorOffsets;
	IOByteCount         * _descriptorLengths;
	bool                  _descriptorPrepared;

	virtual void free() APPLE_KEXT_OVERRIDE;

public:



	static OSPtr<IOInterleavedMemoryDescriptor>  withCapacity( IOByteCount           capacity,
	    IODirection           direction);



	virtual bool initWithCapacity( IOByteCount           capacity,
	    IODirection           direction );



	virtual void clearMemoryDescriptors( IODirection direction = kIODirectionNone );



	virtual bool setMemoryDescriptor( IOMemoryDescriptor * descriptor,
	    IOByteCount offset,
	    IOByteCount length );



	virtual addr64_t getPhysicalSegment( IOByteCount   offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0 ) APPLE_KEXT_OVERRIDE;



	virtual IOReturn prepare(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;



	virtual IOReturn complete(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;
};
void interruptAccountingInit(void);
void interruptAccountingDataAddToList(IOInterruptAccountingData * data);
void interruptAccountingDataRemoveFromList(IOInterruptAccountingData * data);
void interruptAccountingDataUpdateChannels(IOInterruptAccountingData * data, IOSimpleReporter * reporter);
void interruptAccountingDataInheritChannels(IOInterruptAccountingData * data, IOSimpleReporter * reporter);
class IOInterruptController : public IOService
{
	OSDeclareAbstractStructors(IOInterruptController);

protected:
	IOInterruptVector *vectors;
	IOSimpleLock      *controllerLock;

	struct ExpansionData { };
	ExpansionData *ioic_reserved;

public:
	virtual IOReturn registerInterrupt(IOService *nub, int source,
	    void *target,
	    IOInterruptHandler handler,
	    void *refCon);
	virtual IOReturn unregisterInterrupt(IOService *nub, int source);

	virtual IOReturn getInterruptType(IOService *nub, int source,
	    int *interruptType);

	virtual IOReturn enableInterrupt(IOService *nub, int source);
	virtual IOReturn disableInterrupt(IOService *nub, int source);
	virtual IOReturn causeInterrupt(IOService *nub, int source);

	virtual IOInterruptAction getInterruptHandlerAddress(void);
	virtual IOReturn handleInterrupt(void *refCon, IOService *nub,
	    int source);



	virtual bool vectorCanBeShared(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual void initVector(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual int  getVectorType(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual void disableVectorHard(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual void enableVector(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual void causeVector(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	virtual void setCPUInterruptProperties(IOService *service);

	virtual void sendIPI(unsigned int cpu_id, bool deferred);
	virtual void cancelDeferredIPI(unsigned int cpu_id);

	OSMetaClassDeclareReservedUsedX86(IOInterruptController, 0);
	OSMetaClassDeclareReservedUsedX86(IOInterruptController, 1);
	OSMetaClassDeclareReservedUsedX86(IOInterruptController, 2);
	OSMetaClassDeclareReservedUnused(IOInterruptController, 3);
	OSMetaClassDeclareReservedUnused(IOInterruptController, 4);
	OSMetaClassDeclareReservedUnused(IOInterruptController, 5);

public:


	void timeStampSpuriousInterrupt(void);
	void timeStampInterruptHandlerStart(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
	void timeStampInterruptHandlerEnd(IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);

private:
	void timeStampInterruptHandlerInternal(bool isStart, IOInterruptVectorNumber vectorNumber, IOInterruptVector *vector);
};
class IOSharedInterruptController : public IOInterruptController
{
	OSDeclareDefaultStructors(IOSharedInterruptController);

private:
	IOService         *provider;
	int               numVectors;
	int               vectorsRegistered;
	int               vectorsEnabled;
	volatile int      controllerDisabled;
	bool              sourceIsLevel;

	struct ExpansionData { };
	ExpansionData *iosic_reserved __unused;

public:
	virtual IOReturn initInterruptController(IOInterruptController *parentController, OSData *parentSource);

	virtual IOReturn registerInterrupt(IOService *nub, int source,
	    void *target,
	    IOInterruptHandler handler,
	    void *refCon) APPLE_KEXT_OVERRIDE;
	virtual IOReturn unregisterInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;

	virtual IOReturn getInterruptType(IOService *nub, int source,
	    int *interruptType) APPLE_KEXT_OVERRIDE;

	virtual IOReturn enableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;
	virtual IOReturn disableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;

	virtual IOInterruptAction getInterruptHandlerAddress(void) APPLE_KEXT_OVERRIDE;
	virtual IOReturn handleInterrupt(void *refCon, IOService *nub, int source) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(IOSharedInterruptController, 0);
	OSMetaClassDeclareReservedUnused(IOSharedInterruptController, 1);
	OSMetaClassDeclareReservedUnused(IOSharedInterruptController, 2);
	OSMetaClassDeclareReservedUnused(IOSharedInterruptController, 3);
};
class IOInterruptEventSource : public IOEventSource
{
	OSDeclareDefaultStructors(IOInterruptEventSource);

public:

	typedef void (*Action)(OSObject *owner, IOInterruptEventSource *sender, int count);




protected:

	IOService *provider;


	int intIndex;


	volatile unsigned int producerCount;


	unsigned int consumerCount;


	bool autoDisable;


	bool explicitDisable;


	struct ExpansionData {
		IOInterruptAccountingData * statistics;
	};


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;


	virtual void free() APPLE_KEXT_OVERRIDE;


	virtual bool checkForWork() APPLE_KEXT_OVERRIDE;


	virtual void setWorkLoop(IOWorkLoop *inWorkLoop) APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<IOInterruptEventSource>
	interruptEventSource(OSObject *owner,
	    Action action,
	    IOService *provider = NULL,
	    int intIndex = 0);



	static void actionToBlock(OSObject *owner, IOInterruptEventSource *sender, int count);


	virtual bool init(OSObject *owner,
	    Action action,
	    IOService *provider = NULL,
	    int intIndex = 0);


	virtual void enable() APPLE_KEXT_OVERRIDE;


	virtual void disable() APPLE_KEXT_OVERRIDE;


	virtual const IOService *getProvider() const;


	virtual int getIntIndex() const;


	virtual bool getAutoDisable() const;


	virtual void interruptOccurred(void *, IOService *nub, int ind);


	virtual void normalInterruptOccurred(void *, IOService *nub, int ind);


	virtual void disableInterruptOccurred(void *, IOService *nub, int ind);


	IOReturn warmCPU(uint64_t abstime);



	void enablePrimaryInterruptTimestamp(bool enable);



	uint64_t getPrimaryInterruptTimestamp();

private:
	IOReturn registerInterruptHandler(IOService *inProvider, int inIntIndex);
	void unregisterInterruptHandler(IOService *inProvider, int inIntIndex);

private:
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 0);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 1);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 2);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 3);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 4);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 5);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 6);
	OSMetaClassDeclareReservedUnused(IOInterruptEventSource, 7);
};
class IOReporter : public OSObject
{
	OSDeclareDefaultStructors(IOReporter);

protected:

	virtual bool init(IOService *reportingService,
	    IOReportChannelType channelType,
	    IOReportUnit unit);

public:


	IOReturn addChannel(uint64_t channelID, const char *channelName = NULL);


	OSPtr<IOReportLegendEntry> createLegend(void);


	IOReturn configureReport(IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);


	IOReturn updateReport(IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);


	virtual void free(void) APPLE_KEXT_OVERRIDE;


	static void initialize(void);









	static IOReturn configureAllReports(OSSet *reporters,
	    IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);



	static IOReturn updateAllReports(OSSet *reporters,
	    IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);




protected:


	void lockReporterConfig(void);
	void unlockReporterConfig(void);


	void lockReporter(void);
	void unlockReporter(void);



	virtual IOReturn handleSwapPrepare(int newNChannels);


	virtual IOReturn handleAddChannelSwap(uint64_t channel_id,
	    const OSSymbol *symChannelName);


	virtual void handleSwapCleanup(int swapNChannels);


	virtual IOReturn handleConfigureReport(IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);


	virtual IOReturn handleUpdateReport(IOReportChannelList *channelList,
	    IOReportConfigureAction action,
	    void *result,
	    void *destination);


	virtual OSPtr<IOReportLegendEntry> handleCreateLegend(void);


	virtual IOReturn updateChannelValues(int channel_index);



	IOReturn updateReportChannel(int channel_index,
	    int *nElements,
	    IOBufferMemoryDescriptor *destination);



	virtual IOReturn setElementValues(int element_index,
	    IOReportElementValues *values,
	    uint64_t record_time = 0);


	virtual const IOReportElementValues* getElementValues(int element_index);


	virtual IOReturn getFirstElementIndex(uint64_t channel_id,
	    int *element_index);


	virtual IOReturn getChannelIndex(uint64_t channel_id,
	    int *channel_index);


	virtual IOReturn getChannelIndices(uint64_t channel_id,
	    int *channel_index,
	    int *element_index);


	virtual IOReturn copyElementValues(int element_index,
	    IOReportElementValues *elementValues);


	static OSPtr<IOReportLegendEntry> legendWith(const uint64_t *channelIDs,
	    const char **channelNames,
	    int channelCount,
	    IOReportChannelType channelType,
	    IOReportUnit unit);


private:

	OSPtr<OSArray> copyChannelIDs(void);


	static OSPtr<IOReportLegendEntry> legendWith(OSArray *channelIDs,
	    OSArray *channelNames,
	    IOReportChannelType channelType,
	    IOReportUnit unit);


protected:
	IOReportChannelType _channelType;
	uint64_t            _driver_id;     


	IOReportElement    *_elements;
	int                *_enableCounts;  
	uint16_t            _channelDimension;
	int                 _nElements;
	int                 _nChannels;     
	OSPtr<OSArray>      _channelNames;


	bool                _reporterIsLocked;
	bool                _reporterConfigIsLocked;


	IOReportElement    *_swapElements;
	int                *_swapEnableCounts;


private:
	IOReportUnit       _unit;

	int                 _enabled;

	IOLock             *_configLock;
	IOInterruptState    _interruptState;
	IOSimpleLock       *_reporterLock;
};
class IOSimpleReporter : public IOReporter
{
	OSDeclareDefaultStructors(IOSimpleReporter);

public:


	static OSPtr<IOSimpleReporter> with(IOService *reportingService,
	    IOReportCategories categories,
	    IOReportUnit unit);


	IOReturn setValue(uint64_t channel_id,
	    int64_t value);


	IOReturn incrementValue(uint64_t channel_id,
	    int64_t increment);


	int64_t getValue(uint64_t channel_id);


	static OSPtr<IOReportLegendEntry> createLegend(const uint64_t *channelIDs,
	    const char **channelNames,
	    int channelCount,
	    IOReportCategories categories,
	    IOReportUnit unit);

protected:


	virtual bool initWith(IOService *reportingService,
	    IOReportCategories categories,
	    IOReportUnit unit);

private:
};
class IOStateReporter : public IOReporter
{
	OSDeclareDefaultStructors(IOStateReporter);

public:


	static OSPtr<IOStateReporter> with(IOService *reportingService,
	    IOReportCategories categories,
	    int nstates,
	    IOReportUnit unit = kIOReportUnitHWTicks);


	IOReturn setStateID(uint64_t channel_id,
	    int state_index,
	    uint64_t state_id);


	IOReturn setChannelState(uint64_t channel_id,
	    uint64_t new_state_id,
	    uint64_t last_intransition,
	    uint64_t prev_state_residency) __deprecated;


	IOReturn setChannelState(uint64_t channel_id,
	    uint64_t new_state_id);



	IOReturn setState(uint64_t new_state_id);


	IOReturn setState(uint64_t new_state_id,
	    uint64_t last_intransition,
	    uint64_t prev_state_residency) __deprecated;


	IOReturn overrideChannelState(uint64_t channel_id,
	    uint64_t state_id,
	    uint64_t time_in_state,
	    uint64_t intransitions,
	    uint64_t last_intransition = 0);


	IOReturn incrementChannelState(uint64_t channel_id,
	    uint64_t state_id,
	    uint64_t time_in_state,
	    uint64_t intransitions,
	    uint64_t last_intransition = 0);


	IOReturn setStateByIndices(int channel_index,
	    int new_state_index);


	IOReturn setStateByIndices(int channel_index,
	    int new_state_index,
	    uint64_t last_intransition,
	    uint64_t prev_state_residency) __deprecated;


	uint64_t getStateInTransitions(uint64_t channel_id,
	    uint64_t state_id);


	uint64_t getStateResidencyTime(uint64_t channel_id,
	    uint64_t state_id);


	uint64_t getStateLastTransitionTime(uint64_t channel_id, uint64_t state_id);


	uint64_t getStateLastChannelUpdateTime(uint64_t channel_id) __deprecated;


	static OSPtr<IOReportLegendEntry> createLegend(const uint64_t *channelIDs,
	    const char **channelNames,
	    int channelCount,
	    int nstates,
	    IOReportCategories categories,
	    IOReportUnit unit);


	virtual void free(void) APPLE_KEXT_OVERRIDE;

protected:


	virtual bool initWith(IOService *reportingService,
	    IOReportCategories categories,
	    int16_t nstates, IOReportUnit unit);



	virtual IOReturn handleSwapPrepare(int newNChannels) APPLE_KEXT_OVERRIDE;


	virtual IOReturn handleAddChannelSwap(uint64_t channel_id,
	    const OSSymbol *symChannelName) APPLE_KEXT_OVERRIDE;


	virtual void handleSwapCleanup(int swapNChannels) APPLE_KEXT_OVERRIDE;


	virtual IOReturn updateChannelValues(int channel_index) APPLE_KEXT_OVERRIDE;


	virtual IOReturn handleSetStateByIndices(int channel_index,
	    int new_state_index,
	    uint64_t last_intransition,
	    uint64_t prev_state_residency);


	virtual IOReturn handleSetStateID(uint64_t channel_id,
	    int state_index,
	    uint64_t state_id);


	virtual IOReturn handleOverrideChannelStateByIndices(int channel_index,
	    int state_index,
	    uint64_t time_in_state,
	    uint64_t intransitions,
	    uint64_t last_intransition = 0);


	virtual IOReturn handleIncrementChannelStateByIndices(int channel_index,
	    int state_index,
	    uint64_t time_in_state,
	    uint64_t intransitions,
	    uint64_t last_intransition = 0);
private:

	int            *_currentStates;     
	uint64_t       *_lastUpdateTimes;   


	int            *_swapCurrentStates;
	uint64_t       *_swapLastUpdateTimes;

	enum valueSelector {
		kInTransitions,
		kResidencyTime,
		kLastTransitionTime
	};
	uint64_t _getStateValue(uint64_t channel_id,
	    uint64_t state_id,
	    enum valueSelector value);

	IOReturn _getStateIndices(uint64_t channel_id,
	    uint64_t state_id,
	    int *channel_index,
	    int *state_index);
};
class IOHistogramReporter : public IOReporter
{
	OSDeclareDefaultStructors(IOHistogramReporter);

public:

	static OSPtr<IOHistogramReporter> with(IOService *reportingService,
	    IOReportCategories categories,
	    uint64_t channelID,
	    const char *channelName,
	    IOReportUnit unit,
	    int nSegments,
	    IOHistogramSegmentConfig *config);


	IOReturn
	addChannel(__unused uint64_t channelID, __unused const char *channelName = NULL)
	{
		return kIOReturnUnsupported;
	}



	IOReturn overrideBucketValues(unsigned int index,
	    uint64_t bucket_hits,
	    int64_t bucket_min,
	    int64_t bucket_max,
	    int64_t bucket_sum);


	int tallyValue(int64_t value);


	static OSPtr<IOReportLegendEntry> createLegend(const uint64_t channelID,
	    const char *channelName,
	    int segmentCount,
	    IOHistogramSegmentConfig *config,
	    IOReportCategories categories,
	    IOReportUnit unit);


	virtual void free(void) APPLE_KEXT_OVERRIDE;

protected:


	virtual bool initWith(IOService *reportingService,
	    IOReportCategories categories,
	    uint64_t channelID,
	    const OSSymbol *channelName,
	    IOReportUnit unit,
	    int nSegments,
	    IOHistogramSegmentConfig  *config);


	OSPtr<IOReportLegendEntry> handleCreateLegend(void) APPLE_KEXT_OVERRIDE;


private:

	int                         _segmentCount;
	int64_t                    *_bucketBounds;
	int                         _bucketCount;
	IOHistogramSegmentConfig   *_histogramSegmentsConfig;
};
class IOReportLegend : public OSObject
{
	OSDeclareDefaultStructors(IOReportLegend);

public:

	static OSPtr<IOReportLegend> with(OSArray *legend);


	IOReturn addLegendEntry(IOReportLegendEntry *legendEntry,
	    const char *groupName,
	    const char *subGroupName);


	IOReturn addReporterLegend(IOReporter *reporter,
	    const char *groupName,
	    const char *subGroupName);


	static  IOReturn addReporterLegend(IOService *reportingService,
	    IOReporter *reporter,
	    const char *groupName,
	    const char *subGroupName);


	OSArray* getLegend(void);


	void free(void) APPLE_KEXT_OVERRIDE;



protected:

private:

	OSPtr<OSArray>     _reportLegend;

	IOReturn initWith(OSArray *legend);


	IOReturn organizeLegend(IOReportLegendEntry *legendEntry,
	    const OSSymbol *groupName,
	    const OSSymbol *subGroupName);





};
extern void    IOPrintPlane(
	);
extern void    OSPrintMemory( void );
class IOKitDiagnosticsClient : public IOUserClient2022
{
	OSDeclareDefaultStructors(IOKitDiagnosticsClient);

public:
	static  IOUserClient * withTask(task_t owningTask);
	virtual IOReturn       clientClose(void) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       setProperties(OSObject * properties) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       externalMethod(uint32_t selector, IOExternalMethodArgumentsOpaque * args) APPLE_KEXT_OVERRIDE;
};
extern void iokit_add_reference( io_object_t obj, ipc_kobject_type_t type );
extern ipc_port_t iokit_port_for_object(io_object_t obj,
    ipc_kobject_type_t type, ipc_kobject_t * kobj);
extern kern_return_t iokit_client_died( io_object_t obj,
    ipc_port_t port, ipc_kobject_type_t type, mach_port_mscount_t * mscount );
extern kern_return_t
iokit_client_memory_for_type(
	io_object_t     connect,
	unsigned int    type,
	unsigned int *  flags,
	vm_address_t *  address,
	vm_size_t    *  size );
extern ipc_port_t iokit_alloc_object_port( io_kobject_t obj,
    ipc_kobject_type_t type );
extern void iokit_remove_object_port( ipc_port_t port,
    ipc_kobject_type_t type );
extern kern_return_t iokit_destroy_object_port( ipc_port_t port,
    ipc_kobject_type_t type );
extern ipc_kobject_type_t iokit_port_type(ipc_port_t port);
extern mach_port_name_t iokit_make_send_right( task_t task,
    io_object_t obj, ipc_kobject_type_t type );
extern mach_port_t ipc_kobject_make_send(mach_port_t, ipc_kobject_t, ipc_kobject_type_t) __result_use_check;
extern mach_port_t ipc_kobject_copy_send(mach_port_t, ipc_kobject_t, ipc_kobject_type_t) __result_use_check;
extern mach_port_t ipc_port_make_send_mqueue(mach_port_t) __result_use_check;
extern mach_port_t ipc_port_copy_send_mqueue(mach_port_t) __result_use_check;
extern void ipc_port_release_send(ipc_port_t port);
extern io_object_t iokit_lookup_io_object(ipc_port_t port, ipc_kobject_type_t type);
extern kern_return_t iokit_mod_send_right( task_t task, mach_port_name_t name, mach_port_delta_t delta );
extern io_object_t iokit_lookup_object_with_port_name(mach_port_name_t name, ipc_kobject_type_t type, task_t task);
extern io_object_t iokit_lookup_connect_ref_current_task(mach_port_name_t name);
extern io_object_t iokit_lookup_uext_ref_current_task(mach_port_name_t name);
extern void iokit_retain_port( ipc_port_t port );
extern void iokit_release_port( ipc_port_t port );
extern void iokit_release_port_send( ipc_port_t port );
extern void iokit_lock_port(ipc_port_t port);
extern void iokit_unlock_port(ipc_port_t port);
extern kern_return_t iokit_lookup_raw_current_task(mach_port_name_t name, ipc_kobject_type_t type, ipc_port_t *port);
extern kern_return_t
uext_server(ipc_port_t receiver, ipc_kmsg_t request, ipc_kmsg_t * preply);
extern kern_return_t
iokit_label_dext_task(task_t task);
extern void
iokit_clear_registered_ports(task_t task);
extern ppnum_t IOGetLastPageNumber(void);
extern kern_return_t IOMapPages(vm_map_t map, mach_vm_address_t va, mach_vm_address_t pa,
    mach_vm_size_t length, unsigned int mapFlags);
extern kern_return_t IOUnmapPages(vm_map_t map, mach_vm_address_t va, mach_vm_size_t length);
extern kern_return_t IOProtectCacheMode(vm_map_t map, mach_vm_address_t va,
    mach_vm_size_t length, unsigned int options);
extern unsigned int IODefaultCacheBits(addr64_t pa);
__BEGIN_DECLS







static inline vm_size_t
IOMallocArraySize(vm_size_t hdr_size, vm_size_t elem_size, vm_size_t elem_count)
{
	
	const vm_size_t limit = 1ull << (8 * sizeof(vm_size_t) - 1);
	vm_size_t s = hdr_size;

	if (os_mul_and_add_overflow(elem_size, elem_count, s, &s) || (s & limit)) {
		return limit;
	}
	return s;
}



typedef thread_t IOThread;
extern void *
IOMalloc_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	vm_size_t            size,
	zalloc_flags_t       flags);
__attribute__((alloc_size(2)))
static inline void *
__IOMalloc_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	vm_size_t            size,
	zalloc_flags_t       flags)
{
	void *addr = (IOMalloc_internal)(kalloc_heap_cfg, size, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}






extern void
IOFree_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	void               * inAddress,
	vm_size_t            size);
void   IOFree(void * address, vm_size_t size);
extern void *
IOMallocAligned_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	vm_size_t            size,
	vm_size_t            alignment,
	zalloc_flags_t       flags);
__attribute__((alloc_size(2)))
static inline void *
__IOMallocAligned_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	vm_size_t            size,
	vm_size_t            alignment,
	zalloc_flags_t       flags)
{
	void *addr = (IOMallocAligned_internal)(kalloc_heap_cfg, size, alignment, flags);
	if (flags & Z_NOFAIL) {
		__builtin_assume(addr != NULL);
	}
	return addr;
}









extern void
IOFreeAligned_internal(
	struct kalloc_heap * kalloc_heap_cfg,
	void               * address,
	vm_size_t            size);
void   IOFreeAligned(void * address, vm_size_t size);
void * IOMallocContiguous(vm_size_t size, vm_size_t alignment,
    IOPhysicalAddress * physicalAddress) __attribute__((deprecated)) __attribute__((alloc_size(1)));
void   IOFreeContiguous(void * address, vm_size_t size) __attribute__((deprecated));
void * IOMallocPageable(vm_size_t size, vm_size_t alignment) __attribute__((alloc_size(1)));
void * IOMallocPageableZero(vm_size_t size, vm_size_t alignment) __attribute__((alloc_size(1)));
void IOFreePageable(void * address, vm_size_t size);
void IOFreeData(void * address, vm_size_t size);
UInt8 IOMappedRead8(IOPhysicalAddress address);
UInt16 IOMappedRead16(IOPhysicalAddress address);
UInt32 IOMappedRead32(IOPhysicalAddress address);
UInt64 IOMappedRead64(IOPhysicalAddress address);
void IOMappedWrite8(IOPhysicalAddress address, UInt8 value);
void IOMappedWrite16(IOPhysicalAddress address, UInt16 value);
void IOMappedWrite32(IOPhysicalAddress address, UInt32 value);
void IOMappedWrite64(IOPhysicalAddress address, UInt64 value);
IOReturn IOSetProcessorCacheMode( task_t task, IOVirtualAddress address,
    IOByteCount length, IOOptionBits cacheMode ) __attribute__((deprecated));
IOReturn IOFlushProcessorCache( task_t task, IOVirtualAddress address,
    IOByteCount length );
IOThread IOCreateThread(IOThreadFunc function, void *argument) __attribute__((deprecated));
void IOExitThread(void) __attribute__((deprecated));
void IOSleep(unsigned milliseconds);
void IOSleepWithLeeway(unsigned intervalMilliseconds, unsigned leewayMilliseconds);
void IODelay(unsigned microseconds);
void IOPause(unsigned nanoseconds);
void IOLog(const char *format, ...)
__attribute__((format(printf, 1, 2)));
void IOLogv(const char *format, va_list ap)
__attribute__((format(printf, 1, 0)));
void kprintf(const char *format, ...) __printflike(1, 2);
const char *IOFindNameForValue(int value,
    const IONamedValue *namedValueArray);
IOReturn IOFindValueForName(const char *string,
    const IONamedValue *regValueArray,
    int *value);
void Debugger(const char * reason);
IOBSDNameMatching( const char * name );
IOOFPathMatching( const char * path, char * buf, int maxLen ) __attribute__((deprecated));
IOAlignment IOSizeToAlignment(unsigned int size);
unsigned int IOAlignmentToSize(IOAlignment align);
static inline IOFixed
IOFixedMultiply(IOFixed a, IOFixed b)
{
	return (IOFixed)((((SInt64) a) * ((SInt64) b)) >> 16);
}

static inline IOFixed
IOFixedDivide(IOFixed a, IOFixed b)
{
	return (IOFixed)((((SInt64) a) << 16) / ((SInt64) b));
}






vm_tag_t
IOMemoryTag(vm_map_t map);
vm_size_t
log2up(vm_size_t size);
void *
IOMallocTypeImpl(kalloc_type_view_t kt_view);
void
IOFreeTypeImpl(kalloc_type_view_t kt_view, void * address);
void *
IOMallocTypeVarImpl(kalloc_type_var_view_t kt_view, vm_size_t size);
void
IOFreeTypeVarImpl(kalloc_type_var_view_t kt_view, void * address, vm_size_t size);
IOLock * IOLockAlloc( void );
void    IOLockFree( IOLock * lock);
lck_mtx_t * IOLockGetMachLock( IOLock * lock);
int     IOLockSleep( IOLock * lock, void *event, UInt32 interType) __DARWIN14_ALIAS(IOLockSleep);
int     IOLockSleepDeadline( IOLock * lock, void *event,
    AbsoluteTime deadline, UInt32 interType) __DARWIN14_ALIAS(IOLockSleepDeadline);
void    IOLockWakeup(IOLock * lock, void *event, bool oneThread) __DARWIN14_ALIAS(IOLockWakeup);
int     IOLockSleepWithInheritor( IOLock * lock, UInt32 lck_sleep_action,
    void *event, thread_t inheritor, UInt32 interType, uint64_t deadline);
void    IOLockWakeupAllWithInheritor(IOLock * lock, void *event);
void    IOLockInlineInit(IOLock *);
void    IOLockInlineDestroy(IOLock *);
IORecursiveLock * IORecursiveLockAlloc( void );
void            IORecursiveLockFree( IORecursiveLock * lock);
lck_mtx_t * IORecursiveLockGetMachLock( IORecursiveLock * lock);
void            IORecursiveLockLock( IORecursiveLock * lock);
boolean_t       IORecursiveLockTryLock( IORecursiveLock * lock);
void            IORecursiveLockUnlock( IORecursiveLock * lock);
boolean_t       IORecursiveLockHaveLock( const IORecursiveLock * lock);
extern int      IORecursiveLockSleep( IORecursiveLock *_lock,
    void *event, UInt32 interType);
extern int      IORecursiveLockSleepDeadline( IORecursiveLock * _lock, void *event,
    AbsoluteTime deadline, UInt32 interType);
extern void     IORecursiveLockWakeup( IORecursiveLock *_lock,
    void *event, bool oneThread);
IORWLock * IORWLockAlloc( void );
void    IORWLockFree( IORWLock * lock);
lck_rw_t * IORWLockGetMachLock( IORWLock * lock);
void    IORWLockInlineInit(IORWLock *);
void    IORWLockInlineDestroy(IORWLock *);
IOSimpleLock * IOSimpleLockAlloc( void );
void IOSimpleLockFree( IOSimpleLock * lock );
lck_spin_t * IOSimpleLockGetMachLock( IOSimpleLock * lock);
void IOSimpleLockInit( IOSimpleLock * lock );
void IOSimpleLockDestroy( IOSimpleLock * lock );
IORecursiveLock *
IORecursiveLockAllocWithLockGroup( lck_grp_t * lockGroup );
__BEGIN_DECLS


ppnum_t IOMapperIOVMAlloc(unsigned pages);
void IOMapperIOVMFree(ppnum_t addr, unsigned pages);
ppnum_t IOMapperInsertPage(ppnum_t addr, unsigned offset, ppnum_t page);
class IOMemoryCursor : public OSObject
{
	OSDeclareDefaultStructors(IOMemoryCursor);

public:

	struct PhysicalSegment {
		IOPhysicalAddress location;
		IOPhysicalLength  length;
	};




	typedef void (*SegmentFunction)(PhysicalSegment segment,
	    void *          segments,
	    UInt32          segmentIndex);



protected:

	SegmentFunction outSeg;


	IOPhysicalLength  maxSegmentSize;


	IOPhysicalLength  maxTransferSize;


	IOPhysicalLength  alignMask;

public:

	static OSPtr<IOMemoryCursor>
	withSpecification(SegmentFunction  outSegFunc,
	    IOPhysicalLength maxSegmentSize = 0,
	    IOPhysicalLength maxTransferSize = 0,
	    IOPhysicalLength alignment = 1);


	virtual bool initWithSpecification(SegmentFunction  outSegFunc,
	    IOPhysicalLength maxSegmentSize = 0,
	    IOPhysicalLength maxTransferSize = 0,
	    IOPhysicalLength alignment = 1);


	virtual UInt32 genPhysicalSegments(
		IOMemoryDescriptor *descriptor,
		IOByteCount         fromPosition,
		void *              segments,
		UInt32              maxSegments,
		UInt32              maxTransferSize = 0,
		IOByteCount        *transferSize = NULL);
};
class IONaturalMemoryCursor : public IOMemoryCursor
{
	OSDeclareDefaultStructors(IONaturalMemoryCursor);

public:

	static void outputSegment(PhysicalSegment segment,
	    void *          segments,
	    UInt32          segmentIndex);




	static OSPtr<IONaturalMemoryCursor>
	withSpecification(IOPhysicalLength maxSegmentSize,
	    IOPhysicalLength maxTransferSize,
	    IOPhysicalLength alignment = 1);


	virtual bool initWithSpecification(IOPhysicalLength  maxSegmentSize,
	    IOPhysicalLength  maxTransferSize,
	    IOPhysicalLength  alignment = 1);



	virtual UInt32
	getPhysicalSegments(IOMemoryDescriptor *descriptor,
	    IOByteCount         fromPosition,
	    PhysicalSegment    *segments,
	    UInt32              maxSegments,
	    UInt32              inMaxTransferSize = 0,
	    IOByteCount        *transferSize = NULL)
	{
		return genPhysicalSegments(descriptor, fromPosition, segments,
		           maxSegments, inMaxTransferSize, transferSize);
	}
};
class IOBigMemoryCursor : public IOMemoryCursor
{
	OSDeclareDefaultStructors(IOBigMemoryCursor);

public:

	static void outputSegment(PhysicalSegment segment,
	    void *          segments,
	    UInt32          segmentIndex);




	static OSPtr<IOBigMemoryCursor>
	withSpecification(IOPhysicalLength maxSegmentSize,
	    IOPhysicalLength maxTransferSize,
	    IOPhysicalLength alignment = 1);


	virtual bool initWithSpecification(IOPhysicalLength  maxSegmentSize,
	    IOPhysicalLength  maxTransferSize,
	    IOPhysicalLength  alignment = 1);



	virtual UInt32
	getPhysicalSegments(IOMemoryDescriptor * descriptor,
	    IOByteCount          fromPosition,
	    PhysicalSegment *    segments,
	    UInt32               maxSegments,
	    UInt32               inMaxTransferSize = 0,
	    IOByteCount       *  transferSize = NULL)
	{
		return genPhysicalSegments(descriptor, fromPosition, segments,
		           maxSegments, inMaxTransferSize, transferSize);
	}
};
class IOLittleMemoryCursor : public IOMemoryCursor
{
	OSDeclareDefaultStructors(IOLittleMemoryCursor);

public:

	static void outputSegment(PhysicalSegment segment,
	    void *          segments,
	    UInt32          segmentIndex);




	static OSPtr<IOLittleMemoryCursor>
	withSpecification(IOPhysicalLength maxSegmentSize,
	    IOPhysicalLength maxTransferSize,
	    IOPhysicalLength alignment = 1);


	virtual bool initWithSpecification(IOPhysicalLength  maxSegmentSize,
	    IOPhysicalLength  maxTransferSize,
	    IOPhysicalLength  alignment = 1);



	virtual UInt32
	getPhysicalSegments(IOMemoryDescriptor * descriptor,
	    IOByteCount          fromPosition,
	    PhysicalSegment *    segments,
	    UInt32               maxSegments,
	    UInt32               inMaxTransferSize = 0,
	    IOByteCount       *  transferSize = NULL)
	{
		return genPhysicalSegments(descriptor, fromPosition, segments,
		           maxSegments, inMaxTransferSize, transferSize);
	}
};
class IOMemoryDescriptor : public OSObject
{
	friend class IOMemoryMap;
	friend class IOMultiMemoryDescriptor;

	OSDeclareDefaultStructorsWithDispatch(IOMemoryDescriptor);

protected:


	struct IOMemoryDescriptorReserved * reserved;

protected:
	OSPtr<OSSet>        _mappings;
	IOOptionBits        _flags;


public:
	struct IOMemoryReference *  _memRef;
	vm_tag_t _kernelTag;
	vm_tag_t _userTag;
	int16_t _dmaReferences;
	uint16_t _internalFlags;
	kern_allocation_name_t _mapName;
protected:

	uint16_t            _iomapperOptions;
	uint16_t            __iomd_reserved3[3];
	uintptr_t           __iomd_reserved4;

	IOByteCount         _length;       
	IOOptionBits        _tag;

public:
	typedef IOOptionBits DMACommandOps;


	virtual bool initWithOptions(void *         buffers,
	    UInt32         count,
	    UInt32         offset,
	    task_t         task,
	    IOOptionBits   options,
	    IOMapper *     mapper = kIOMapperSystem);




	virtual IOReturn setPurgeable( IOOptionBits newState,
	    IOOptionBits * oldState );



	IOReturn setOwnership( task_t newOwner,
	    int newLedgerTag,
	    IOOptionBits newLedgerOptions );



	IOReturn getPageCounts( IOByteCount * residentPageCount,
	    IOByteCount * dirtyPageCount);



	virtual IOReturn performOperation( IOOptionBits options,
	    IOByteCount offset, IOByteCount length );


	virtual IOReturn dmaCommandOperation(DMACommandOps op, void *vData, UInt dataSize) const;



	virtual addr64_t getPhysicalSegment( IOByteCount   offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0 ) = 0;

	virtual uint64_t getPreparationID( void );
	void             setPreparationID( void );

	void     setVMTags(uint32_t kernelTag, uint32_t userTag);
	uint32_t getVMTag(vm_map_t map);

	uint64_t getDescriptorID( void );
	void     setDescriptorID( void );

	IOReturn ktraceEmitPhysicalSegments( void );

	IOMemoryDescriptorReserved * getKernelReserved( void );
	void                         cleanKernelReserved(IOMemoryDescriptorReserved * reserved);
	IOReturn dmaMap(
		IOMapper                    * mapper,
		IOMemoryDescriptor          * memory,
		IODMACommand                * command,
		const IODMAMapSpecification * mapSpec,
		uint64_t                      offset,
		uint64_t                      length,
		uint64_t                    * mapAddress,
		uint64_t                    * mapLength);
	IOReturn dmaUnmap(
		IOMapper                    * mapper,
		IODMACommand                * command,
		uint64_t                      offset,
		uint64_t                      mapAddress,
		uint64_t                      mapLength);
	void dmaMapRecord(
		IOMapper                    * mapper,
		IODMACommand                * command,
		uint64_t                      mapLength);

private:
	OSMetaClassDeclareReservedUsedX86(IOMemoryDescriptor, 0);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 1);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 2);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 3);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 4);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 5);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 6);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 7);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 8);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 9);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 10);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 11);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 12);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 13);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 14);
	OSMetaClassDeclareReservedUnused(IOMemoryDescriptor, 15);

protected:
	virtual void free(void) APPLE_KEXT_OVERRIDE;
public:
	static void initialize( void );

public:


	static OSPtr<IOMemoryDescriptor>  withAddress(void *       address,
	    IOByteCount  withLength,
	    IODirection  withDirection);




	static OSPtr<IOMemoryDescriptor>  withPhysicalAddress(
		IOPhysicalAddress       address,
		IOByteCount             withLength,
		IODirection             withDirection );




	static OSPtr<IOMemoryDescriptor>  withAddressRange(
		mach_vm_address_t address,
		mach_vm_size_t    length,
		IOOptionBits      options,
		task_t            task);



	static OSPtr<IOMemoryDescriptor>  withAddressRanges(
		IOAddressRange * ranges,
		UInt32           rangeCount,
		IOOptionBits     options,
		task_t           task);



	static OSPtr<IOMemoryDescriptor> withOptions(void *       buffers,
	    UInt32       count,
	    UInt32       offset,
	    task_t       task,
	    IOOptionBits options,
	    IOMapper *   mapper = kIOMapperSystem);




	static OSPtr<IOMemoryDescriptor>
	withPersistentMemoryDescriptor(IOMemoryDescriptor *originalMD);




	virtual IODirection getDirection() const;



	virtual IOByteCount getLength() const;

	uint64_t getDMAMapLength(uint64_t * offset = NULL);



	virtual void setTag( IOOptionBits tag );



	virtual IOOptionBits getTag( void );



	uint64_t getFlags(void);



	virtual IOByteCount readBytes(IOByteCount offset,
	    void * bytes, IOByteCount withLength);



	virtual IOByteCount writeBytes(IOByteCount offset,
	    const void * bytes, IOByteCount withLength);




	IOPhysicalAddress getPhysicalAddress();




	virtual IOReturn prepare(IODirection forDirection = kIODirectionNone) = 0;



	virtual IOReturn complete(IODirection forDirection = kIODirectionNone) = 0;





	OSPtr<IOMemoryMap>        createMappingInTask(
		task_t                  intoTask,
		mach_vm_address_t       atAddress,
		IOOptionBits            options,
		mach_vm_size_t          offset = 0,
		mach_vm_size_t          length = 0 );




	virtual OSPtr<IOMemoryMap>       map(
		IOOptionBits            options = 0 );



	virtual OSPtr<IOMemoryMap>       setMapping(
		task_t          task,
		IOVirtualAddress        mapAddress,
		IOOptionBits            options = 0 );



	void setMapperOptions( uint16_t options );



	uint16_t getMapperOptions( void );



	virtual
	IOReturn redirect( task_t safeTask, bool redirect );

	IOReturn handleFault(
		void *                  _pager,
		mach_vm_size_t          sourceOffset,
		mach_vm_size_t          length);

	IOReturn populateDevicePager(
		void *                  pager,
		vm_map_t                addressMap,
		mach_vm_address_t       address,
		mach_vm_size_t          sourceOffset,
		mach_vm_size_t          length,
		IOOptionBits            options );

	virtual LIBKERN_RETURNS_NOT_RETAINED IOMemoryMap *      makeMapping(
		IOMemoryDescriptor *    owner,
		task_t                  intoTask,
		IOVirtualAddress        atAddress,
		IOOptionBits            options,
		IOByteCount             offset,
		IOByteCount             length );


	OSObject * copyContext(void) const;


	void setContext(OSObject * context);

protected:
	virtual void                addMapping(
		IOMemoryMap *           mapping );

	virtual void                removeMapping(
		IOMemoryMap *           mapping );

	virtual IOReturn doMap(
		vm_map_t                addressMap,
		IOVirtualAddress *      atAddress,
		IOOptionBits            options,
		IOByteCount             sourceOffset = 0,
		IOByteCount             length = 0 );

	virtual IOReturn doUnmap(
		vm_map_t                addressMap,
		IOVirtualAddress        logical,
		IOByteCount             length );
};
class IOMemoryMap : public OSObject
{
	OSDeclareDefaultStructorsWithDispatch(IOMemoryMap);
public:
	IOOptionBits         fOptions;
	OSPtr<IOMemoryDescriptor>  fMemory;
	OSPtr<IOMemoryMap>         fSuperMap;
	mach_vm_size_t       fOffset;
	mach_vm_address_t    fAddress;
	mach_vm_size_t       fLength;
	task_t               fAddressTask;
	vm_map_t             fAddressMap;
	upl_t                fRedirUPL;
	uint8_t              fUserClientUnmap;

protected:
	virtual void taggedRelease(const void *tag = NULL) const APPLE_KEXT_OVERRIDE;
	virtual void free(void) APPLE_KEXT_OVERRIDE;

public:


	virtual IOVirtualAddress    getVirtualAddress(void);



	virtual IOPhysicalAddress   getPhysicalSegment(IOByteCount offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0);



	IOPhysicalAddress getPhysicalAddress(void);



	virtual IOByteCount         getLength(void);



	virtual task_t              getAddressTask();



	virtual IOMemoryDescriptor * getMemoryDescriptor();



	virtual IOOptionBits        getMapOptions();



	virtual IOReturn            unmap();

	virtual void                taskDied();



	virtual IOReturn            redirect(IOMemoryDescriptor * newBackingMemory,
	    IOOptionBits         options,
	    mach_vm_size_t       offset = 0);


	inline mach_vm_address_t    getAddress() __attribute__((always_inline));

	inline mach_vm_size_t       getSize() __attribute__((always_inline));


	IOMemoryMap *  copyCompatible( IOMemoryMap * newMapping );

	bool init(
		task_t                  intoTask,
		mach_vm_address_t       toAddress,
		IOOptionBits            options,
		mach_vm_size_t          offset,
		mach_vm_size_t          length );

	bool    setMemoryDescriptor(IOMemoryDescriptor * _memory, mach_vm_size_t _offset);

	IOReturn redirect(
		task_t                  intoTask, bool redirect );

	IOReturn userClientUnmap();

	IOReturn wireRange(
		uint32_t                options,
		mach_vm_size_t          offset,
		mach_vm_size_t          length);

	OSMetaClassDeclareReservedUnused(IOMemoryMap, 0);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 1);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 2);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 3);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 4);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 5);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 6);
	OSMetaClassDeclareReservedUnused(IOMemoryMap, 7);
};
class IOGeneralMemoryDescriptor : public IOMemoryDescriptor
{
	OSDeclareDefaultStructors(IOGeneralMemoryDescriptor);

public:
	union Ranges {
		IOVirtualRange   *v;
		IOAddressRange   *v64;
		IOPhysicalRange  *p;
		void             *uio;
	};
protected:
	Ranges              _ranges;
	unsigned            _rangesCount;   

	task_t              _task;           

	union {
		IOVirtualRange  v;
		IOPhysicalRange p;
	}                   _singleRange;  

	unsigned            _wireCount;    


	bool                _initialized;  

public:
	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual IOReturn dmaCommandOperation(DMACommandOps op, void *vData, UInt dataSize) const APPLE_KEXT_OVERRIDE;

	virtual uint64_t getPreparationID( void ) APPLE_KEXT_OVERRIDE;


	IOReturn wireVirtual(IODirection forDirection);
	IOReturn dmaMap(
		IOMapper                    * mapper,
		IOMemoryDescriptor          * memory,
		IODMACommand                * command,
		const IODMAMapSpecification * mapSpec,
		uint64_t                      offset,
		uint64_t                      length,
		uint64_t                    * mapAddress,
		uint64_t                    * mapLength);
	bool initMemoryEntries(size_t size, IOMapper * mapper);

	IOMemoryReference * memoryReferenceAlloc(uint32_t capacity,
	    IOMemoryReference * realloc);
	void memoryReferenceFree(IOMemoryReference * ref);
	void memoryReferenceRelease(IOMemoryReference * ref);

	IOReturn memoryReferenceCreate(
		IOOptionBits         options,
		IOMemoryReference ** reference);

	IOReturn memoryReferenceMap(IOMemoryReference * ref,
	    vm_map_t            map,
	    mach_vm_size_t      inoffset,
	    mach_vm_size_t      size,
	    IOOptionBits        options,
	    mach_vm_address_t * inaddr);

	IOReturn memoryReferenceMapNew(IOMemoryReference * ref,
	    vm_map_t            map,
	    mach_vm_size_t      inoffset,
	    mach_vm_size_t      size,
	    IOOptionBits        options,
	    mach_vm_address_t * inaddr);

	static IOReturn memoryReferenceSetPurgeable(
		IOMemoryReference * ref,
		IOOptionBits newState,
		IOOptionBits * oldState);
	static IOReturn memoryReferenceSetOwnership(
		IOMemoryReference * ref,
		task_t newOwner,
		int newLedgerTag,
		IOOptionBits newLedgerOptions);
	static IOReturn memoryReferenceGetPageCounts(
		IOMemoryReference * ref,
		IOByteCount       * residentPageCount,
		IOByteCount       * dirtyPageCount);

	static uint64_t memoryReferenceGetDMAMapLength(
		IOMemoryReference * ref,
		uint64_t * offset);

	IOByteCount readBytes(IOByteCount offset,
	    void * bytes, IOByteCount withLength) override;
	IOByteCount writeBytes(IOByteCount offset,
	    const void * bytes, IOByteCount withLength) override;


private:



	OSPtr<_IOMemoryDescriptorMixedData> _memoryEntries;
	unsigned int    _pages;
	ppnum_t         _highestPage;
	uint32_t        __iomd_reservedA;
	uint32_t        __iomd_reservedB;

	IOLock *        _prepareLock;

public:



	virtual bool initWithOptions(void *         buffers,
	    UInt32         count,
	    UInt32         offset,
	    task_t         task,
	    IOOptionBits   options,
	    IOMapper *     mapper = kIOMapperSystem) APPLE_KEXT_OVERRIDE;


	virtual IOReturn setPurgeable( IOOptionBits newState,
	    IOOptionBits * oldState ) APPLE_KEXT_OVERRIDE;

	IOReturn setOwnership( task_t newOwner,
	    int newLedgerTag,
	    IOOptionBits newLedgerOptions );

	virtual addr64_t getPhysicalSegment( IOByteCount   offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0 ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn prepare(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;

	virtual IOReturn complete(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;

	virtual LIBKERN_RETURNS_NOT_RETAINED IOMemoryMap *      makeMapping(
		IOMemoryDescriptor *    owner,
		task_t                  intoTask,
		IOVirtualAddress        atAddress,
		IOOptionBits            options,
		IOByteCount             offset,
		IOByteCount             length ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn doMap(
		vm_map_t                addressMap,
		IOVirtualAddress *      atAddress,
		IOOptionBits            options,
		IOByteCount             sourceOffset = 0,
		IOByteCount             length = 0 ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn doUnmap(
		vm_map_t                addressMap,
		IOVirtualAddress        logical,
		IOByteCount             length ) APPLE_KEXT_OVERRIDE;

	virtual bool serialize(OSSerialize *s) const APPLE_KEXT_OVERRIDE;


	static OSPtr<IOMemoryDescriptor>
	withPersistentMemoryDescriptor(IOGeneralMemoryDescriptor *originalMD);

	IOOptionBits memoryReferenceCreateOptions(IOOptionBits options, IOMemoryMap * map);
};
mach_vm_address_t
IOMemoryMap::getAddress()
{
	return getVirtualAddress();
}

mach_vm_size_t
IOMemoryMap::getSize()
{
	return getLength();
}



extern bool iokit_iomd_setownership_enabled;
class IOMultiMemoryDescriptor : public IOMemoryDescriptor
{
	OSDeclareDefaultStructors(IOMultiMemoryDescriptor);

protected:

	IOMemoryDescriptor ** _descriptors;
	UInt32                _descriptorsCount;
	bool                  _descriptorsIsAllocated;

	virtual void free() APPLE_KEXT_OVERRIDE;

public:



	static OSPtr<IOMultiMemoryDescriptor>  withDescriptors(
		IOMemoryDescriptor ** descriptors,
		UInt32                withCount,
		IODirection           withDirection,
		bool                  asReference = false );



	virtual bool initWithDescriptors(
		IOMemoryDescriptor ** descriptors,
		UInt32                withCount,
		IODirection           withDirection,
		bool                  asReference = false );



	virtual addr64_t getPhysicalSegment( IOByteCount   offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0 ) APPLE_KEXT_OVERRIDE;



	virtual IOReturn prepare(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;



	virtual IOReturn complete(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;

	virtual IOReturn setPurgeable(IOOptionBits newState, IOOptionBits * oldState) APPLE_KEXT_OVERRIDE;

	IOReturn setOwnership(task_t newOwner, int newLedgerTag, IOOptionBits newOptions);



	IOReturn getPageCounts(IOByteCount * residentPageCount,
	    IOByteCount * dirtyPageCount);

	virtual uint64_t getPreparationID( void ) APPLE_KEXT_OVERRIDE;


private:
	virtual IOReturn doMap(vm_map_t           addressMap,
	    IOVirtualAddress * atAddress,
	    IOOptionBits       options,
	    IOByteCount        sourceOffset = 0,
	    IOByteCount        length = 0 ) APPLE_KEXT_OVERRIDE;
};
class IONotifier : public OSObject
{
	OSDeclareAbstractStructors(IONotifier);

public:



	virtual void remove() = 0;



	virtual bool disable() = 0;



	virtual void enable( bool was ) = 0;
};
#pragma once


extern "C" kern_return_t IOCPURunPlatformQuiesceActions(void);
extern "C" kern_return_t IOCPURunPlatformActiveActions(void);
extern "C" kern_return_t IOCPURunPlatformHaltRestartActions(uint32_t message);
extern "C" kern_return_t IOCPURunPlatformPanicActions(uint32_t message, uint32_t details);
extern "C" kern_return_t IOCPURunPlatformPanicSyncAction(void *addr, uint32_t offset, uint32_t len);
void IOPlatformActionsPreSleep(void);
void IOPlatformActionsPostResume(void);
extern boolean_t PEGetMachineName( char * name, int maxLength );
extern boolean_t PEGetModelName( char * name, int maxLength );
extern boolean_t PEGetTargetName( char * name, int maxLength );
extern boolean_t PEGetProductName( char * name, int maxLength );
extern int PEGetPlatformEpoch( void );
extern int PEHaltRestart(unsigned int type);
extern int PEHaltRestartInternal(unsigned int type, uint32_t details);
extern void IOSystemShutdownNotification(int howto, int stage);
extern boolean_t IOPMRootDomainGetWillShutdown(void);
extern void PEInitiatePanic(void);
extern UInt32 PESavePanicInfo(UInt8 *buffer, UInt32 length);
extern void PESavePanicInfoAction(void *buffer, UInt32 offset, UInt32 length);
extern long PEGetGMTTimeOfDay( void );
extern void PESetGMTTimeOfDay( long secs );
extern void PEGetUTCTimeOfDay( clock_sec_t * secs, clock_usec_t * usecs );
extern void PESetUTCTimeOfDay( clock_sec_t secs, clock_usec_t usecs );
extern boolean_t PEWriteNVRAMBooleanProperty(const char *symbol, boolean_t value);
extern boolean_t PEWriteNVRAMProperty(const char *symbol, const void *value, const unsigned int len);
extern boolean_t PEWriteNVRAMPropertyWithCopy(const char *symbol, const void *value, const unsigned int len);
extern boolean_t PEReadNVRAMProperty(const char *symbol, void *value, unsigned int *len);
extern boolean_t PEReadNVRAMBooleanProperty(const char *symbol, boolean_t *value);
extern boolean_t PERemoveNVRAMProperty(const char *symbol);
extern boolean_t PESyncNVRAM(void);
extern coprocessor_type_t PEGetCoprocessorVersion( void );
#pragma once




class IOPMGR : public IOService
{
	OSDeclareAbstractStructors(IOPMGR);

public:
	
	virtual void enableCPUCore(unsigned int cpu_id, uint64_t entry_pa);

	
	virtual void enableCPUCore(unsigned int cpu_id);

	
	virtual void disableCPUCore(unsigned int cpu_id) = 0;

	
	virtual void enableCPUCluster(unsigned int cluster_id) = 0;

	
	virtual void disableCPUCluster(unsigned int cluster_id) = 0;

	
	virtual void initCPUIdle(ml_processor_info_t *info) = 0;

	
	virtual void enterCPUIdle(UInt64 *newIdleTimeoutTicks) = 0;

	
	virtual void exitCPUIdle(UInt64 *newIdleTimeoutTicks) = 0;

	
	virtual void updateCPUIdle(UInt64 *newIdleTimeoutTicks) = 0;
};
extern __C IOReturn IOPolledFileSeek(IOPolledFileIOVars * vars, uint64_t position);
extern __C IOReturn IOPolledFileWrite(IOPolledFileIOVars * vars,
    const uint8_t * bytes, IOByteCount size,
    IOPolledFileCryptVars * cryptvars);
extern __C IOReturn IOPolledFileRead(IOPolledFileIOVars * vars,
    uint8_t * bytes, IOByteCount size,
    IOPolledFileCryptVars * cryptvars);
extern __C IOReturn IOPolledFileFlush(IOPolledFileIOVars * vars);
extern __C IOReturn IOPolledFilePollersOpen(IOPolledFileIOVars * vars, uint32_t state, bool abortable);
extern __C IOReturn IOPolledFilePollersClose(IOPolledFileIOVars * vars, uint32_t state);
extern __C IOReturn IOPolledFilePollersSetEncryptionKey(IOPolledFileIOVars * vars,
    const uint8_t * key, size_t keySize);
class IOProviderPropertyMerger : public IOService
{
	OSDeclareDefaultStructors(IOProviderPropertyMerger);
public:
	virtual bool init(OSDictionary * dictionary = NULL) APPLE_KEXT_OVERRIDE;
	virtual bool setProperty(const OSSymbol * aKey, OSObject * anObject) APPLE_KEXT_OVERRIDE;
	virtual void setPropertyTable( OSDictionary * dict ) APPLE_KEXT_OVERRIDE;
};
class IORangeAllocator : public OSObject {
	OSDeclareDefaultStructors(IORangeAllocator);

protected:
	UInt32              numElements;
	UInt32              capacity;
	UInt32              capacityIncrement;
	IORangeScalar       defaultAlignmentMask;
	IOOptionBits        options;

	struct IORangeAllocatorElement *    elements;

private:
	virtual bool allocElement( UInt32 index );

	virtual void deallocElement( UInt32 index );

public:
	enum {
		kLocking        = 0x00000001
	};



	virtual bool init( IORangeScalar endOfRange,
	    IORangeScalar defaultAlignment,
	    UInt32 capacity,
	    IOOptionBits options );



	static OSPtr<IORangeAllocator>  withRange( IORangeScalar endOfRange,
	    IORangeScalar defaultAlignment = 0,
	    UInt32 capacity = 0,
	    IOOptionBits options = 0 );

	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual bool serialize(OSSerialize *s) const APPLE_KEXT_OVERRIDE;



	virtual UInt32 getFragmentCount( void );



	virtual UInt32 getFragmentCapacity( void );



	virtual void setFragmentCapacityIncrement( UInt32 count );



	virtual IORangeScalar getFreeCount( void );



	virtual bool allocate( IORangeScalar size,
	    IORangeScalar * result,
	    IORangeScalar alignment = 0 );



	virtual bool allocateRange( IORangeScalar start,
	    IORangeScalar size );



	virtual void deallocate( IORangeScalar start,
	    IORangeScalar size );
};
class IORegistryEntry : public OSObject
{
	friend class IORegistryIterator;

	OSDeclareDefaultStructors(IORegistryEntry);

protected:

	struct ExpansionData;


	ExpansionData * reserved;

private:

	OSDictionary *      fRegistryTable;
	OSDictionary *      fPropertyTable;

public:




	virtual OSPtr<OSObject> copyProperty( const char *           aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	virtual OSPtr<OSObject> copyProperty( const OSString *        aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	virtual OSPtr<OSObject> copyProperty( const OSSymbol *        aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	virtual OSPtr<IORegistryEntry> copyParentEntry( const IORegistryPlane * plane ) const;



	virtual OSPtr<IORegistryEntry> copyChildEntry( const IORegistryPlane * plane ) const;



	typedef IOReturn (*Action)(OSObject *target,
	    void *arg0, void *arg1,
	    void *arg2, void *arg3);


	virtual IOReturn runPropertyAction(Action action, OSObject *target,
	    void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);


private:
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 0);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 1);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 2);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 3);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 4);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 5);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 6);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 7);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 8);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 9);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 10);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 11);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 12);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 13);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 14);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 15);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 16);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 17);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 18);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 19);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 20);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 21);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 22);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 23);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 24);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 25);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 26);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 27);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 28);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 29);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 30);
	OSMetaClassDeclareReservedUnused(IORegistryEntry, 31);

public:





	static IORegistryEntry * getRegistryRoot( void );



	static SInt32            getGenerationCount( void );



	static const IORegistryPlane * getPlane( const char * name );





	virtual bool init( OSDictionary * dictionary = NULL );



	virtual void free( void ) APPLE_KEXT_OVERRIDE;



	virtual void setPropertyTable( OSDictionary * dict );





	virtual bool setProperty(const OSSymbol * aKey,
	    OSObject * anObject);

	OSObject * setIndexedProperty(uint32_t index, OSObject * anObject);
	OSObject * getIndexedProperty(uint32_t index) const;



	virtual bool setProperty(const OSString * aKey, OSObject * anObject);



	virtual bool setProperty(const char * aKey, OSObject * anObject);



	virtual bool setProperty(const char * aKey, const char * aString);



	virtual bool setProperty(const char * aKey, bool aBoolean);



	virtual bool setProperty( const char *       aKey,
	    unsigned long long aValue,
	    unsigned int       aNumberOfBits);



	virtual bool setProperty( const char *       aKey,
	    void *             bytes,
	    unsigned int       length);



	virtual void removeProperty( const OSSymbol * aKey);



	virtual void removeProperty( const OSString * aKey);



	virtual void removeProperty( const char * aKey);



	virtual OSObject * getProperty( const OSSymbol * aKey) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	virtual OSObject * getProperty( const OSString * aKey) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	virtual OSObject * getProperty( const char * aKey) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	bool propertyExists(const OSSymbol * aKey);



	bool propertyExists(const OSString * aKey);



	bool propertyExists(const char * aKey);



	bool propertyHasValue(const OSSymbol * aKey,
	    const OSObject * value);



	bool propertyHasValue(const OSString * aKey,
	    const OSObject * value);



	bool propertyHasValue(const char * aKey,
	    const OSObject * value);



	virtual OSObject * getProperty( const OSSymbol *        aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	virtual OSObject * getProperty( const OSString *        aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	virtual OSObject * getProperty( const char *            aKey,
	    const IORegistryPlane * plane,
	    IOOptionBits            options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const APPLE_KEXT_DEPRECATED_WITH_SHARED_PTR;



	bool propertyExists( const OSSymbol *        aKey,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	bool propertyExists( const OSString *        aKey,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	bool propertyExists( const char *            aKey,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	bool propertyHasValue( const OSSymbol *        aKey,
	    const OSObject        * value,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	bool propertyHasValue( const OSString *        aKey,
	    const OSObject        * value,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;



	bool propertyHasValue( const char *            aKey,
	    const OSObject        * value,
	    const IORegistryPlane * plane,
	    uint32_t                options =
	    kIORegistryIterateRecursively |
	    kIORegistryIterateParents) const;




	virtual OSPtr<OSObject> copyProperty( const OSSymbol * aKey) const;



	virtual OSPtr<OSObject> copyProperty( const OSString * aKey) const;



	virtual OSPtr<OSObject> copyProperty( const char * aKey) const;



	virtual OSPtr<OSDictionary> dictionaryWithProperties( void ) const;



	virtual bool serializeProperties( OSSerialize * serialize ) const;





 OSDictionary * getPropertyTable( void ) const;






	virtual IOReturn setProperties( OSObject * properties );





	virtual OSPtr<OSIterator> getParentIterator( const IORegistryPlane * plane )
	const;
	virtual void applyToParents( IORegistryEntryApplierFunction applier,
	    void * context,
	    const IORegistryPlane * plane ) const;



	virtual IORegistryEntry * getParentEntry( const IORegistryPlane * plane ) const;



	virtual OSPtr<OSIterator> getChildIterator( const IORegistryPlane * plane )
	const;

	uint32_t getChildCount( const IORegistryPlane * plane ) const;
	OSPtr<OSArray> copyPropertyKeys(void) const;

	virtual void applyToChildren( IORegistryEntryApplierFunction applier,
	    void * context,
	    const IORegistryPlane * plane ) const;



	virtual IORegistryEntry * getChildEntry( const IORegistryPlane * plane ) const;



	virtual bool isChild( IORegistryEntry * child,
	    const IORegistryPlane * plane,
	    bool onlyChild = false ) const;



	virtual bool isParent( IORegistryEntry * parent,
	    const IORegistryPlane * plane,
	    bool onlyParent = false ) const;



	virtual bool inPlane( const IORegistryPlane * plane = NULL) const;



	virtual unsigned int getDepth( const IORegistryPlane * plane ) const;





	virtual bool attachToParent( IORegistryEntry * parent,
	    const IORegistryPlane * plane );



	virtual void detachFromParent( IORegistryEntry * parent,
	    const IORegistryPlane * plane );



	virtual bool attachToChild( IORegistryEntry * child,
	    const IORegistryPlane * plane );



	virtual void detachFromChild( IORegistryEntry * child,
	    const IORegistryPlane * plane );



	virtual void detachAbove( const IORegistryPlane * plane );



	virtual void detachAll( const IORegistryPlane * plane );





	virtual const char * getName( const IORegistryPlane * plane = NULL ) const;



	virtual OSPtr<const OSSymbol> copyName(
		const IORegistryPlane * plane = NULL ) const;



	virtual bool compareNames( OSObject * name, OSString ** matched = NULL ) const;

	bool compareNames( OSObject * name, OSSharedPtr<OSString>& matched) const;



	virtual bool compareName( OSString * name, OSString ** matched = NULL ) const;

	bool compareName( OSString * name, OSSharedPtr<OSString>& matched) const;



	virtual void setName( const OSSymbol * name,
	    const IORegistryPlane * plane = NULL );



	virtual void setName( const char * name,
	    const IORegistryPlane * plane = NULL );



	virtual const char * getLocation( const IORegistryPlane * plane = NULL ) const;



	virtual OSPtr<const OSSymbol> copyLocation(
		const IORegistryPlane * plane = NULL ) const;



	virtual void setLocation( const OSSymbol * location,
	    const IORegistryPlane * plane = NULL );
	virtual void setLocation( const char * location,
	    const IORegistryPlane * plane = NULL );



	virtual bool getPath( char * path, int * length,
	    const IORegistryPlane * plane) const;



	virtual bool getPathComponent( char * path, int * length,
	    const IORegistryPlane * plane ) const;



	static OSPtr<IORegistryEntry> fromPath(  const char * path,
	    const IORegistryPlane * plane = NULL,
	    char * residualPath = NULL,
	    int * residualLength = NULL,
	    IORegistryEntry * fromEntry = NULL );



	virtual OSPtr<IORegistryEntry> childFromPath( const char * path,
	    const IORegistryPlane * plane = NULL,
	    char * residualPath = NULL,
	    int * residualLength = NULL );



	static const char * dealiasPath( const char ** opath,
	    const IORegistryPlane * plane );



	static OSPtr<const IORegistryPlane> makePlane( const char * name );



	uint64_t getRegistryEntryID( void );





	virtual bool init( IORegistryEntry * from,
	    const IORegistryPlane * inPlane );

public:
	static LIBKERN_RETURNS_NOT_RETAINED IORegistryEntry * initialize( void );

	SInt32 getRegistryEntryParentGenerationCount( void ) const;
	void setName(const OSString * name,
	    const IORegistryPlane * plane = NULL);

private:
	inline bool arrayMember( OSArray * set,
	    const IORegistryEntry * member,
	    unsigned int * index = NULL ) const;

	bool makeLink( IORegistryEntry * to,
	    unsigned int relation,
	    const IORegistryPlane * plane ) const;
	void breakLink( IORegistryEntry * to,
	    unsigned int relation,
	    const IORegistryPlane * plane ) const;

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	OSArray * getParentSetReference( const IORegistryPlane * plane )
	const;

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	OSArray * getChildSetReference( const IORegistryPlane * plane )
	const;

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IORegistryEntry * getChildFromComponent( const char ** path,
	    const IORegistryPlane * plane );

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	LIBKERN_RETURNS_NOT_RETAINED const OSSymbol * hasAlias(
		const IORegistryPlane * plane,
		char * opath = NULL, int * length = NULL ) const;

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	const char * matchPathLocation( const char * cmp,
	    const IORegistryPlane * plane );
};
class IORegistryIterator : public OSIterator
{
	OSDeclareAbstractStructors(IORegistryIterator);

private:
	struct IORegCursor {
		IORegCursor          *  next;
		IORegistryEntry      *  current;
		OSIterator           *  iter;
	};
	IORegCursor                 start;
	IORegCursor           *     where;
	IORegistryEntry       *     root;
	OSOrderedSet          *     done;
	const IORegistryPlane *     plane;
	IOOptionBits                options;

	virtual void free( void ) APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<IORegistryIterator> iterateOver( IORegistryEntry * start,
	    const IORegistryPlane * plane,
	    IOOptionBits options = 0 );



	static OSPtr<IORegistryIterator> iterateOver( const IORegistryPlane * plane,
	    IOOptionBits options = 0 );



	virtual IORegistryEntry * getNextObject( void ) APPLE_KEXT_OVERRIDE;



	virtual IORegistryEntry * getNextObjectFlat( void );



	virtual IORegistryEntry * getNextObjectRecursive( void );



	virtual IORegistryEntry * getCurrentEntry( void );



	virtual void enterEntry( void );



	virtual void enterEntry( const IORegistryPlane * plane );



	virtual bool exitEntry( void );



	virtual void reset( void ) APPLE_KEXT_OVERRIDE;



	virtual bool isValid( void ) APPLE_KEXT_OVERRIDE;



	virtual OSPtr<OSOrderedSet> iterateAll( void );
};
#pragma pack(push, 4)

#pragma pack(pop)
typedef struct IORPCMessage IORPCMessage;
#pragma pack(4)

#pragma pack()




typedef struct IORPC IORPC;
extern SInt32 IOServiceOrdering( const OSMetaClassBase * inObj1, const OSMetaClassBase * inObj2, void * ref );
class IOService : public IORegistryEntry
{
	OSDeclareDefaultStructorsWithDispatch(IOService);

public:

	struct ExpansionData {
		uint64_t authorizationID;
		
		IOLock interruptStatisticsLock;
		IOInterruptAccountingReporter * interruptStatisticsArray;
		int interruptStatisticsArrayCount;

		OSObjectUserVars * uvars;
		IOServiceStateChangeVars * svars;

		IOInterruptSourcePrivate * interruptSourcesPrivate;
	};


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData * reserved;
	APPLE_KEXT_WSHADOW_POP;

private:
	IOService *     __provider;
	SInt32      __providerGeneration;
	uint32_t         __resv1;
	IOService *     __owner;
	IOOptionBits    __state[2];
	uint64_t        __timeBusy;
	uint64_t        __accumBusy;
	IOServicePM *   pwrMgt;

protected:

	bool            initialized;
	bool            __machPortHoldDestroy;
	uint8_t         __resv2[6];

public:

	void *          pm_vars;

public:



	virtual bool requestTerminate( IOService * provider, IOOptionBits options );



	virtual bool willTerminate( IOService * provider, IOOptionBits options );



	virtual bool didTerminate( IOService * provider, IOOptionBits options, bool * defer );



	virtual SInt32 nextIdleTimeout(AbsoluteTime currentTime,
	    AbsoluteTime lastActivity, unsigned int powerState);



	virtual void systemWillShutdown( IOOptionBits specifier );



	virtual IOService * copyClientWithCategory( const OSSymbol * category );

public:

	virtual IOReturn configureReport(IOReportChannelList   *channels,
	    IOReportConfigureAction action,
	    void                  *result,
	    void                  *destination);


	virtual IOReturn updateReport(IOReportChannelList      *channels,
	    IOReportUpdateAction      action,
	    void                     *result,
	    void                     *destination);

protected:

	
	IOReturn _ConfigureReport(IOReportChannelList   *channels,
	    IOReportConfigureAction action,
	    void                  *result,
	    void                  *destination);
	IOReturn _UpdateReport(IOReportChannelList      *channels,
	    IOReportUpdateAction      action,
	    void                     *result,
	    void                     *destination);

private:
	OSMetaClassDeclareReservedUsedX86(IOService, 0);
	OSMetaClassDeclareReservedUsedX86(IOService, 1);
	OSMetaClassDeclareReservedUnused(IOService, 2);
	OSMetaClassDeclareReservedUnused(IOService, 3);
	OSMetaClassDeclareReservedUnused(IOService, 4);
	OSMetaClassDeclareReservedUnused(IOService, 5);
	OSMetaClassDeclareReservedUnused(IOService, 6);
	OSMetaClassDeclareReservedUnused(IOService, 7);

	OSMetaClassDeclareReservedUnused(IOService, 8);
	OSMetaClassDeclareReservedUnused(IOService, 9);
	OSMetaClassDeclareReservedUnused(IOService, 10);
	OSMetaClassDeclareReservedUnused(IOService, 11);
	OSMetaClassDeclareReservedUnused(IOService, 12);
	OSMetaClassDeclareReservedUnused(IOService, 13);
	OSMetaClassDeclareReservedUnused(IOService, 14);
	OSMetaClassDeclareReservedUnused(IOService, 15);
	OSMetaClassDeclareReservedUnused(IOService, 16);
	OSMetaClassDeclareReservedUnused(IOService, 17);
	OSMetaClassDeclareReservedUnused(IOService, 18);
	OSMetaClassDeclareReservedUnused(IOService, 19);
	OSMetaClassDeclareReservedUnused(IOService, 20);
	OSMetaClassDeclareReservedUnused(IOService, 21);
	OSMetaClassDeclareReservedUnused(IOService, 22);
	OSMetaClassDeclareReservedUnused(IOService, 23);
	OSMetaClassDeclareReservedUnused(IOService, 24);
	OSMetaClassDeclareReservedUnused(IOService, 25);
	OSMetaClassDeclareReservedUnused(IOService, 26);
	OSMetaClassDeclareReservedUnused(IOService, 27);
	OSMetaClassDeclareReservedUnused(IOService, 28);
	OSMetaClassDeclareReservedUnused(IOService, 29);
	OSMetaClassDeclareReservedUnused(IOService, 30);
	OSMetaClassDeclareReservedUnused(IOService, 31);
	OSMetaClassDeclareReservedUnused(IOService, 32);
	OSMetaClassDeclareReservedUnused(IOService, 33);
	OSMetaClassDeclareReservedUnused(IOService, 34);
	OSMetaClassDeclareReservedUnused(IOService, 35);
	OSMetaClassDeclareReservedUnused(IOService, 36);
	OSMetaClassDeclareReservedUnused(IOService, 37);
	OSMetaClassDeclareReservedUnused(IOService, 38);
	OSMetaClassDeclareReservedUnused(IOService, 39);
	OSMetaClassDeclareReservedUnused(IOService, 40);
	OSMetaClassDeclareReservedUnused(IOService, 41);
	OSMetaClassDeclareReservedUnused(IOService, 42);
	OSMetaClassDeclareReservedUnused(IOService, 43);
	OSMetaClassDeclareReservedUnused(IOService, 44);
	OSMetaClassDeclareReservedUnused(IOService, 45);
	OSMetaClassDeclareReservedUnused(IOService, 46);
	OSMetaClassDeclareReservedUnused(IOService, 47);

public:


	virtual IOOptionBits getState( void ) const;



	bool isInactive( void ) const;





	virtual void registerService( IOOptionBits options = 0 );



	virtual LIBKERN_RETURNS_NOT_RETAINED IOService * probe(  IOService *     provider,
	    SInt32    *     score );



	virtual bool start( IOService * provider );



	virtual void stop( IOService * provider );





	virtual bool open(   IOService *       forClient,
	    IOOptionBits      options = 0,
	    void *        arg = NULL );



	virtual void close(  IOService *       forClient,
	    IOOptionBits      options = 0 );



	virtual bool isOpen( const IOService * forClient = NULL ) const;



	virtual bool handleOpen(    IOService *   forClient,
	    IOOptionBits      options,
	    void *        arg );



	virtual void handleClose(   IOService *       forClient,
	    IOOptionBits      options );



	virtual bool handleIsOpen(  const IOService * forClient ) const;





	virtual bool terminate( IOOptionBits options = 0 );



	virtual bool finalize( IOOptionBits options );


	virtual bool init( OSDictionary * dictionary = NULL ) APPLE_KEXT_OVERRIDE;


	virtual bool init( IORegistryEntry * from,
	    const IORegistryPlane * inPlane ) APPLE_KEXT_OVERRIDE;



	virtual void free( void ) APPLE_KEXT_OVERRIDE;



	virtual bool lockForArbitration( bool isSuccessRequired = true );



	virtual void unlockForArbitration( void );

	static uint32_t isLockedForArbitration(IOService * service);
	void setMachPortHoldDestroy(bool holdDestroy);
	bool machPortHoldDestroy();




	virtual bool terminateClient( IOService * client, IOOptionBits options );





	virtual UInt32 getBusyState( void );



	virtual void adjustBusy( SInt32 delta );



	IOReturn waitQuietWithOptions(uint64_t timeout = UINT64_MAX, IOOptionBits options = 0);

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn waitQuiet(mach_timespec_t * timeout)
	APPLE_KEXT_DEPRECATED;



	IOReturn waitQuiet(uint64_t timeout = UINT64_MAX);





	virtual bool matchPropertyTable( OSDictionary * table,
	    SInt32       * score );

	virtual bool matchPropertyTable( OSDictionary * table );



	virtual LIBKERN_RETURNS_NOT_RETAINED IOService * matchLocation( IOService * client );





	static void publishResource( const OSSymbol * key, OSObject * value = NULL );
	static void publishUserResource( const OSSymbol * key, OSObject * value = NULL );



	static void publishResource( const char * key, OSObject * value = NULL );
	virtual bool addNeededResource( const char * key );





	static OSPtr<IONotifier>  addNotification(
		const OSSymbol * type, OSDictionary * matching,
		IOServiceNotificationHandler handler,
		void * target, void * ref = NULL,
		SInt32 priority = 0 )
	APPLE_KEXT_DEPRECATED;



	static IONotifier * addMatchingNotification(
		const OSSymbol * type, OSDictionary * matching,
		IOServiceMatchingNotificationHandler handler,
		void * target, void * ref = NULL,
		SInt32 priority = 0 );





	static LIBKERN_RETURNS_NOT_RETAINED IOService * waitForService(
		LIBKERN_CONSUMED OSDictionary * matching,
		mach_timespec_t * timeout = NULL);



	static OSPtr<IOService>  waitForMatchingService( OSDictionary * matching,
	    uint64_t timeout = UINT64_MAX);

	static IOService * waitForMatchingServiceWithToken( OSDictionary * matching,
	    uint64_t timeout, IOUserServerCheckInToken * token );



	static OSPtr<OSIterator> getMatchingServices( OSDictionary * matching );



	static OSPtr<IOService>  copyMatchingService( OSDictionary * matching );

public:




	static OSPtr<OSDictionary>  serviceMatching( const char * className,
	    OSDictionary * table = NULL );




	static OSPtr<OSDictionary>  serviceMatching( const OSString * className,
	    OSDictionary * table = NULL );




	static OSPtr<OSDictionary>  nameMatching( const char * name,
	    OSDictionary * table = NULL );




	static OSPtr<OSDictionary>  nameMatching( const OSString* name,
	    OSDictionary * table = NULL );




	static OSPtr<OSDictionary>  resourceMatching( const char * name,
	    OSDictionary * table = NULL );




	static OSPtr<OSDictionary>  resourceMatching( const OSString * name,
	    OSDictionary * table = NULL );





	static OSPtr<OSDictionary>  propertyMatching( const OSSymbol * key, const OSObject * value,
	    OSDictionary * table = NULL );




	static OSDictionary * registryEntryIDMatching( uint64_t entryID,
	    OSDictionary * table = NULL );





	static OSPtr<OSDictionary>  addLocation( OSDictionary * table );





	virtual bool compareProperty(   OSDictionary   * matching,
	    const char     * key );


	virtual bool compareProperty(   OSDictionary   * matching,
	    const OSString * key );



	virtual bool compareProperties( OSDictionary   * matching,
	    OSCollection   * keys );





	virtual bool attach( IOService * provider );



	virtual void detach( IOService * provider );



	virtual IOService * getProvider( void ) const;



	virtual IOWorkLoop * getWorkLoop() const;



	virtual OSPtr<OSIterator> getProviderIterator( void ) const;



	virtual OSPtr<OSIterator> getOpenProviderIterator( void ) const;



	virtual IOService * getClient( void ) const;



	virtual OSPtr<OSIterator> getClientIterator( void ) const;



	virtual OSPtr<OSIterator> getOpenClientIterator( void ) const;



	virtual IOReturn callPlatformFunction( const OSSymbol * functionName,
	    bool waitForFunction,
	    void *param1, void *param2,
	    void *param3, void *param4 );

	virtual IOReturn callPlatformFunction( const char * functionName,
	    bool waitForFunction,
	    void *param1, void *param2,
	    void *param3, void *param4 );






	static IOPlatformExpert * getPlatform( void );



	static class IOPMrootDomain * getPMRootDomain( void );



	static IOService * getServiceRoot( void );



	static IOService * getResourceService( void );

	static IOService * getSystemStateNotificationService(void);





	virtual IOReturn getResources( void );





	virtual IOItemCount getDeviceMemoryCount( void );



	virtual IODeviceMemory * getDeviceMemoryWithIndex( unsigned int index );



	virtual IOMemoryMap * mapDeviceMemoryWithIndex( unsigned int index,
	    IOOptionBits options = 0 );



	virtual OSArray * getDeviceMemory( void );



	virtual void setDeviceMemory( OSArray * array );





	virtual IOReturn registerInterrupt(int source, OSObject *target,
	    IOInterruptAction handler,
	    void *refCon = NULL);




	virtual IOReturn unregisterInterrupt(int source);


	IOReturn addInterruptStatistics(IOInterruptAccountingData * statistics, int source);


	IOReturn removeInterruptStatistics(int source);



	virtual IOReturn getInterruptType(int source, int *interruptType);



	virtual IOReturn enableInterrupt(int source);



	virtual IOReturn disableInterrupt(int source);



	virtual IOReturn causeInterrupt(int source);



	virtual IOReturn requestProbe( IOOptionBits options );





	virtual IOReturn message( UInt32 type, IOService * provider,
	    void * argument = NULL );



	virtual IOReturn messageClient( UInt32 messageType, OSObject * client,
	    void * messageArgument = NULL, vm_size_t argSize = 0 );



	virtual IOReturn messageClients( UInt32 type,
	    void * argument = NULL, vm_size_t argSize = 0 );

	virtual OSPtr<IONotifier> registerInterest( const OSSymbol * typeOfInterest,
	    IOServiceInterestHandler handler,
	    void * target, void * ref = NULL );


	virtual void applyToProviders( IOServiceApplierFunction applier,
	    void * context );

	virtual void applyToClients( IOServiceApplierFunction applier,
	    void * context );


	virtual void applyToInterested( const OSSymbol * typeOfInterest,
	    OSObjectApplierFunction applier,
	    void * context );

	virtual IOReturn acknowledgeNotification( IONotificationRef notification,
	    IOOptionBits response );





	virtual IOReturn newUserClient( task_t owningTask, void * securityID,
	    UInt32 type, OSDictionary * properties,
	    LIBKERN_RETURNS_RETAINED IOUserClient ** handler );

	virtual IOReturn newUserClient( task_t owningTask, void * securityID,
	    UInt32 type,
	    LIBKERN_RETURNS_RETAINED IOUserClient ** handler );

	IOReturn newUserClient( task_t owningTask, void * securityID,
	    UInt32 type, OSDictionary * properties,
	    OSSharedPtr<IOUserClient>& handler );

	IOReturn newUserClient( task_t owningTask, void * securityID,
	    UInt32 type,
	    OSSharedPtr<IOUserClient>& handler );





	virtual const char * stringFromReturn( IOReturn rtn );



	virtual int errnoFromReturn( IOReturn rtn );



	struct IOExclaveProxyState;

	bool
	exclaveStart(IOService * provider, IOExclaveProxyState ** state);

	
	uint64_t
	exclaveEndpoint(IOExclaveProxyState * pRef);

	
	kern_return_t exclaveAsyncNotificationRegister(IOExclaveProxyState * pRef, IOInterruptEventSource *notification, uint32_t *notificationID);


	
	bool
	exclaveRegisterInterrupt(IOExclaveProxyState * pRef, int index, bool noProvider);
	bool
	exclaveRemoveInterrupt(IOExclaveProxyState * pRef, int index);
	bool
	exclaveEnableInterrupt(IOExclaveProxyState * pRef, int index, bool enable);

	
	bool
	exclaveRegisterTimer(IOExclaveProxyState * pRef, uint32_t *timer_id);
	bool
	exclaveRemoveTimer(IOExclaveProxyState * pRef, uint32_t timer_id);
	bool
	exclaveEnableTimer(IOExclaveProxyState * pRef, uint32_t timer_id, bool enable);
	bool
	exclaveTimerCancelTimeout(IOExclaveProxyState * pRef, uint32_t timer_id);
	bool
	exclaveTimerSetTimeout(IOExclaveProxyState * pRef, uint32_t timer_id, uint32_t options, AbsoluteTime interval, AbsoluteTime leeway, kern_return_t *kr);

	
	void
	exclaveInterruptOccurred(IOInterruptEventSource *eventSource, int count);
	void
	exclaveTimerFired(IOTimerEventSource *eventSource);

	kern_return_t exclaveAsyncNotificationSignal(IOExclaveProxyState * pRef, uint32_t notificationID);








	int               _numInterruptSources;
	IOInterruptSource *_interruptSources;


	virtual bool serializeProperties( OSSerialize * s ) const APPLE_KEXT_OVERRIDE;

	IOReturn   requireMaxBusStall(UInt32 ns);
	IOReturn   requireMaxInterruptDelay(uint32_t ns);





public:

	static void initialize( void );
	static void setPlatform( IOPlatformExpert * platform);
	static void setPMRootDomain( class IOPMrootDomain * rootDomain );
	static void publishPMRootDomain( void );
	static IOReturn catalogNewDrivers( OSOrderedSet * newTables );
	uint64_t getAccumulatedBusyTime( void );
	static void updateConsoleUsers(OSArray * consoleUsers, IOMessage systemMessage,
	    bool afterUserspaceReboot = false);
	static void consoleLockTimer(thread_call_param_t p0, thread_call_param_t p1);
	void setTerminateDefer(IOService * provider, bool defer);
	uint64_t getAuthorizationID( void );
	IOReturn setAuthorizationID( uint64_t authorizationID );
	void cpusRunning(void);
	void scheduleFinalize(bool now);
	static void willShutdown();
	static void startDeferredMatches();
	static void iokitDaemonLaunched();
	void resetRematchProperties();
	bool hasUserServer() const;
	static void userSpaceWillReboot();
	static void userSpaceDidReboot();
	kern_return_t CopyProperties_Local(OSDictionary ** properties);

	IOStateNotificationItem * stateNotificationItemCopy(OSString * itemName, OSDictionary * schema);
	kern_return_t stateNotificationListenerAdd(OSArray * items,
	    IOStateNotificationListenerRef * outRef,
	    IOStateNotificationHandler handler);
	kern_return_t stateNotificationListenerRemove(IOStateNotificationListenerRef ref);

private:
	static IOReturn waitMatchIdle( UInt32 ms );
	static OSPtr<IONotifier>  installNotification(
		const OSSymbol * type, OSDictionary * matching,
		IOServiceMatchingNotificationHandler handler,
		void * target, void * ref,
		SInt32 priority,
		LIBKERN_RETURNS_RETAINED OSIterator ** existing);

private:
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	bool checkResources( void );
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	bool checkResource( OSObject * matching );

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void probeCandidates( LIBKERN_CONSUMED OSOrderedSet * matches );
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	bool startCandidate( IOService * candidate );

public:
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOService * getClientWithCategory( const OSSymbol * category )
	APPLE_KEXT_DEPRECATED;



public:
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	bool passiveMatch( OSDictionary * matching, bool changesOK = false);
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void startMatching( IOOptionBits options = 0 );
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void doServiceMatch( IOOptionBits options );
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void doServiceTerminate( IOOptionBits options );

	bool hasParent(IOService * root);
	static void setRootMedia(IOService * root);
	static void publishHiddenMedia(IOService * parent);
	static bool publishHiddenMediaApplier(const OSObject * entry, void * context);
	bool canTerminateForReplacement(IOService * client);
	void unregisterAllInterrupts(void);

private:

	bool matchPassive(OSDictionary * table, uint32_t options);
	bool matchInternal(OSDictionary * table, uint32_t options, unsigned int * did);
	static bool instanceMatch(const OSObject * entry, void * context);
	OSDictionary * _copyPropertiesForMatching(void);

	static OSPtr<OSObject>  copyExistingServices( OSDictionary * matching,
	    IOOptionBits inState, IOOptionBits options = 0 );

	static OSPtr<IONotifier>  setNotification(
		const OSSymbol * type, OSDictionary * matching,
		IOServiceMatchingNotificationHandler handler,
		void * target, void * ref,
		SInt32 priority = 0 );

	static OSPtr<IONotifier>  doInstallNotification(
		const OSSymbol * type, OSDictionary * matching,
		IOServiceMatchingNotificationHandler handler,
		void * target, void * ref,
		SInt32 priority, OSIterator ** existing );

	static bool syncNotificationHandler( void * target, void * ref,
	    IOService * newService, IONotifier * notifier  );

	static void userServerCheckInTokenCancellationHandler(
		IOUserServerCheckInToken * token,
		void * ref);

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void deliverNotification( const OSSymbol * type,
	    IOOptionBits orNewState, IOOptionBits andNewState );

	OSPtr<OSArray>  copyNotifiers(const OSSymbol * type,
	    IOOptionBits orNewState, IOOptionBits andNewState);

	bool invokeNotifiers(OSArray * willSend[]);
	bool invokeNotifier( class _IOServiceNotifier * notify );

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void unregisterAllInterest( void );

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn waitForState( UInt32 mask, UInt32 value,
	    mach_timespec_t * timeout = NULL );

	IOReturn waitForState( UInt32 mask, UInt32 value, uint64_t timeout );

	UInt32 _adjustBusy(SInt32 delta);
	UInt32 _adjustBusy(SInt32 delta, bool unlock);

	bool terminatePhase1( IOOptionBits options = 0 );
	void scheduleTerminatePhase2( IOOptionBits options = 0 );
	void scheduleStop( IOService * provider );

	static void waitToBecomeTerminateThread( void );
	static void __attribute__((__noreturn__)) terminateThread( void * arg, wait_result_t unused );
	static void terminateWorker( IOOptionBits options );
	static void actionWillTerminate( IOService * victim, IOOptionBits options,
	    OSArray * doPhase2List, bool, void * );
	static void actionDidTerminate( IOService * victim, IOOptionBits options,
	    void *, void *, void *);

	static void actionWillStop( IOService * victim, IOOptionBits options,
	    void *, void *, void *);
	static void actionDidStop( IOService * victim, IOOptionBits options,
	    void *, void *, void *);

	static void actionFinalize( IOService * victim, IOOptionBits options,
	    void *, void *, void *);
	static void actionStop( IOService * client, IOService * provider,
	    void *, void *, void *);

	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn resolveInterrupt(IOService *nub, int source);
	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn lookupInterrupt(
		int source, bool resolve,
		LIBKERN_RETURNS_NOT_RETAINED IOInterruptController *
		*interruptController);




public:



	virtual void PMinit( void );



	virtual void PMstop( void );



	virtual void joinPMtree( IOService * driver );



	virtual IOReturn registerPowerDriver(
		IOService *      controllingDriver,
		IOPMPowerState * powerStates,
		unsigned long    numberOfStates );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOPMPowerFlags registerInterestedDriver( IOService * theDriver );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn deRegisterInterestedDriver( IOService * theDriver );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn acknowledgePowerChange( IOService * whichDriver );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn acknowledgeSetPowerState( void );



	virtual IOReturn requestPowerDomainState(
		IOPMPowerFlags desiredState,
		IOPowerConnection * whichChild,
		unsigned long specificationFlags );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn makeUsable( void );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn temporaryPowerClampOn( void );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOReturn changePowerStateTo( unsigned long ordinal );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	IOPMPowerFlags currentCapability( void );



	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	unsigned long currentPowerConsumption( void );



	virtual bool activityTickle(
		unsigned long type,
		unsigned long stateNumber = 0 );



	virtual IOReturn setAggressiveness(
		unsigned long type,
		unsigned long newLevel );



	virtual IOReturn getAggressiveness(
		unsigned long type,
		unsigned long * currentLevel );




	virtual IOReturn addPowerChild( IOService * theChild );



	virtual IOReturn removePowerChild( IOPowerConnection * theChild );




	APPLE_KEXT_COMPATIBILITY_VIRTUAL
	void start_PM_idle_timer( void );




	virtual IOReturn setIdleTimerPeriod( unsigned long period );




	UInt32 getPowerState( void );



	virtual IOReturn setPowerState(
		unsigned long powerStateOrdinal,
		IOService *   whatDevice );




	virtual unsigned long maxCapabilityForDomainState( IOPMPowerFlags domainState );



	virtual unsigned long initialPowerStateForDomainState( IOPMPowerFlags domainState );



	virtual unsigned long powerStateForDomainState( IOPMPowerFlags domainState );



	virtual IOReturn powerStateWillChangeTo(
		IOPMPowerFlags  capabilities,
		unsigned long   stateNumber,
		IOService *     whatDevice );



	virtual IOReturn powerStateDidChangeTo(
		IOPMPowerFlags  capabilities,
		unsigned long   stateNumber,
		IOService *     whatDevice );


	virtual bool askChangeDown( unsigned long );
	virtual bool tellChangeDown( unsigned long );
	virtual void tellNoChangeDown( unsigned long );
	virtual void tellChangeUp( unsigned long );
	virtual IOReturn allowPowerChange( unsigned long refcon );
	virtual IOReturn cancelPowerChange( unsigned long refcon );

protected:

public:
	IOReturn changePowerStateToPriv( unsigned long ordinal );



	IOReturn powerOverrideOnPriv( void );



	IOReturn powerOverrideOffPriv( void );



	virtual void powerChangeDone( unsigned long stateNumber );

public:
	void idleTimerExpired( void );
	void settleTimerExpired( void );
	IOReturn synchronizePowerTree( IOOptionBits options = 0, IOService * notifyRoot = NULL );
	bool assertPMDriverCall( IOPMDriverCallEntry * callEntry, IOOptionBits method, const IOPMinformee * inform = NULL, IOOptionBits options = 0 );
	void deassertPMDriverCall( IOPMDriverCallEntry * callEntry );
	IOReturn changePowerStateWithOverrideTo( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
	IOReturn changePowerStateWithTagToPriv( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
	IOReturn changePowerStateWithTagTo( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
	IOReturn changePowerStateForRootDomain( IOPMPowerStateIndex ordinal );
	IOReturn setIgnoreIdleTimer( bool ignore );
	IOReturn quiescePowerTree( void * target, IOPMCompletionAction action, void * param );
	IOPMPowerStateIndex getPowerStateForClient( const OSSymbol * client );
	static const char * getIOMessageString( uint32_t msg );
	static void setAdvisoryTickleEnable( bool enable );
	void reset_watchdog_timer(IOService *obj, int timeout);
	void reset_watchdog_timer(int timeout = 0);
	void start_watchdog_timer( void );
	void stop_watchdog_timer( void );
	void start_watchdog_timer(uint64_t deadline);
	uint64_t get_watchdog_elapsed_time(void);
	IOReturn registerInterestForNotifier( IONotifier *notify, const OSSymbol * typeOfInterest,
	    IOServiceInterestHandler handler, void * target, void * ref );

	static IOWorkLoop * getIOPMWorkloop( void );
	bool getBlockingDriverCall(thread_t *thread, const void **callMethod);
	void cancelIdlePowerDown(IOService * service);
	void cancelIdlePowerDownSync( void );

protected:
	bool tellClientsWithResponse( int messageType );
	void tellClients( int messageType );
	void PMDebug( uint32_t event, uintptr_t param1, uintptr_t param2 );

private:
	static void allocPMInitLock( void );
	void PMfree( void );
	bool tellChangeDown1( unsigned long );
	bool tellChangeDown2( unsigned long );
	IOReturn startPowerChange( IOPMPowerChangeFlags, IOPMPowerStateIndex, IOPMPowerFlags, IOPowerConnection *, IOPMPowerFlags );
	void setParentInfo( IOPMPowerFlags, IOPowerConnection *, bool );
	IOReturn notifyAll( uint32_t nextMS );
	bool notifyChild( IOPowerConnection * child );
	IOPMPowerStateIndex getPowerStateForDomainFlags( IOPMPowerFlags flags );


	void OurChangeStart( void );
	void OurSyncStart( void );
	void OurChangeTellClientsPowerDown( void );
	void OurChangeTellUserPMPolicyPowerDown( void );
	void OurChangeTellPriorityClientsPowerDown( void );
	void OurChangeTellCapabilityWillChange( void );
	void OurChangeNotifyInterestedDriversWillChange( void );
	void OurChangeSetPowerState( void );
	void OurChangeWaitForPowerSettle( void );
	void OurChangeNotifyInterestedDriversDidChange( void );
	void OurChangeTellCapabilityDidChange( void );
	void OurChangeFinish( void );


	IOReturn ParentChangeStart( void );
	void ParentChangeTellPriorityClientsPowerDown( void );
	void ParentChangeTellCapabilityWillChange( void );
	void ParentChangeNotifyInterestedDriversWillChange( void );
	void ParentChangeSetPowerState( void );
	void ParentChangeWaitForPowerSettle( void );
	void ParentChangeNotifyInterestedDriversDidChange( void );
	void ParentChangeTellCapabilityDidChange( void );
	void ParentChangeAcknowledgePowerChange( void );
	void ParentChangeRootChangeDown( void );

	void all_done( void );
	void start_ack_timer( void );
	void stop_ack_timer( void );
	void start_ack_timer( UInt32 value, UInt32 scale );
	void startSettleTimer( void );
	void start_spindump_timer( const char * delay_type );
	void stop_spindump_timer( void );
	bool checkForDone( void );
	bool responseValid( uint32_t x, int pid );
	void updateClientResponses( void );
	void computeDesiredState( unsigned long tempDesire, bool computeOnly );
	void trackSystemSleepPreventers( IOPMPowerStateIndex, IOPMPowerStateIndex, IOPMPowerChangeFlags );
	void tellSystemCapabilityChange( uint32_t nextMS );
	void restartIdleTimer( void );
	void startDriverCalloutTimer( void );
	void stopDriverCalloutTimer( void );

	static void ack_timer_expired( thread_call_param_t, thread_call_param_t );
	static void watchdog_timer_expired( thread_call_param_t arg0, thread_call_param_t arg1 );
	static void spindump_timer_expired( thread_call_param_t arg0, thread_call_param_t arg1 );
	static IOReturn actionAckTimerExpired(OSObject *, void *, void *, void *, void * );
	static IOReturn actionSpinDumpTimerExpired(OSObject *, void *, void *, void *, void * );

	static IOReturn actionDriverCalloutDone(OSObject *, void *, void *, void *, void * );
	static IOPMRequest * acquirePMRequest( IOService * target, IOOptionBits type, IOPMRequest * active = NULL );
	static void releasePMRequest( IOPMRequest * request );
	static void pmDriverCallout( IOService * from, thread_call_param_t );
	static void pmDriverCalloutTimer( thread_call_param_t, thread_call_param_t );
	static void pmTellAppWithResponse( OSObject * object, void * context );
	static void pmTellClientWithResponse( OSObject * object, void * context );
	static void pmTellCapabilityAppWithResponse( OSObject * object, void * arg );
	static void pmTellCapabilityClientWithResponse( OSObject * object, void * arg );
	static void submitPMRequest(LIBKERN_CONSUMED IOPMRequest * request );
	static void submitPMRequests( IOPMRequest * requests[], IOItemCount count );
	bool ackTimerTick( void );
	void addPowerChild1( IOPMRequest * request );
	void addPowerChild2( IOPMRequest * request );
	void addPowerChild3( IOPMRequest * request );
	void adjustPowerState( IOPMPowerStateIndex clamp = 0 );
	void handlePMstop( IOPMRequest * request );
	void handleRegisterPowerDriver( IOPMRequest * request );
	bool handleAcknowledgePowerChange( IOPMRequest * request );
	bool handleAcknowledgeSetPowerState( IOPMRequest * request );
	bool handleCancelIdlePowerDown( void );
	void handlePowerDomainWillChangeTo( IOPMRequest * request );
	void handlePowerDomainDidChangeTo( IOPMRequest * request );
	void handleRequestPowerState( IOPMRequest * request );
	void handlePowerOverrideChanged( IOPMRequest * request );
	bool _activityTickle( unsigned long type, unsigned long stateNumber );
	void handleDeferredActivityTickle( IOPMRequest * request );
	void handleActivityTickle( IOPMRequest * request );
	void handleInterestChanged( IOPMRequest * request );
	void handleSynchronizePowerTree( IOPMRequest * request );
	void executePMRequest( IOPMRequest * request );
	bool actionPMWorkQueueInvoke( IOPMRequest * request, IOPMWorkQueue * queue );
	bool actionPMWorkQueueRetire( IOPMRequest * request, IOPMWorkQueue * queue );
	bool actionPMRequestQueue( IOPMRequest * request, IOPMRequestQueue * queue );
	bool actionPMReplyQueue( IOPMRequest * request, IOPMRequestQueue * queue );
	bool actionPMCompletionQueue( LIBKERN_CONSUMED IOPMRequest * request, IOPMCompletionQueue * queue );
	bool notifyInterestedDrivers( void );
	void notifyInterestedDriversDone( void );
	bool notifyControllingDriver( void );
	void notifyControllingDriverDone( void );
	void driverSetPowerState( void );
	void driverInformPowerChange( void );
	unsigned long driverMaxCapabilityForDomainState( IOPMPowerFlags domainState );
	unsigned long driverInitialPowerStateForDomainState( IOPMPowerFlags domainState );
	bool isPMBlocked( IOPMRequest * request, int count );
	void notifyChildren( void );
	void notifyChildrenOrdered( void );
	void notifyChildrenDelayed( void );
	void notifyRootDomain( void );
	void notifyRootDomainDone( void );
	void cleanClientResponses( bool logErrors );
	void updatePowerClient( const OSSymbol * client, IOPMPowerStateIndex powerState );
	void removePowerClient( const OSSymbol * client );
	IOReturn requestPowerState( const OSSymbol * client, IOPMPowerStateIndex state, IOPMRequestTag tag = 0 );
	IOReturn requestDomainPower( IOPMPowerStateIndex ourPowerState, IOOptionBits options = 0 );
	IOReturn configurePowerStatesReport( IOReportConfigureAction action, void *result );
	IOReturn updatePowerStatesReport( IOReportConfigureAction action, void *result, void *destination );
	IOReturn configureSimplePowerReport(IOReportConfigureAction action, void *result );
	IOReturn updateSimplePowerReport( IOReportConfigureAction action, void *result, void *destination );
	void waitForPMDriverCall( IOService * target = NULL );

	friend class IOUserServer;
};
class IOServiceCompatibility : public IOService
{
	OSDeclareDefaultStructors(IOServiceCompatibility);
};
class IOPMprot : public OSObject
{
	friend class IOService;

	OSDeclareDefaultStructors(IOPMprot);

public:
	const char *            ourName;
	IOPlatformExpert *      thePlatform;
	unsigned long           theNumberOfPowerStates;
	IOPMPowerState          thePowerStates[IOPMMaxPowerStates];
	IOService *             theControllingDriver;
	unsigned long           aggressiveness;
	unsigned long           current_aggressiveness_values[kMaxType + 1];
	bool                    current_aggressiveness_valid[kMaxType + 1];
	unsigned long           myCurrentState;
};
class IOServiceStateNotificationEventSource : public IOEventSource
{
	OSDeclareDefaultStructors(IOServiceStateNotificationEventSource);

public:
	typedef void (^ActionBlock)();

protected:
	IOService                    * fStateNotification;
	IOStateNotificationListenerRef fListener;
	bool                           fEnable;
	bool                           fArmed;




	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;


	virtual void free() APPLE_KEXT_OVERRIDE;


	virtual bool checkForWork() APPLE_KEXT_OVERRIDE;


	virtual void setWorkLoop(IOWorkLoop *inWorkLoop) APPLE_KEXT_OVERRIDE;

public:

	
	static OSPtr<IOServiceStateNotificationEventSource>
	serviceStateNotificationEventSource(IOService *service,
	    OSArray * items,
	    ActionBlock action);


	virtual void enable() APPLE_KEXT_OVERRIDE;


	virtual void disable() APPLE_KEXT_OVERRIDE;

private:
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 0);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 1);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 2);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 3);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 4);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 5);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 6);
	OSMetaClassDeclareReservedUnused(IOServiceStateNotificationEventSource, 7);
};
class IOSharedDataQueue : public IODataQueue
{
	OSDeclareDefaultStructors(IOSharedDataQueue);

	struct ExpansionData {
		UInt32 queueSize;
	};

	ExpansionData * _reserved;

protected:
	virtual void free() APPLE_KEXT_OVERRIDE;


	UInt32 getQueueSize();


	Boolean setQueueSize(UInt32 size);

public:

	static OSPtr<IOSharedDataQueue> withCapacity(UInt32 size __xnu_data_size);


	static OSPtr<IOSharedDataQueue> withEntries(UInt32 numEntries, UInt32 entrySize __xnu_data_size);


	virtual Boolean initWithCapacity(UInt32 size) APPLE_KEXT_OVERRIDE;


	virtual OSPtr<IOMemoryDescriptor> getMemoryDescriptor() APPLE_KEXT_OVERRIDE;


	virtual IODataQueueEntry * peek();


	virtual Boolean dequeue(void *data, UInt32 *dataSize);


	virtual Boolean enqueue(void *data, UInt32 dataSize) APPLE_KEXT_OVERRIDE;


	__inline__ Boolean
	enqueue_tail(void *data, UInt32 dataSize)
	{
		return IOSharedDataQueue::enqueue(data, dataSize);
	}

};
class IOSubMemoryDescriptor : public IOMemoryDescriptor
{
	OSDeclareDefaultStructors(IOSubMemoryDescriptor);

protected:
	IOMemoryDescriptor * _parent;
	IOByteCount          _start;

	virtual void free() APPLE_KEXT_OVERRIDE;

public:


	static OSPtr<IOSubMemoryDescriptor>       withSubRange(IOMemoryDescriptor *of,
	    IOByteCount offset,
	    IOByteCount length,
	    IOOptionBits options);


	virtual bool initSubRange( IOMemoryDescriptor * parent,
	    IOByteCount offset, IOByteCount length,
	    IODirection withDirection );



	virtual addr64_t getPhysicalSegment( IOByteCount   offset,
	    IOByteCount * length,
	    IOOptionBits  options = 0 ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn prepare(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;

	virtual IOReturn complete(IODirection forDirection = kIODirectionNone) APPLE_KEXT_OVERRIDE;

	virtual IOReturn redirect( task_t safeTask, bool redirect ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn setPurgeable( IOOptionBits newState,
	    IOOptionBits * oldState ) APPLE_KEXT_OVERRIDE;

	IOReturn setOwnership( task_t newOwner,
	    int newLedgerTag,
	    IOOptionBits newLedgerOptions );


	virtual IOMemoryMap *       makeMapping(
		IOMemoryDescriptor *    owner,
		task_t                  intoTask,
		IOVirtualAddress        atAddress,
		IOOptionBits            options,
		IOByteCount             offset,
		IOByteCount             length ) APPLE_KEXT_OVERRIDE;

	virtual uint64_t getPreparationID( void ) APPLE_KEXT_OVERRIDE;



	IOReturn getPageCounts(IOByteCount * residentPageCount,
	    IOByteCount * dirtyPageCount);
};
class IOSyncer : public OSObject
{
	OSDeclareDefaultStructors(IOSyncer);

private:

	IOSimpleLock *guardLock;
	volatile bool threadMustStop;
	IOReturn fResult;
	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void privateSignal();

public:

	static IOSyncer * create(bool twoRetains = true)
	APPLE_KEXT_DEPRECATED;

	virtual bool init(bool twoRetains)
	APPLE_KEXT_DEPRECATED;
	virtual void reinit()
	APPLE_KEXT_DEPRECATED;
	virtual IOReturn wait(bool autoRelease = true)
	APPLE_KEXT_DEPRECATED;
	virtual void signal(IOReturn res = kIOReturnSuccess,
	    bool autoRelease = true)
	APPLE_KEXT_DEPRECATED;
};
class IOTimerEventSource : public IOEventSource
{
	OSDeclareDefaultStructors(IOTimerEventSource);

protected:

	void *calloutEntry;


	AbsoluteTime abstime;


	struct ExpansionData {
		SInt32       calloutGeneration;
		SInt32       calloutGenerationSignaled;
		IOWorkLoop * workLoop;
	};


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;


	static void timeout(void *self);


	virtual void setTimeoutFunc();


	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual void setWorkLoop(IOWorkLoop *workLoop) APPLE_KEXT_OVERRIDE;

public:


	typedef void (*Action)(OSObject *owner, IOTimerEventSource *sender);


	static OSPtr<IOTimerEventSource>
	timerEventSource(OSObject *owner, Action action = NULL);


	static OSPtr<IOTimerEventSource>
	timerEventSource(uint32_t options, OSObject *owner, Action action = NULL);


	__inline__ void invokeAction(IOEventSource::Action action, IOTimerEventSource * ts,
	    OSObject * owner, IOWorkLoop * workLoop);


	virtual bool init(OSObject *owner, Action action = NULL);


	virtual void enable() APPLE_KEXT_OVERRIDE;


	virtual void disable() APPLE_KEXT_OVERRIDE;


	virtual bool checkForWork() APPLE_KEXT_OVERRIDE;


	virtual IOReturn setTimeoutTicks(UInt32 ticks);


	virtual IOReturn setTimeoutMS(UInt32 ms);


	virtual IOReturn setTimeoutUS(UInt32 us);


	virtual IOReturn setTimeout(UInt32 interval,
	    UInt32 scale_factor = kNanosecondScale);



	virtual IOReturn setTimeout(AbsoluteTime interval);


	virtual IOReturn wakeAtTimeTicks(UInt32 ticks);


	virtual IOReturn wakeAtTimeMS(UInt32 ms);


	virtual IOReturn wakeAtTimeUS(UInt32 us);


	virtual IOReturn wakeAtTime(UInt32 abstime,
	    UInt32 scale_factor = kNanosecondScale);



	virtual IOReturn wakeAtTime(AbsoluteTime abstime);


	virtual void cancelTimeout();


	virtual bool init(uint32_t options, OSObject *inOwner, Action inAction);


	virtual IOReturn setTimeout(uint32_t options, AbsoluteTime interval, AbsoluteTime leeway);


	virtual IOReturn wakeAtTime(uint32_t options, AbsoluteTime abstime, AbsoluteTime leeway);

private:
	static void timeoutAndRelease(void *self, void *c);
	static void timeoutSignaled(void *self, void *c);

private:
	OSMetaClassDeclareReservedUsedX86(IOTimerEventSource, 0);
	OSMetaClassDeclareReservedUsedX86(IOTimerEventSource, 1);
	OSMetaClassDeclareReservedUsedX86(IOTimerEventSource, 2);
	OSMetaClassDeclareReservedUnused(IOTimerEventSource, 3);
	OSMetaClassDeclareReservedUnused(IOTimerEventSource, 4);
	OSMetaClassDeclareReservedUnused(IOTimerEventSource, 5);
	OSMetaClassDeclareReservedUnused(IOTimerEventSource, 6);
	OSMetaClassDeclareReservedUnused(IOTimerEventSource, 7);
};
static inline void
IOTimeStampStartConstant(unsigned int csc,
    uintptr_t a = 0, uintptr_t b = 0,
    uintptr_t c = 0, uintptr_t d = 0)
{
	(void)csc;
	(void)a;
	(void)b;
	(void)c;
	(void)d;
	KERNEL_DEBUG_CONSTANT(((uint32_t)csc) | DBG_FUNC_START, a, b, c, d, 0);
}

static inline void
IOTimeStampEndConstant(uintptr_t csc,
    uintptr_t a = 0, uintptr_t b = 0,
    uintptr_t c = 0, uintptr_t d = 0)
{
	(void)csc;
	(void)a;
	(void)b;
	(void)c;
	(void)d;
	KERNEL_DEBUG_CONSTANT(((uint32_t)csc) | DBG_FUNC_END, a, b, c, d, 0);
}

static inline void
IOTimeStampConstant(uintptr_t csc,
    uintptr_t a = 0, uintptr_t b = 0,
    uintptr_t c = 0, uintptr_t d = 0)
{
	(void)csc;
	(void)a;
	(void)b;
	(void)c;
	(void)d;
	KERNEL_DEBUG_CONSTANT(((uint32_t)csc) | DBG_FUNC_NONE, a, b, c, d, 0);
}

static inline void
IOTimeStampConstantFiltered(uintptr_t csc,
    uintptr_t a = 0, uintptr_t b = 0,
    uintptr_t c = 0, uintptr_t d = 0)
{
	(void)csc;
	(void)a;
	(void)b;
	(void)c;
	(void)d;
	KERNEL_DEBUG_CONSTANT_FILTERED(((uint32_t)csc) | DBG_FUNC_NONE, a, b, c, d, 0);
}


class IOTimeStampIntervalConstantFiltered
{
public:
	IOTimeStampIntervalConstantFiltered(unsigned int csc,
	    uintptr_t arg1 = 0, uintptr_t arg2 = 0,
	    uintptr_t arg3 = 0, uintptr_t arg4 = 0)
		: _csc(csc), _endArg1(0), _endArg2(0), _endArg3(0), _endArg4(0)
	{
		(void)csc;
		(void)arg1;
		(void)arg2;
		(void)arg3;
		(void)arg4;
		KDBG_FILTERED(((uint32_t)csc) | DBG_FUNC_START, arg1, arg2, arg3, arg4);
	}
	
	void
	setEndCodes(uintptr_t arg1 = 0, uintptr_t arg2 = 0,
	    uintptr_t arg3 = 0, uintptr_t arg4 = 0)
	{
		_endArg1 = arg1;
		_endArg2 = arg2;
		_endArg3 = arg3;
		_endArg4 = arg4;
	}
	void
	setEndArg1(uintptr_t arg)
	{
		_endArg1  = arg;
	}
	void
	setEndArg2(uintptr_t arg)
	{
		_endArg2  = arg;
	}
	void
	setEndArg3(uintptr_t arg)
	{
		_endArg3  = arg;
	}
	void
	setEndArg4(uintptr_t arg)
	{
		_endArg4  = arg;
	}
	~IOTimeStampIntervalConstantFiltered()
	{
		KDBG_FILTERED(((uint32_t)_csc) | DBG_FUNC_END, _endArg1, _endArg2, _endArg3, _endArg4);
	}
private:
	unsigned int _csc;
	
	uintptr_t _endArg1, _endArg2, _endArg3, _endArg4;
};
class IOUserClient : public IOService
{
	OSDeclareAbstractStructorsWithDispatch(IOUserClient);

public:

	struct ExpansionData {
		IOUCFilterPolicy * filterPolicies;
	};


	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData * reserved;
	APPLE_KEXT_WSHADOW_POP;

	bool reserve();


public:
	UInt8        __opaque_start[0];

	OSSet * mappings;
	UInt8   sharedInstance;
	UInt8   closed;
	UInt8   __ipcFinal;
	UInt8   messageAppSuspended:1,
	    uc2022:1,
	    defaultLocking:1,
	    defaultLockingSingleThreadExternalMethod:1,
	    defaultLockingSetProperties:1,
	    opened:1,
	    __reservedA:2;
	volatile SInt32 __ipc;
	queue_head_t owners;
	IORWLock     lock;
	IOLock       filterLock;
	void        *__reserved[1];

	UInt8        __opaque_end[0];

public:
	MIG_SERVER_ROUTINE virtual IOReturn
	externalMethod(uint32_t selector, IOExternalMethodArguments * arguments,
	    IOExternalMethodDispatch *dispatch = NULL,
	    OSObject *target = NULL, void *reference = NULL);

	MIG_SERVER_ROUTINE virtual IOReturn registerNotificationPort(
		mach_port_t port, UInt32 type, io_user_reference_t refCon);

private:
	OSMetaClassDeclareReservedUnused(IOUserClient, 0);
	OSMetaClassDeclareReservedUnused(IOUserClient, 1);
	OSMetaClassDeclareReservedUnused(IOUserClient, 2);
	OSMetaClassDeclareReservedUnused(IOUserClient, 3);
	OSMetaClassDeclareReservedUnused(IOUserClient, 4);
	OSMetaClassDeclareReservedUnused(IOUserClient, 5);
	OSMetaClassDeclareReservedUnused(IOUserClient, 6);
	OSMetaClassDeclareReservedUnused(IOUserClient, 7);
	OSMetaClassDeclareReservedUnused(IOUserClient, 8);
	OSMetaClassDeclareReservedUnused(IOUserClient, 9);
	OSMetaClassDeclareReservedUnused(IOUserClient, 10);
	OSMetaClassDeclareReservedUnused(IOUserClient, 11);
	OSMetaClassDeclareReservedUnused(IOUserClient, 12);
	OSMetaClassDeclareReservedUnused(IOUserClient, 13);
	OSMetaClassDeclareReservedUnused(IOUserClient, 14);
	OSMetaClassDeclareReservedUnused(IOUserClient, 15);



public:
	static void initialize( void );
	static void destroyUserReferences( OSObject * obj );
	static bool finalizeUserReferences( OSObject * obj );
	void ipcEnter(int locking);
	void ipcExit(int locking);
	OSPtr<IOMemoryMap>  mapClientMemory64( IOOptionBits type,
	    task_t task,
	    IOOptionBits mapFlags = kIOMapAnywhere,
	    mach_vm_address_t atAddress = 0 );
	IOReturn registerOwner(task_t task);
	void     noMoreSenders(void);
	io_filter_policy_t filterForTask(task_t task, io_filter_policy_t addFilterPolicy);
	MIG_SERVER_ROUTINE IOReturn
	callExternalMethod(uint32_t selector, IOExternalMethodArguments * arguments);


public:
	static IOReturn registerFilterCallbacks(const struct io_filter_callbacks *callbacks, size_t size);

protected:
	static IOReturn sendAsyncResult(OSAsyncReference reference,
	    IOReturn result, void *args[], UInt32 numArgs);
	static void setAsyncReference(OSAsyncReference asyncRef,
	    mach_port_t wakePort,
	    void *callback, void *refcon);

	static IOReturn sendAsyncResult64(OSAsyncReference64 reference,
	    IOReturn result, io_user_reference_t args[], UInt32 numArgs);


	static IOReturn sendAsyncResult64WithOptions(OSAsyncReference64 reference,
	    IOReturn result, io_user_reference_t args[], UInt32 numArgs,
	    IOOptionBits options);

	static void setAsyncReference64(OSAsyncReference64 asyncRef,
	    mach_port_t wakePort,
	    mach_vm_address_t callback, io_user_reference_t refcon);

	static void setAsyncReference64(OSAsyncReference64 asyncRef,
	    mach_port_t wakePort,
	    mach_vm_address_t callback, io_user_reference_t refcon,
	    task_t task);

public:

	static IOReturn clientHasAuthorization( task_t task,
	    IOService * service );

	static IOReturn clientHasPrivilege( void * securityToken,
	    const char * privilegeName );

	static OSPtr<OSObject>  copyClientEntitlement(task_t task, const char *entitlement);
	static OSPtr<OSObject>  copyClientEntitlementVnode(struct vnode *vnode, off_t offset, const char *entitlement);

	static OSPtr<OSDictionary>  copyClientEntitlements(task_t task);
	static OSPtr<OSDictionary>  copyClientEntitlementsVnode(struct vnode *vnode, off_t offset);


	static IOReturn releaseAsyncReference64(OSAsyncReference64 reference);

	static IOReturn releaseNotificationPort(mach_port_t port);

	virtual bool init() APPLE_KEXT_OVERRIDE;
	virtual bool init( OSDictionary * dictionary ) APPLE_KEXT_OVERRIDE;

	virtual bool initWithTask(
		task_t owningTask, void * securityToken, UInt32 type,
		OSDictionary * properties);

	virtual bool initWithTask(
		task_t owningTask, void * securityToken, UInt32 type);

	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual IOReturn clientClose( void );
	virtual IOReturn clientDied( void );

	virtual IOService * getService( void );

	MIG_SERVER_ROUTINE virtual IOReturn registerNotificationPort(
		mach_port_t port, UInt32 type, UInt32 refCon );

	MIG_SERVER_ROUTINE virtual IOReturn getNotificationSemaphore( UInt32 notification_type,
	    semaphore_t * semaphore );

	virtual IOReturn connectClient( IOUserClient * client );


	virtual IOReturn clientMemoryForType( UInt32 type,
	    IOOptionBits * options,
	    IOMemoryDescriptor ** memory );

	IOReturn clientMemoryForType( UInt32 type,
	    IOOptionBits * options,
	    OSSharedPtr<IOMemoryDescriptor>& memory );


	static IOReturn _sendAsyncResult64(OSAsyncReference64 reference,
	    IOReturn result, io_user_reference_t args[], UInt32 numArgs, IOOptionBits options);
public:


	OSPtr<IOMemoryMap>  removeMappingForDescriptor(IOMemoryDescriptor * memory);


	virtual IOReturn exportObjectToClient(task_t task,
	    LIBKERN_CONSUMED OSObject *obj, io_object_t *clientObj);



	static IOReturn copyPortNameForObjectInTask(task_t task, OSObject *object,
	    mach_port_name_t * port_name);


	static IOReturn copyObjectForPortNameInTask(task_t task, mach_port_name_t port_name,
	    OSObject **object);

	static IOReturn copyObjectForPortNameInTask(task_t task, mach_port_name_t port_name,
	    OSSharedPtr<OSObject>& object);


	static IOReturn adjustPortNameReferencesInTask(task_t task, mach_port_name_t port_name, mach_port_delta_t delta);




	virtual IOExternalMethod *
	getExternalMethodForIndex( UInt32 index )
	APPLE_KEXT_DEPRECATED;
	virtual IOExternalAsyncMethod *
	getExternalAsyncMethodForIndex( UInt32 index )
	APPLE_KEXT_DEPRECATED;


	virtual IOExternalMethod *
	getTargetAndMethodForIndex(
		LIBKERN_RETURNS_NOT_RETAINED IOService ** targetP, UInt32 index );
	virtual IOExternalAsyncMethod *
	getAsyncTargetAndMethodForIndex(
		LIBKERN_RETURNS_NOT_RETAINED IOService ** targetP, UInt32 index );
	IOExternalMethod *
	getTargetAndMethodForIndex(
		OSSharedPtr<IOService>& targetP, UInt32 index );
	IOExternalAsyncMethod *
	getAsyncTargetAndMethodForIndex(
		OSSharedPtr<IOService>& targetP, UInt32 index );


	virtual IOExternalTrap *
	getExternalTrapForIndex( UInt32 index )
	APPLE_KEXT_DEPRECATED;

	virtual IOExternalTrap *
	getTargetAndTrapForIndex(
		LIBKERN_RETURNS_NOT_RETAINED IOService **targetP, UInt32 index );
};
class IOUserClient2022 : public IOUserClient
{
	OSDeclareDefaultStructors(IOUserClient2022);

private:
	MIG_SERVER_ROUTINE virtual IOReturn
	externalMethod(uint32_t selector, IOExternalMethodArguments * arguments,
	    IOExternalMethodDispatch *dispatch = NULL,
	    OSObject *target = NULL, void *reference = NULL) APPLE_KEXT_OVERRIDE;

protected:
	IOReturn
	dispatchExternalMethod(uint32_t selector, IOExternalMethodArgumentsOpaque * arguments,
	    const IOExternalMethodDispatch2022 dispatchArray[], size_t dispatchArrayCount,
	    OSObject * target, void * reference);

public:

	MIG_SERVER_ROUTINE virtual IOReturn
	externalMethod(uint32_t selector, IOExternalMethodArgumentsOpaque * arguments) = 0;


	OSMetaClassDeclareReservedUnused(IOUserClient2022, 0);
	OSMetaClassDeclareReservedUnused(IOUserClient2022, 1);
	OSMetaClassDeclareReservedUnused(IOUserClient2022, 2);
	OSMetaClassDeclareReservedUnused(IOUserClient2022, 3);
};
class IOUserIterator : public OSIterator
{
	OSDeclareDefaultStructors(IOUserIterator);
public:
	OSObject    *       userIteratorObject;
	IOLock              lock;

	static IOUserIterator * withIterator(LIBKERN_CONSUMED OSIterator * iter);
	virtual bool init( void ) APPLE_KEXT_OVERRIDE;
	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual void reset() APPLE_KEXT_OVERRIDE;
	virtual bool isValid() APPLE_KEXT_OVERRIDE;
	virtual OSObject * getNextObject() APPLE_KEXT_OVERRIDE;
	virtual OSObject * copyNextObject();
};
class IOUserNotification : public IOUserIterator
{
	OSDeclareDefaultStructors(IOUserNotification);

public:

	virtual void free() APPLE_KEXT_OVERRIDE;

	virtual void setNotification( IONotifier * obj );

	virtual void reset() APPLE_KEXT_OVERRIDE;
	virtual bool isValid() APPLE_KEXT_OVERRIDE;
};
namespace IOServicePH
{
void serverAdd(IOUserServer * server);
void serverRemove(IOUserServer * server);
void serverAck(IOUserServer * server);
bool serverSlept(void);
void systemHalt(int howto);
bool checkPMReady(void);
};
class IOUserServer : public IOUserClient2022
{
	OSDeclareDefaultStructorsWithDispatch(IOUserServer);

	IOLock       *        fLock;
	IOSimpleLock *        fInterruptLock;
	OSDictionary  *       fEntitlements;
	OSDictionary  *       fClasses;
	IODispatchQueue     * fRootQueue;
	OSArray             * fServices;

	uint8_t               fRootNotifier;
	uint8_t               fSystemPowerAck;
	uint8_t               fSystemOff;
	IOUserServerCheckInToken * fCheckInToken;
	OSDextStatistics    * fStatistics;
	bool                  fPlatformDriver;
	OSString            * fTeamIdentifier;
	unsigned int          fCSValidationCategory;
	IOWorkLoop          * fWorkLoop;
public:
	kern_allocation_name_t fAllocationName;
	task_t                 fOwningTask;
	os_reason_t            fTaskCrashReason;

public:

	
	static  IOUserServer * launchUserServer(OSString * bundleID, const OSSymbol * serverName, OSNumber * serverTag, bool reuseIfExists, IOUserServerCheckInToken ** token, OSData *serverDUI);
	static  IOUserClient * withTask(task_t owningTask);
	virtual IOReturn       clientClose(void) APPLE_KEXT_OVERRIDE;
	virtual bool           finalize(IOOptionBits options) APPLE_KEXT_OVERRIDE;
	virtual void           stop(IOService * provider) APPLE_KEXT_OVERRIDE;
	virtual void           free() APPLE_KEXT_OVERRIDE;
	virtual IOWorkLoop   * getWorkLoop() const APPLE_KEXT_OVERRIDE;

	virtual IOReturn       setProperties(OSObject * properties) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       externalMethod(uint32_t selector, IOExternalMethodArgumentsOpaque * args) APPLE_KEXT_OVERRIDE;
	static IOReturn        externalMethodStart(OSObject * target, void * reference, IOExternalMethodArguments * arguments);
	static IOReturn        externalMethodRegisterClass(OSObject * target, void * reference, IOExternalMethodArguments * arguments);

	virtual IOExternalTrap * getTargetAndTrapForIndex(IOService ** targetP, UInt32 index) APPLE_KEXT_OVERRIDE;

	IOReturn               serviceAttach(IOService * service, IOService * provider);
	IOReturn               serviceStop(IOService * service, IOService * provider);
	void                   serviceFree(IOService * service);
	IOReturn               serviceStarted(IOService * service, IOService * provider, bool result);
	static void            serviceWillTerminate(IOService * client, IOService * provider, IOOptionBits options);
	static void            serviceDidTerminate(IOService * client, IOService * provider, IOOptionBits options, bool * defer);
	static void            serviceDidStop(IOService * client, IOService * provider);
	IOReturn               serviceOpen(IOService * provider, IOService * client);
	IOReturn               serviceClose(IOService * provider, IOService * client);
	IOReturn               serviceJoinPMTree(IOService * service);
	IOReturn               serviceSetPowerState(IOService * controllingDriver, IOService * service, IOPMPowerFlags flags, IOPMPowerStateIndex powerState);
	IOReturn               serviceNewUserClient(IOService * service, task_t owningTask, void * securityID,
	    uint32_t type, OSDictionary * properties, IOUserClient ** handler);
	IOReturn               serviceNewUserClient(IOService * service, task_t owningTask, void * securityID,
	    uint32_t type, OSDictionary * properties, OSSharedPtr<IOUserClient>& handler);
	IOReturn               exit(const char * reason);
	IOReturn               kill(const char * reason);

	bool                   serviceMatchesCheckInToken(IOUserServerCheckInToken *token);
	bool                   checkEntitlements(IOService * provider, IOService * dext);
	bool                   checkEntitlements(LIBKERN_CONSUMED OSObject * prop,
	    IOService * provider, IOService * dext);
	static bool            checkEntitlements(OSDictionary * entitlements, LIBKERN_CONSUMED OSObject * prop,
	    IOService * provider, IOService * dext);

	void                   setTaskLoadTag(OSKext *kext);
	void                   setDriverKitUUID(OSKext *kext);
	void                   setDriverKitStatistics(OSKext *kext);
	IOReturn               setCheckInToken(IOUserServerCheckInToken *token);
	void                   systemPower(bool powerOff, bool hibernate);
	void                               systemHalt(int howto);
	static void            powerSourceChanged(bool acAttached);
	bool                   checkPMReady();

	IOReturn                                setPowerState(unsigned long state, IOService * service) APPLE_KEXT_OVERRIDE;
	IOReturn                                powerStateWillChangeTo(IOPMPowerFlags flags, unsigned long state, IOService * service) APPLE_KEXT_OVERRIDE;
	IOReturn                                powerStateDidChangeTo(IOPMPowerFlags flags, unsigned long state, IOService * service) APPLE_KEXT_OVERRIDE;

	IOPStrings *           copyInStringArray(const char * string, uint32_t userSize);
	uint32_t               stringArrayIndex(IOPStrings * array, const char * look);
	IOReturn               registerClass(OSClassDescription * desc, uint32_t size, OSUserMetaClass ** cls);
	IOReturn               registerClass(OSClassDescription * desc, uint32_t size, OSSharedPtr<OSUserMetaClass>& cls);
	IOReturn               setRootQueue(IODispatchQueue * queue);

	OSObjectUserVars     * varsForObject(OSObject * obj);
	LIBKERN_RETURNS_NOT_RETAINED IODispatchQueue      * queueForObject(OSObject * obj, uint64_t msgid);

	static ipc_port_t      copySendRightForObject(OSObject * object, natural_t  type);
	static OSObject      * copyObjectForSendRight(ipc_port_t port, natural_t  type);

	IOReturn               copyOutObjects(IORPCMessageMach * mach, IORPCMessage * message,
	    size_t size, bool consume);
	IOReturn               copyInObjects(IORPCMessageMach * mach, IORPCMessage * message,
	    size_t size, bool copyObjects, bool consumePorts);

	IOReturn               consumeObjects(IORPCMessageMach *mach, IORPCMessage * message, size_t messageSize);

	IOReturn               objectInstantiate(OSObject * obj, IORPC rpc, IORPCMessage * message);
	IOReturn               kernelDispatch(OSObject * obj, IORPC rpc);
	static OSObject      * target(OSAction * action, IORPCMessage * message);

	IOReturn               rpc(IORPC rpc);
	IOReturn               server(ipc_kmsg_t requestkmsg, IORPCMessage * message, ipc_kmsg_t * preply);
	kern_return_t          waitInterruptTrap(void * p1, void * p2, void * p3, void * p4, void * p5, void * p6);
	static bool            shouldLeakObjects();
	static void            beginLeakingObjects();
	bool                   isPlatformDriver();
	int                    getCSValidationCategory();
};
class _IOUserServerCheckInCancellationHandler : public OSObject {
	OSDeclareDefaultStructors(_IOUserServerCheckInCancellationHandler);
public:
	static _IOUserServerCheckInCancellationHandler *
	withHandler(IOUserServerCheckInCancellationHandler handler, void * args);

	void call(IOUserServerCheckInToken * token);
private:
	IOUserServerCheckInCancellationHandler fHandler;
	void                                 * fHandlerArgs;
};
class IOUserServerCheckInToken : public OSObject
{
	enum State {
		kIOUserServerCheckInPending,
		kIOUserServerCheckInCanceled,
		kIOUserServerCheckInComplete,
	};

	OSDeclareDefaultStructors(IOUserServerCheckInToken);
public:
	virtual void free() APPLE_KEXT_OVERRIDE;

	
	static void cancelAll();

	
	_IOUserServerCheckInCancellationHandler * setCancellationHandler(IOUserServerCheckInCancellationHandler handler, void *handlerArgs);

	
	void removeCancellationHandler(_IOUserServerCheckInCancellationHandler * handler);

	
	void cancel();

	
	IOReturn complete();

	const OSSymbol * copyServerName() const;
	OSNumber * copyServerTag() const;

private:
	static IOUserServerCheckInToken * findExistingToken(const OSSymbol * serverName);
	bool init(const OSSymbol * userServerName, OSNumber * serverTag, OSKext *driverKext, OSData *serverDUI);
	bool dextTerminate(void);

	friend class IOUserServer;



private:
	IOUserServerCheckInToken::State          fState;
	size_t                                   fPendingCount;
	const OSSymbol                         * fServerName;
	const OSSymbol                         * fExecutableName;
	OSNumber                               * fServerTag;
	OSSet                                  * fHandlers;
	OSString                               * fKextBundleID;
	bool                                     fNeedDextDec;
};
extern "C" kern_return_t
IOUserServerUEXTTrap(OSObject * object, void * p1, void * p2, void * p3, void * p4, void * p5, void * p6);
extern "C" void
IOUserServerRecordExitReason(task_t task, os_reason_t reason);
class IOUserUserClient : public IOUserClient
{
	OSDeclareDefaultStructors(IOUserUserClient);
public:
	task_t          fTask;
	OSDictionary  * fWorkGroups;
	OSDictionary  * fEventLinks;
	IOLock        * fLock;

	IOReturn                   setTask(task_t task);
	IOReturn                   eventlinkConfigurationTrap(void * p1, void * p2, void * p3, void * p4, void * p5, void * p6);
	IOReturn                   workgroupConfigurationTrap(void * p1, void * p2, void * p3, void * p4, void * p5, void * p6);

	virtual bool           init( OSDictionary * dictionary ) APPLE_KEXT_OVERRIDE;
	virtual void           free() APPLE_KEXT_OVERRIDE;
	virtual void           stop(IOService * provider) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       clientClose(void) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       setProperties(OSObject * properties) APPLE_KEXT_OVERRIDE;
	virtual IOReturn       externalMethod(uint32_t selector, IOExternalMethodArguments * args,
	    IOExternalMethodDispatch * dispatch, OSObject * target, void * reference) APPLE_KEXT_OVERRIDE;
	virtual IOReturn           clientMemoryForType(UInt32 type,
	    IOOptionBits * options,
	    IOMemoryDescriptor ** memory) APPLE_KEXT_OVERRIDE;
	virtual IOExternalTrap * getTargetAndTrapForIndex( IOService **targetP, UInt32 index ) APPLE_KEXT_OVERRIDE;
};
class IOWorkLoop : public OSObject
{
	OSDeclareDefaultStructors(IOWorkLoop);

public:

	typedef IOReturn (*Action)(OSObject *target,
	    void *arg0, void *arg1,
	    void *arg2, void *arg3);


	enum {
		kPreciousStack  = 0x00000001,
		kTimeLockPanics = 0x00000002,
	};

private:

	static void threadMainContinuation(IOWorkLoop *self);


	bool eventSourcePerformsWork(IOEventSource *inEventSource);


	static void releaseEventChain(LIBKERN_CONSUMED IOEventSource *eventChain);

protected:


	typedef enum { mAddEvent, mRemoveEvent } maintCommandEnum;


	IORecursiveLock *gateLock;


	IOEventSource *eventChain;


	IOCommandGate *controlG;


	IOSimpleLock *workToDoLock;


	IOThread workThread;


	volatile bool workToDo;


	bool loopRestart;


	struct ExpansionData {
		IOOptionBits options;
		IOEventSource *passiveEventChain;
		uint64_t lockInterval;
		uint64_t lockTime;
	};


	ExpansionData *reserved;


	virtual IOReturn _maintRequest(void *command, void *data, void *, void *);


	virtual void free() APPLE_KEXT_OVERRIDE;


	virtual void threadMain();

public:


	static OSPtr<IOWorkLoop> workLoop();


	static OSPtr<IOWorkLoop> workLoopWithOptions(IOOptionBits options);


	virtual bool init() APPLE_KEXT_OVERRIDE;


	virtual IOThread getThread() const;


	virtual bool onThread() const;


	virtual bool inGate() const;


	virtual IOReturn addEventSource(IOEventSource *newEvent);


	virtual IOReturn removeEventSource(IOEventSource *toRemove);


	virtual void enableAllEventSources() const;


	virtual void disableAllEventSources() const;


	virtual void enableAllInterrupts() const;


	virtual void disableAllInterrupts() const;


protected:

	friend class IOEventSource;
	friend class IOTimerEventSource;
	friend class IOCommandGate;
	virtual void signalWorkAvailable();
	virtual void openGate();
	virtual void closeGate();
	virtual bool tryCloseGate();
	virtual int  sleepGate(void *event, UInt32 interuptibleType);
	virtual void wakeupGate(void *event, bool oneThread);

public:



	virtual IOReturn runAction(Action action, OSObject *target,
	    void *arg0 = NULL, void *arg1 = NULL,
	    void *arg2 = NULL, void *arg3 = NULL);



	virtual bool runEventSources();


	void setMaximumLockTime(uint64_t interval, uint32_t options);

protected:

	virtual int sleepGate(void *event, AbsoluteTime deadline, UInt32 interuptibleType);

	void lockTime(void);

protected:
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 0);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 1);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 2);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 3);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 4);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 5);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 6);
	OSMetaClassDeclareReservedUnused(IOWorkLoop, 7);
};
#pragma pack(4)

#pragma pack()





typedef struct OSNotificationHeader OSNotificationHeader;
class PassthruInterruptController : public IOInterruptController
{
	OSDeclareDefaultStructors(PassthruInterruptController);

public:
	virtual bool     init(void) APPLE_KEXT_OVERRIDE;

	virtual void     *waitForChildController(void);

	virtual void     setCPUInterruptProperties(IOService *service) APPLE_KEXT_OVERRIDE;

	virtual IOReturn registerInterrupt(IOService *nub, int source,
	    void *target,
	    IOInterruptHandler handler,
	    void *refCon) APPLE_KEXT_OVERRIDE;

	virtual IOReturn getInterruptType(IOService *nub, int source,
	    int *interruptType) APPLE_KEXT_OVERRIDE;

	virtual IOReturn enableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;
	virtual IOReturn disableInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;
	virtual IOReturn causeInterrupt(IOService *nub, int source) APPLE_KEXT_OVERRIDE;

	virtual IOReturn handleInterrupt(void *refCon, IOService *nub,
	    int source) APPLE_KEXT_OVERRIDE;

	virtual void externalInterrupt(void);

protected:
	IOInterruptHandler child_handler;
	void               *child_target;
	void               *child_refCon;
	IOService          *child_nub;
	semaphore_t        child_sentinel;
};
__BEGIN_DECLS




extern void     _doprnt( const char *format, va_list *arg,
    void (*lputc)(char), int radix );
uint32_t
hibernate_sum_page(uint8_t *buf, uint32_t ppnum);
void IOLibInit(void);
kern_return_t IOIteratePageableMaps(vm_size_t size,
    IOIteratePageableMapsCallback callback, void * ref);
vm_map_t IOPageableMapForAddress(uintptr_t address);
kern_return_t
IOMemoryDescriptorMapAlloc(vm_map_t map, void * ref);
mach_vm_address_t
IOKernelAllocateWithPhysicalRestrict(
	kalloc_heap_t       kheap,
	mach_vm_size_t      size,
	mach_vm_address_t   maxPhys,
	mach_vm_size_t      alignment,
	bool                contiguous);
void
IOKernelFreePhysical(
	kalloc_heap_t       kheap,
	mach_vm_address_t   address,
	mach_vm_size_t      size);
extern void bcopy_phys(addr64_t from, addr64_t to, vm_size_t size);
extern void bcopy_phys_with_options(addr64_t from, addr64_t to, vm_size_t nbytes, int options);
extern "C" void IOKitInitializeTime( void );
extern void IOMachPortInitialize(void);
extern "C" OSString * IOCopyLogNameForPID(int pid);
extern "C" void IOKitKernelLogBuffer(const char * title, const void * buffer, size_t size,
    void (*output)(const char *format, ...));
void IOScreenLockTimeUpdate(clock_sec_t secs);
void     IOCPUInitialize(void);
IOReturn IOInstallServicePlatformActions(IOService * service);
IOReturn IOInstallServiceSleepPlatformActions(IOService * service);
IOReturn IORemoveServicePlatformActions(IOService * service);
void     IOCPUSleepKernel(void);
void     IOPlatformActionsInitialize(void);
class IOSystemStateNotification : public IOService
{
	OSDeclareDefaultStructors(IOSystemStateNotification);
public:
	static IOService * initialize(void);
	virtual IOReturn setProperties( OSObject * properties) APPLE_KEXT_OVERRIDE;
	virtual bool serializeProperties(OSSerialize * serialize) const APPLE_KEXT_OVERRIDE;
};
class IOPMPowerStateQueue : public IOEventSource
{
	OSDeclareDefaultStructors(IOPMPowerStateQueue);

private:
	struct PowerEventEntry {
		queue_chain_t   chain;
		uint32_t        eventType;
		void *          arg0;
		uint64_t        arg1;
	};

	queue_head_t    queueHead;
	IOLock *        queueLock;

protected:
	virtual bool checkForWork( void ) APPLE_KEXT_OVERRIDE;
	virtual bool init( OSObject * owner, Action action ) APPLE_KEXT_OVERRIDE;

public:
	static IOPMPowerStateQueue * PMPowerStateQueue( OSObject * owner, Action action );

	bool submitPowerEvent( uint32_t eventType, void * arg0 = NULL, uint64_t arg1 = 0 );
};
#pragma pack(4)


typedef enum{
	vUnknown            = 0,        
	vBool               = 1,        
	vInt                = 2,        
	vUInt               = 3,        
	vChars              = 4,        
	vInvalid            = -1        
} pmioctlVarType_t;
class IOServicePM : public OSObject
{
	friend class IOService;
	friend class IOPMWorkQueue;

	OSDeclareDefaultStructors( IOServicePM );

private:

	queue_chain_t           WorkChain;


	queue_head_t            RequestHead;


	IOService *             Owner;


	IOPMinformeeList *      InterestedDrivers;


	IOReturn                DriverTimer;


	uint32_t                MachineState;

	thread_call_t           AckTimer;
	thread_call_t           SettleTimer;
	thread_call_t           IdleTimer;
	thread_call_t           WatchdogTimer;
	thread_call_t           SpinDumpTimer;

	IOLock  *               WatchdogLock;
	OSArray *               BlockedArray;
	uint64_t                PendingResponseDeadline;
	uint64_t                WatchdogDeadline;
	uint64_t                WatchdogStart;


	uint32_t                SettleTimeUS;
	IOPMPowerStateIndex     IdleTimerGeneration;


	IOPMPowerChangeFlags    HeadNoteChangeFlags;


	IOPMPowerStateIndex     HeadNotePowerState;


	IOPMPSEntry *           HeadNotePowerArrayEntry;


	IOPMPowerFlags          HeadNoteDomainFlags;


	IOPMPowerFlags          HeadNoteDomainTargetFlags;


	IOPowerConnection *     HeadNoteParentConnection;


	IOPMPowerFlags          HeadNoteParentFlags;


	uint32_t                HeadNotePendingAcks;


	IOLock *                PMLock;

	unsigned int            InitialPowerChange          :1;
	unsigned int            InitialSetPowerState        :1;
	unsigned int            DeviceOverrideEnabled       :1;
	unsigned int            DoNotPowerDown              :1;
	unsigned int            ParentsKnowState            :1;
	unsigned int            StrictTreeOrder             :1;
	unsigned int            IdleTimerStopped            :1;
	unsigned int            AdjustPowerScheduled        :1;

	unsigned int            IsPreChange                 :1;
	unsigned int            DriverCallBusy              :1;
	unsigned int            PCDFunctionOverride         :1;
	unsigned int            IdleTimerIgnored            :1;
	unsigned int            HasAdvisoryDesire           :1;
	unsigned int            AdvisoryTickleUsed          :1;
	unsigned int            ResetPowerStateOnWake       :1;


	AbsoluteTime            DeviceActiveTimestamp;
	AbsoluteTime            MaxPowerStateEntryTime;
	AbsoluteTime            MaxPowerStateExitTime;


	IOLock *                ActivityLock;


	int                     IdleTimerPeriod;
	int                     NextIdleTimerPeriod;
	IOPMPowerStateIndex     IdleTimerMinPowerState;
	AbsoluteTime            IdleTimerStartTime;


	IOPMPowerStateIndex     DeviceDesire;


	IOPMPowerStateIndex     DesiredPowerState;


	IOPMPowerFlags          PreviousRequestPowerFlags;


	const char *            Name;


	IOPMPowerStateIndex     NumberOfPowerStates;


	IOPMPowerStateIndex     HighestPowerState;


	IOPMPSEntry *           PowerStates;


	IOService *             ControllingDriver;


	IOPMPowerStateIndex     CurrentPowerState;


	IOPMPowerFlags          ParentsCurrentPowerFlags;


	IOPMPowerStateIndex     MaxPowerState;


	IOPMPowerFlags          MergedOutputPowerFlags;


	OSArray *               ResponseArray;
	OSArray *               NotifyClientArray;


	uint16_t                SerialNumber;



	int                     OutOfBandParameter;

	AbsoluteTime            DriverCallStartTime;
	IOPMPowerFlags          CurrentCapabilityFlags;
	unsigned long           CurrentPowerConsumption;
	IOPMPowerStateIndex     TempClampPowerState;
	OSArray *               NotifyChildArray;
	OSDictionary *          PowerClients;
	thread_call_t           DriverCallEntry;
	void *                  DriverCallParamPtr;
	IOItemCount             DriverCallParamCount;
	IOItemCount             DriverCallParamSlots;
	uint32_t                DriverCallReason;
	uint32_t                OutOfBandMessage;
	uint32_t                TempClampCount;
	IOPMPowerStateIndex     OverrideMaxPowerState;
	IOPMPowerStateIndex     DeviceUsablePowerState;


	thread_call_t           DriverCallTimer;


	IOPMPowerStateIndex     ActivityTicklePowerState;
	IOPMPowerStateIndex     AdvisoryTicklePowerState;
	uint32_t                ActivityTickleCount;
	uint32_t                DeviceWasActive     : 1;
	uint32_t                AdvisoryTickled     : 1;


	uint32_t                WaitReason;
	uint32_t                SavedMachineState;


	struct {
		uint32_t            PMStop              : 1;
		uint32_t            PMDriverCallWait    : 1;
	} LockedFlags;

	queue_head_t            PMDriverCallQueue;
	OSSet *                 InsertInterestSet;
	OSSet *                 RemoveInterestSet;


	uint32_t                ReportClientCnt;
	void *                  ReportBuf;



	IOPMActions             PMActions;


	IOReturn gatedSerialize( OSSerialize * s ) const;
	virtual bool serialize( OSSerialize * s ) const APPLE_KEXT_OVERRIDE;


	void pmPrint( uint32_t event, uintptr_t param1, uintptr_t param2 ) const;
	void pmTrace( uint32_t event, uint32_t eventFunc, uintptr_t param1, uintptr_t param2 ) const;
};
class IOPMClientAck : public OSObject {
	OSDeclareDefaultStructors( IOPMClientAck );
public:
	uint64_t completionTimestamp;   
	uint32_t maxTimeRequested;              
};
class IOPMRequest : public IOCommand
{
	OSDeclareDefaultStructors( IOPMRequest );

protected:
	IOService *          fTarget;           
	IOPMRequest *        fRequestNext;      
	IOPMRequest *        fRequestRoot;      
	uint32_t             fWorkWaitCount;    
	uint32_t             fFreeWaitCount;    
	uint64_t             fTimestamp;        
	uint32_t             fRequestType;      
	bool                 fIsQuiesceBlocker;

	IOPMCompletionAction fCompletionAction;
	void *               fCompletionTarget;
	void *               fCompletionParam;

public:
	uint32_t             fTag;
	void *               fArg0;
	void *               fArg1;
	void *               fArg2;

	inline bool
	isWorkBlocked( void ) const
	{
		return fWorkWaitCount != 0;
	}

	inline bool
	isFreeBlocked( void ) const
	{
		return fFreeWaitCount != 0;
	}

	inline IOPMRequest *
	getNextRequest( void ) const
	{
		return fRequestNext;
	}

	inline IOPMRequest *
	getRootRequest( void ) const
	{
		if (fRequestRoot) {
			return fRequestRoot;
		}
		return NULL;
	}

	inline uint32_t
	getType( void ) const
	{
		return fRequestType;
	}

	inline uint32_t
	getTag( void ) const
	{
		return fTag;
	}

	inline bool
	isReplyType( void ) const
	{
		return fRequestType > kIOPMRequestTypeReplyStart;
	}

	inline IOService *
	getTarget( void ) const
	{
		return fTarget;
	}

	inline bool
	isQuiesceBlocker( void ) const
	{
		return fIsQuiesceBlocker;
	}

	inline bool
	isQuiesceType( void ) const
	{
		return (kIOPMRequestTypeQuiescePowerTree == fRequestType) &&
		       (fCompletionAction != NULL) && (fCompletionTarget != NULL);
	}

	inline void
	installCompletionAction(
		void *               target,
		IOPMCompletionAction action,
		void *               param )
	{
		fCompletionTarget = target;
		fCompletionAction = action;
		fCompletionParam  = param;
	}

	inline void
	setTimestamp( uint64_t time )
	{
		fTimestamp = time;
	}

	inline uint64_t
	getTimestamp( void ) const
	{
		return fTimestamp;
	}

	static IOPMRequest * create( void );
	bool   init( IOService * owner, IOOptionBits type );
	void   reset( void );
	bool   attachNextRequest( IOPMRequest * next );
	bool   detachNextRequest( void );
	bool   attachRootRequest( IOPMRequest * root );
	bool   detachRootRequest( void );
};
class IOPMRequestQueue : public IOEventSource
{
	OSDeclareDefaultStructors( IOPMRequestQueue );

public:
	typedef bool (*Action)( IOService *, IOPMRequest *, IOPMRequestQueue * );

protected:
	queue_head_t    fQueue;
	IOLock *        fLock;

	enum { kMaxDequeueCount = 256 };

	virtual bool checkForWork( void ) APPLE_KEXT_OVERRIDE;
	virtual void free( void ) APPLE_KEXT_OVERRIDE;
	virtual bool init( IOService * inOwner, Action inAction );

public:
	static  IOPMRequestQueue * create( IOService * inOwner, Action inAction );
	void    queuePMRequest( LIBKERN_CONSUMED IOPMRequest * request );
	void    queuePMRequestChain( IOPMRequest ** requests, IOItemCount count );
};
class IOPMWorkQueue : public IOEventSource
{
	OSDeclareDefaultStructors( IOPMWorkQueue );

public:
	typedef bool (*Action)( IOService *, IOPMRequest *, IOPMWorkQueue * );

	uint64_t            fStatCheckForWork;
	uint64_t            fStatScanEntries;
	uint64_t            fStatQueueEmpty;
	uint64_t            fStatNoWorkDone;

protected:
	queue_head_t        fWorkQueue;
	Action              fInvokeAction;
	Action              fRetireAction;
	uint32_t            fQueueLength;
	uint32_t            fConsumerCount;
	volatile uint32_t   fProducerCount;
	IOPMRequest *       fQuiesceRequest;
	AbsoluteTime        fQuiesceStartTime;
	AbsoluteTime        fQuiesceFinishTime;

	virtual bool checkForWork( void ) APPLE_KEXT_OVERRIDE;
	virtual bool init( IOService * inOwner, Action invoke, Action retire );
	bool    checkRequestQueue( queue_head_t * queue, bool * empty );

public:
	static  IOPMWorkQueue * create( IOService * inOwner, Action invoke, Action retire );
	bool    queuePMRequest( IOPMRequest * request, IOServicePM * pwrMgt );
	void    signalWorkAvailable( void );
	void    incrementProducerCount( void );
	void    attachQuiesceRequest( IOPMRequest * quiesceRequest );
	void    finishQuiesceRequest( IOPMRequest * quiesceRequest );
};
class IOPMCompletionQueue : public IOEventSource
{
	OSDeclareDefaultStructors( IOPMCompletionQueue );

public:
	typedef bool (*Action)( IOService *, IOPMRequest *, IOPMCompletionQueue * );

protected:
	queue_head_t    fQueue;

	virtual bool checkForWork( void ) APPLE_KEXT_OVERRIDE;
	virtual bool init( IOService * inOwner, Action inAction );

public:
	static  IOPMCompletionQueue * create( IOService * inOwner, Action inAction );
	bool    queuePMRequest( IOPMRequest * request );
};
class _IOServiceNotifier : public IONotifier
{
	friend class IOService;

	OSDeclareDefaultStructors(_IOServiceNotifier);

public:
	OSOrderedSet *                      whence;

	OSDictionary *                      matching;
	const OSSymbol *                    type;
	IOServiceMatchingNotificationHandler handler;
	IOServiceNotificationHandler        compatHandler;
	void *                              target;
	void *                              ref;
	SInt32                              priority;
	queue_head_t                        handlerInvocations;
	IOOptionBits                        state;

	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void remove() APPLE_KEXT_OVERRIDE;
	virtual bool disable() APPLE_KEXT_OVERRIDE;
	virtual void enable( bool was ) APPLE_KEXT_OVERRIDE;
	virtual void wait();
};
class _IOServiceInterestNotifier : public IONotifier
{
	friend class IOService;

	OSDeclareDefaultStructors(_IOServiceInterestNotifier);

public:
	queue_chain_t               chain;

	IOServiceInterestHandler    handler;
	void *                      target;
	void *                      ref;
	queue_head_t                handlerInvocations;
	IOOptionBits                state;

	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void remove() APPLE_KEXT_OVERRIDE;
	virtual bool disable() APPLE_KEXT_OVERRIDE;
	virtual void enable( bool was ) APPLE_KEXT_OVERRIDE;
	virtual void wait();
	virtual bool init() APPLE_KEXT_OVERRIDE;
};
class _IOServiceNullNotifier : public IONotifier
{
	OSDeclareDefaultStructors(_IOServiceNullNotifier);

public:
	virtual void taggedRetain(const void *tag) const APPLE_KEXT_OVERRIDE;
	virtual void taggedRelease(const void *tag, const int when) const APPLE_KEXT_OVERRIDE;
	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void remove() APPLE_KEXT_OVERRIDE;
	virtual bool disable() APPLE_KEXT_OVERRIDE;
	virtual void enable( bool was ) APPLE_KEXT_OVERRIDE;
	virtual void wait();
};
class _IOConfigThread : public OSObject
{
	friend class IOService;

	OSDeclareDefaultStructors(_IOConfigThread);

public:
	static void configThread( const char * name );
	static void main( void * arg, wait_result_t result );
};
class _IOServiceJob : public OSObject
{
	friend class IOService;

	OSDeclareDefaultStructors(_IOServiceJob);

public:
	int                 type;
	IOService *         nub;
	IOOptionBits        options;

	static LIBKERN_RETURNS_NOT_RETAINED _IOServiceJob * startJob( IOService * nub, int type,
	    IOOptionBits options = 0 );
	static void pingConfig( LIBKERN_CONSUMED class _IOServiceJob * job );
};
class IOResources : public IOService
{
	friend class IOService;

	OSDeclareDefaultStructors(IOResources);

public:
	static IOService * resources( void );
	virtual bool init( OSDictionary * dictionary = NULL ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn newUserClient(task_t owningTask, void * securityID,
	    UInt32 type, OSDictionary * properties,
	    IOUserClient ** handler) APPLE_KEXT_OVERRIDE;
	virtual IOWorkLoop * getWorkLoop() const APPLE_KEXT_OVERRIDE;
	virtual bool matchPropertyTable( OSDictionary * table ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn setProperties( OSObject * properties ) APPLE_KEXT_OVERRIDE;
};
class IOUserResources : public IOService
{
	friend class IOService;

	OSDeclareDefaultStructors(IOUserResources);

public:
	static IOService * resources( void );
	virtual bool init( OSDictionary * dictionary = NULL ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn newUserClient(task_t owningTask, void * securityID,
	    UInt32 type, OSDictionary * properties,
	    IOUserClient ** handler) APPLE_KEXT_OVERRIDE;
	virtual IOWorkLoop * getWorkLoop() const APPLE_KEXT_OVERRIDE;
	virtual bool matchPropertyTable( OSDictionary * table ) APPLE_KEXT_OVERRIDE;
};
class _IOOpenServiceIterator : public OSIterator
{
	friend class IOService;

	OSDeclareDefaultStructors(_IOOpenServiceIterator);

	OSIterator *        iter;
	const IOService *   client;
	const IOService *   provider;
	IOService *         last;

public:
	static OSIterator * iterator(LIBKERN_CONSUMED OSIterator * _iter,
	    const IOService * client,
	    const IOService * provider );
	virtual void free() APPLE_KEXT_OVERRIDE;
	virtual void reset() APPLE_KEXT_OVERRIDE;
	virtual bool isValid() APPLE_KEXT_OVERRIDE;
	virtual OSObject * getNextObject() APPLE_KEXT_OVERRIDE;
};
class IOExclaveProxy : public IOService
{
	OSDeclareDefaultStructors(IOExclaveProxy);

	IOExclaveProxyState * exclaveState;

	bool start(IOService * provider) APPLE_KEXT_OVERRIDE;
};
class _IOServiceStateNotification : public IOService
{
	friend class IOService;

	IOLock * fLock;
	OSDictionary * fItems;

	OSDeclareDefaultStructors(_IOServiceStateNotification);

public:
};
class RootDomainUserClient : public IOUserClient2022
{
	OSDeclareDefaultStructors(RootDomainUserClient);

	friend class IOPMrootDomain;
private:
	IOPMrootDomain *    fOwner;
	task_t              fOwningTask;

	IOReturn            secureSleepSystem( uint32_t *return_code );

	IOReturn            secureSleepSystemOptions( const void  *inOptions,
	    IOByteCount  inOptionsSize,
	    uint32_t  *returnCode);

	IOReturn            secureSetAggressiveness( unsigned long type,
	    unsigned long newLevel,
	    int *return_code );

	IOReturn            secureSetMaintenanceWakeCalendar(
		IOPMCalendarStruct  *inCalendar,
		uint32_t            *returnCode);

	IOReturn            secureSetUserAssertionLevels(uint32_t    assertionBitfield);

	IOReturn            secureGetSystemSleepType( uint32_t *sleepType, uint32_t *sleepTimer);

	IOReturn            secureAttemptIdleSleepAbort( uint32_t *outReverted);

public:

	virtual IOReturn clientClose( void ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn externalMethod(uint32_t selector,
	    IOExternalMethodArgumentsOpaque * args) APPLE_KEXT_OVERRIDE;

	static IOReturn externalMethodDispatched(OSObject * target, void * reference, IOExternalMethodArguments * args);

	virtual bool start( IOService * provider ) APPLE_KEXT_OVERRIDE;

	virtual bool initWithTask(task_t owningTask, void *security_id,
	    UInt32 type, OSDictionary * properties) APPLE_KEXT_OVERRIDE;


	void setPreventative(UInt32 on_off, UInt32 types_of_sleep);


	virtual IOExternalMethod * getTargetAndMethodForIndex( IOService ** targetP, UInt32 index ) APPLE_KEXT_OVERRIDE;
	virtual void stop( IOService *provider) APPLE_KEXT_OVERRIDE;
};
#line 61 "OSUnserializeXML.y"







typedef 



typedef 


static int              OSUnserializeerror(parser_state_t *state, const char *s);
static int              yylex(YYSTYPE *lvalp, parser_state_t *state);
static object_t         *newObject(parser_state_t *state);
static void             freeObject(parser_state_t *state, object_t *o);
static void             rememberObject(parser_state_t *state, int tag, OSObject *o);
static object_t         *retrieveObject(parser_state_t *state, int tag);
static void             cleanupObjects(parser_state_t *state);
static object_t         *buildDictionary(parser_state_t *state, object_t *o);
static object_t         *buildArray(parser_state_t *state, object_t *o);
static object_t         *buildSet(parser_state_t *state, object_t *o);
static object_t         *buildString(parser_state_t *state, object_t *o);
static object_t         *buildSymbol(parser_state_t *state, object_t *o);
static object_t         *buildData(parser_state_t *state, object_t *o);
static object_t         *buildNumber(parser_state_t *state, object_t *o);
static object_t         *buildBoolean(parser_state_t *state, object_t *o);
__BEGIN_DECLS
__END_DECLS

static inline void *
malloc_impl(size_t size)
{
	if (size == 0) {
		return NULL;
	}
	return kalloc_data(size,
	           Z_VM_TAG_BT(Z_WAITOK_ZERO, VM_KERN_MEMORY_LIBKERN));
}

static inline void
free_impl(void *addr)
{
	kfree_data_addr(addr);
}
static inline void
safe_free(void *addr, size_t size)
{
	kfree_data(addr, size);
}

static inline void *
realloc_impl(void *addr, size_t osize, size_t nsize)
{
	return krealloc_data(addr, osize, nsize,
	           Z_VM_TAG_BT(Z_WAITOK_ZERO, VM_KERN_MEMORY_LIBKERN));
}




# define YYDEBUG 0




# define YYTOKEN_TABLE 0








#line 283 "OSUnserializeXML.tab.c"






# ifdef __SIZE_TYPE__
#  define YYSIZE_T __SIZE_TYPE__
# elif defined size_t
#  define YYSIZE_T size_t
# elif !defined YYSIZE_T && (defined __STDC__ || defined __C99__FUNC__         || defined __cplusplus || defined _MSC_VER)
#  include <stddef.h> 
#  define YYSIZE_T size_t
# else
#  define YYSIZE_T unsigned int
# endif


# if defined YYENABLE_NLS && YYENABLE_NLS
#  if ENABLE_NLS
#   include <libintl.h> 
#   define YY_(msgid) dgettext ("bison-runtime", msgid)
#  endif
# endif
# ifndef YY_
#  define YY_(msgid) msgid
# endif




# define YYID(n) (n)
















static const yytype_uint8 yytranslate[] =
{
	0, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	15, 16, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 17, 2, 18, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 13, 2, 14, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
	2, 2, 2, 2, 2, 2, 1, 2, 3, 4,
	5, 6, 7, 8, 9, 10, 11, 12
};
# define YYLLOC_DEFAULT(Current, Rhs, N)                                    do                                                                        if (YYID (N))                                                    	{                                                               	  (Current).first_line   = YYRHSLOC (Rhs, 1).first_line;        	  (Current).first_column = YYRHSLOC (Rhs, 1).first_column;      	  (Current).last_line    = YYRHSLOC (Rhs, N).last_line;         	  (Current).last_column  = YYRHSLOC (Rhs, N).last_column;       	}                                                                     else                                                              	{                                                               	  (Current).first_line   = (Current).last_line   =              	    YYRHSLOC (Rhs, 0).last_line;                                	  (Current).first_column = (Current).last_column =              	    YYRHSLOC (Rhs, 0).last_column;                              	}                                                                   while (YYID (0))




# if defined YYLTYPE_IS_TRIVIAL && YYLTYPE_IS_TRIVIAL
#  define YY_LOCATION_PRINT(File, Loc)                       fprintf (File, "%d.%d-%d.%d",                      	      (Loc).first_line, (Loc).first_column,     	      (Loc).last_line,  (Loc).last_column)
# else
#  define YY_LOCATION_PRINT(File, Loc) ((void) 0)
# endif









# define YYINITDEPTH 200



# define YYMAXDEPTH 10000





/*-----------------------------------------------.
 | Release the memory associated to this symbol.  |
 |   `-----------------------------------------------*/

/*ARGSUSED*/
{
	YYUSE(yyvaluep);

	if (!yymsg) {
		yymsg = "Deleting";
	}
	YY_SYMBOL_PRINT(yymsg, yytype, yyvaluep, yylocationp);

	switch (yytype) {
	default:
		break;
	}
}


/* Prevent warnings from -Wmissing-prototypes.  */







/*----------.
 | yyparse.  |
 |   `----------*/

{
	/* The look-ahead symbol.  */
	int yychar;

/* The semantic value of the look-ahead symbol.  */
	YYSTYPE yylval;

/* Number of syntax errors so far.  */
	int yynerrs;

	int yystate;
	int yyn;
	int yyresult;
	/* Number of tokens to shift before error messages enabled.  */
	int yyerrstatus;
	/* Look-ahead token as an internal (translated) token number.  */
	int yytoken = 0;

	/* Three stacks and their tools:
	 *  `yyss': related to states,
	 *  `yyvs': related to semantic values,
	 *  `yyls': related to locations.
	 *
	 *  Refer to the stacks thru separate pointers, to allow yyoverflow
	 *  to reallocate them elsewhere.  */

	/* The state stack.  */
	yytype_int16 yyssa[YYINITDEPTH];
	yytype_int16 *yyss = yyssa;
	yytype_int16 *yyssp;

	/* The semantic value stack.  */
	YYSTYPE yyvsa[YYINITDEPTH];
	YYSTYPE *yyvs = yyvsa;
	YYSTYPE *yyvsp;




	YYSIZE_T yystacksize = YYINITDEPTH;

	/* The variables used to return semantic value and location from the
	 *  action routines.  */
	YYSTYPE yyval;


	/* The number of symbols on the RHS of the reduced rule.
	 *  Keep to zero when no symbol should be popped.  */
	int yylen = 0;

	YYDPRINTF((stderr, "Starting parse\n"));

	yystate = 0;
	yyerrstatus = 0;
	yynerrs = 0;
	yychar = YYEMPTY;       /* Cause a token to be read.  */

	/* Initialize stack pointers.
	 *  Waste one element of value and location stack
	 *  so that they stay on the same level as the state stack.
	 *  The wasted elements are never initialized.  */

	yyssp = yyss;
	yyvsp = yyvs;

	goto yysetstate;

/*------------------------------------------------------------.
 | yynewstate -- Push a new state, which is found in yystate.  |
 |   `------------------------------------------------------------*/
yynewstate:
	/* In all cases, when you get here, the value and location stacks
	 *  have just been pushed.  So pushing a state here evens the stacks.  */
	yyssp++;

yysetstate:
	*yyssp = yystate;

	if (yyss + yystacksize - 1 <= yyssp) {
		/* Get the current used size of the three stacks, in elements.  */
		YYSIZE_T yysize = yyssp - yyss + 1;


		yyssp = yyss + yysize - 1;
		yyvsp = yyvs + yysize - 1;


		YYDPRINTF((stderr, "Stack size increased to %lu\n",
		    (unsigned long int) yystacksize));

		if (yyss + yystacksize - 1 <= yyssp) {
			YYABORT;
		}
	}

	YYDPRINTF((stderr, "Entering state %d\n", yystate));

	goto yybackup;

/*-----------.
 | yybackup.  |
 |   `-----------*/
yybackup:

	/* Do appropriate processing given the current state.  Read a
	 *  look-ahead token if we need one and don't already have one.  */

	/* First try to decide what to do without reference to look-ahead token.  */
	yyn = yypact[yystate];
	if (yyn == YYPACT_NINF) {
		goto yydefault;
	}

	/* Not known => get a look-ahead token if don't already have one.  */

	/* YYCHAR is either YYEMPTY or YYEOF or a valid look-ahead symbol.  */
	if (yychar == YYEMPTY) {
		YYDPRINTF((stderr, "Reading a token: "));
		yychar = YYLEX;
	}

	if (yychar <= YYEOF) {
		yychar = yytoken = YYEOF;
		YYDPRINTF((stderr, "Now at end of input.\n"));
	} else {
		yytoken = YYTRANSLATE(yychar);
		YY_SYMBOL_PRINT("Next token is", yytoken, &yylval, &yylloc);
	}

	/* If the proper action on seeing token YYTOKEN is to reduce or to
	 *  detect an error, take that action.  */
	yyn += yytoken;
	if (yyn < 0 || YYLAST < yyn || yycheck[yyn] != yytoken) {
		goto yydefault;
	}
	yyn = yytable[yyn];
	if (yyn <= 0) {
		if (yyn == 0 || yyn == YYTABLE_NINF) {
			goto yyerrlab;
		}
		yyn = -yyn;
		goto yyreduce;
	}

	if (yyn == YYFINAL) {
		YYACCEPT;
	}

	/* Count tokens shifted since error; after three, turn off error
	 *  status.  */
	if (yyerrstatus) {
		yyerrstatus--;
	}

	/* Shift the look-ahead token.  */
	YY_SYMBOL_PRINT("Shifting", yytoken, &yylval, &yylloc);

	/* Discard the shifted token unless it is eof.  */
	if (yychar != YYEOF) {
		yychar = YYEMPTY;
	}

	yystate = yyn;
	*++yyvsp = yylval;

	goto yynewstate;


/*-----------------------------------------------------------.
 | yydefault -- do the default action for the current state.  |
 |   `-----------------------------------------------------------*/
yydefault:
	yyn = yydefact[yystate];
	if (yyn == 0) {
		goto yyerrlab;
	}
	goto yyreduce;


/*-----------------------------.
 | yyreduce -- Do a reduction.  |
 |   `-----------------------------*/
yyreduce:
	/* yyn is the number of a rule to reduce with.  */
	yylen = yyr2[yyn];

	/* If YYLEN is nonzero, implement the default value of the action:
	 *  `$$ = $1'.
	 *
	 *  Otherwise, the following line sets YYVAL to garbage.
	 *  This behavior is undocumented and Bison
	 *  users should not rely upon it.  Assigning to YYVAL
	 *  unconditionally makes the parser a bit smaller, and it avoids a
	 *  GCC warning that YYVAL may be used uninitialized.  */
	yyval = yyvsp[1 - yylen];


	YY_REDUCE_PRINT(yyn);
	switch (yyn) {
	case 2:
#line 217 "OSUnserializeXML.y"
		{ yyerror("unexpected end of buffer");
		  YYERROR;
		  ;}
		break;

	case 3:
#line 220 "OSUnserializeXML.y"
		{ STATE->parsedObject = (yyvsp[(1) - (1)])->object;
		  (yyvsp[(1) - (1)])->object = 0;
		  freeObject(STATE, (yyvsp[(1) - (1)]));
		  YYACCEPT;
		  ;}
		break;

	case 4:
#line 225 "OSUnserializeXML.y"
		{ yyerror("syntax error");
		  YYERROR;
		  ;}
		break;

	case 5:
#line 230 "OSUnserializeXML.y"
		{ (yyval) = buildDictionary(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildDictionary");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 6:
#line 242 "OSUnserializeXML.y"
		{ (yyval) = buildArray(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildArray");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 7:
#line 254 "OSUnserializeXML.y"
		{ (yyval) = buildSet(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildSet");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 8:
#line 266 "OSUnserializeXML.y"
		{ (yyval) = buildString(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildString");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 9:
#line 278 "OSUnserializeXML.y"
		{ (yyval) = buildData(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildData");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 10:
#line 290 "OSUnserializeXML.y"
		{ (yyval) = buildNumber(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildNumber");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 11:
#line 302 "OSUnserializeXML.y"
		{ (yyval) = buildBoolean(STATE, (yyvsp[(1) - (1)]));

		  if (!yyval->object) {
			  yyerror("buildBoolean");
			  YYERROR;
		  }
		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 12:
#line 314 "OSUnserializeXML.y"
		{ (yyval) = retrieveObject(STATE, (yyvsp[(1) - (1)])->idref);
		  if ((yyval)) {
			  STATE->retrievedObjectCount++;
			  (yyval)->object->retain();
			  if (STATE->retrievedObjectCount > MAX_REFED_OBJECTS) {
				  yyerror("maximum object reference count");
				  YYERROR;
			  }
		  } else {
			  yyerror("forward reference detected");
			  YYERROR;
		  }
		  freeObject(STATE, (yyvsp[(1) - (1)]));

		  STATE->parsedObjectCount++;
		  if (STATE->parsedObjectCount > MAX_OBJECTS) {
			  yyerror("maximum object count");
			  YYERROR;
		  }
		  ;}
		break;

	case 13:
#line 338 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 14:
#line 341 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 17:
#line 348 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(2) - (2)]);
		  (yyval)->next = (yyvsp[(1) - (2)]);

		  object_t *o;
		  o = (yyval)->next;
		  while (o) {
			  if (o->key == (yyval)->key) {
				  yyerror("duplicate dictionary key");
				  YYERROR;
			  }
			  o = o->next;
		  }
		  ;}
		break;

	case 18:
#line 363 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->key = (OSSymbol *)(yyval)->object;
		  (yyval)->object = (yyvsp[(2) - (2)])->object;
		  (yyval)->next = NULL;
		  (yyvsp[(2) - (2)])->object = 0;
		  freeObject(STATE, (yyvsp[(2) - (2)]));
		  ;}
		break;

	case 19:
#line 372 "OSUnserializeXML.y"
		{ (yyval) = buildSymbol(STATE, (yyvsp[(1) - (1)]));

//				  STATE->parsedObjectCount++;
//				  if (STATE->parsedObjectCount > MAX_OBJECTS) {
//				    yyerror("maximum object count");
//				    YYERROR;
//				  }
		  ;}
		break;

	case 20:
#line 384 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 21:
#line 387 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 23:
#line 393 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (2)]);
		  (yyval)->elements = NULL;
		  ;}
		break;

	case 24:
#line 396 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (3)]);
		  (yyval)->elements = (yyvsp[(2) - (3)]);
		  ;}
		break;

	case 26:
#line 402 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(1) - (1)]);
		  (yyval)->next = NULL;
		  ;}
		break;

	case 27:
#line 405 "OSUnserializeXML.y"
		{ (yyval) = (yyvsp[(2) - (2)]);
		  (yyval)->next = (yyvsp[(1) - (2)]);
		  ;}
		break;


/* Line 1267 of yacc.c.  */
#line 1772 "OSUnserializeXML.tab.c"
	default: break;
	}
	YY_SYMBOL_PRINT("-> $$ =", yyr1[yyn], &yyval, &yyloc);

	YYPOPSTACK(yylen);
	yylen = 0;
	YY_STACK_PRINT(yyss, yyssp);

	*++yyvsp = yyval;


	/* Now `shift' the result of the reduction.  Determine what state
	 *  that goes to, based on the state we popped back to and the rule
	 *  number reduced by.  */

	yyn = yyr1[yyn];

	yystate = yypgoto[yyn - YYNTOKENS] + *yyssp;
	if (0 <= yystate && yystate <= YYLAST && yycheck[yystate] == *yyssp) {
		yystate = yytable[yystate];
	} else {
		yystate = yydefgoto[yyn - YYNTOKENS];
	}

	goto yynewstate;


/*------------------------------------.
 | yyerrlab -- here on detecting error |
 |   `------------------------------------*/
yyerrlab:
	/* If not already recovering from an error, report this error.  */
	if (!yyerrstatus) {
		++yynerrs;
	}



	if (yyerrstatus == 3) {
		/* If just tried and failed to reuse look-ahead token after an
		 *  error, discard it.  */

		if (yychar <= YYEOF) {
			/* Return failure if at end of input.  */
			if (yychar == YYEOF) {
				YYABORT;
			}
		} else {
			yydestruct("Error: discarding",
			    yytoken, &yylval);
			yychar = YYEMPTY;
		}
	}

	/* Else will try to reuse look-ahead token after shifting the error
	 *  token.  */
	goto yyerrlab1;


/*---------------------------------------------------.
 | yyerrorlab -- error raised explicitly by YYERROR.  |
 |   `---------------------------------------------------*/
yyerrorlab:

	/* Pacify compilers like GCC when the user code never invokes
	 *  YYERROR and the label yyerrorlab therefore never appears in user
	 *  code.  */
	if (/*CONSTCOND*/ 0) {
		goto yyerrorlab;
	}

	/* Do not reclaim the symbols of the rule which action triggered
	 *  this YYERROR.  */
	YYPOPSTACK(yylen);
	yylen = 0;
	YY_STACK_PRINT(yyss, yyssp);
	yystate = *yyssp;
	goto yyerrlab1;


/*-------------------------------------------------------------.
 | yyerrlab1 -- common code for both syntax error and YYERROR.  |
 |   `-------------------------------------------------------------*/
yyerrlab1:
	yyerrstatus = 3; /* Each real token shifted decrements this.  */

	for (;;) {
		yyn = yypact[yystate];
		if (yyn != YYPACT_NINF) {
			yyn += YYTERROR;
			if (0 <= yyn && yyn <= YYLAST && yycheck[yyn] == YYTERROR) {
				yyn = yytable[yyn];
				if (0 < yyn) {
					break;
				}
			}
		}

		/* Pop the current state because it cannot handle the error token.  */
		if (yyssp == yyss) {
			YYABORT;
		}


		yydestruct("Error: popping",
		    yystos[yystate], yyvsp);
		YYPOPSTACK(1);
		yystate = *yyssp;
		YY_STACK_PRINT(yyss, yyssp);
	}

	if (yyn == YYFINAL) {
		YYACCEPT;
	}

	*++yyvsp = yylval;


	/* Shift the error token.  */
	YY_SYMBOL_PRINT("Shifting", yystos[yyn], yyvsp, yylsp);

	yystate = yyn;
	goto yynewstate;


/*-------------------------------------.
 | yyacceptlab -- YYACCEPT comes here.  |
 |   `-------------------------------------*/
yyacceptlab:
	yyresult = 0;
	goto yyreturn;

/*-----------------------------------.
 | yyabortlab -- YYABORT comes here.  |
 |   `-----------------------------------*/
yyabortlab:
	yyresult = 1;
	goto yyreturn;

/*-------------------------------------------------.
 | yyexhaustedlab -- memory exhaustion comes here.  |
 |   `-------------------------------------------------*/
yyexhaustedlab:
	yyerror(YY_("memory exhausted"));
	yyresult = 2;
	/* Fall through.  */

yyreturn:
	if (yychar != YYEOF && yychar != YYEMPTY) {
		yydestruct("Cleanup: discarding lookahead",
		    yytoken, &yylval);
	}
	/* Do not reclaim the symbols of the rule which action triggered
	 *  this YYABORT or YYACCEPT.  */
	YYPOPSTACK(yylen);
	YY_STACK_PRINT(yyss, yyssp);
	while (yyssp != yyss) {
		yydestruct("Cleanup: popping",
		    yystos[*yyssp], yyvsp);
		YYPOPSTACK(1);
	}
	if (yyss != yyssa) {
		YYSTACK_FREE(yyss);
	}
	/* Make sure YYID is used.  */
	return YYID(yyresult);
}


#line 427 "OSUnserializeXML.y"


int
OSUnserializeerror(parser_state_t * state, const char *s)  /* Called by yyparse on errors */
{
	if (state->errorString) {
		char tempString[128];
		snprintf(tempString, 128, "OSUnserializeXML: %s near line %d\n", s, state->lineNumber);
		*(state->errorString) = OSString::withCString(tempString);
	}

	return 0;
}




static int
getTag(parser_state_t *state,
    char tag[TAG_MAX_LENGTH],
    int *attributeCount,
    char attributes[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH],
    char values[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH] )
{
	int length = 0;
	int c = currentChar();
	int tagType = TAG_START;

	*attributeCount = 0;

	if (c != '<') {
		return TAG_BAD;
	}
	c = nextChar();         // skip '<'


	// <!TAG   declarations     >
	// <!--     comments      -->
	if (c == '!') {
		c = nextChar();
		bool isComment = (c == '-') && ((c = nextChar()) != 0) && (c == '-');
		if (!isComment && !isAlpha(c)) {
			return TAG_BAD;                      // <!1, <!-A, <!eos
		}
		while (c && (c = nextChar()) != 0) {
			if (c == '\n') {
				state->lineNumber++;
			}
			if (isComment) {
				if (c != '-') {
					continue;
				}
				c = nextChar();
				if (c != '-') {
					continue;
				}
				c = nextChar();
			}
			if (c == '>') {
				(void)nextChar();
				return TAG_IGNORE;
			}
			if (isComment) {
				break;
			}
		}
		return TAG_BAD;
	} else
	// <? Processing Instructions  ?>
	if (c == '?') {
		while ((c = nextChar()) != 0) {
			if (c == '\n') {
				state->lineNumber++;
			}
			if (c != '?') {
				continue;
			}
			c = nextChar();
			if (!c) {
				return TAG_IGNORE;
			}
			if (c == '>') {
				(void)nextChar();
				return TAG_IGNORE;
			}
		}
		return TAG_BAD;
	} else
	// </ end tag >
	if (c == '/') {
		c = nextChar();         // skip '/'
		tagType = TAG_END;
	}
	if (!isAlpha(c)) {
		return TAG_BAD;
	}

	/* find end of tag while copying it */
	while (isAlphaNumeric(c)) {
		tag[length++] = c;
		c = nextChar();
		if (length >= (TAG_MAX_LENGTH - 1)) {
			return TAG_BAD;
		}
	}

	tag[length] = 0;

//	printf("tag %s, type %d\n", tag, tagType);

	// look for attributes of the form attribute = "value" ...
	while ((c != '>') && (c != '/')) {
		while (isSpace(c)) {
			c = nextChar();
		}

		length = 0;
		while (isAlphaNumeric(c)) {
			attributes[*attributeCount][length++] = c;
			if (length >= (TAG_MAX_LENGTH - 1)) {
				return TAG_BAD;
			}
			c = nextChar();
		}
		attributes[*attributeCount][length] = 0;

		while (isSpace(c)) {
			c = nextChar();
		}

		if (c != '=') {
			return TAG_BAD;
		}
		c = nextChar();

		while (isSpace(c)) {
			c = nextChar();
		}

		if (c != '"') {
			return TAG_BAD;
		}
		c = nextChar();
		length = 0;
		while (c != '"') {
			values[*attributeCount][length++] = c;
			if (length >= (TAG_MAX_LENGTH - 1)) {
				return TAG_BAD;
			}
			c = nextChar();
			if (!c) {
				return TAG_BAD;
			}
		}
		values[*attributeCount][length] = 0;

		c = nextChar(); // skip closing quote

//		printf("	attribute '%s' = '%s', nextchar = '%c'\n",
//		       attributes[*attributeCount], values[*attributeCount], c);

		(*attributeCount)++;
		if (*attributeCount >= TAG_MAX_ATTRIBUTES) {
			return TAG_BAD;
		}
	}

	if (c == '/') {
		c = nextChar();         // skip '/'
		tagType = TAG_EMPTY;
	}
	if (c != '>') {
		return TAG_BAD;
	}
	c = nextChar();         // skip '>'

	return tagType;
}

static char *
getString(parser_state_t *state, int *alloc_lengthp)
{
	int c = currentChar();
	int start, length, i, j;
	char * tempString;

	start = state->parseBufferIndex;
	/* find end of string */

	while (c != 0) {
		if (c == '\n') {
			state->lineNumber++;
		}
		if (c == '<') {
			break;
		}
		c = nextChar();
	}

	if (c != '<') {
		return 0;
	}

	length = state->parseBufferIndex - start;

	/* copy to null terminated buffer */
	tempString = (char *)malloc(length + 1);
	if (tempString == NULL) {
		printf("OSUnserializeXML: can't alloc temp memory\n");
		goto error;
	}
	if (alloc_lengthp) {
		*alloc_lengthp = length + 1;
	}

	// copy out string in tempString
	// "&amp;" -> '&', "&lt;" -> '<', "&gt;" -> '>'

	i = j = 0;
	while (i < length) {
		c = state->parseBuffer[start + i++];
		if (c != '&') {
			tempString[j++] = c;
		} else {
			if ((i + 3) > length) {
				goto error;
			}
			c = state->parseBuffer[start + i++];
			if (c == 'l') {
				if (state->parseBuffer[start + i++] != 't') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '<';
				continue;
			}
			if (c == 'g') {
				if (state->parseBuffer[start + i++] != 't') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '>';
				continue;
			}
			if ((i + 3) > length) {
				goto error;
			}
			if (c == 'a') {
				if (state->parseBuffer[start + i++] != 'm') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != 'p') {
					goto error;
				}
				if (state->parseBuffer[start + i++] != ';') {
					goto error;
				}
				tempString[j++] = '&';
				continue;
			}
			goto error;
		}
	}
	tempString[j] = 0;

//	printf("string %s\n", tempString);

	return tempString;

error:
	if (tempString) {
		safe_free(tempString, length + 1);
		if (alloc_lengthp) {
			*alloc_lengthp = 0;
		}
	}
	return 0;
}

static long long
getNumber(parser_state_t *state)
{
	unsigned long long n = 0;
	int base = 10;
	bool negate = false;
	int c = currentChar();

	if (c == '0') {
		c = nextChar();
		if (c == 'x') {
			base = 16;
			c = nextChar();
		}
	}
	if (base == 10) {
		if (c == '-') {
			negate = true;
			c = nextChar();
		}
		while (isDigit(c)) {
			n = (n * base + c - '0');
			c = nextChar();
		}
		if (negate) {
			n = (unsigned long long)((long long)n * (long long)-1);
		}
	} else {
		while (isHexDigit(c)) {
			if (isDigit(c)) {
				n = (n * base + c - '0');
			} else {
				n = (n * base + 0xa + c - 'a');
			}
			c = nextChar();
		}
	}
//	printf("number 0x%x\n", (unsigned long)n);
	return n;
}

// taken from CFXMLParsing/CFPropertyList.c

static const signed char __CFPLDataDecodeTable[128] = {
	/* 000 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 010 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 020 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* 030 */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* ' ' */ -1, -1, -1, -1, -1, -1, -1, -1,
	/* '(' */ -1, -1, -1, 62, -1, -1, -1, 63,
	/* '0' */ 52, 53, 54, 55, 56, 57, 58, 59,
	/* '8' */ 60, 61, -1, -1, -1, 0, -1, -1,
	/* '@' */ -1, 0, 1, 2, 3, 4, 5, 6,
	/* 'H' */ 7, 8, 9, 10, 11, 12, 13, 14,
	/* 'P' */ 15, 16, 17, 18, 19, 20, 21, 22,
	/* 'X' */ 23, 24, 25, -1, -1, -1, -1, -1,
	/* '`' */ -1, 26, 27, 28, 29, 30, 31, 32,
	/* 'h' */ 33, 34, 35, 36, 37, 38, 39, 40,
	/* 'p' */ 41, 42, 43, 44, 45, 46, 47, 48,
	/* 'x' */ 49, 50, 51, -1, -1, -1, -1, -1
};
static void *
getCFEncodedData(parser_state_t *state, unsigned int *size)
{
	int numeq = 0, cntr = 0;
	unsigned int acc = 0;
	int tmpbufpos = 0;
	size_t tmpbuflen = DATA_ALLOC_SIZE;
	unsigned char *tmpbuf = (unsigned char *)malloc(tmpbuflen);

	int c = currentChar();
	*size = 0;

	while (c != '<') {
		c &= 0x7f;
		if (c == 0) {
			safe_free(tmpbuf, tmpbuflen);
			return 0;
		}
		if (c == '=') {
			numeq++;
		} else {
			numeq = 0;
		}
		if (c == '\n') {
			state->lineNumber++;
		}
		if (__CFPLDataDecodeTable[c] < 0) {
			c = nextChar();
			continue;
		}
		cntr++;
		acc <<= 6;
		acc += __CFPLDataDecodeTable[c];
		if (0 == (cntr & 0x3)) {
			if (tmpbuflen <= tmpbufpos + 2) {
				size_t oldsize = tmpbuflen;
				tmpbuflen *= 2;
				tmpbuf = (unsigned char *)realloc(tmpbuf, oldsize, tmpbuflen);
			}
			tmpbuf[tmpbufpos++] = (acc >> 16) & 0xff;
			if (numeq < 2) {
				tmpbuf[tmpbufpos++] = (acc >> 8) & 0xff;
			}
			if (numeq < 1) {
				tmpbuf[tmpbufpos++] = acc & 0xff;
			}
		}
		c = nextChar();
	}
	*size = tmpbufpos;
	if (*size == 0) {
		safe_free(tmpbuf, tmpbuflen);
		return 0;
	}
	return tmpbuf;
}

static void *
getHexData(parser_state_t *state, unsigned int *size)
{
	int c;
	unsigned char *d, *start;

	size_t buflen = DATA_ALLOC_SIZE; // initial buffer size
	start = d = (unsigned char *)malloc(buflen);
	c = currentChar();

	while (c != '<') {
		if (isSpace(c)) {
			while ((c = nextChar()) != 0 && isSpace(c)) {
			}
		}
		;
		if (c == '\n') {
			state->lineNumber++;
			c = nextChar();
			continue;
		}

		// get high nibble
		if (isDigit(c)) {
			*d = (c - '0') << 4;
		} else if (isAlphaDigit(c)) {
			*d =  (0xa + (c - 'a')) << 4;
		} else {
			goto error;
		}

		// get low nibble
		c = nextChar();
		if (isDigit(c)) {
			*d |= c - '0';
		} else if (isAlphaDigit(c)) {
			*d |= 0xa + (c - 'a');
		} else {
			goto error;
		}

		d++;
		size_t oldsize = d - start;
		if (oldsize >= buflen) {
			assert(oldsize == buflen);
			buflen *= 2;
			start = (unsigned char *)realloc(start, oldsize, buflen);
			d = start + oldsize;
		}
		c = nextChar();
	}

	*size = d - start;
	return start;

error:

	*size = 0;
	safe_free(start, buflen);
	return 0;
}

static int
yylex(YYSTYPE *lvalp, parser_state_t *state)
{
	int c, i;
	int tagType;
	char tag[TAG_MAX_LENGTH];
	int attributeCount;
	char attributes[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH];
	char values[TAG_MAX_ATTRIBUTES][TAG_MAX_LENGTH];
	object_t *object;
	int alloc_length;
top:
	c = currentChar();

	/* skip white space  */
	if (isSpace(c)) {
		while ((c = nextChar()) != 0 && isSpace(c)) {
		}
	}
	;

	/* keep track of line number, don't return \n's */
	if (c == '\n') {
		STATE->lineNumber++;
		(void)nextChar();
		goto top;
	}

	// end of the buffer?
	if (!c) {
		return 0;
	}

	tagType = getTag(STATE, tag, &attributeCount, attributes, values);
	if (tagType == TAG_BAD) {
		return SYNTAX_ERROR;
	}
	if (tagType == TAG_IGNORE) {
		goto top;
	}

	// handle allocation and check for "ID" and "IDREF" tags up front
	*lvalp = object = newObject(STATE);
	object->idref = -1;
	for (i = 0; i < attributeCount; i++) {
		if (attributes[i][0] == 'I' && attributes[i][1] == 'D') {
			// check for idref's, note: we ignore the tag, for
			
			
			if (attributes[i][2] == 'R' && attributes[i][3] == 'E' &&
			    attributes[i][4] == 'F' && !attributes[i][5]) {
				if (tagType != TAG_EMPTY) {
					return SYNTAX_ERROR;
				}
				object->idref = strtol(values[i], NULL, 0);
				return IDREF;
			}
			
			if (!attributes[i][2]) {
				object->idref = strtol(values[i], NULL, 0);
			} else {
				return SYNTAX_ERROR;
			}
		}
	}

	switch (*tag) {
	case 'a':
		if (!strcmp(tag, "array")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return ARRAY;
			}
			return (tagType == TAG_START) ? '(' : ')';
		}
		break;
	case 'd':
		if (!strcmp(tag, "dict")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return DICTIONARY;
			}
			return (tagType == TAG_START) ? '{' : '}';
		}
		if (!strcmp(tag, "data")) {
			unsigned int size;
			if (tagType == TAG_EMPTY) {
				object->data = NULL;
				object->size = 0;
				return DATA;
			}

			bool isHexFormat = false;
			for (i = 0; i < attributeCount; i++) {
				if (!strcmp(attributes[i], "format") && !strcmp(values[i], "hex")) {
					isHexFormat = true;
					break;
				}
			}
			
			if (isHexFormat) {
				object->data = getHexData(STATE, &size);
			} else {
				object->data = getCFEncodedData(STATE, &size);
			}
			object->size = size;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END) || strcmp(tag, "data")) {
				return SYNTAX_ERROR;
			}
			return DATA;
		}
		break;
	case 'f':
		if (!strcmp(tag, "false")) {
			if (tagType == TAG_EMPTY) {
				object->number = 0;
				return BOOLEAN;
			}
		}
		break;
	case 'i':
		if (!strcmp(tag, "integer")) {
			object->size = 64;      
			for (i = 0; i < attributeCount; i++) {
				if (!strcmp(attributes[i], "size")) {
					object->size = strtoul(values[i], NULL, 0);
				}
			}
			if (tagType == TAG_EMPTY) {
				object->number = 0;
				return NUMBER;
			}
			object->number = getNumber(STATE);
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END) || strcmp(tag, "integer")) {
				return SYNTAX_ERROR;
			}
			return NUMBER;
		}
		break;
	case 'k':
		if (!strcmp(tag, "key")) {
			if (tagType == TAG_EMPTY) {
				return SYNTAX_ERROR;
			}
			object->string = getString(STATE, &alloc_length);
			if (!object->string) {
				return SYNTAX_ERROR;
			}
			object->string_alloc_length = alloc_length;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END)
			    || strcmp(tag, "key")) {
				return SYNTAX_ERROR;
			}
			return KEY;
		}
		break;
	case 'p':
		if (!strcmp(tag, "plist")) {
			freeObject(STATE, object);
			goto top;
		}
		break;
	case 's':
		if (!strcmp(tag, "string")) {
			if (tagType == TAG_EMPTY) {
				object->string = (char *)malloc(1);
				object->string_alloc_length = 1;
				object->string[0] = 0;
				return STRING;
			}
			object->string = getString(STATE, &alloc_length);
			if (!object->string) {
				return SYNTAX_ERROR;
			}
			object->string_alloc_length = alloc_length;
			if ((getTag(STATE, tag, &attributeCount, attributes, values) != TAG_END)
			    || strcmp(tag, "string")) {
				return SYNTAX_ERROR;
			}
			return STRING;
		}
		if (!strcmp(tag, "set")) {
			if (tagType == TAG_EMPTY) {
				object->elements = NULL;
				return SET;
			}
			if (tagType == TAG_START) {
				return '[';
			} else {
				return ']';
			}
		}
		break;
	case 't':
		if (!strcmp(tag, "true")) {
			if (tagType == TAG_EMPTY) {
				object->number = 1;
				return BOOLEAN;
			}
		}
		break;
	}

	return SYNTAX_ERROR;
}











object_t *
newObject(parser_state_t *state)
{
	object_t *o;

	if (state->freeObjects) {
		o = state->freeObjects;
		state->freeObjects = state->freeObjects->next;
	} else {
		o = malloc_type(object_t);

		o->free = state->objects;
		state->objects = o;
	}

	return o;
}

void
freeObject(parser_state_t * state, object_t *o)
{
	o->next = state->freeObjects;
	state->freeObjects = o;
}

void
cleanupObjects(parser_state_t *state)
{
	object_t *t, *o = state->objects;

	while (o) {
		if (o->object) {

			o->object->release();
		}
		if (o->data) {

			free(o->data);
		}
		if (o->key) {

			o->key->release();
		}
		if (o->string) {

			free(o->string);
		}

		t = o;
		o = o->free;
		free_type(object_t, t);

	}

}





static void
rememberObject(parser_state_t *state, int tag, OSObject *o)
{
	char key[16];
	snprintf(key, 16, "%u", tag);



	state->tags->setObject(key, o);
}

static object_t *
retrieveObject(parser_state_t *state, int tag)
{
	OSObject *ref;
	object_t *o;
	char key[16];
	snprintf(key, 16, "%u", tag);



	ref = state->tags->getObject(key);
	if (!ref) {
		return 0;
	}

	o = newObject(state);
	o->object = ref;
	return o;
}





object_t *
buildDictionary(parser_state_t *state, object_t * header)
{
	object_t *o, *t;
	int count = 0;
	OSDictionary *dict;

	
	o = header->elements;
	header->elements = 0;
	while (o) {
		count++;
		t = o;
		o = o->next;

		t->next = header->elements;
		header->elements = t;
	}

	dict = OSDictionary::withCapacity(count);
	if (header->idref >= 0) {
		rememberObject(state, header->idref, dict);
	}

	o = header->elements;
	while (o) {
		dict->setObject(o->key, o->object);

		o->key->release();
		o->object->release();
		o->key = 0;
		o->object = 0;

		t = o;
		o = o->next;
		freeObject(state, t);
	}
	o = header;
	o->object = dict;
	return o;
};
object_t *
buildArray(parser_state_t *state, object_t * header)
{
	object_t *o, *t;
	int count = 0;
	OSArray *array;

	
	o = header->elements;
	header->elements = 0;
	while (o) {
		count++;
		t = o;
		o = o->next;

		t->next = header->elements;
		header->elements = t;
	}

	array = OSArray::withCapacity(count);
	if (header->idref >= 0) {
		rememberObject(state, header->idref, array);
	}

	o = header->elements;
	while (o) {
		array->setObject(o->object);

		o->object->release();
		o->object = 0;

		t = o;
		o = o->next;
		freeObject(state, t);
	}
	o = header;
	o->object = array;
	return o;
};
object_t *
buildSet(parser_state_t *state, object_t *header)
{
	object_t *o = buildArray(state, header);

	OSArray *array = (OSArray *)o->object;
	OSSet *set = OSSet::withArray(array, array->getCapacity());

	
	if (header->idref >= 0) {
		rememberObject(state, header->idref, set);
	}

	array->release();
	o->object = set;
	return o;
};
object_t *
buildString(parser_state_t *state, object_t *o)
{
	OSString *string;

	string = OSString::withCString(o->string);
	if (o->idref >= 0) {
		rememberObject(state, o->idref, string);
	}

	free(o->string);
	o->string = 0;
	o->object = string;

	return o;
};
object_t *
buildSymbol(parser_state_t *state, object_t *o)
{
	OSSymbol *symbol;

	symbol = const_cast < OSSymbol * > (OSSymbol::withCString(o->string));
	if (o->idref >= 0) {
		rememberObject(state, o->idref, symbol);
	}

	safe_free(o->string, o->string_alloc_length);
	o->string = 0;
	o->object = symbol;

	return o;
};
object_t *
buildData(parser_state_t *state, object_t *o)
{
	OSData *data;

	if (o->size) {
		data = OSData::withBytes(o->data, o->size);
	} else {
		data = OSData::withCapacity(0);
	}
	if (o->idref >= 0) {
		rememberObject(state, o->idref, data);
	}

	if (o->size) {
		free(o->data);
	}
	o->data = 0;
	o->object = data;
	return o;
};
object_t *
buildNumber(parser_state_t *state, object_t *o)
{
	OSNumber *number = OSNumber::withNumber(o->number, o->size);

	if (o->idref >= 0) {
		rememberObject(state, o->idref, number);
	}

	o->object = number;
	return o;
};
object_t *
buildBoolean(parser_state_t *state __unused, object_t *o)
{
	o->object = ((o->number == 0) ? kOSBooleanFalse : kOSBooleanTrue);
	o->object->retain();
	return o;
};
extern int IOMemoryDescriptorTest(int x);
class IONVRAMController : public IOService
{
	OSDeclareAbstractStructors(IONVRAMController);

public:
	virtual void registerService(IOOptionBits options = 0) APPLE_KEXT_OVERRIDE;

	virtual void sync(void);
	virtual IOReturn select(uint32_t bank);
	virtual IOReturn eraseBank(void);

	virtual IOReturn read(IOByteCount offset, UInt8 *buffer,
	    IOByteCount length) = 0;
	virtual IOReturn write(IOByteCount offset, UInt8 *buffer,
	    IOByteCount length) = 0;
};
class AppleMacIO : public IOService
{
	OSDeclareAbstractStructors(AppleMacIO);

	IOService *         fNub;
	IOMemoryMap *       fMemory;

	struct ExpansionData { };
	ExpansionData *fReserved;

protected:
	virtual bool selfTest( void );

public:
	virtual bool start( IOService * provider ) APPLE_KEXT_OVERRIDE;

	virtual IOService * createNub( IORegistryEntry * from );

	virtual void processNub( IOService * nub );

	virtual void publishBelow( IORegistryEntry * root );

	virtual const char * deleteList( void );
	virtual const char * excludeList( void );

	virtual bool compareNubName( const IOService * nub, OSString * name,
	    OSString ** matched = 0 ) const;

	virtual IOReturn getNubResources( IOService * nub );

	OSMetaClassDeclareReservedUnused(AppleMacIO, 0);
	OSMetaClassDeclareReservedUnused(AppleMacIO, 1);
	OSMetaClassDeclareReservedUnused(AppleMacIO, 2);
	OSMetaClassDeclareReservedUnused(AppleMacIO, 3);
};
class AppleMacIODevice : public IOService
{
	OSDeclareDefaultStructors(AppleMacIODevice);

private:
	struct ExpansionData { };
	ExpansionData *reserved;

public:
	virtual bool compareName( OSString * name, OSString ** matched = 0 ) const APPLE_KEXT_OVERRIDE;
	virtual IOService *matchLocation(IOService *client) APPLE_KEXT_OVERRIDE;
	virtual IOReturn getResources( void ) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(AppleMacIODevice, 0);
	OSMetaClassDeclareReservedUnused(AppleMacIODevice, 1);
	OSMetaClassDeclareReservedUnused(AppleMacIODevice, 2);
	OSMetaClassDeclareReservedUnused(AppleMacIODevice, 3);
};
class AppleNMI : public IOService
{
	OSDeclareDefaultStructors(AppleNMI);

private:
	bool enable_debugger;
	bool mask_NMI;

	struct ExpansionData { };
	ExpansionData * reserved; 

public:
	IOService *rootDomain;
	virtual bool start(IOService *provider) APPLE_KEXT_OVERRIDE;
	virtual IOReturn initNMI(IOInterruptController *parentController, OSData *parentSource);
	virtual IOReturn handleInterrupt(void *refCon, IOService *nub, int source);


	virtual IOReturn powerStateWillChangeTo(IOPMPowerFlags, unsigned long, IOService*) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(AppleNMI, 0);
	OSMetaClassDeclareReservedUnused(AppleNMI, 1);
	OSMetaClassDeclareReservedUnused(AppleNMI, 2);
	OSMetaClassDeclareReservedUnused(AppleNMI, 3);
};
class ApplePlatformExpert : public IODTPlatformExpert
{
	OSDeclareAbstractStructors(ApplePlatformExpert);

private:
	SInt32 _timeToGMT;

	struct ExpansionData { };
	ExpansionData *reserved;

public:
	virtual bool start( IOService * provider ) APPLE_KEXT_OVERRIDE;
	virtual bool configure( IOService * provider ) APPLE_KEXT_OVERRIDE;
	virtual const char * deleteList( void ) APPLE_KEXT_OVERRIDE;
	virtual const char * excludeList( void ) APPLE_KEXT_OVERRIDE;

	virtual void registerNVRAMController( IONVRAMController * nvram ) APPLE_KEXT_OVERRIDE;

	virtual long getGMTTimeOfDay(void) APPLE_KEXT_OVERRIDE;
	virtual void setGMTTimeOfDay(long secs) APPLE_KEXT_OVERRIDE;

	virtual bool getMachineName(char *name, int maxLength) APPLE_KEXT_OVERRIDE;

	OSMetaClassDeclareReservedUnused(ApplePlatformExpert, 0);
	OSMetaClassDeclareReservedUnused(ApplePlatformExpert, 1);
	OSMetaClassDeclareReservedUnused(ApplePlatformExpert, 2);
	OSMetaClassDeclareReservedUnused(ApplePlatformExpert, 3);
};
extern "C" {
}



class IOPlatformIO : public IOService
{
	OSDeclareAbstractStructors(IOPlatformIO);

public:
	virtual bool start(IOService * provider) APPLE_KEXT_OVERRIDE;

	
	virtual bool handlePlatformError(vm_offset_t far) = 0;
};
class IOPwrController : public IOService
{
	OSDeclareAbstractStructors(IOPwrController);

public:
};
class IOPMinformee : public OSObject
{
	OSDeclareDefaultStructors(IOPMinformee);
	friend class IOPMinformeeList;

public:
	static IOPMinformee * withObject( IOService * theObject );

	void initialize( IOService * theObject );

	void free( void ) APPLE_KEXT_OVERRIDE;

public:
	IOService *     whatObject; 
	int32_t         timer;      
	IOPMinformee *  nextInList; 
	AbsoluteTime    startTime;  
	bool            active;     
};
class IOPMinformeeList : public OSObject
{
	OSDeclareDefaultStructors(IOPMinformeeList);
	friend class IOPMinformee;

private:

	IOPMinformee       *firstItem;

	unsigned long       length;

public:
	void initialize( void );
	void free( void ) APPLE_KEXT_OVERRIDE;

	unsigned long numberOfItems( void );

	LIBKERN_RETURNS_NOT_RETAINED IOPMinformee *appendNewInformee( IOService * newObject );



	IOReturn addToList(LIBKERN_CONSUMED IOPMinformee *   newInformee );
	IOReturn removeFromList( IOService * theItem );

	LIBKERN_RETURNS_NOT_RETAINED IOPMinformee * firstInList( void );
	LIBKERN_RETURNS_NOT_RETAINED IOPMinformee * nextInList( IOPMinformee * currentItem );

	LIBKERN_RETURNS_NOT_RETAINED IOPMinformee * findItem( IOService * driverOrChild );


	static IORecursiveLock * getSharedRecursiveLock( void );
};
class IOPMPowerSource : public IOService
{
	OSDeclareDefaultStructors(IOPMPowerSource);

	friend class IOPMPowerSourceList;

protected:


	bool settingsChangedSinceUpdate;


	OSDictionary            *properties;

	const OSSymbol *externalConnectedKey;
	const OSSymbol *externalChargeCapableKey;
	const OSSymbol *batteryInstalledKey;
	const OSSymbol *chargingKey;
	const OSSymbol *warnLevelKey;
	const OSSymbol *criticalLevelKey;
	const OSSymbol *currentCapacityKey;
	const OSSymbol *maxCapacityKey;
	const OSSymbol *timeRemainingKey;
	const OSSymbol *amperageKey;
	const OSSymbol *voltageKey;
	const OSSymbol *cycleCountKey;
	const OSSymbol *adapterInfoKey;
	const OSSymbol *locationKey;
	const OSSymbol *errorConditionKey;
	const OSSymbol *manufacturerKey;
	const OSSymbol *modelKey;
	const OSSymbol *serialKey;
	const OSSymbol *batteryInfoKey;


	IOPMPowerSource         *nextInList;

public:


	static IOPMPowerSource *powerSource(void);

	virtual bool init(void) APPLE_KEXT_OVERRIDE;

	virtual void free(void) APPLE_KEXT_OVERRIDE;


	virtual void updateStatus(void);


	bool externalConnected(void);
	bool externalChargeCapable(void);
	bool batteryInstalled(void);
	bool isCharging(void);
	bool atWarnLevel(void);
	bool atCriticalLevel(void);

	unsigned int currentCapacity(void);
	unsigned int maxCapacity(void);
	unsigned int capacityPercentRemaining(void);
	int timeRemaining(void);
	int amperage(void);
	unsigned int voltage(void);
	unsigned int cycleCount(void);
	int adapterInfo(void);
	int location(void);

	OSSymbol *errorCondition(void);
	OSSymbol *manufacturer(void);
	OSSymbol *model(void);
	OSSymbol *serial(void);
	OSDictionary *legacyIOBatteryInfo(void);

	OSObject *getPSProperty(const OSSymbol *);

protected:


	void setExternalConnected(bool);
	void setExternalChargeCapable(bool);
	void setBatteryInstalled(bool);
	void setIsCharging(bool);
	void setAtWarnLevel(bool);
	void setAtCriticalLevel(bool);

	void setCurrentCapacity(unsigned int);
	void setMaxCapacity(unsigned int);
	void setTimeRemaining(int);
	void setAmperage(int);
	void setVoltage(unsigned int);
	void setCycleCount(unsigned int);
	void setAdapterInfo(int);
	void setLocation(int);

	void setErrorCondition(OSSymbol *);
	void setManufacturer(OSSymbol *);
	void setModel(OSSymbol *);
	void setSerial(OSSymbol *);
	void setLegacyIOBatteryInfo(OSDictionary *);


	void setPSProperty(const OSSymbol *, OSObject *);
};
class IOPMPowerSourceList : public OSObject
{
	OSDeclareDefaultStructors(IOPMPowerSourceList);
private:

	IOPMPowerSource         *firstItem;


	unsigned long           length;

public:
	void initialize(void);
	void free(void) APPLE_KEXT_OVERRIDE;

	unsigned long numberOfItems(void);
	IOReturn addToList(IOPMPowerSource *newPowerSource);
	IOReturn removeFromList(IOPMPowerSource *theItem);

	LIBKERN_RETURNS_NOT_RETAINED IOPMPowerSource *firstInList(void);
	LIBKERN_RETURNS_NOT_RETAINED IOPMPowerSource *nextInList(IOPMPowerSource *currentItem);
};
#pragma mark Stray Bitfields



enum {
    kIOPMSetValue                   = (1<<16),
    
    kIOPMSetDesktopMode             = (1<<17),
    
    kIOPMSetACAdaptorConnected      = (1<<18)
};
class IOPowerConnection : public IOService
{
	OSDeclareDefaultStructors(IOPowerConnection);

protected:

	bool            stateKnown;


	IOPMPowerFlags      currentPowerFlags;


	unsigned long       desiredDomainState;


	bool            requestFlag;


	unsigned long       preventIdleSleepFlag;


	unsigned long       preventSystemSleepFlag;


	bool            awaitingAck;


	bool            readyFlag;

public:
	bool            delayChildNotification;

public:

	void setParentKnowsState(bool );


	void setParentCurrentPowerFlags(IOPMPowerFlags );


	bool parentKnowsState(void );


	IOPMPowerFlags parentCurrentPowerFlags(void );


	void setDesiredDomainState(unsigned long );


	unsigned long getDesiredDomainState( void );


	void setChildHasRequestedPower( void );


	bool childHasRequestedPower( void );


	void setPreventIdleSleepFlag(unsigned long );


	bool getPreventIdleSleepFlag( void );


	void setPreventSystemSleepFlag(unsigned long );


	bool getPreventSystemSleepFlag( void );


	void setAwaitingAck( bool );


	bool getAwaitingAck( void );


	void setReadyFlag( bool flag );


	bool getReadyFlag( void ) const;
};
__BEGIN_DECLS
IONotifier *    registerSleepWakeInterest(
	IOServiceInterestHandler, void *, void * = NULL);
IONotifier *    registerPrioritySleepWakeInterest(
	IOServiceInterestHandler handler,
	void * self, void * ref = NULL);
IOReturn        acknowledgeSleepWakeNotification(void * );
IOReturn        vetoSleepWakeNotification(void * PMrefcon);
__END_DECLS


class IOPMrootDomain : public IOService
{
	OSDeclareFinalStructors(IOPMrootDomain);

public:
	static IOPMrootDomain * construct( void );

	virtual bool        start( IOService * provider ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn    setAggressiveness( unsigned long, unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn    getAggressiveness( unsigned long, unsigned long * ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn    sleepSystem( void );
	IOReturn            sleepSystemOptions( OSDictionary *options );

	virtual IOReturn    setProperties( OSObject * ) APPLE_KEXT_OVERRIDE;
	virtual bool        serializeProperties( OSSerialize * s ) const APPLE_KEXT_OVERRIDE;
	virtual OSPtr<OSObject>  copyProperty( const char * aKey ) const APPLE_KEXT_OVERRIDE;



	IOReturn            systemPowerEventOccurred(
		const OSSymbol *event,
		uint32_t intValue );

	IOReturn            systemPowerEventOccurred(
		const OSSymbol *event,
		OSObject *value );


	void                claimSystemWakeEvent(
		IOService     *device,
		IOOptionBits  flags,
		const char    *reason,
		OSObject      *details = NULL );

	void                claimSystemBootEvent(
		IOService     *device,
		IOOptionBits  flags,
		const char    *reason,
		OSObject      *details = NULL );

	void                claimSystemShutdownEvent(
		IOService     *device,
		IOOptionBits  flags,
		const char    *reason,
		OSObject      *details = NULL );

	virtual IOReturn    receivePowerNotification( UInt32 msg );

	virtual void        setSleepSupported( IOOptionBits flags );

	virtual IOOptionBits getSleepSupported( void );

	void                wakeFromDoze( void );

	void                requestUserActive(IOService *driver, const char *reason);



	void                publishFeature( const char *feature );







	void                publishFeature( const char *feature,
	    uint32_t supportedWhere,
	    uint32_t *uniqueFeatureID);




	IOReturn            removePublishedFeature( uint32_t removeFeatureID );



	OSPtr<OSObject>           copyPMSetting( OSSymbol *whichSetting );



	IOReturn            registerPMSettingController(
		const OSSymbol *settings[],
		IOPMSettingControllerCallback callout,
		OSObject   *target,
		uintptr_t  refcon,
		OSObject   **handle);                     



	IOReturn            registerPMSettingController(
		const OSSymbol *settings[],
		uint32_t   supportedPowerSources,
		IOPMSettingControllerCallback callout,
		OSObject   *target,
		uintptr_t  refcon,
		OSObject   **handle);                     

	virtual OSPtr<IONotifier> registerInterest(
		const OSSymbol * typeOfInterest,
		IOServiceInterestHandler handler,
		void * target, void * ref = NULL ) APPLE_KEXT_OVERRIDE;

	virtual IOReturn    callPlatformFunction(
		const OSSymbol *functionName,
		bool waitForFunction,
		void *param1, void *param2,
		void *param3, void *param4 ) APPLE_KEXT_OVERRIDE;


	IOPMDriverAssertionID createPMAssertion(
		IOPMDriverAssertionType whichAssertionsBits,
		IOPMDriverAssertionLevel assertionLevel,
		IOService *ownerService,
		const char *ownerDescription);


	IOReturn setPMAssertionLevel(IOPMDriverAssertionID assertionID, IOPMDriverAssertionLevel assertionLevel);


	IOPMDriverAssertionLevel getPMAssertionLevel(IOPMDriverAssertionType whichAssertionBits);


	IOReturn releasePMAssertion(IOPMDriverAssertionID releaseAssertion);


	IOReturn restartWithStackshot();

	IOReturn    setWakeTime(uint64_t wakeContinuousTime);

	IOReturn acquireDriverKitMatchingAssertion();
	void releaseDriverKitMatchingAssertion();

	void        copyWakeReasonString( char * outBuf, size_t bufSize );

private:
	unsigned long getRUN_STATE(void);

	virtual IOReturn    changePowerStateTo( unsigned long ordinal ) APPLE_KEXT_COMPATIBILITY_OVERRIDE;
	virtual IOReturn    changePowerStateToPriv( unsigned long ordinal );
	virtual IOReturn    requestPowerDomainState( IOPMPowerFlags, IOPowerConnection *, unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual void        powerChangeDone( unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual bool        tellChangeDown( unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual bool        askChangeDown( unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual void        tellChangeUp( unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual void        tellNoChangeDown( unsigned long ) APPLE_KEXT_OVERRIDE;
	virtual IOReturn configureReport(IOReportChannelList   *channels,
	    IOReportConfigureAction action,
	    void                    *result,
	    void                    *destination) APPLE_KEXT_OVERRIDE;
	virtual IOReturn updateReport(IOReportChannelList      *channels,
	    IOReportUpdateAction     action,
	    void                     *result,
	    void                     *destination) APPLE_KEXT_OVERRIDE;

	void             configureReportGated(uint64_t channel_id,
	    uint64_t action,
	    void     *result);
	IOReturn         updateReportGated(uint64_t ch_id,
	    void *result,
	    IOBufferMemoryDescriptor *dest);


public:
	void        tagPowerPlaneService(
		IOService *             service,
		IOPMActions *           actions,
		IOPMPowerStateIndex     maxPowerState );

	void        overrideOurPowerChange(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex *   inOutPowerState,
		IOPMPowerChangeFlags *  inOutChangeFlags );

	void        handleOurPowerChangeStart(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex     powerState,
		IOPMPowerChangeFlags *  inOutChangeFlags );

	void        handleOurPowerChangeDone(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex     powerState,
		IOPMPowerChangeFlags    changeFlags );

	void        overridePowerChangeForService(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex *   inOutPowerState,
		IOPMPowerChangeFlags *  inOutChangeFlags );

	void        handleActivityTickleForDisplayWrangler(
		IOService *             service,
		IOPMActions *           actions );

	void        handleUpdatePowerClientForDisplayWrangler(
		IOService *             service,
		IOPMActions *           actions,
		const OSSymbol *        powerClient,
		IOPMPowerStateIndex     oldPowerState,
		IOPMPowerStateIndex     newPowerState );

	bool        shouldDelayChildNotification(
		IOService *     service );

	void        handlePowerChangeStartForPCIDevice(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex     powerState,
		IOPMPowerChangeFlags *  inOutChangeFlags );

	void        handlePowerChangeDoneForPCIDevice(
		IOService *             service,
		IOPMActions *           actions,
		const IOPMRequest *     request,
		IOPMPowerStateIndex     powerState,
		IOPMPowerChangeFlags    changeFlags );

	void        askChangeDownDone(
		IOPMPowerChangeFlags * inOutChangeFlags,
		bool * cancel );

	void        handlePublishSleepWakeUUID(
		bool shouldPublish);

	void        handleQueueSleepWakeUUID(
		OSObject *obj);

	void        willTellSystemCapabilityDidChange(void);

	void        handleSetDisplayPowerOn(bool powerOn);

	void        willNotifyPowerChildren( IOPMPowerStateIndex newPowerState );

	IOReturn    setMaintenanceWakeCalendar(
		const IOPMCalendarStruct * calendar );

	IOReturn    getSystemSleepType(uint32_t * sleepType, uint32_t * standbyTimer);


	void        acknowledgeSystemWillShutdown( IOService * from );


	void        handlePlatformHaltRestart( UInt32 pe_type );

	IOReturn    shutdownSystem( void );
	IOReturn    restartSystem( void );
	void        handleSleepTimerExpiration( void );
	void        setIdleSleepRevertible( bool revertible );

	bool        activitySinceSleep(void);
	bool        abortHibernation(void);
	void        updateConsoleUsers(void);

	IOReturn    joinAggressiveness( IOService * service );
	void        handleAggressivesRequests( void );

	void        kdebugTrace(uint32_t event, uint64_t regId,
	    uintptr_t param1, uintptr_t param2, uintptr_t param3 = 0);
	void        tracePoint(uint8_t point);
	void        traceDetail(uint32_t msgType, uint32_t msgIndex, uint32_t delay);
	void        traceNotification(OSObject *notifier, bool start, uint64_t ts = 0, uint32_t msgIndex = UINT_MAX);
	void        traceNotificationAck(OSObject *notifier, uint32_t delay_ms);
	void        traceNotificationResponse(OSObject *object, uint32_t delay_ms, uint32_t ack_time_us);
	void        traceFilteredNotification(OSObject *notifier);
	const char * getNotificationClientName(OSObject *notifier);

	void        startSpinDump(uint32_t spindumpKind);

	bool        systemMessageFilter(
		void * object, void * arg1, void * arg2, void * arg3 );

	bool        updatePreventIdleSleepList(
		IOService * service, bool addNotRemove );
	void        updatePreventSystemSleepList(
		IOService * service, bool addNotRemove );

	bool        updatePreventIdleSleepListInternal(
		IOService * service, bool addNotRemove, unsigned int oldCount);
	unsigned int idleSleepPreventersCount();

	void        publishPMSetting(
		const OSSymbol * feature, uint32_t where, uint32_t * featureID );

	void        pmStatsRecordEvent(
		int             eventIndex,
		AbsoluteTime    timestamp);

	void        pmStatsRecordApplicationResponse(
		const OSSymbol      *response,
		const char          *name,
		int                 messageType,
		uint32_t            delay_ms,
		uint64_t            id,
		OSObject            *object,
		IOPMPowerStateIndex ps = 0,
		bool                async = false);

	void        copyShutdownReasonString( char * outBuf, size_t bufSize );
	void        lowLatencyAudioNotify(uint64_t time, boolean_t state);

	void        takeStackshot(bool restart);
	void        sleepWakeDebugTrig(bool restart);
	void        sleepWakeDebugEnableWdog();
	bool        sleepWakeDebugIsWdogEnabled();
	void        sleepWakeDebugSaveSpinDumpFile();
	bool        checkShutdownTimeout();
	void        panicWithShutdownLog(uint32_t timeoutInMs) __abortlike;
	uint32_t    getWatchdogTimeout();
	void        deleteStackshot();

private:
	friend class PMSettingObject;
	friend class RootDomainUserClient;
	friend class PMAssertionsTracker;

	static IOReturn sysPowerDownHandler( void * target, void * refCon,
	    UInt32 messageType, IOService * service,
	    void * messageArgument, vm_size_t argSize );

	static IOReturn displayWranglerNotification( void * target, void * refCon,
	    UInt32 messageType, IOService * service,
	    void * messageArgument, vm_size_t argSize );

	static IOReturn rootBusyStateChangeHandler( void * target, void * refCon,
	    UInt32 messageType, IOService * service,
	    void * messageArgument, vm_size_t argSize );

	void initializeBootSessionUUID( void );

	void fullWakeDelayedWork( void );

	OSPtr<IOService>        wrangler;
	OSPtr<OSDictionary>     wranglerIdleSettings;

	IOLock                  *featuresDictLock;
	IOLock                  *wakeEventLock;
	IOPMPowerStateQueue     *pmPowerStateQueue;

	OSPtr<OSArray>           allowedPMSettings;
	OSPtr<OSArray>           noPublishPMSettings;
	OSPtr<PMTraceWorker>     pmTracer;
	PMAssertionsTracker     *pmAssertions;


	IOLock                  *settingsCtrlLock;
	OSPtr<OSDictionary>      settingsCallbacks;
	OSPtr<OSDictionary>      fPMSettingsDict;


	OSPtr<const OSSymbol>   _statsNameKey;
	OSPtr<const OSSymbol>   _statsPIDKey;
	OSPtr<const OSSymbol>   _statsTimeMSKey;
	OSPtr<const OSSymbol>   _statsResponseTypeKey;
	OSPtr<const OSSymbol>   _statsMessageTypeKey;
	OSPtr<const OSSymbol>   _statsPowerCapsKey;
	uint32_t                sleepCnt;
	uint32_t                darkWakeCnt;
	uint32_t                displayWakeCnt;

	OSPtr<OSString>         queuedSleepWakeUUIDString;
	OSPtr<OSArray>          pmStatsAppResponses;
	IOLock                  *pmStatsLock;

	void                    *sleepDelaysReport; 
	uint32_t                sleepDelaysClientCnt;
	uint64_t                ts_sleepStart;
	uint64_t                wake2DarkwakeDelay;  

	void                    *assertOnWakeReport;
	uint32_t                assertOnWakeClientCnt;
	clock_sec_t             assertOnWakeSecs;   

	bool                    uuidPublished;


	bool                    idleSleepEnabled;
	uint32_t                sleepSlider;
	uint32_t                idleMilliSeconds;
	bool                    idleSleepRevertible;


	uint32_t                extraSleepDelay;


	thread_call_t           extraSleepTimer;
	thread_call_t           powerButtonDown;
	thread_call_t           powerButtonUp;
	thread_call_t           diskSyncCalloutEntry;
	thread_call_t           fullWakeThreadCall;
	thread_call_t           updateConsoleUsersEntry;


	uint32_t                _desiredCapability;
	uint32_t                _currentCapability;
	uint32_t                _pendingCapability;
	uint32_t                _highestCapability;
	OSPtr<OSSet>            _joinedCapabilityClients;
	uint32_t                _systemStateGeneration;


	enum {
		kSystemMessageClientPowerd    = 0x01,
		kSystemMessageClientLegacyApp = 0x02,
		kSystemMessageClientKernel    = 0x04,
		kSystemMessageClientAll       = 0x07
	};
	uint32_t                _systemMessageClientMask;


	enum {
		kSystemTransitionNone         = 0,
		kSystemTransitionSleep        = 1,
		kSystemTransitionWake         = 2,
		kSystemTransitionCapability   = 3,
		kSystemTransitionNewCapClient = 4
	}                       _systemTransitionType;

	unsigned int            systemBooting           :1;
	unsigned int            systemShutdown          :1;
	unsigned int            systemDarkWake          :1;
	unsigned int            clamshellExists         :1;
	unsigned int            clamshellClosed         :1;
	unsigned int            clamshellDisabled       :1;
	unsigned int            desktopMode             :1;
	unsigned int            acAdaptorConnected      :1;

	unsigned int            clamshellIgnoreClose    :1;
	unsigned int            idleSleepTimerPending   :1;
	unsigned int            userDisabledAllSleep    :1;
	unsigned int            ignoreTellChangeDown    :1;
	unsigned int            wranglerAsleep          :1;
	unsigned int            darkWakeExit            :1;
	unsigned int            _preventUserActive      :1;
	unsigned int            darkWakePowerClamped    :1;

	unsigned int            capabilityLoss          :1;
	unsigned int            pciCantSleepFlag        :1;
	unsigned int            pciCantSleepValid       :1;
	unsigned int            darkWakeLogClamp        :1;
	unsigned int            darkWakeToSleepASAP     :1;
	unsigned int            darkWakeMaintenance     :1;
	unsigned int            darkWakeSleepService    :1;
	unsigned int            darkWakePostTickle      :1;

	unsigned int            sleepTimerMaintenance   :1;
	unsigned int            sleepToStandby          :1;
	unsigned int            lowBatteryCondition     :1;
	unsigned int            hibernateDisabled       :1;
	unsigned int            hibernateRetry          :1;
	unsigned int            wranglerTickled         :1;
	unsigned int            userIsActive            :1;
	unsigned int            userWasActive           :1;

	unsigned int            displayIdleForDemandSleep :1;
	unsigned int            darkWakeHibernateError  :1;
	unsigned int            thermalWarningState     :1;
	unsigned int            toldPowerdCapWillChange :1;
	unsigned int            displayPowerOnRequested :1;
	unsigned int            isRTCAlarmWake          :1;
	unsigned int            wranglerPowerOff        :1;
	unsigned int            thermalEmergencyState   :1;

	uint8_t                 tasksSuspended;
	uint8_t                 tasksSuspendState;
	uint32_t                hibernateMode;
	AbsoluteTime            userActivityTime;
	AbsoluteTime            userActivityTime_prev;
	uint32_t                userActivityCount;
	uint32_t                userActivityAtSleep;
	uint32_t                lastSleepReason;
	uint32_t                fullToDarkReason;
	uint32_t                hibernateAborted;
	uint8_t                 standbyNixed;
	uint8_t                 resetTimers;

	enum FullWakeReason {
		kFullWakeReasonNone = 0,
		kFullWakeReasonLocalUser = 1,
		kFullWakeReasonDisplayOn = 2,
		fFullWakeReasonDisplayOnAndLocalUser = 3
	};
	uint32_t                fullWakeReason;

	enum {
		kClamshellSleepDisableInternal = 0x01,
		kClamshellSleepDisablePowerd   = 0x02
	};
	uint32_t                clamshellSleepDisableMask;


	int32_t                 idxPMCPUClamshell;
	int32_t                 idxPMCPULimitedPower;

	IOOptionBits            platformSleepSupport;
	uint32_t                _debugWakeSeconds;

	queue_head_t            aggressivesQueue;
	thread_call_t           aggressivesThreadCall;
	OSPtr<OSData>                aggressivesData;

	AbsoluteTime            userBecameInactiveTime;


	OSPtr<IOService>        pciHostBridgeDevice;
	OSPtr<IOService>        pciHostBridgeDriver;

	OSPtr<IONotifier>       systemCapabilityNotifier;

	typedef struct {
		uint32_t            pid;
		uint32_t            refcount;
	} PMNotifySuspendedStruct;

	uint32_t                pmSuspendedCapacity;
	uint32_t                pmSuspendedSize;
	PMNotifySuspendedStruct *pmSuspendedPIDS;

	OSPtr<OSSet>            preventIdleSleepList;
	OSPtr<OSSet>            preventSystemSleepList;

	OSPtr<const OSSymbol>   _nextScheduledAlarmType;
	clock_sec_t             _nextScheduledAlarmUTC;
	clock_sec_t             _calendarWakeAlarmUTC;
	UInt32                  _scheduledAlarmMask;
	UInt32                  _userScheduledAlarmMask;

	volatile uint32_t   swd_lock;
	void  *             swd_buffer;
	uint32_t            swd_flags;
	void *              swd_compressed_buffer;
	void  *             swd_spindump_buffer;
	thread_t            notifierThread;
	OSPtr<OSObject>     notifierObject;

	OSPtr<IOBufferMemoryDescriptor>    swd_spindump_memDesc;
	OSPtr<IOBufferMemoryDescriptor>    swd_memDesc;


	OSPtr<OSArray>       _systemWakeEventsArray;
	bool                 _acceptSystemWakeEvents;

	
	IOPMCalendarStruct   _aotWakeTimeCalendar;
	OSPtr<IOTimerEventSource> _aotTimerES;
	clock_sec_t          _aotWakeTimeUTC;
	uint64_t             _aotTestTime;
	uint64_t             _aotTestInterval;
	uint32_t             _aotPendingFlags;
public:
	IOPMAOTMetrics     * _aotMetrics;
	uint8_t              _aotMode;
private:
	uint8_t              _aotNow;
	uint8_t              _aotTasksSuspended;
	uint8_t              _aotTimerScheduled;
	uint8_t              _aotReadyToFullWake;
	uint64_t             _aotLastWakeTime;
	uint64_t             _aotWakeTimeContinuous;
	uint64_t             _aotWakePreWindow;
	uint64_t             _aotWakePostWindow;
	uint64_t             _aotLingerTime;

	size_t               _driverKitMatchingAssertionCount;
	IOPMDriverAssertionID _driverKitMatchingAssertion;

	bool        aotShouldExit(bool software);
	void        aotExit(bool cps);
	void        aotEvaluate(IOTimerEventSource * timer);
public:
	bool        isAOTMode(void);
private:
	
	enum {
		kTasksSuspendUnsuspended = 0,
		kTasksSuspendSuspended   = 1,
		kTasksSuspendNoChange    = -1,
	};
	bool        updateTasksSuspend(int newTasksSuspended, int newAOTTasksSuspended);
	int         findSuspendedPID(uint32_t pid, uint32_t *outRefCount);


	IOReturn    privateSleepSystem( uint32_t sleepReason );
	void        reportUserInput( void );
	void        updateUserActivity( void );
	void        setDisableClamShellSleep( bool );
	void        setClamShellSleepDisable(bool disable, uint32_t bitmask);
	bool        checkSystemSleepAllowed( IOOptionBits options,
	    uint32_t sleepReason );
	bool        checkSystemSleepEnabled( void );
	bool        checkSystemCanSleep( uint32_t sleepReason );
	bool        checkSystemCanSustainFullWake( void );
	bool        checkSystemCanAbortIdleSleep( void );
	void        adjustPowerState( bool sleepASAP = false );
	bool        attemptIdleSleepAbort( void );
	void        setQuickSpinDownTimeout( void );
	void        restoreUserSpinDownTimeout( void );

	bool        shouldSleepOnClamshellClosed(void );
	bool        shouldSleepOnRTCAlarmWake(void );
	void        sendClientClamshellNotification( void );


	void        informCPUStateChange( uint32_t type, uint32_t value );

	void        dispatchPowerEvent( uint32_t event, void * arg0, uint64_t arg1 );
	void        handlePowerNotification( UInt32 msg );

	IOReturn    setPMSetting(const OSSymbol *, OSObject *);

	void        startIdleSleepTimer( uint32_t inMilliSeconds );
	void        cancelIdleSleepTimer( void );
	uint32_t    getTimeToIdleSleep( void );

	IOReturn    setAggressiveness(
		unsigned long type,
		unsigned long value,
		IOOptionBits  options );

	void        synchronizeAggressives(
		queue_head_t * services,
		const AggressivesRecord * array,
		int count );

	void        broadcastAggressives(
		const AggressivesRecord * array,
		int count );

	IOReturn    setPMAssertionUserLevels(IOPMDriverAssertionType);

	void        publishSleepWakeUUID( bool shouldPublish );

	void        evaluatePolicy( int stimulus, uint32_t arg = 0 );
	void        requestFullWake( FullWakeReason reason );
	void        willEnterFullWake( void );

	void        evaluateAssertions(IOPMDriverAssertionType newAssertions,
	    IOPMDriverAssertionType oldAssertions);

	void        deregisterPMSettingObject( PMSettingObject * pmso );

	uint32_t    checkForValidDebugData(const char *fname, vfs_context_t *ctx,
	    void *tmpBuf, struct vnode **vp);
	void        getFailureData(thread_t *thread, char *failureStr, size_t strLen);
	void        saveFailureData2File();
	void        tracePhase2String(uint32_t tracePhase, const char **phaseString, const char **description);
	void        sleepWakeDebugMemAlloc();
	void        sleepWakeDebugSpinDumpMemAlloc();
	errno_t     sleepWakeDebugSaveFile(const char *name, char *buf, int len);

	IOReturn    changePowerStateWithOverrideTo( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
	IOReturn    changePowerStateWithTagToPriv( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );
	IOReturn    changePowerStateWithTagTo( IOPMPowerStateIndex ordinal, IOPMRequestTag tag );


	bool        latchDisplayWranglerTickle( bool latch );
	void        setDisplayPowerOn( uint32_t options );

	void        acceptSystemWakeEvents( uint32_t control );
	void        systemDidNotSleep( void );
	void        preventTransitionToUserActive( bool prevent );
	void        setThermalState(OSObject *value);
	void        copySleepPreventersList(OSArray  **idleSleepList, OSArray  **systemSleepList);
	void        copySleepPreventersListWithID(OSArray  **idleSleepList, OSArray  **systemSleepList);
	void        recordRTCAlarm(const OSSymbol *type, OSObject *object);
	void        scheduleImmediateDebugWake( void );

	
	OSPtr<OSDictionary>   lowLatencyAudioNotifierDict;
	OSPtr<OSNumber>       lowLatencyAudioNotifyStateVal;
	OSPtr<OSNumber>       lowLatencyAudioNotifyTimestampVal;
	OSPtr<const OSSymbol> lowLatencyAudioNotifyStateSym;
	OSPtr<const OSSymbol> lowLatencyAudioNotifyTimestampSym;
};
class IORootParent : public IOService
{
	OSDeclareFinalStructors(IORootParent);

public:
	static void initialize( void );
	virtual OSPtr<OSObject> copyProperty( const char * aKey ) const APPLE_KEXT_OVERRIDE;
	bool start( IOService * nub ) APPLE_KEXT_OVERRIDE;
	void shutDownSystem( void );
	void restartSystem( void );
	void sleepSystem( void );
	void dozeSystem( void );
	void sleepToDoze( void );
	void wakeSystem( void );
};
void     IOHibernateSystemInit(IOPMrootDomain * rootDomain);
IOReturn IOHibernateSystemSleep(void);
IOReturn IOHibernateIOKitSleep(void);
IOReturn IOHibernateSystemHasSlept(void);
IOReturn IOHibernateSystemWake(void);
IOReturn IOHibernateSystemPostWake(bool now);
uint32_t IOHibernateWasScreenLocked(void);
void     IOHibernateSetScreenLocked(uint32_t lockState);
void     IOHibernateSetWakeCapabilities(uint32_t capability);
void     IOHibernateSystemRestart(void);
class IORTCController : public IOService
{
	OSDeclareAbstractStructors(IORTCController);

public:

	virtual IOReturn getRealTimeClock( UInt8 * currentTime, IOByteCount * length ) = 0;
	virtual IOReturn setRealTimeClock( UInt8 * newTime ) = 0;
};
class IORTC : public IOService
{
	OSDeclareAbstractStructors(IORTC);

protected:


	struct ExpansionData { };
	ExpansionData *iortc_reserved __unused;

public:

	virtual long            getGMTTimeOfDay( void ) = 0;
	virtual void            setGMTTimeOfDay( long secs ) = 0;

	virtual void                    getUTCTimeOfDay( clock_sec_t * secs, clock_nsec_t * nsecs );
	virtual void                    setUTCTimeOfDay( clock_sec_t secs, clock_nsec_t nsecs );

	virtual void            setAlarmEnable( IOOptionBits message ) = 0;

	virtual IOReturn        getMonotonicClockOffset( int64_t * usecs );
	virtual IOReturn        setMonotonicClockOffset( int64_t usecs );
	virtual IOReturn        getMonotonicClockAndTimestamp( uint64_t * usecs, uint64_t *mach_absolute_time );


	OSMetaClassDeclareReservedUnused(IORTC, 0);
	OSMetaClassDeclareReservedUnused(IORTC, 1);
	OSMetaClassDeclareReservedUnused(IORTC, 2);
	OSMetaClassDeclareReservedUnused(IORTC, 3);
	OSMetaClassDeclareReservedUnused(IORTC, 4);
	OSMetaClassDeclareReservedUnused(IORTC, 5);
	OSMetaClassDeclareReservedUnused(IORTC, 6);
	OSMetaClassDeclareReservedUnused(IORTC, 7);
};
IOSKMemoryArrayRef  IOSKMemoryArrayCreate( const IOSKMemoryRef refs[__counted_by(count)],
    uint32_t count );
void                IOSKMemoryDestroy( IOSK_CONSUMED IOSKMemoryRef reference );
IOSKMemoryMapRef    IOSKMemoryMapToTask( IOSKMemoryRef reference,
    task_t intoTask,
    mach_vm_address_t * mapAddr,
    mach_vm_size_t * mapSize );
IOSKMemoryMapRef    IOSKMemoryMapToKernelTask( IOSKMemoryRef reference,
    mach_vm_address_t * mapAddr,
    mach_vm_size_t * mapSize );
void                IOSKMemoryMapDestroy(
	IOSK_CONSUMED IOSKMemoryMapRef reference );
IOReturn            IOSKMemoryReclaim( IOSKMemoryRef reference );
IOReturn            IOSKMemoryDiscard( IOSKMemoryRef reference );
IOReturn            IOSKMemoryWire( IOSKMemoryRef reference );
IOReturn            IOSKMemoryUnwire( IOSKMemoryRef reference );
IOSKArenaRef
IOSKArenaCreate( IOSKRegionRef * regionList, IOSKCount regionCount );
void
IOSKArenaDestroy( IOSK_CONSUMED IOSKArenaRef arena );
void
IOSKArenaRedirect( IOSKArenaRef arena );
IOSKRegionRef
IOSKRegionCreate( const IOSKRegionSpec * regionSpec,
    IOSKSize segmentSize, IOSKCount segmentCount );
void
IOSKRegionDestroy( IOSK_CONSUMED IOSKRegionRef region );
IOReturn
IOSKRegionSetBuffer( IOSKRegionRef region, IOSKIndex segmentIndex,
    IOSKMemoryBufferRef buffer );
void
IOSKRegionClearBuffer( IOSKRegionRef region, IOSKIndex segmentIndex );
void
IOSKRegionClearBufferDebug( IOSKRegionRef region, IOSKIndex segmentIndex,
    IOSKMemoryBufferRef * prevBufferRef );
IOSKMapperRef
IOSKMapperCreate( IOSKArenaRef arena, task_t task );
void
IOSKMapperDestroy( IOSK_CONSUMED IOSKMapperRef mapper );
void
IOSKMapperRedirect( IOSKMapperRef mapper );
IOReturn
IOSKMapperGetAddress( IOSKMapperRef mapper,
    mach_vm_address_t * address, mach_vm_size_t * size );
boolean_t
IOSKBufferIsWired( IOSKMemoryBufferRef buffer );
class IOWatchDogTimer : public IOService
{
	OSDeclareAbstractStructors(IOWatchDogTimer);

protected:
	IONotifier *notifier;
	struct ExpansionData { };
	APPLE_KEXT_WSHADOW_PUSH;
	ExpansionData *reserved;
	APPLE_KEXT_WSHADOW_POP;

public:
	virtual bool start(IOService *provider) APPLE_KEXT_OVERRIDE;
	virtual void stop(IOService *provider) APPLE_KEXT_OVERRIDE;
	virtual IOReturn setProperties(OSObject *properties) APPLE_KEXT_OVERRIDE;
	virtual void setWatchDogTimer(UInt32 timeOut) = 0;

	OSMetaClassDeclareReservedUnused(IOWatchDogTimer, 0);
	OSMetaClassDeclareReservedUnused(IOWatchDogTimer, 1);
	OSMetaClassDeclareReservedUnused(IOWatchDogTimer, 2);
	OSMetaClassDeclareReservedUnused(IOWatchDogTimer, 3);
};
class GenericInterruptController : public IOInterruptController
{
	IODeclareDefaultStructors(GenericInterruptController);

public:


	virtual bool start(IOService *provider);



	virtual IOReturn getInterruptType(IOService *nub, int source,
	    int *interruptType);



	virtual IOInterruptAction getInterruptHandlerAddress(void);


	virtual IOReturn handleInterrupt(void *refCon,
	    IOService *nub, int source);






	virtual bool vectorCanBeShared(long vectorNumber, IOInterruptVector *vector);



	virtual void initVector(long vectorNumber, IOInterruptVector *vector);


	virtual void disableVectorHard(long vectorNumber, IOInterruptVector *vector);


	virtual void enableVector(long vectorNumber, IOInterruptVector *vector);


	virtual void causeVector(long vectorNumber, IOInterruptVector *vector);
};
aes_rval aes_encrypt_key128(const unsigned char *key, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt_key256(const unsigned char *key, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt(const unsigned char *in, unsigned char *out, aes_encrypt_ctx cx[1]);
aes_rval aes_encrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
    unsigned char *out_blk, aes_encrypt_ctx cx[1]);
aes_rval aes_decrypt_key(const unsigned char *key, int key_len, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_key128(const unsigned char *key, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_key256(const unsigned char *key, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt(const unsigned char *in, unsigned char *out, aes_decrypt_ctx cx[1]);
aes_rval aes_decrypt_cbc(const unsigned char *in_blk, const unsigned char *in_iv, unsigned int num_blk,
    unsigned char *out_blk, aes_decrypt_ctx cx[1]);
aes_rval aes_encrypt_key_gcm(const unsigned char *key, int key_len, ccgcm_ctx *ctx);
aes_rval aes_encrypt_key_with_iv_gcm(const unsigned char *key, int key_len, const unsigned char *in_iv, ccgcm_ctx *ctx);
aes_rval aes_encrypt_set_iv_gcm(const unsigned char *in_iv, unsigned int len, ccgcm_ctx *ctx);
aes_rval aes_encrypt_reset_gcm(ccgcm_ctx *ctx);
aes_rval aes_encrypt_inc_iv_gcm(unsigned char *out_iv, ccgcm_ctx *ctx);
aes_rval aes_encrypt_aad_gcm(const unsigned char *aad, unsigned int aad_bytes, ccgcm_ctx *ctx);
aes_rval aes_encrypt_gcm(const unsigned char *in_blk, unsigned int num_bytes, unsigned char *out_blk, ccgcm_ctx *ctx);
aes_rval aes_encrypt_finalize_gcm(unsigned char *tag, size_t tag_bytes, ccgcm_ctx *ctx);
size_t aes_encrypt_get_ctx_size_gcm(void);
aes_rval aes_decrypt_key_gcm(const unsigned char *key, int key_len, ccgcm_ctx *ctx);
aes_rval aes_decrypt_key_with_iv_gcm(const unsigned char *key, int key_len, const unsigned char *in_iv, ccgcm_ctx *ctx);
aes_rval aes_decrypt_set_iv_gcm(const unsigned char *in_iv, size_t len, ccgcm_ctx *ctx);
aes_rval aes_decrypt_reset_gcm(ccgcm_ctx *ctx);
aes_rval aes_decrypt_inc_iv_gcm(unsigned char *out_iv, ccgcm_ctx *ctx);
aes_rval aes_decrypt_aad_gcm(const unsigned char *aad, unsigned int aad_bytes, ccgcm_ctx *ctx);
aes_rval aes_decrypt_gcm(const unsigned char *in_blk, unsigned int num_bytes, unsigned char *out_blk, ccgcm_ctx *ctx);
aes_rval aes_decrypt_finalize_gcm(unsigned char *tag, size_t tag_bytes, ccgcm_ctx *ctx);
size_t aes_decrypt_get_ctx_size_gcm(void);
__BEGIN_DECLS



typedef 

extern void MD5Init(MD5_CTX *);
extern void MD5Update(MD5_CTX *, const void *, unsigned int);
extern void MD5Final(unsigned char [MD5_DIGEST_LENGTH], MD5_CTX *);
extern void SHA1Update(SHA1_CTX *, const void *, size_t);
extern void SHA1Final(void *, SHA1_CTX *);
void SHA256_Init(SHA256_CTX *ctx);
void SHA256_Update(SHA256_CTX *ctx, const void *data, size_t len);
void SHA256_Final(void *digest, SHA256_CTX *ctx);
void SHA384_Init(SHA384_CTX *ctx);
void SHA384_Update(SHA384_CTX *ctx, const void *data, size_t len);
void SHA384_Final(void *digest, SHA384_CTX *ctx);
void SHA512_Init(SHA512_CTX *ctx);
void SHA512_Update(SHA512_CTX *ctx, const void *data, size_t len);
void SHA512_Final(void *digest, SHA512_CTX *ctx);
static __inline__
unsigned short 
NXSwapShort(
    unsigned short inv
)
{
    return (unsigned short)OSSwapInt16((uint16_t)inv);
}

static __inline__
unsigned int
NXSwapInt(
    unsigned int inv
)
{
    return (unsigned int)OSSwapInt32((uint32_t)inv);
}

static __inline__
unsigned long
NXSwapLong(
    unsigned long inv
)
{
    return (unsigned long)OSSwapInt32((uint32_t)inv);
}

static __inline__
unsigned long long
NXSwapLongLong(
    unsigned long long inv
)
{
    return (unsigned long long)OSSwapInt64((uint64_t)inv);
}

static __inline__ NXSwappedFloat
NXConvertHostFloatToSwapped(float x)
{
    union fconv {
        float number;
        NXSwappedFloat sf;
    } u;
    u.number = x;
    return u.sf;
}

static __inline__ float
NXConvertSwappedFloatToHost(NXSwappedFloat x)
{
    union fconv {
        float number;
        NXSwappedFloat sf;
    } u;
    u.sf = x;
    return u.number;
}

static __inline__ NXSwappedDouble
NXConvertHostDoubleToSwapped(double x)
{
    union dconv {
        double number;
        NXSwappedDouble sd;
    } u;
    u.number = x;
    return u.sd;
}

static __inline__ double
NXConvertSwappedDoubleToHost(NXSwappedDouble x)
{
    union dconv {
        double number;
        NXSwappedDouble sd;
    } u;
    u.sd = x;
    return u.number;
}

static __inline__ NXSwappedFloat
NXSwapFloat(NXSwappedFloat x)
{ 
    return (NXSwappedFloat)OSSwapInt32((uint32_t)x);  
}

static __inline__ NXSwappedDouble   
NXSwapDouble(NXSwappedDouble x)
{  
    return (NXSwappedDouble)OSSwapInt64((uint64_t)x);
}



enum NXByteOrder {
    NX_UnknownByteOrder,
    NX_LittleEndian,
    NX_BigEndian
};
CC_PTRCHECK_CAPABLE_HEADER()



























CC_NONNULL((2))
void cc_clear(size_t len, void *cc_sized_by(len) dst);
CC_INLINE CC_NONNULL((2))
void cc_xor(size_t size, void *cc_sized_by(size) r, const void *cc_sized_by(size) s, const void *cc_sized_by(size) t) {
    uint8_t *_r=(uint8_t *)r;
    const uint8_t *_s=(const uint8_t *)s;
    const uint8_t *_t=(const uint8_t *)t;
    size_t _size = size;
    while (_size--) {
        _r[_size] = _s[_size] ^ _t[_size];
    }
}


CC_NONNULL((2, 3))
int cc_cmp_safe (size_t num, const void * cc_sized_by(num) ptr1, const void * cc_sized_by(num) ptr2);
CC_PTRCHECK_CAPABLE_HEADER()



extern const struct ccmode_ecb ccaes_ltc_ecb_decrypt_mode;
const struct ccmode_ecb *ccaes_ecb_encrypt_mode(void);
const struct ccmode_cbc *ccaes_cbc_encrypt_mode(void);
const struct ccmode_cfb *ccaes_cfb_encrypt_mode(void);
const struct ccmode_cfb8 *ccaes_cfb8_encrypt_mode(void);
const struct ccmode_xts *ccaes_xts_encrypt_mode(void);
const struct ccmode_gcm *ccaes_gcm_encrypt_mode(void);
const struct ccmode_ccm *ccaes_ccm_encrypt_mode(void);
const struct ccmode_ecb *ccaes_ecb_decrypt_mode(void);
const struct ccmode_cbc *ccaes_cbc_decrypt_mode(void);
const struct ccmode_cfb *ccaes_cfb_decrypt_mode(void);
const struct ccmode_cfb8 *ccaes_cfb8_decrypt_mode(void);
const struct ccmode_xts *ccaes_xts_decrypt_mode(void);
const struct ccmode_gcm *ccaes_gcm_decrypt_mode(void);
const struct ccmode_ccm *ccaes_ccm_decrypt_mode(void);
const struct ccmode_ctr *ccaes_ctr_crypt_mode(void);
const struct ccmode_ofb *ccaes_ofb_crypt_mode(void);
const struct ccmode_siv *ccaes_siv_encrypt_mode(void);
const struct ccmode_siv *ccaes_siv_decrypt_mode(void);
const struct ccmode_siv_hmac *ccaes_siv_hmac_sha256_encrypt_mode(void);
const struct ccmode_siv_hmac *ccaes_siv_hmac_sha256_decrypt_mode(void);
int ccaes_unwind(size_t key_nbytes, const void *cc_sized_by(key_nbytes) key, void *cc_sized_by(key_nbytes) out);
CC_PTRCHECK_CAPABLE_HEADER()


enum {
    CCASN1_EOL               = 0x00,
    CCASN1_BOOLEAN           = 0x01,
    CCASN1_INTEGER           = 0x02,
    CCASN1_BIT_STRING        = 0x03,
    CCASN1_OCTET_STRING      = 0x04,
    CCASN1_NULL              = 0x05,
    CCASN1_OBJECT_IDENTIFIER = 0x06,
    CCASN1_OBJECT_DESCRIPTOR = 0x07,
    
    CCASN1_REAL              = 0x09,
    CCASN1_ENUMERATED        = 0x0a,
    CCASN1_EMBEDDED_PDV      = 0x0b,
    CCASN1_UTF8_STRING       = 0x0c,
    
    
    
    CCASN1_SEQUENCE          = 0x10,
    CCASN1_SET               = 0x11,
    CCASN1_NUMERIC_STRING    = 0x12,
    CCASN1_PRINTABLE_STRING  = 0x13,
    CCASN1_T61_STRING        = 0x14,
    CCASN1_VIDEOTEX_STRING   = 0x15,
    CCASN1_IA5_STRING        = 0x16,
    CCASN1_UTC_TIME          = 0x17,
    CCASN1_GENERALIZED_TIME  = 0x18,
    CCASN1_GRAPHIC_STRING    = 0x19,
    CCASN1_VISIBLE_STRING    = 0x1a,
    CCASN1_GENERAL_STRING    = 0x1b,
    CCASN1_UNIVERSAL_STRING  = 0x1c,
    
    CCASN1_BMP_STRING        = 0x1e,
    CCASN1_HIGH_TAG_NUMBER   = 0x1f,
    CCASN1_TELETEX_STRING    = CCASN1_T61_STRING,

    CCASN1_TAG_MASK			 = 0xff,
    CCASN1_TAGNUM_MASK		 = 0x1f,

    CCASN1_METHOD_MASK		 = 0x20,
    CCASN1_PRIMITIVE		 = 0x00,
    CCASN1_CONSTRUCTED		 = 0x20,

    CCASN1_CLASS_MASK		 = 0xc0,
    CCASN1_UNIVERSAL		 = 0x00,
    CCASN1_APPLICATION		 = 0x40,
    CCASN1_CONTEXT_SPECIFIC	 = 0x80,
    CCASN1_PRIVATE			 = 0xc0,

    CCASN1_CONSTRUCTED_SET = CCASN1_SET | CCASN1_CONSTRUCTED,
    CCASN1_CONSTRUCTED_SEQUENCE = CCASN1_SEQUENCE | CCASN1_CONSTRUCTED,
};
CC_PURE CC_NONNULL((1))
size_t ccoid_size(ccoid_t oid);
CC_PURE CC_NONNULL((1))
const unsigned char *cc_indexable ccoid_payload(ccoid_t oid);
CC_PURE
bool ccoid_equal(ccoid_t oid1, ccoid_t oid2);
CC_PTRCHECK_CAPABLE_HEADER()


typedef 


typedef 











typedef 





const struct ccchacha20poly1305_info *ccchacha20poly1305_info(void);
int	ccchacha20poly1305_init(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_KEY_NBYTES) key);
int ccchacha20poly1305_reset(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx);
int ccchacha20poly1305_setnonce(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_NONCE_NBYTES) nonce);
int ccchacha20poly1305_incnonce(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, uint8_t *cc_counted_by(CCCHACHA20POLY1305_NONCE_NBYTES) nonce);
int	ccchacha20poly1305_aad(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) aad);
int	ccchacha20poly1305_encrypt(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) ptext, void *cc_sized_by(nbytes) ctext);
int	ccchacha20poly1305_finalize(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, uint8_t *cc_counted_by(CCCHACHA20POLY1305_TAG_NBYTES) tag);
int	ccchacha20poly1305_decrypt(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) ctext, void *cc_sized_by(nbytes) ptext);
int	ccchacha20poly1305_verify(const struct ccchacha20poly1305_info *info, ccchacha20poly1305_ctx *ctx, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_TAG_NBYTES) tag);
int ccchacha20poly1305_encrypt_oneshot(const struct ccchacha20poly1305_info *info, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_KEY_NBYTES) key, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_NONCE_NBYTES) nonce, size_t aad_nbytes, const void *cc_sized_by(aad_nbytes) aad, size_t ptext_nbytes, const void *cc_sized_by(ptext_nbytes) ptext, void *cc_sized_by(ptext_nbytes) ctext, uint8_t *cc_counted_by(CCCHACHA20POLY1305_TAG_NBYTES) tag);
int ccchacha20poly1305_decrypt_oneshot(const struct ccchacha20poly1305_info *info, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_KEY_NBYTES) key, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_NONCE_NBYTES) nonce, size_t aad_nbytes, const void *cc_sized_by(aad_nbytes) aad, size_t ctext_nbytes, const void *cc_sized_by(ctext_nbytes) ctext, void *cc_sized_by(ctext_nbytes) ptext, const uint8_t *cc_counted_by(CCCHACHA20POLY1305_TAG_NBYTES) tag);
CC_PTRCHECK_CAPABLE_HEADER()




typedef struct cccmac_ctx* cccmac_ctx_t;
int cccmac_one_shot_generate(const struct ccmode_cbc *cbc,
                        size_t key_nbytes, const void *cc_sized_by(key_nbytes) key,
                        size_t data_nbytes, const void *cc_sized_by(data_nbytes) data,
                        size_t mac_nbytes, void *cc_sized_by(mac_nbytes) mac);
int cccmac_one_shot_verify(const struct ccmode_cbc *cbc,
                           size_t key_nbytes, const void *cc_sized_by(key_nbytes) key,
                           size_t data_nbytes, const void *cc_sized_by(data_nbytes) data,
                           size_t expected_mac_nbytes, const void *cc_sized_by(expected_mac_nbytes) expected_mac);
int cccmac_init(const struct ccmode_cbc *cbc,
                cccmac_ctx_t ctx,
                size_t key_nbytes, const void *cc_sized_by(key_nbytes) key);
int cccmac_update(cccmac_ctx_t ctx,
                  size_t data_nbytes, const void *cc_sized_by(data_nbytes) data);
int cccmac_final_generate(cccmac_ctx_t ctx,
                     size_t mac_nbytes, void *cc_sized_by(mac_nbytes) mac);
int cccmac_final_verify(cccmac_ctx_t ctx,
                        size_t expected_mac_nbytes, const void *cc_sized_by(expected_mac_nbytes) expected_mac);
CC_CONST
size_t ccder_sizeof(ccder_tag tag, size_t len);
CC_NONNULL_ALL
size_t ccder_sizeof_overflow(ccder_tag tag, size_t nbytes, bool *overflowed);
CC_PURE
size_t ccder_sizeof_implicit_integer(ccder_tag implicit_tag, cc_size n, const cc_unit *s);
CC_PURE
size_t ccder_sizeof_implicit_octet_string(ccder_tag implicit_tag, cc_size n, const cc_unit *s);
CC_CONST
size_t ccder_sizeof_implicit_raw_octet_string(ccder_tag implicit_tag, size_t s_size);
CC_NONNULL_ALL
size_t ccder_sizeof_implicit_raw_octet_string_overflow(ccder_tag implicit_tag, size_t s_size, bool *overflowed);
CC_CONST
size_t ccder_sizeof_implicit_uint64(ccder_tag implicit_tag, uint64_t value);
CC_PURE
size_t ccder_sizeof_integer(cc_size n, const cc_unit *s);
CC_CONST
size_t ccder_sizeof_len(size_t len);
CC_PURE
size_t ccder_sizeof_octet_string(cc_size n, const cc_unit *s);
CC_PURE
size_t ccder_sizeof_oid(ccoid_t oid);
CC_CONST
size_t ccder_sizeof_raw_octet_string(size_t s_size);
CC_CONST
size_t ccder_sizeof_tag(ccder_tag tag);
CC_CONST
size_t ccder_sizeof_uint64(uint64_t value);
CC_PURE
size_t ccder_sizeof_eckey(size_t priv_size, ccoid_t oid, size_t pub_size);
CC_PURE
size_t ccder_encode_eckey_size(size_t priv_size, ccoid_t oid, size_t pub_size);
CC_NONNULL((2)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_tag(ccder_tag tag, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_len(size_t len, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_tl(ccder_tag tag, size_t len, const uint8_t *der, uint8_t *der_end);
CC_PURE CC_NONNULL((2)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_body_nocopy(size_t size, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2, 3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_constructed_tl(ccder_tag tag, const uint8_t *body_end, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((1, 2)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_oid(ccoid_t oid, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((3, 4)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_implicit_integer(ccder_tag implicit_tag, cc_size n, const cc_unit *s, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2, 3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_integer(cc_size n, const cc_unit *s, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_implicit_uint64(ccder_tag implicit_tag, uint64_t value, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_uint64(uint64_t value, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((3, 4)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_implicit_octet_string(ccder_tag implicit_tag, cc_size n, const cc_unit *s, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2, 3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_octet_string(cc_size n, const cc_unit *s, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((3, 4)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_implicit_raw_octet_string(ccder_tag implicit_tag,
                                                size_t s_size,
                                                const uint8_t *s,
                                                const uint8_t *der,
                                                uint8_t *der_end);
CC_NONNULL((2, 3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_raw_octet_string(size_t s_size, const uint8_t *s, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((2, 5, 6)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_eckey(size_t priv_size,
                            const uint8_t *priv_key,
                            ccoid_t oid,
                            size_t pub_size,
                            const uint8_t *pub_key,
                            uint8_t *der,
                            uint8_t *der_end);
CC_NONNULL((3)) cc_ptrcheck_unavailable()
uint8_t *ccder_encode_body(size_t size, const uint8_t *body, const uint8_t *der, uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_tag(ccder_tag *tagp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_len(size_t *lenp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_len_strict(size_t *lenp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((2, 4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_tl(ccder_tag expected_tag, size_t *lenp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((2, 4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_tl_strict(ccder_tag expected_tag, size_t *lenp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((2, 4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_constructed_tl(ccder_tag expected_tag,
                                           const uint8_t **body_end,
                                           const uint8_t *der,
                                           const uint8_t *der_end);
CC_NONNULL((2, 4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_constructed_tl_strict(ccder_tag expected_tag, const uint8_t **body_end, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_sequence_tl(const uint8_t **body_end, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_sequence_tl_strict(const uint8_t **body_end, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_uint_n(cc_size *n, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_uint(cc_size n, cc_unit *r, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_uint_strict(cc_size n, cc_unit *r, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_uint64(uint64_t *r, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((2, 3, 5)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_seqii(cc_size n, cc_unit *r, cc_unit *s, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((2, 3, 5)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_seqii_strict(cc_size n, cc_unit *r, cc_unit *s, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 3)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_oid(ccoid_t *oidp, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 2, 4)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_bitstring(const uint8_t **bit_string, size_t *bit_length, const uint8_t *der, const uint8_t *der_end);
CC_NONNULL((1, 2, 3, 4, 5, 6, 7)) cc_ptrcheck_unavailable()
const uint8_t *ccder_decode_eckey(uint64_t *version,
                                  size_t *priv_size,
                                  const uint8_t **priv_key,
                                  ccoid_t *oid,
                                  size_t *pub_size,
                                  const uint8_t **pub_key,
                                  const uint8_t *der,
                                  const uint8_t *der_end);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_encode_len(ccder_blob *into, size_t len);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_encode_tl(ccder_blob *into, ccder_tag tag, size_t len);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_encode_body(ccder_blob *into, size_t size, const uint8_t *cc_sized_by(size) body);
CC_NONNULL((1, 4)) CC_NODISCARD
bool ccder_blob_encode_body_tl(ccder_blob *into, ccder_tag tag, size_t size, const uint8_t *cc_sized_by(size) body);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_reserve(ccder_blob *into, size_t reserve_size, ccder_blob *out_reserved);
CC_NONNULL((1, 4)) CC_NODISCARD
bool ccder_blob_reserve_tl(ccder_blob *into, ccder_tag tag, size_t reserve_size, ccder_blob *out_reserved);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_encode_oid(ccder_blob *into, ccoid_t oid);
CC_NONNULL((1, 4)) CC_NODISCARD
bool ccder_blob_encode_implicit_integer(ccder_blob *into, ccder_tag implicit_tag, cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_encode_integer(ccder_blob *into, cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_encode_implicit_uint64(ccder_blob *into, ccder_tag implicit_tag, uint64_t value);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_encode_uint64(ccder_blob *into, uint64_t value);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_encode_octet_string(ccder_blob *into, cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((1, 4)) CC_NODISCARD
bool ccder_blob_encode_implicit_octet_string(ccder_blob *into, ccder_tag implicit_tag, cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((1, 4)) CC_NODISCARD
bool ccder_blob_encode_implicit_raw_octet_string(ccder_blob *into, ccder_tag implicit_tag, size_t s_size, const uint8_t *cc_sized_by(s_size) s);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_encode_raw_octet_string(ccder_blob *into, size_t s_size, const uint8_t *cc_sized_by(s_size) s);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_encode_eckey(ccder_blob *into, size_t priv_byte_size, const uint8_t *cc_sized_by(priv_byte_size) priv_key, ccoid_t oid, size_t pub_byte_size, const uint8_t *cc_sized_by(pub_byte_size) pub_key);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_tag(ccder_read_blob *from, ccder_tag *tag);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_len(ccder_read_blob *from, size_t *size);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_len_strict(ccder_read_blob *from, size_t *size);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_tl(ccder_read_blob *from, ccder_tag expected_tag, size_t *size);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_tl_strict(ccder_read_blob *from, ccder_tag expected_tag, size_t *size);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_range(ccder_read_blob *from, ccder_tag expected_tag, ccder_read_blob *range_blob);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_range_strict(ccder_read_blob *from, ccder_tag expected_tag, ccder_read_blob *range_blob);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_sequence_tl(ccder_read_blob *from, ccder_read_blob *range_blob);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_sequence_tl_strict(ccder_read_blob *from, ccder_read_blob *range_blob);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_uint_n(ccder_read_blob *from, cc_size *n);
CC_NONNULL((1)) CC_NODISCARD
bool ccder_blob_decode_uint64(ccder_read_blob *from, uint64_t *r);
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_uint(ccder_read_blob *from, cc_size n, cc_unit *cc_counted_by(n));
CC_NONNULL((1, 3)) CC_NODISCARD
bool ccder_blob_decode_uint_strict(ccder_read_blob *from, cc_size n, cc_unit *cc_counted_by(n));
CC_NONNULL((1, 3, 4)) CC_NODISCARD
bool ccder_blob_decode_seqii(ccder_read_blob *from, size_t n, cc_unit *cc_counted_by(n) r, cc_unit *cc_counted_by(n) s);
CC_NONNULL((1, 3, 4)) CC_NODISCARD
bool ccder_blob_decode_seqii_strict(ccder_read_blob *from, size_t n, cc_unit *cc_counted_by(n) r, cc_unit *cc_counted_by(n) s);
CC_NONNULL((1, 2)) CC_NODISCARD
bool ccder_blob_decode_oid(ccder_read_blob *from, ccoid_t *oidp);
CC_NONNULL((1, 2, 3)) CC_NODISCARD
bool ccder_blob_decode_bitstring(ccder_read_blob *from, ccder_read_blob *bit_string_range, size_t *bit_count);
CC_NONNULL((1, 2, 3, 4, 5, 6, 7)) CC_NODISCARD
bool ccder_blob_decode_eckey(ccder_read_blob *from, uint64_t *version, size_t *priv_key_byte_size, const uint8_t *cc_sized_by(*priv_key_byte_size) *priv_key, ccoid_t *oid, size_t *pub_key_byte_size, const uint8_t *cc_sized_by(*pub_key_byte_size) *pub_key, size_t *pub_key_bit_count);
const struct ccmode_ecb *ccdes_ecb_decrypt_mode(void);
const struct ccmode_ecb *ccdes_ecb_encrypt_mode(void);
const struct ccmode_cbc *ccdes_cbc_decrypt_mode(void);
const struct ccmode_cbc *ccdes_cbc_encrypt_mode(void);
const struct ccmode_cfb *ccdes_cfb_decrypt_mode(void);
const struct ccmode_cfb *ccdes_cfb_encrypt_mode(void);
const struct ccmode_cfb8 *ccdes_cfb8_decrypt_mode(void);
const struct ccmode_cfb8 *ccdes_cfb8_encrypt_mode(void);
const struct ccmode_ctr *ccdes_ctr_crypt_mode(void);
const struct ccmode_ofb *ccdes_ofb_crypt_mode(void);
const struct ccmode_ecb *ccdes3_ecb_decrypt_mode(void);
const struct ccmode_ecb *ccdes3_ecb_encrypt_mode(void);
const struct ccmode_cbc *ccdes3_cbc_decrypt_mode(void);
const struct ccmode_cbc *ccdes3_cbc_encrypt_mode(void);
const struct ccmode_cfb *ccdes3_cfb_decrypt_mode(void);
const struct ccmode_cfb *ccdes3_cfb_encrypt_mode(void);
const struct ccmode_cfb8 *ccdes3_cfb8_decrypt_mode(void);
const struct ccmode_cfb8 *ccdes3_cfb8_encrypt_mode(void);
const struct ccmode_ctr *ccdes3_ctr_crypt_mode(void);
const struct ccmode_ofb *ccdes3_ofb_crypt_mode(void);
int ccdes_key_is_weak( void *key, size_t  length);
void ccdes_key_set_odd_parity(void *key, size_t length);
uint32_t
ccdes_cbc_cksum(const void *in, void *out, size_t length,
                const void *key, size_t key_nbytes, const void *ivec);
void ccdigest_init(const struct ccdigest_info *di, ccdigest_ctx_t ctx);
void ccdigest_update(const struct ccdigest_info *di, ccdigest_ctx_t ctx,
                     size_t len, const void *data);
CC_INLINE
void ccdigest_final(const struct ccdigest_info *di, ccdigest_ctx_t ctx, unsigned char *digest)
{
    di->final(di,ctx,digest);
}

void ccdigest(const struct ccdigest_info *di, size_t len,
              const void *data, void *digest);
const struct ccdigest_info *ccdigest_oid_lookup(ccoid_t oid, ...);
int ccdrbg_init(const struct ccdrbg_info *info,
                struct ccdrbg_state *drbg,
                size_t entropyLength, const void* entropy,
                size_t nonceLength, const void* nonce,
                size_t psLength, const void* ps);
int ccdrbg_reseed(const struct ccdrbg_info *info,
                  struct ccdrbg_state *drbg,
                  size_t entropyLength, const void *entropy,
                  size_t additionalLength, const void *additional);
int ccdrbg_generate(const struct ccdrbg_info *info,
                    struct ccdrbg_state *drbg,
                    size_t dataOutLength, void *dataOut,
                    size_t additionalLength, const void *additional);
void ccdrbg_done(const struct ccdrbg_info *info,
                 struct ccdrbg_state *drbg);
size_t ccdrbg_context_size(const struct ccdrbg_info *info);
bool ccdrbg_must_reseed(const struct ccdrbg_info *info,
                        const struct ccdrbg_state *drbg);
void ccdrbg_factory_nistctr(struct ccdrbg_info *info, const struct ccdrbg_nistctr_custom *custom);
void ccdrbg_factory_nisthmac(struct ccdrbg_info *info, const struct ccdrbg_nisthmac_custom *custom);
CC_WARN_RESULT
CC_NONNULL_ALL
int ccdrbg_df_bc_init(ccdrbg_df_bc_ctx_t *ctx,
                      const struct ccmode_cbc *cbc_info,
                      size_t key_nbytes);
int ccentropy_add_entropy(ccentropy_ctx_t *ctx,
                          uint32_t entropy_nsamples,
                          size_t entropy_nbytes,
                          const void *entropy);
int cchkdf(const struct ccdigest_info *di,
           size_t ikm_nbytes,
           const void *ikm,
           size_t salt_nbytes,
           const void *salt,
           size_t info_nbytes,
           const void *info,
           size_t dk_nbytes,
           void *dk);
int cchkdf_extract(const struct ccdigest_info *di,
                   size_t salt_nbytes,
                   const void *salt,
                   size_t ikm_nbytes,
                   const void *ikm,
                   void *prk);
int cchkdf_expand(const struct ccdigest_info *di,
                  size_t prk_nbytes,
                  const void *prk,
                  size_t info_nbytes,
                  const void *info,
                  size_t dk_nbytes,
                  void *dk);
void cchmac_init(const struct ccdigest_info *di, cchmac_ctx_t ctx,
                 size_t key_len, const void *key);
void cchmac_update(const struct ccdigest_info *di, cchmac_ctx_t ctx,
                   size_t data_len, const void *data);
void cchmac_final(const struct ccdigest_info *di, cchmac_ctx_t ctx,
                  unsigned char *mac);
void cchmac(const struct ccdigest_info *di, size_t key_len,
            const void *key, size_t data_len, const void *data,
            unsigned char *mac);
void cckprng_init(struct cckprng_ctx *ctx,
                  size_t seed_nbytes,
                  const void *seed,
                  size_t nonce_nbytes,
                  const void *nonce,
                  cckprng_getentropy getentropy,
                  void *getentropy_arg);
void cckprng_init_with_getentropy(struct cckprng_ctx *ctx,
                                  unsigned max_ngens,
                                  size_t seed_nbytes,
                                  const void *seed,
                                  size_t nonce_nbytes,
                                  const void *nonce,
                                  cckprng_getentropy getentropy,
                                  void *getentropy_arg);
void cckprng_initgen(struct cckprng_ctx *ctx, unsigned gen_idx);
void cckprng_reseed(struct cckprng_ctx *ctx, size_t nbytes, const void *seed);
void cckprng_refresh(struct cckprng_ctx *ctx);
void cckprng_generate(struct cckprng_ctx *ctx, unsigned gen_idx, size_t nbytes, void *out);
CC_PTRCHECK_CAPABLE_HEADER()





size_t ccecb_context_size(const struct ccmode_ecb *mode);
size_t ccecb_block_size(const struct ccmode_ecb *mode);
int ccecb_init(const struct ccmode_ecb *mode, ccecb_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key);
int ccecb_update(const struct ccmode_ecb *mode, const ccecb_ctx *ctx, size_t nblocks, const void *cc_indexable in, void *cc_indexable out);
cc_ptrcheck_unavailable_r(ccecb_one_shot_explicit)
int ccecb_one_shot(const struct ccmode_ecb *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   size_t nblocks,
                   const void *cc_unsafe_indexable in,
                   void *cc_unsafe_indexable out);
int ccecb_one_shot_explicit(const struct ccmode_ecb *mode,
                            size_t key_len,
                            size_t block_size,
                            size_t nblocks,
                            const void *cc_sized_by(key_len) key,
                            const void *cc_sized_by(block_size * nblocks) in,
                            void *cc_sized_by(block_size * nblocks) out);
size_t cccbc_context_size(const struct ccmode_cbc *mode);
size_t cccbc_block_size(const struct ccmode_cbc *mode);
int cccbc_init(const struct ccmode_cbc *mode, cccbc_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key);
int cccbc_copy_iv(cccbc_iv *cc_sized_by(len) iv_ctx, const void *cc_sized_by(len) iv, size_t len);
int cccbc_clear_iv(cccbc_iv *cc_sized_by(len) iv_ctx, size_t len);
cc_ptrcheck_unavailable() 
int cccbc_set_iv(const struct ccmode_cbc *mode, cccbc_iv *iv_ctx, const void *iv);
int cccbc_update(const struct ccmode_cbc *mode, const cccbc_ctx *ctx, cccbc_iv *iv, size_t nblocks, const void *cc_indexable in, void *cc_indexable out);
cc_ptrcheck_unavailable_r(cccbc_one_shot_explicit)
int cccbc_one_shot(const struct ccmode_cbc *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   const void *iv,
                   size_t nblocks,
                   const void *cc_unsafe_indexable in,
                   void *cc_unsafe_indexable out);
int cccbc_one_shot_explicit(const struct ccmode_cbc *mode,
                            size_t key_len,
                            size_t iv_len,
                            size_t block_size,
                            size_t nblocks,
                            const void *cc_sized_by(key_len) key,
                            const void *cc_sized_by(iv_len) iv,
                            const void *cc_sized_by(block_size * nblocks) in,
                            void *cc_sized_by(block_size * nblocks) out);
size_t cccfb_context_size(const struct ccmode_cfb *mode);
size_t cccfb_block_size(const struct ccmode_cfb *mode);
int cccfb_init(const struct ccmode_cfb *mode, cccfb_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key, const void *cc_indexable iv);
int cccfb_update(const struct ccmode_cfb *mode, cccfb_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int cccfb_one_shot(const struct ccmode_cfb *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   const void *cc_indexable iv,
                   size_t nbytes,
                   const void *cc_sized_by(nbytes) in,
                   void *cc_sized_by(nbytes) out);
size_t cccfb8_context_size(const struct ccmode_cfb8 *mode);
size_t cccfb8_block_size(const struct ccmode_cfb8 *mode);
int cccfb8_init(const struct ccmode_cfb8 *mode, cccfb8_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key, const void *cc_indexable iv);
int cccfb8_update(const struct ccmode_cfb8 *mode, cccfb8_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int cccfb8_one_shot(const struct ccmode_cfb8 *mode,
                    size_t key_len,
                    const void *cc_sized_by(key_len) key,
                    const void *cc_indexable iv,
                    size_t nbytes,
                    const void *cc_sized_by(nbytes) in,
                    void *cc_sized_by(nbytes) out);
size_t ccctr_context_size(const struct ccmode_ctr *mode);
size_t ccctr_block_size(const struct ccmode_ctr *mode);
int ccctr_init(const struct ccmode_ctr *mode, ccctr_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key, const void *cc_indexable iv);
int ccctr_update(const struct ccmode_ctr *mode, ccctr_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int ccctr_one_shot(const struct ccmode_ctr *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   const void *cc_indexable iv,
                   size_t nbytes,
                   const void *cc_sized_by(nbytes) in,
                   void *cc_sized_by(nbytes) out);
size_t ccofb_context_size(const struct ccmode_ofb *mode);
size_t ccofb_block_size(const struct ccmode_ofb *mode);
int ccofb_init(const struct ccmode_ofb *mode, ccofb_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key, const void *cc_indexable iv);
int ccofb_update(const struct ccmode_ofb *mode, ccofb_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int ccofb_one_shot(const struct ccmode_ofb *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   const void *cc_indexable iv,
                   size_t nbytes,
                   const void *cc_sized_by(nbytes) in,
                   void *cc_sized_by(nbytes) out);
size_t ccxts_context_size(const struct ccmode_xts *mode);
size_t ccxts_block_size(const struct ccmode_xts *mode);
int ccxts_init(const struct ccmode_xts *mode, ccxts_ctx *ctx, size_t key_nbytes, const void *cc_sized_by(key_nbytes) data_key, const void *cc_sized_by(key_nbytes) tweak_key);
int ccxts_set_tweak(const struct ccmode_xts *mode, ccxts_ctx *ctx, ccxts_tweak *tweak, const void *cc_indexable iv);
void *cc_unsafe_indexable
ccxts_update(const struct ccmode_xts *mode, ccxts_ctx *ctx, ccxts_tweak *tweak, size_t nblocks, const void *cc_indexable in, void *cc_indexable out);
int ccxts_one_shot(const struct ccmode_xts *mode,
                   size_t key_nbytes,
                   const void *cc_sized_by(key_nbytes) data_key,
                   const void *cc_sized_by(key_nbytes) tweak_key,
                   const void *cc_unsafe_indexable iv,
                   size_t nblocks,
                   const void *cc_unsafe_indexable in,
                   void *cc_unsafe_indexable out);
size_t ccgcm_context_size(const struct ccmode_gcm *mode);
size_t ccgcm_block_size(const struct ccmode_gcm *mode);
int ccgcm_init(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t key_nbytes, const void *cc_sized_by(key_nbytes) key);
int ccgcm_init_with_iv(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t key_nbytes, const void *cc_sized_by(key_nbytes) key, const void *cc_unsafe_indexable iv);
int ccgcm_set_iv(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t iv_nbytes, const void *cc_sized_by(iv_nbytes) iv);
int ccgcm_set_iv_legacy(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t iv_nbytes, const void *cc_sized_by(iv_nbytes) iv);
int ccgcm_inc_iv(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, void *cc_unsafe_indexable iv);
int ccgcm_aad(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) additional_data);
int ccgcm_gmac(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in)
cc_deprecate_with_replacement("ccgcm_aad", 13.0, 10.15, 13.0, 6.0, 4.0);
int ccgcm_update(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int ccgcm_finalize(const struct ccmode_gcm *mode, ccgcm_ctx *ctx, size_t tag_nbytes, void *cc_sized_by(tag_nbytes) tag);
int ccgcm_reset(const struct ccmode_gcm *mode, ccgcm_ctx *ctx);
int ccgcm_one_shot(const struct ccmode_gcm *mode,
                   size_t key_nbytes,
                   const void *cc_sized_by(key_nbytes) key,
                   size_t iv_nbytes,
                   const void *cc_sized_by(iv_nbytes) iv,
                   size_t adata_nbytes,
                   const void *cc_sized_by(adata_nbytes) adata,
                   size_t nbytes,
                   const void *cc_sized_by(nbytes) in,
                   void *cc_sized_by(nbytes) out,
                   size_t tag_nbytes,
                   void *cc_sized_by(tag_nbytes) tag);
int ccgcm_one_shot_legacy(const struct ccmode_gcm *mode,
                          size_t key_nbytes,
                          const void *cc_sized_by(key_nbytes) key,
                          size_t iv_nbytes,
                          const void *cc_sized_by(iv_nbytes) iv,
                          size_t adata_nbytes,
                          const void *cc_sized_by(adata_nbytes) adata,
                          size_t nbytes,
                          const void *cc_sized_by(nbytes) in,
                          void *cc_sized_by(nbytes) out,
                          size_t tag_nbytes,
                          void *cc_sized_by(tag_nbytes) tag);
size_t ccccm_context_size(const struct ccmode_ccm *mode);
size_t ccccm_block_size(const struct ccmode_ccm *mode);
int ccccm_init(const struct ccmode_ccm *mode, ccccm_ctx *ctx, size_t key_len, const void *cc_sized_by(key_len) key);
int ccccm_set_iv(const struct ccmode_ccm *mode,
                 ccccm_ctx *ctx,
                 ccccm_nonce *nonce_ctx,
                 size_t nonce_len,
                 const void *cc_sized_by(nonce_len) nonce,
                 size_t mac_size,
                 size_t auth_len,
                 size_t data_len);
int ccccm_cbcmac(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const void *cc_sized_by(nbytes) in);
int ccccm_aad(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t ad_nbytes, const uint8_t *cc_sized_by(ad_nbytes) ad);
int ccccm_update(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const void *cc_sized_by(nbytes) in, void *cc_sized_by(nbytes) out);
int ccccm_encrypt(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const uint8_t *cc_sized_by(nbytes) plaintext, uint8_t *cc_sized_by(nbytes) encrypted_plaintext);
int ccccm_decrypt(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, size_t nbytes, const uint8_t *cc_sized_by(nbytes) encrypted_plaintext, uint8_t *cc_sized_by(nbytes) plaintext);
int ccccm_finalize(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, void *cc_indexable mac);
int ccccm_finalize_and_generate_tag(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, uint8_t *cc_indexable mac);
int ccccm_finalize_and_verify_tag(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx, const uint8_t *cc_indexable mac);
int ccccm_reset(const struct ccmode_ccm *mode, ccccm_ctx *ctx, ccccm_nonce *nonce_ctx);
int ccccm_one_shot(const struct ccmode_ccm *mode,
                   size_t key_len,
                   const void *cc_sized_by(key_len) key,
                   size_t nonce_len,
                   const void *cc_sized_by(nonce_len) nonce,
                   size_t nbytes,
                   const void *cc_sized_by(nbytes) in,
                   void *cc_sized_by(nbytes) out,
                   size_t adata_len,
                   const void *cc_sized_by(adata_len) adata,
                   size_t mac_size,
                   void *cc_sized_by(mac_size) mac);
int ccccm_one_shot_encrypt(const struct ccmode_ccm *mode,
                             size_t key_nbytes,
                             const uint8_t *cc_sized_by(key_nbytes) key,
                             size_t nonce_nbytes,
                             const uint8_t *cc_sized_by(nonce_nbytes) nonce,
                             size_t nbytes,
                             const uint8_t *cc_sized_by(nbytes) plaintext,
                             uint8_t *cc_sized_by(nbytes) encrypted_plaintext,
                             size_t adata_nbytes,
                             const uint8_t *cc_sized_by(adata_nbytes) adata,
                             size_t mac_tag_nbytes,
                             uint8_t *cc_sized_by(mac_tag_nbytes) mac_tag);
int ccccm_one_shot_decrypt(const struct ccmode_ccm *mode,
                             size_t key_nbytes,
                             const uint8_t *cc_sized_by(key_nbytes) key,
                             size_t nonce_nbytes,
                             const uint8_t *cc_sized_by(nonce_nbytes) nonce,
                             size_t nbytes,
                             const uint8_t *cc_sized_by(nbytes) encrypted_plaintext,
                             uint8_t *cc_sized_by(nbytes) plaintext,
                             size_t adata_nbytes,
                             const uint8_t *cc_sized_by(adata_nbytes) adata,
                             size_t mac_tag_nbytes,
                             const uint8_t *cc_sized_by(mac_tag_nbytes) mac_tag);
size_t ccomac_context_size(const struct ccmode_omac *mode);
size_t ccomac_block_size(const struct ccmode_omac *mode);
int ccomac_init(const struct ccmode_omac *mode, ccomac_ctx *ctx, size_t tweak_len, size_t key_len, const void *cc_sized_by(key_len) key);
int ccomac_update(const struct ccmode_omac *mode, ccomac_ctx *ctx, size_t nblocks, const void *tweak, const void *cc_indexable in, void *cc_indexable out);
int ccomac_one_shot(const struct ccmode_omac *mode,
                    size_t tweak_len,
                    size_t key_len,
                    const void *cc_sized_by(key_len) key,
                    const void *cc_sized_by(tweak_len) tweak,
                    size_t nblocks,
                    const void *cc_indexable in,
                    void *cc_indexable out);
void ccmode_factory_cbc_decrypt(struct ccmode_cbc *cbc,
                                const struct ccmode_ecb *ecb);
void ccmode_factory_cbc_encrypt(struct ccmode_cbc *cbc,
                                const struct ccmode_ecb *ecb);
void ccmode_factory_cfb_decrypt(struct ccmode_cfb *cfb,
                                const struct ccmode_ecb *ecb);
void ccmode_factory_cfb_encrypt(struct ccmode_cfb *cfb,
                                const struct ccmode_ecb *ecb);
void ccmode_factory_cfb8_decrypt(struct ccmode_cfb8 *cfb8,
                                 const struct ccmode_ecb *ecb);
void ccmode_factory_cfb8_encrypt(struct ccmode_cfb8 *cfb8,
                                 const struct ccmode_ecb *ecb);
void ccmode_factory_ctr_crypt(struct ccmode_ctr *ctr,
                              const struct ccmode_ecb *ecb);
void ccmode_factory_gcm_decrypt(struct ccmode_gcm *gcm,
                                const struct ccmode_ecb *ecb_encrypt);
void ccmode_factory_gcm_encrypt(struct ccmode_gcm *gcm,
                                const struct ccmode_ecb *ecb_encrypt);
void ccmode_factory_ccm_decrypt(struct ccmode_ccm *ccm,
                                const struct ccmode_ecb *ecb_encrypt);
void ccmode_factory_ccm_encrypt(struct ccmode_ccm *ccm,
                                const struct ccmode_ecb *ecb_encrypt);
void ccmode_factory_ofb_crypt(struct ccmode_ofb *ofb,
                              const struct ccmode_ecb *ecb);
void ccmode_factory_omac_decrypt(struct ccmode_omac *omac,
                                 const struct ccmode_ecb *ecb);
void ccmode_factory_omac_encrypt(struct ccmode_omac *omac,
                                 const struct ccmode_ecb *ecb);
void ccmode_factory_xts_decrypt(struct ccmode_xts *xts,
                                const struct ccmode_ecb *ecb,
                                const struct ccmode_ecb *ecb_encrypt);
void ccmode_factory_xts_encrypt(struct ccmode_xts *xts,
                                const struct ccmode_ecb *ecb,
                                const struct ccmode_ecb *ecb_encrypt);
cc_aligned_struct(16) ccecb_ctx;
cc_aligned_struct(16) cccbc_ctx;
cc_aligned_struct(16) cccbc_iv;
cc_aligned_struct(16) cccfb_ctx;
cc_aligned_struct(16) cccfb8_ctx;
cc_aligned_struct(16) ccctr_ctx;
cc_aligned_struct(16) ccofb_ctx;
cc_aligned_struct(16) ccxts_ctx;
cc_aligned_struct(16) ccxts_tweak;
cc_aligned_struct(16) ccgcm_ctx;
cc_aligned_struct(16) ccccm_ctx;
cc_aligned_struct(16) ccccm_nonce;
cc_aligned_struct(16) ccomac_ctx;
cc_aligned_struct(16) ccsiv_ctx;
cc_aligned_struct(16) ccsiv_hmac_ctx;
size_t ccsiv_context_size(const struct ccmode_siv *mode);
size_t ccsiv_block_size(const struct ccmode_siv *mode);
size_t ccsiv_ciphertext_size(const struct ccmode_siv *mode, size_t plaintext_size);
size_t ccsiv_plaintext_size(const struct ccmode_siv *mode, size_t ciphertext_size);
int ccsiv_init(const struct ccmode_siv *mode, ccsiv_ctx *ctx,
               size_t key_byte_len, const uint8_t *key);
int ccsiv_set_nonce(const struct ccmode_siv *mode, ccsiv_ctx *ctx,
                    size_t nbytes, const uint8_t *in);
int ccsiv_aad(const struct ccmode_siv *mode, ccsiv_ctx *ctx,
              size_t nbytes, const uint8_t *in);
int ccsiv_crypt(const struct ccmode_siv *mode, ccsiv_ctx *ctx,
                size_t nbytes, const uint8_t *in, uint8_t *out);
int ccsiv_reset(const struct ccmode_siv *mode, ccsiv_ctx *ctx);
int ccsiv_one_shot(const struct ccmode_siv *mode,
                   size_t key_len, const uint8_t *key,
                   unsigned nonce_nbytes, const uint8_t* nonce,
                   unsigned adata_nbytes, const uint8_t* adata,
                   size_t in_nbytes, const uint8_t *in, uint8_t *out);
size_t ccsiv_hmac_context_size(const struct ccmode_siv_hmac *mode);
size_t ccsiv_hmac_block_size(const struct ccmode_siv_hmac *mode);
size_t ccsiv_hmac_ciphertext_size(ccsiv_hmac_ctx *ctx, size_t plaintext_size);
size_t ccsiv_hmac_plaintext_size(ccsiv_hmac_ctx *ctx, size_t ciphertext_size);
int ccsiv_hmac_init(const struct ccmode_siv_hmac *mode, ccsiv_hmac_ctx *ctx, size_t key_byte_len, const uint8_t *key, size_t tag_size);
int ccsiv_hmac_aad(const struct ccmode_siv_hmac *mode, ccsiv_hmac_ctx *ctx, size_t nbytes, const uint8_t *in);
int ccsiv_hmac_set_nonce(const struct ccmode_siv_hmac *mode, ccsiv_hmac_ctx *ctx, size_t nbytes, const uint8_t *in);
int ccsiv_hmac_crypt(const struct ccmode_siv_hmac *mode, ccsiv_hmac_ctx *ctx, size_t nbytes, const uint8_t *in, uint8_t *out);
int ccsiv_hmac_reset(const struct ccmode_siv_hmac *mode, ccsiv_hmac_ctx *ctx);
int ccsiv_hmac_one_shot(const struct ccmode_siv_hmac *mode,
                        size_t key_len,
                        const uint8_t *key,
                        size_t tag_length,
                        unsigned nonce_nbytes,
                        const uint8_t *nonce,
                        unsigned adata_nbytes,
                        const uint8_t *adata,
                        size_t in_nbytes,
                        const uint8_t *in,
                        uint8_t *out);
CC_PTRCHECK_CAPABLE_HEADER()

typedef size_t  cc_size;
CC_NONNULL_ALL
void ccn_set_bit(cc_unit *cc_indexable x, size_t k, cc_unit v);
CC_NONNULL((2))
size_t ccn_bitlen(cc_size n, const cc_unit *cc_counted_by(n) s);
CC_PURE CC_NONNULL((2, 3))
int ccn_cmp(cc_size n, const cc_unit *cc_counted_by(n) s, const cc_unit *cc_counted_by(n) t) __asm__("_ccn_cmp");
CC_NONNULL_ALL
int ccn_cmpn(cc_size ns, const cc_unit *cc_counted_by(ns) s, cc_size nt, const cc_unit *cc_counted_by(nt) t);
CC_NONNULL((2, 3, 4))
cc_unit ccn_sub(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit *cc_counted_by(n) t) __asm__("_ccn_sub");
CC_NONNULL((2, 3, 4))
cc_unit ccn_add(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit *cc_counted_by(n) t) __asm__("_ccn_add");
CC_NONNULL((2, 3))
cc_unit ccn_add1(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, cc_unit v);
CC_NONNULL((2, 4))
int ccn_read_uint(cc_size n, cc_unit *cc_counted_by(n) r, size_t data_nbytes, const uint8_t *cc_sized_by(data_nbytes) data);
CC_PURE CC_NONNULL((2)) size_t ccn_write_uint_size(cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((2, 4))
void ccn_write_uint(cc_size n, const cc_unit *cc_counted_by(n) s, size_t out_size, void *cc_sized_by(out_size) out);
CC_NONNULL((2, 4))
int ccn_write_uint_padded_ct(cc_size n, const cc_unit *cc_sized_by(n) s, size_t out_size, uint8_t *cc_counted_by(out_size) out);
CC_NONNULL((2, 4))
size_t ccn_write_uint_padded(cc_size n, const cc_unit *cc_counted_by(n) s, size_t out_size, uint8_t *cc_sized_by(out_size) out);
CC_PURE CC_NONNULL((2))
size_t ccn_write_int_size(cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((2, 4))
void ccn_write_int(cc_size n, const cc_unit *cc_counted_by(n) s, size_t out_size, void *cc_sized_by(out_size) out);
CC_NONNULL((2))
void ccn_zero(cc_size n, cc_unit *cc_sized_by(n) r);
CC_NONNULL((2))
void ccn_seti(cc_size n, cc_unit *cc_counted_by(n) r, cc_unit v);
CC_NONNULL((2))
void ccn_swap(cc_size n, cc_unit *cc_counted_by(n) r);
CC_NONNULL((2, 3, 4))
void ccn_xor(cc_size n, cc_unit *cc_counted_by(n) r, const cc_unit *cc_counted_by(n) s, const cc_unit *cc_counted_by(n) t);
CC_NONNULL((2))
void ccn_print(cc_size n, const cc_unit *cc_counted_by(n) s);
CC_NONNULL((3))
void ccn_lprint(cc_size n, const char *cc_cstring label, const cc_unit *cc_counted_by(n) s);
size_t ccpad_cts1_decrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_cts1_encrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_cts2_decrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_cts2_encrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_cts3_decrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_cts3_encrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                       size_t nbytes, const void *in, void *out);
size_t ccpad_pkcs7_decrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                           size_t nbytes, const void *in, void *out);
size_t ccpad_pkcs7_encrypt(const struct ccmode_cbc *cbc, cccbc_ctx *ctx, cccbc_iv *iv,
                         size_t nbytes, const void *in, void *out);
size_t ccpad_pkcs7_ecb_decrypt(const struct ccmode_ecb *ecb, ccecb_ctx *ecb_key,
                               size_t nbytes, const void *in, void *out);
size_t ccpad_pkcs7_ecb_encrypt(const struct ccmode_ecb *ecb, ccecb_ctx *ctx,
                             size_t nbytes, const void *in, void *out);
size_t ccpad_pkcs7_decode(const size_t block_size, const uint8_t* last_block);
size_t ccpad_xts_decrypt(const struct ccmode_xts *xts, ccxts_ctx *ctx, ccxts_tweak *tweak,
                       size_t nbytes, const void *in, void *out);
void ccpad_xts_encrypt(const struct ccmode_xts *xts, ccxts_ctx *ctx, ccxts_tweak *tweak,
                       size_t nbytes, const void *in, void *out);
int
ccrng_crypto_generate(ccrng_crypto_ctx_t *ctx,
                      size_t nbytes,
                      void *rand);
int
ccrng_crypto_reseed(ccrng_crypto_ctx_t *ctx,
                    size_t seed_nbytes,
                    const void *seed,
                    size_t nonce_nbytes,
                    const void *nonce);
void ccrng_fortuna_init(struct ccrng_fortuna_ctx *ctx,
                        ccrng_fortuna_getentropy getentropy,
                        void *getentropy_arg);
bool ccrng_fortuna_refresh(struct ccrng_fortuna_ctx *ctx);
int ccrng_fortuna_generate(struct ccrng_fortuna_ctx *ctx, size_t nbytes, void *out);
void ccrng_schedule_notify_reseed(ccrng_schedule_ctx_t *ctx);
void ccrng_schedule_atomic_flag_set(ccrng_schedule_atomic_flag_ctx_t *ctx);
CC_PTRCHECK_CAPABLE_HEADER()








typedef struct ccrsa_full_ctx* ccrsa_full_ctx_t;
cczp_t ccrsa_ctx_private_zp(ccrsa_full_ctx_t fk);
ccrsa_pub_ctx_t ccrsa_ctx_public(ccrsa_full_ctx_t fk);
CC_NONNULL_ALL
size_t ccrsa_pubkeylength(ccrsa_pub_ctx_t pubk);
CC_NONNULL_ALL
int ccrsa_init_pub(ccrsa_pub_ctx_t pubk, const cc_unit *modulus,
                    const cc_unit *exponent);
int ccrsa_make_priv(ccrsa_full_ctx_t full_ctx,
                    size_t e_nbytes, const uint8_t *cc_counted_by(e_nbytes) e_bytes,
                    size_t p_nbytes, const uint8_t *cc_counted_by(p_nbytes) p_bytes,
                    size_t q_nbytes, const uint8_t *cc_counted_by(q_nbytes) q_bytes);
int ccrsa_recover_priv(ccrsa_full_ctx_t full_ctx,
                       size_t m_nbytes, const uint8_t *cc_counted_by(m_nbytes) m_bytes,
                       size_t e_nbytes, const uint8_t *cc_counted_by(e_nbytes) e_bytes,
                       size_t d_nbytes, const uint8_t *cc_counted_by(d_nbytes) d_bytes,
                       struct ccrng_state *rng);
CC_NONNULL((1, 3, 5))
int ccrsa_make_pub(ccrsa_pub_ctx_t pubk,
                   size_t exp_nbytes, const uint8_t *cc_counted_by(exp_nbytes) exp,
                   size_t mod_nbytes, const uint8_t *cc_counted_by(mod_nbytes) mod);
CC_NONNULL((1, 2, 3))
int ccrsa_pub_crypt(ccrsa_pub_ctx_t key, cc_unit *cc_unsafe_indexable out, const cc_unit *cc_unsafe_indexable in);
CC_NONNULL_ALL
int ccrsa_generate_key(size_t nbits, ccrsa_full_ctx_t fk,
                       size_t e_nbytes, const void *cc_sized_by(e_nbytes) e_bytes, struct ccrng_state *rng) CC_WARN_RESULT;
CC_NONNULL((2, 4, 6, 10))
int ccrsa_generate_key_deterministic(size_t nbits, ccrsa_full_ctx_t fk,
                                     size_t e_nbytes, const uint8_t *cc_sized_by(e_nbytes) e,
                                     size_t entropy_nbytes, const uint8_t *cc_sized_by(entropy_nbytes) entropy,
                                     size_t nonce_nbytes, const uint8_t *cc_sized_by(nonce_nbytes) nonce,
                                     uint32_t flags,
                                     struct ccrng_state *rng);
CC_NONNULL_ALL int
ccrsa_generate_fips186_key(size_t nbits, ccrsa_full_ctx_t fk,
                           size_t e_nbytes, const void *cc_sized_by(e_nbytes) e_bytes,
                           struct ccrng_state *rng, struct ccrng_state *rng_mr) CC_WARN_RESULT;
CC_NONNULL((1, 2, 3, 5, 7, 8, 9))
int ccrsa_sign_pss(ccrsa_full_ctx_t key,
                   const struct ccdigest_info* hashAlgorithm, const struct ccdigest_info* MgfHashAlgorithm,
                   size_t salt_nbytes, struct ccrng_state *rng,
                   size_t digest_nbytes, const uint8_t *cc_counted_by(digest_nbytes) digest,
                   size_t *sig_nbytes, uint8_t *cc_unsafe_indexable sig);
CC_NONNULL((1, 2, 3, 5, 7, 8, 9))
int ccrsa_sign_pss_msg(ccrsa_full_ctx_t key,
                   const struct ccdigest_info* hashAlgorithm, const struct ccdigest_info* MgfHashAlgorithm,
                   size_t salt_nbytes, struct ccrng_state *rng,
                   size_t msg_nbytes, const uint8_t *cc_counted_by(msg_nbytes) msg,
                   size_t *sig_nbytes, uint8_t *cc_unsafe_indexable sig);
CC_NONNULL((1, 2, 3, 5, 7))
int ccrsa_verify_pss_digest(ccrsa_pub_ctx_t key,
                            const struct ccdigest_info* di,
                            const struct ccdigest_info* mgfdi,
                            size_t digest_nbytes, const uint8_t *cc_counted_by(digest_nbytes) digest,
                            size_t sig_nbytes, const uint8_t *cc_unsafe_indexable sig,
                            size_t salt_nbytes, cc_fault_canary_t fault_canary_out);
CC_NONNULL((1, 2, 3, 5, 7))
int ccrsa_verify_pss_msg(ccrsa_pub_ctx_t key,
                         const struct ccdigest_info* di,
                         const struct ccdigest_info* mgfdi,
                         size_t msg_nbytes, const uint8_t *cc_counted_by(msg_nbytes) msg,
                         size_t sig_nbytes, const uint8_t *cc_counted_by(sig_nbytes) sig,
                         size_t salt_nbytes, cc_fault_canary_t fault_canary_out);
CC_NONNULL((1, 4, 5, 6))
int ccrsa_sign_pkcs1v15(ccrsa_full_ctx_t key, const uint8_t *oid,
                        size_t digest_len, const uint8_t *cc_counted_by(digest_len) digest,
                        size_t *sig_len, uint8_t *cc_unsafe_indexable sig);
CC_NONNULL((1, 2, 4, 5, 6))
int ccrsa_sign_pkcs1v15_msg(ccrsa_full_ctx_t key, const struct ccdigest_info* di,
                            size_t msg_len, const uint8_t *cc_counted_by(msg_len) msg,
                            size_t *sig_len, uint8_t *cc_unsafe_indexable sig);
CC_NONNULL((1, 4, 6, 7))
int ccrsa_verify_pkcs1v15(ccrsa_pub_ctx_t key, const uint8_t *oid,
                          size_t digest_len, const uint8_t *cc_counted_by(digest_len) digest,
                          size_t sig_len, const uint8_t *cc_counted_by(sig_len) sig,
                          bool *valid);
CC_NONNULL((1, 4, 6))
int ccrsa_verify_pkcs1v15_digest(ccrsa_pub_ctx_t key, const uint8_t *oid,
                          size_t digest_len, const uint8_t *cc_counted_by(digest_len) digest,
                          size_t sig_len, const uint8_t *cc_counted_by(sig_len) sig,
                          cc_fault_canary_t fault_canary_out);
CC_NONNULL((1, 2, 4, 6))
int ccrsa_verify_pkcs1v15_msg(ccrsa_pub_ctx_t key, const struct ccdigest_info* di,
                          size_t msg_len, const uint8_t *cc_counted_by(msg_len) msg,
                          size_t sig_len, const uint8_t *cc_counted_by(sig_len) sig,
                          cc_fault_canary_t fault_canary_out);
CC_NONNULL((1))
size_t ccder_encode_rsa_pub_size(const ccrsa_pub_ctx_t key);
CC_NONNULL((1, 2, 3))
uint8_t *ccder_encode_rsa_pub(const ccrsa_pub_ctx_t key, uint8_t *cc_ended_by(der_end) der, uint8_t *der_end);
CC_NONNULL((1))
size_t ccder_encode_rsa_priv_size(const ccrsa_full_ctx_t key);
CC_NONNULL((1, 2, 3))
uint8_t *ccder_encode_rsa_priv(const ccrsa_full_ctx_t key, const uint8_t *cc_ended_by(der_end) der, uint8_t *der_end);
CC_NONNULL((1, 2))
cc_size ccder_decode_rsa_pub_n(const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1, 2, 3))
const uint8_t *ccder_decode_rsa_pub(const ccrsa_pub_ctx_t key, const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1, 2))
cc_size ccder_decode_rsa_pub_x509_n(const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1, 2, 3))
const uint8_t *ccder_decode_rsa_pub_x509(const ccrsa_pub_ctx_t key, const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1, 2))
cc_size ccder_decode_rsa_priv_n(const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1, 2, 3))
const uint8_t *ccder_decode_rsa_priv(const ccrsa_full_ctx_t key, const uint8_t *cc_ended_by(der_end) der, const uint8_t *der_end);
CC_NONNULL((1))
size_t ccrsa_export_pub_size(const ccrsa_pub_ctx_t key);
CC_NONNULL((1, 3))
int ccrsa_export_pub(const ccrsa_pub_ctx_t key, size_t out_len, uint8_t *cc_counted_by(out_len) out);
CC_NONNULL((2))
cc_size ccrsa_import_pub_n(size_t inlen, const uint8_t *cc_sized_by(inlen) der);
CC_NONNULL((1, 3))
int ccrsa_import_pub(ccrsa_pub_ctx_t key, size_t inlen, const uint8_t *cc_sized_by(inlen) der);
CC_NONNULL((1))
size_t ccrsa_export_priv_size(const ccrsa_full_ctx_t key);
CC_NONNULL((1, 3))
int ccrsa_export_priv(const ccrsa_full_ctx_t key, size_t out_len, uint8_t *cc_sized_by(out_len) out);
CC_NONNULL((2))
cc_size ccrsa_import_priv_n(size_t inlen, const uint8_t *cc_sized_by(inlen) der);
CC_NONNULL_ALL
int ccrsa_import_priv(ccrsa_full_ctx_t key, size_t inlen, const uint8_t *cc_sized_by(inlen) der);
CC_NONNULL((1, 2))
int ccrsa_get_pubkey_components(const ccrsa_pub_ctx_t pubkey, uint8_t *cc_unsafe_indexable modulus, size_t *modulusLength, uint8_t *cc_unsafe_indexable exponent, size_t *exponentLength);
CC_NONNULL((1, 2))
int ccrsa_get_fullkey_components(const ccrsa_full_ctx_t key, uint8_t *cc_unsafe_indexable modulus, size_t *modulusLength,
                                 uint8_t *cc_unsafe_indexable d, size_t *dLength,
                                 uint8_t *cc_unsafe_indexable p, size_t *pLength,
                                 uint8_t *cc_unsafe_indexable q, size_t *qLength);
void ccrsa_dump_public_key(ccrsa_pub_ctx_t key);
void ccrsa_dump_full_key(ccrsa_full_ctx_t key);
const struct ccdigest_info *ccsha1_di(void);
CC_PTRCHECK_CAPABLE_HEADER()


const struct ccdigest_info *ccsha224_di(void);
const struct ccdigest_info *ccsha256_di(void);
const struct ccdigest_info *ccsha384_di(void);
const struct ccdigest_info *ccsha512_di(void);
const struct ccdigest_info *ccsha512_256_di(void);
CC_PTRCHECK_CAPABLE_HEADER()

struct cczp;
CC_NONNULL((1))
cc_size cczp_n(cczp_const_t zp);
CC_NONNULL((1))
const cc_unit * cc_indexable cczp_prime(cczp_const_t zp);
CC_NONNULL((1))
size_t cczp_bitlen(cczp_const_t zp);
CC_PTRCHECK_CAPABLE_HEADER()

typedef uint8_t cc_fault_canary_t[CC_FAULT_CANARY_SIZE];
const char *cc_impl_name(cc_impl_t impl);
int cc_lock_init(cc_lock_ctx_t *ctx, const char *group_name);
CC_PTRCHECK_CAPABLE_HEADER()


















CC_INLINE void cc_store16_be(uint16_t x, uint8_t cc_sized_by(2) * y)
{
    y[0] = (uint8_t)(x >> 8);
    y[1] = (uint8_t)(x);
}
CC_INLINE uint16_t cc_load16_be(const uint8_t cc_sized_by(2) * y)
{
    return (uint16_t) (((uint16_t)(y[0])) << 8) | ((uint16_t)(y[1]));
}


CC_INLINE void cc_store64_le(uint64_t x, uint8_t cc_sized_by(8) * y)
{
    y[7] = (uint8_t)(x >> 56);
    y[6] = (uint8_t)(x >> 48);
    y[5] = (uint8_t)(x >> 40);
    y[4] = (uint8_t)(x >> 32);
    y[3] = (uint8_t)(x >> 24);
    y[2] = (uint8_t)(x >> 16);
    y[1] = (uint8_t)(x >> 8);
    y[0] = (uint8_t)(x);
}
CC_INLINE uint64_t cc_load64_le(const uint8_t cc_sized_by(8) * y)
{
    return (((uint64_t)(y[7])) << 56) | (((uint64_t)(y[6])) << 48) | (((uint64_t)(y[5])) << 40) | (((uint64_t)(y[4])) << 32) |
           (((uint64_t)(y[3])) << 24) | (((uint64_t)(y[2])) << 16) | (((uint64_t)(y[1])) << 8) | ((uint64_t)(y[0]));
}


CC_INLINE void cc_store32_le(uint32_t x, uint8_t cc_sized_by(4) * y)
{
    y[3] = (uint8_t)(x >> 24);
    y[2] = (uint8_t)(x >> 16);
    y[1] = (uint8_t)(x >> 8);
    y[0] = (uint8_t)(x);
}
CC_INLINE uint32_t cc_load32_le(const uint8_t cc_sized_by(4) * y)
{
    return (((uint32_t)(y[3])) << 24) | (((uint32_t)(y[2])) << 16) | (((uint32_t)(y[1])) << 8) | ((uint32_t)(y[0]));
}










CC_NORETURN
void cc_abort(const char *msg);
void cc_try_abort(const char *msg);
void cc_try_abort_if(bool condition, const char *msg);
__ptrcheck_abi_assume_single();
OS_CLOSED_ENUM(CEType, uint32_t,
               kCETypeUnknown = 0,
               kCETypeDictionary = 1,
               kCETypeSequence = 2,
               kCETypeInteger = 3,
               kCETypeString = 4,
               kCETypeBool = 5,
               kCETypeData = 6);
__ptrcheck_abi_assume_single();
CEError_t CEDeserialize(CEQueryContext_t ctx, CESerializedElement_t *__counted_by(elementsLength) elements, size_t elementsLength);
__ptrcheck_abi_assume_single();
der_vm_context_t der_vm_execute(const der_vm_context_t context, CEQueryOperation_t op);
der_vm_context_t der_vm_execute_nocopy(const der_vm_context_t context, const CEQueryOperation_t* op);
der_vm_context_t der_vm_execute_seq_nocopy(const der_vm_context_t context, const CEQueryOperation_t *__counted_by(queryLength) query, size_t queryLength);
CEError_t der_vm_iterate(const der_vm_context_t context, void* user_data, der_vm_iteration_callback callback);
bool der_vm_context_is_valid(const der_vm_context_t context);
CEType_t der_vm_CEType_from_context(const der_vm_context_t context, ccder_tag* tag);
CEType_t der_vm_CEType_from_ccder_tag(const der_vm_context_t context, ccder_tag tag);
int64_t der_vm_integer_from_context(const der_vm_context_t context);
CEBuffer der_vm_string_from_context(const der_vm_context_t context);
bool der_vm_bool_from_context(const der_vm_context_t context);
CEBuffer der_vm_data_from_context(const der_vm_context_t context);
CEBuffer der_vm_buffer_from_context(const der_vm_context_t context);
int CEBuffer_cmp(const CEBuffer left, const CEBuffer right);
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"


__ptrcheck_abi_assume_single();
OS_ENUM(CEVersion, int64_t,
        kCEVersionInvalid = 0,
        kCEVersionZero = 1,
        kCEVersionOne = 2);
CEError_t CEValidateWithOptions(const CERuntime_t rt, CEValidationOptions* options, CEValidationResult* result, const uint8_t *__ended_by(blob_end) blob, const uint8_t* blob_end) __result_use_check;
CEError_t CEAcquireManagedContext(const CERuntime_t rt, CEValidationResult validationResult, CEQueryContext_t* ctx) __result_use_check;
CEError_t CEReleaseManagedContext(CEQueryContext_t* ctx);
OS_ENUM(CEQueryOpOpcode, int64_t,
        kCEOpNoop = 0,
        kCEOpSelectKey = 1,
        kCEOpSelectIndex = 2,
        kCEOpMatchString = 3,
        kCEOpMatchStringPrefix = 4,
        kCEOpMatchBool = 5,
        kCEOpStringValueAllowed = 6,
        kCEOpMatchInteger = 7,
        kCEOpStringPrefixValueAllowed = 8,
        kCEOpSelectKeyWithPrefix = 9,
        kCEOpIntegerValueAllowed = 10,
        kCEOpMatchType = 11,
        kCEOpMatchData = 12,
        kCEOpMatchDataValueAllowed = 13,
        kCEOpMaxOperation = 14, 
        kCEOpDynamic = 0x1LL << 62);
CEError_t CEPrepareQuery(CEPrepareOptions_t options, CEQueryOperation_t *__counted_by(queryLength) query, size_t queryLength);
CEError_t CEContextIsSubset(CEQueryContext_t subset, CEQueryContext_t superset);
#error This is a private API, please consult with the Trusted Execution team before using this. Misusing these functions will lead to security issues.

__ptrcheck_abi_assume_single();
CEError_t CEAcquireUnmanagedContext(const CERuntime_t rt, CEValidationResult validationResult, struct CEQueryContext* ctx);
CEQueryOperation_t* CECreateStringOpInplace(CEQueryOperation_t* storage, CEQueryOpOpcode_t op, const char *__counted_by(len) data, size_t len);
CEQueryOperation_t* CECreateNumericOpInplace(CEQueryOperation_t* storage, CEQueryOpOpcode_t op, int64_t param);
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"

__ptrcheck_abi_assume_single();
CE_DEF_ERROR(kCENoError);
CE_DEF_ERROR(kCEAPIMisuse);
CE_DEF_ERROR(kCEInvalidArgument);
CE_DEF_ERROR(kCEAllocationFailed);
CE_DEF_ERROR(kCEMalformedEntitlements);
CE_DEF_ERROR(kCEQueryCannotBeSatisfied);
CE_DEF_ERROR(kCENotEligibleForAcceleration);
const char *__unsafe_indexable CEGetErrorString(CEError_t error);
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"






typedef 





CEError_t CEIndexSizeForContext(CEQueryContext_t context, size_t* size);
CEError_t CEBuildIndexForContext(CEQueryContext_t context);
CEError_t CEFreeIndexForContext(CEQueryContext_t context);
bool CEContextIsAccelerated(CEQueryContext_t context);
#error This is a private API, please consult with the Trusted Execution team before using this. Misusing these functions will lead to security issues.


OSPtr<OSDictionary> CEQueryContextToOSDictionary(CEQueryContext_t entitlements);
OSPtr<OSObject> CEQueryContextToOSObject(CEQueryContext_t context);
__ptrcheck_abi_assume_single();
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"

__ptrcheck_abi_assume_single();
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"


__ptrcheck_abi_assume_single();
#error "Please include <CoreEntitlements/CoreEntitlements.h> instead of this file"


__ptrcheck_abi_assume_single();
OS_CLOSED_ENUM(CESerializedElementType, int64_t,
               
               kCESerializedBool = 1,
               
               kCESerializedString = 2,
               
               kCESerializedKey = 3,
               
               kCESerializedInteger = 4,
               
               kCESerializedArrayBegin = 5,
               
               kCESerializedArrayEnd = 6,
               
               
               
               
               kCESerializedDictionaryBegin = 7,
               
               kCESerializedDictionaryEnd = 8,
               
               kCESerializedData = 9,
               );
CEError_t CESizeXMLSerialization(CESerializedElement_t elements[__counted_by(elementsCount)], size_t elementsCount, size_t* requiredSize) __result_use_check;
CEError_t CESerializeWithOptions(const CERuntime_t runtime, CEValidationOptions* options, CESerializedElement_t elements[__counted_by(elementsCount)], size_t elementsCount, uint8_t *__ended_by(end) start, uint8_t* end) __result_use_check;
CEError_t CESerialize(const CERuntime_t runtime, CESerializedElement_t elements[__counted_by(elementsCount)], size_t elementsCount, uint8_t *__ended_by(end) start, uint8_t* end) __result_use_check;
CEError_t CESerializeXML(const CERuntime_t runtime, CESerializedElement_t elements[__counted_by(elementsCount)], size_t elementsCount, uint8_t *__ended_by(end) start, uint8_t* end) __result_use_check;
__BEGIN_DECLS

__ptrcheck_abi_assume_single()

typedef 

extern const CTAsn1Item CTOidItemAppleImg4Manifest;
CT_int CTParseCertificateSet(
    const CT_uint8_t * __ended_by(der_end) der,
    const CT_uint8_t *der_end,
    CTAsn1Item * __counted_by(certStorageLen) certStorage,
    CT_size_t certStorageLen,
    CT_size_t *numParsedCerts);
CT_int CTParseExtensionValue(
    const CT_uint8_t * __counted_by(certLen) certData,
    CT_size_t certLen,
    const CT_uint8_t *__counted_by(extensionOidLen) extensionOidData,
    CT_size_t extensionOidLen,
    const CT_uint8_t * __counted_by(*extensionValueLen) *extensionValueData,
    CT_size_t *extensionValueLen);
CT_int CTParseKey(
    const CT_uint8_t * __counted_by(certLen) certData,
    CT_size_t certLen,
    const CT_uint8_t *__counted_by(*keyLen) *keyData,
    CT_size_t *keyLen);
CT_int CTEvaluateSavageCerts(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CT_bool *isProdCert);
CT_int CTEvaluateSavageCertsWithUID(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CT_uint8_t *__counted_by(UIDLen) UIDData, CT_size_t UIDLen,
    CT_bool *isProdCert);
CT_int CTEvaluateYonkersCerts(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CT_uint8_t *__counted_by(UIDLen) UIDData, CT_size_t UIDLen,
    CT_bool *isProdCert);
CT_int CTEvaluateSensorCerts(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(intermediateMarkerLen) intermediateMarker, CT_size_t intermediateMarkerLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CT_uint8_t *__counted_by(UIDLen) UIDData, CT_size_t UIDLen,
    CT_bool *isProdCert);
CT_int CTEvaluateAcrt(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluateUcrt(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluateUcrtTestRoot(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluateBAASystem(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluateBAASystemTestRoot(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CTBAAIdentity *identity);
CT_int CTEvaluateBAAUser(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CTBAAIdentity *identity);
CT_int CTEvaluateBAAUserTestRoot(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    CTBAAIdentity *identity);
CT_int CTEvaluateBAAAccessory(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(rootKeyLen) rootKeyData, CT_size_t rootKeyLen,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    const CT_uint8_t *__counted_by(*propertiesLen) *propertiesData, CT_size_t *propertiesLen);
CT_int CTEvaluateSatori(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    CT_bool allowTestRoot,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluatePragueSignatureCMS(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    const CT_uint8_t *__counted_by(detachedDataLen) detachedData, CT_size_t detachedDataLen,
    CT_bool allowTestRoot,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTEvaluateKDLSignatureCMS(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,                    
    const CT_uint8_t *__counted_by(detachedDataLen) detachedData, CT_size_t detachedDataLen,      
    CT_bool allowTestRoot,                                          
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen);
CT_int CTParseAmfiCMS(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    CoreTrustDigestType maxDigestType,
    const CT_uint8_t *__counted_by(*leafCertLen) *leafCert, CT_size_t *leafCertLen,
    const CT_uint8_t *__counted_by(*contentLen) *contentData, CT_size_t *contentLen,
    CoreTrustDigestType *cmsDigestType,
    CoreTrustPolicyFlags *policyFlags);
CT_int CTVerifyAmfiCMS(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    const CT_uint8_t *__counted_by(digestLen) digestData, CT_size_t digestLen,
    CoreTrustDigestType maxDigestType,
    CoreTrustDigestType *hashAgilityDigestType,
    const CT_uint8_t *__counted_by(*hashAgilityDigestLen) *hashAgilityDigestData, CT_size_t *hashAgilityDigestLen);
CT_int CTVerifyAmfiCertificateChain(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    CT_bool allow_test_hierarchy,
    CoreTrustDigestType maxDigestType,
    CoreTrustPolicyFlags *policyFlags);
CT_int CTEvaluateAMFICodeSignatureCMS(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    const CT_uint8_t *__counted_by(detachedDataLen) detachedData, CT_size_t detachedDataLen,
    CT_bool allow_test_hierarchy,
    const CT_uint8_t *__counted_by(*leafCertLen) *leafCert, CT_size_t *leafCertLen,
    CoreTrustPolicyFlags *policyFlags,
    CoreTrustDigestType *cmsDigestType,
    CoreTrustDigestType *hashAgilityDigestType,
    const CT_uint8_t *__counted_by(*digestLen) *digestData, CT_size_t *digestLen);
CT_int CTEvaluateAMFICodeSignatureCMS_MaxDigestType(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    const CT_uint8_t *__counted_by(detachedDataLen) detachedData, CT_size_t detachedDataLen,
    CT_bool allow_test_hierarchy,
    CoreTrustDigestType maxDigestType,
    const CT_uint8_t *__counted_by(*leafCertLen) *leafCert, CT_size_t *leafCertLen,
    CoreTrustPolicyFlags *policyFlags,
    CoreTrustDigestType *cmsDigestType,
    CoreTrustDigestType *hashAgilityDigestType,
    const CT_uint8_t *__counted_by(*digestLen) *digestData, CT_size_t *digestLen);
int CTEvaluateAMFICodeSignatureCMSPubKey(
    const CT_uint8_t *__counted_by(cmsLen) cmsData, CT_size_t cmsLen,
    const CT_uint8_t *__counted_by(detachedDataLen) detachedData, CT_size_t detachedDataLen,
    const CT_uint8_t *__counted_by(anchorPublicKeyLen) anchorPublicKey, CT_size_t anchorPublicKeyLen,
    CoreTrustDigestType *cmsDigestType,
    CoreTrustDigestType *hashAgilityDigestType,
    const CT_uint8_t *__counted_by(*digestLen) *digestData, CT_size_t *digestLen);
CT_int CTParseAccessoryCerts(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(*leafCertLen) *leafCertData, CT_size_t *leafCertLen,
    const CT_uint8_t *__counted_by(*subCACertLen) *subCACertData, CT_size_t *subCACertLen,
    CoreTrustPolicyFlags *flags);
CT_int CTEvaluateAccessoryCert(
    const CT_uint8_t *__counted_by(leafCertLen) leafCertData, CT_size_t leafCertLen,
    const CT_uint8_t *__counted_by(subCACertLen) subCACertData, CT_size_t subCACertLen,
    const CT_uint8_t *__counted_by(anchorCertLen) anchorCertData, CT_size_t anchorCertLen,
    CoreTrustPolicyFlags policy,
    const CT_uint8_t *__counted_by(*leafKeyLen) *leafKeyData, CT_size_t *leafKeyLen,
    const CT_uint8_t *__counted_by(*extensionValueLen) *extensionValueData, CT_size_t *extensionValueLen);
CT_int CTEvaluateAppleSSL(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(hostnameLen) hostnameData, CT_size_t hostnameLen,
    CT_uint64_t leafMarker,
    CT_bool allowTestRoots);
CT_int CTEvaluateAppleSSLWithOptionalTemporalCheck(
    const CT_uint8_t *__counted_by(certsLen) certsData, CT_size_t certsLen,
    const CT_uint8_t *__counted_by(hostnameLen) hostnameData, CT_size_t hostnameLen,
    CT_uint64_t leafMarker,
    CT_bool allowTestRoots,
    CT_bool checkTemporalValidity);
int CTEvaluateProvisioningProfile(
    const CT_uint8_t *__counted_by(provisioningProfileLen) provisioningProfileData, CT_size_t provisioningProfileLen,
    CT_bool allowTestRoots,
    const CT_uint8_t *__counted_by(*contentLen) *contentData, CT_size_t *contentLen);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_host);
#pragma mark API

IMAGE4_API_AVAILABLE_FALL_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL3
const image4_coprocessor_t *_Nullable
image4_coprocessor_resolve_from_manifest(
	const void *__sized_by(manifest_len) manifest,
	size_t manifest_len,
	const image4_coprocessor_t *_Nullable coprocs[
		_Nonnull __static_size IMAGE4_COPROCESSOR_ARRAY_CNT]);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_coprocessor_resolve_from_manifest);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_environment_init);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT
image4_environment_t *_Nullable
image4_environment_new(
	const image4_coprocessor_t *_Nullable coproc,
	image4_coprocessor_handle_t handle);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_new);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1
void
image4_environment_set_secure_boot(
	image4_environment_t *nv,
	image4_secure_boot_t secure_boot);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_set_secure_boot);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1
void
image4_environment_set_nonce_domain(
	image4_environment_t *nv,
	uint32_t nonce_domain);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_set_nonce_domain);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL2
void
image4_environment_set_callbacks(
	image4_environment_t *nv,
	const image4_environment_callbacks_t *callbacks,
	void *_Nullable _ctx);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_set_callbacks);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1
void
image4_environment_identify(
	const image4_environment_t *nv);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_identify);
IMAGE4_API_AVAILABLE_FALL_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
const struct ccdigest_info *
image4_environment_get_digest_info(
		const image4_environment_t *nv);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_get_digest_info);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL2 OS_NONNULL3
errno_t
image4_environment_copy_nonce_digest(
	const image4_environment_t *nv,
	uint8_t d[__static_size _Nonnull IMAGE4_DIGEST_MAX_LEN],
	size_t *d_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_copy_nonce_digest);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
errno_t
image4_environment_roll_nonce(
	const image4_environment_t *nv);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_roll_nonce);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL2
errno_t
image4_environment_generate_nonce_proposal(
	const image4_environment_t *nv,
	uint8_t d[__static_size _Nonnull IMAGE4_DIGEST_MAX_LEN],
	size_t *d_len,
	uint8_t n[__static_array_or_null(IMAGE4_NONCE_MAX_LEN)],
	size_t *_Nullable n_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_generate_nonce_proposal);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL2
errno_t
image4_environment_commit_nonce_proposal(
	const image4_environment_t *nv,
	const uint8_t d[__static_size _Nonnull IMAGE4_DIGEST_MAX_LEN],
	size_t *d_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_commit_nonce_proposal);
IMAGE4_API_AVAILABLE_FALL_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL2
errno_t
image4_environment_flash(
	const image4_environment_t *nv,
	const void *__sized_by(object_len) object,
	size_t object_len,
	uint8_t n[__static_array_or_null(IMAGE4_NONCE_MAX_LEN)],
	size_t *_Nullable n_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_flash);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1
void
image4_environment_destroy(
	image4_environment_t *_Nonnull *_Nullable nv);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_environment_destroy);
#pragma mark Retired
IMAGE4_XNU_RETIRED_DIRECT(image4_environment_get_nonce_handle);
__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN
OS_ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma mark Types

OS_CLOSED_ENUM(image4_identifier_constraint, uint64_t,
	IMAGE4_IDENTIFIER_CONSTRAINT_EQ,
	IMAGE4_IDENTIFIER_CONSTRAINT_LT,
	IMAGE4_IDENTIFIER_CONSTRAINT_LE,
	IMAGE4_IDENTIFIER_CONSTRAINT_GT,
	IMAGE4_IDENTIFIER_CONSTRAINT_GE,
	IMAGE4_IDENTIFIER_CONSTRAINT_NE,
	IMAGE4_IDENTIFIER_CONSTRAINT_UN,
	IMAGE4_IDENTIFIER_CONSTRAINT_NA,
);
#pragma mark API

IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
image4_identifier_constraint_t
image4_identifier_get_constraint(const image4_identifier_t *id4);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_identifier_get_constraint);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
const char *
image4_identifier_get_constraint_cstr(const image4_identifier_t *id4);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_identifier_get_constraint_cstr);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
uint32_t
image4_identifier_get_fourcc(const image4_identifier_t *id4);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_identifier_get_fourcc);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1
const char *
image4_identifier_get_fourcc_cstr(const image4_identifier_t *id4);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_identifier_get_fourcc_cstr);
__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN
OS_ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma mark Supporting Types

OS_CLOSED_OPTIONS(image4_trust_flags, uint64_t,
	IMAGE4_TRUST_FLAG_INIT = 0,
	IMAGE4_TRUST_FLAG_VIOLATION_PANIC = (1 << 0),
);
OS_CLOSED_ENUM(image4_trust_section, uint64_t,
	IMAGE4_TRUST_SECTION_CERTIFICATE,
	IMAGE4_TRUST_SECTION_MANIFEST,
	IMAGE4_TRUST_SECTION_OBJECT,
	IMAGE4_TRUST_SECTION_RESTORE_INFO,
	IMAGE4_TRUST_SECTION_PAYLOAD_PROPERTIES,
	_IMAGE4_TRUST_SECTION_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_init);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_WARN_RESULT OS_NONNULL1 OS_NONNULL2 OS_NONNULL3
image4_trust_t *_Nullable
image4_trust_new(
	const image4_environment_t *nv,
	const image4_trust_evaluation_t *eval,
	const void *__sized_by(manifest_len) manifest,
	size_t manifest_len,
	image4_trust_flags_t flags);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_new);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL3
void
image4_trust_set_payload(
	image4_trust_t *trst,
	uint32_t type,
	const void *__sized_by(len) bytes,
	size_t len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_set_payload);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL2
void
image4_trust_set_booter(
	image4_trust_t *trst,
	const image4_trust_t *booter);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_set_booter);
IMAGE4_API_AVAILABLE_FALL_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL2
void
image4_trust_set_result_buffer(
	image4_trust_t *trst,
	void *_Nullable __sized_by(p_len) p,
	size_t p_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_set_result_buffer);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL4
void
image4_trust_record_property_bool(
	image4_trust_t *trst,
	image4_trust_section_t type,
	uint32_t tag,
	bool *vp,
	const bool *_Nullable *_Nullable vpp);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_record_property_bool);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL4
void
image4_trust_record_property_integer(
	image4_trust_t *trst,
	image4_trust_section_t type,
	uint32_t tag,
	uint64_t *vp,
	const uint64_t *_Nullable *_Nullable vpp);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_record_property_integer);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL4 OS_NONNULL5
void
image4_trust_record_property_data(
	image4_trust_t *trst,
	image4_trust_section_t type,
	uint32_t tag,
	const void *_Nullable *_Nonnull vp,
	size_t *vp_len);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_record_property_data);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1 OS_NONNULL3
void
image4_trust_evaluate(
	const image4_trust_t *trst,
	void *_Nullable _ctx,
	image4_trust_evaluation_result_t result);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_evaluate);
IMAGE4_API_AVAILABLE_SPRING_2024
OS_EXPORT OS_NONNULL1
void
image4_trust_destroy(
	image4_trust_t *_Nonnull *_Nullable trst);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_trust_destroy);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_evaluation_exec);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_evaluation_preflight);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_evaluation_sign);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_evaluation_boot);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_trust_evaluation_normalize);
OS_CLOSED_ENUM(image4_secure_boot, uint64_t,
	IMAGE4_SECURE_BOOT_FULL,
	IMAGE4_SECURE_BOOT_REDUCED,
	IMAGE4_SECURE_BOOT_LEAST,
	IMAGE4_SECURE_BOOT_NONE,
	_IMAGE4_SECURE_BOOT_CNT,
);
OS_ALWAYS_INLINE OS_WARN_RESULT
static inline int
image4_secure_boot_check(image4_secure_boot_t sb)
{
	if (sb > _IMAGE4_SECURE_BOOT_CNT) {
		__builtin_trap();
	}
	if (sb == _IMAGE4_SECURE_BOOT_CNT) {
		return 1;
	}
	return 0;
}

#pragma mark API Objects

typedef struct _image4_coprocessor image4_coprocessor_t;
OS_CLOSED_OPTIONS(img4_chip_instance_omit, uint64_t,
	IMG4_CHIP_INSTANCE_OMIT_CEPO = (1 << 0),
	IMG4_CHIP_INSTANCE_OMIT_BORD = (1 << 1),
	IMG4_CHIP_INSTANCE_OMIT_CHIP = (1 << 2),
	IMG4_CHIP_INSTANCE_OMIT_SDOM = (1 << 3),
	IMG4_CHIP_INSTANCE_OMIT_ECID = (1 << 4),
	IMG4_CHIP_INSTANCE_OMIT_CPRO = (1 << 5),
	IMG4_CHIP_INSTANCE_OMIT_CSEC = (1 << 6),
	IMG4_CHIP_INSTANCE_OMIT_EPRO = (1 << 7),
	IMG4_CHIP_INSTANCE_OMIT_ESEC = (1 << 8),
	IMG4_CHIP_INSTANCE_OMIT_IUOU = (1 << 9),
	IMG4_CHIP_INSTANCE_OMIT_RSCH = (1 << 10),
	IMG4_CHIP_INSTANCE_OMIT_EUOU = (1 << 11),
	IMG4_CHIP_INSTANCE_OMIT_ESDM = (1 << 12),
	IMG4_CHIP_INSTANCE_OMIT_FPGT = (1 << 13),
	IMG4_CHIP_INSTANCE_OMIT_UDID = (1 << 14),
	IMG4_CHIP_INSTANCE_OMIT_FCHP = (1 << 15),
	IMG4_CHIP_INSTANCE_OMIT_TYPE = (1 << 16),
	IMG4_CHIP_INSTANCE_OMIT_STYP = (1 << 17),
	IMG4_CHIP_INSTANCE_OMIT_CLAS = (1 << 18),
);
IMG4_API_AVAILABLE_20200508
typedef 


IMG4_API_AVAILABLE_20200508
OS_CLOSED_OPTIONS(img4_firmware_flags, uint64_t,
	IMG4_FIRMWARE_FLAG_INIT,
	IMG4_FIRMWARE_FLAG_ATTACHED_MANIFEST = (1 << 0),
	IMG4_FIRMWARE_FLAG_BARE = (1 << 1),
	IMG4_FIRMWARE_FLAG_SUBSEQUENT_STAGE = (1 << 2),
	IMG4_FIRMWARE_FLAG_RESPECT_AMNM = (1 << 3),
	IMG4_FIRMWARE_FLAG_PASSTHROUGH = (1 << 4),
	IMG4_FIRMWARE_FLAG_FORCE_ANTI_REPLAY = (1 << 5),
);
IMG4_API_AVAILABLE_20210521
OS_CLOSED_ENUM(img4_nonce_domain_index, uint64_t,
	IMG4_NONCE_DOMAIN_INDEX_TEST = 0,
	IMG4_NONCE_DOMAIN_INDEX_TRUST_CACHE,
	IMG4_NONCE_DOMAIN_INDEX_PDI,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX,
	IMG4_NONCE_DOMAIN_INDEX_DDI,
	IMG4_NONCE_DOMAIN_INDEX_EPHEMERAL_CRYPTEX,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX1_SNUF_STUB,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX1_BOOT,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX1_ASSET,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX1_GENERIC,
	IMG4_NONCE_DOMAIN_INDEX_CRYPTEX1_SIMULATOR,
	_IMG4_NONCE_DOMAIN_INDEX_CNT,
);
#error "Please #include <img4/firmware.h> instead of this file directly"

__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN
OS_ASSUME_PTR_ABI_SINGLE_BEGIN


IMG4_API_AVAILABLE_20200508
OS_CLOSED_ENUM(img4_identifier, uint64_t,
	IMG4_IDENTIFIER_CEPO,
	IMG4_IDENTIFIER_BORD,
	IMG4_IDENTIFIER_CHIP,
	IMG4_IDENTIFIER_SDOM,
	IMG4_IDENTIFIER_ECID,
	IMG4_IDENTIFIER_CPRO,
	IMG4_IDENTIFIER_CSEC,
	IMG4_IDENTIFIER_EPRO,
	IMG4_IDENTIFIER_ESEC,
	IMG4_IDENTIFIER_IUOU,
	IMG4_IDENTIFIER_RSCH,
	IMG4_IDENTIFIER_CHMH,
	IMG4_IDENTIFIER_AMNM,
	IMG4_IDENTIFIER_EUOU,
	IMG4_IDENTIFIER_LOVE,
	IMG4_IDENTIFIER_ESDM,
	IMG4_IDENTIFIER_FPGT,
	IMG4_IDENTIFIER_UDID,
	IMG4_IDENTIFIER_FCHP,
	IMG4_IDENTIFIER_TYPE,
	IMG4_IDENTIFIER_STYP,
	IMG4_IDENTIFIER_CLAS,
	IMG4_IDENTIFIER_SPIH,
	IMG4_IDENTIFIER_NSPH,
	IMG4_IDENTIFIER_STNG,
	IMG4_IDENTIFIER_VUID,
	_IMG4_IDENTIFIER_CNT,
);
IMG4_API_AVAILABLE_20210521
OS_CLOSED_ENUM(img4_runtime_object_spec_index, uint64_t,
	IMG4_RUNTIME_OBJECT_SPEC_INDEX_MANIFEST,
	IMG4_RUNTIME_OBJECT_SPEC_INDEX_SUPPLEMENTAL_ROOT,
	IMG4_RUNTIME_OBJECT_SPEC_INDEX_SUPPLEMENTAL_OBJECT,
	IMG4_RUNTIME_OBJECT_SPEC_INDEX_LOCAL_POLICY,
	_IMG4_RUNTIME_OBJECT_SPEC_INDEX_CNT,
);
IMG4_API_AVAILABLE_20200508
OS_CLOSED_ENUM(img4_log_level, uint64_t,
	IMG4_LOG_LEVEL_ERROR,
	IMG4_LOG_LEVEL_INFO,
	IMG4_LOG_LEVEL_DEBUG,
	_IMG4_LOG_LEVEL_CNT,
);
OS_FORMAT_PRINTF(4, 5)
IMG4_API_AVAILABLE_20200508
typedef void (*img4_runtime_log_t)(
	const img4_runtime_t *rt,
	void *_Nullable handle,
	img4_log_level_t level,
	const char *fmt,
	...
);
extern int nlist (const char *filename, struct nlist *list);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_ap);
OS_CLOSED_ENUM(image4_coprocessor_handle_ap, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_AP = 0,
	IMAGE4_COPROCESSOR_HANDLE_AP_FF00,
	IMAGE4_COPROCESSOR_HANDLE_AP_FF01,
	IMAGE4_COPROCESSOR_HANDLE_AP_FF06,
	IMAGE4_COPROCESSOR_HANDLE_AP_PDI,
	IMAGE4_COPROCESSOR_HANDLE_AP_SRDP,
	IMAGE4_COPROCESSOR_HANDLE_AP_RESERVED_0,
	IMAGE4_COPROCESSOR_HANDLE_AP_RESERVED_1,
	IMAGE4_COPROCESSOR_HANDLE_AP_RESERVED_2,
	IMAGE4_COPROCESSOR_HANDLE_AP_DDI,
	IMAGE4_COPROCESSOR_HANDLE_AP_BOOTPC,
	_IMAGE4_COPROCESSOR_HANDLE_AP_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_ap_local);
OS_CLOSED_ENUM(image4_coprocessor_handle_ap_local, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_AP_LOCAL = 0,
	IMAGE4_COPROCESSOR_HANDLE_AP_LOCAL_RESERVED_0,
	IMAGE4_COPROCESSOR_HANDLE_AP_LOCAL_RESERVED_1,
	IMAGE4_COPROCESSOR_HANDLE_AP_LOCAL_RESERVED_2,
	_IMAGE4_COPROCESSOR_HANDLE_AP_LOCAL_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_bootpc);
OS_CLOSED_ENUM(image4_coprocessor_handle_bootpc, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_BOOTPC_SHA2_224 = 0,
	IMAGE4_COPROCESSOR_HANDLE_BOOTPC_SHA2_256,
	IMAGE4_COPROCESSOR_HANDLE_BOOTPC_SHA2_384,
	IMAGE4_COPROCESSOR_HANDLE_BOOTPC_SHA2_512,
	_IMAGE4_COPROCESSOR_HANDLE_BOOTPC_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_cryptex1);
OS_CLOSED_ENUM(image4_coprocessor_handle_cryptex1, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_BOOT = 0,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_BOOT_LIVE,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_ASSET_BRAIN,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_GENERIC,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_RESERVED_0,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_RESERVED_1,
	IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_RESERVED_2,
	_IMAGE4_COPROCESSOR_HANDLE_CRYPTEX1_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_sep);
OS_CLOSED_ENUM(image4_coprocessor_handle_sep, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_SEP = 0,
	_IMAGE4_COPROCESSOR_HANDLE_SEP_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_vma2);
OS_CLOSED_ENUM(image4_coprocessor_handle_vma2, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_VMA2 = 0,
	IMAGE4_COPROCESSOR_HANDLE_VMA2_PDI,
	IMAGE4_COPROCESSOR_HANDLE_VMA2_DDI,
	_IMAGE4_COPROCESSOR_HANDLE_VMA2_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_vma3);
OS_CLOSED_ENUM(image4_coprocessor_handle_vma3, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_VMA3 = 0,
	_IMAGE4_COPROCESSOR_HANDLE_VMA3_CNT,
);
IMAGE4_XNU_AVAILABLE_INDIRECT(_image4_coprocessor_x86);
OS_CLOSED_ENUM(image4_coprocessor_handle_x86, image4_coprocessor_handle_t,
	IMAGE4_COPROCESSOR_HANDLE_X86 = 0,
	_IMAGE4_COPROCESSOR_HANDLE_X86_CNT,
);
__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN


OS_CLOSED_ENUM(darwin_cryptex1_nonce, uint32_t,
	DARWIN_CRYPTEX1_NONCE_BOOT = 1,
	DARWIN_CRYPTEX1_NONCE_ASSET_BRAIN = 2,
	DARWIN_CRYPTEX1_NONCE_GENERIC = 3,
	DARWIN_CRYPTEX1_NONCE_SIMULATOR_RUNTIME = 4,
	DARWIN_CRYPTEX1_NONCE_MOBILE_ASSET_DFU = 5,
	DARWIN_CRYPTEX1_NONCE_RESERVED_0 = 6,
	DARWIN_CRYPTEX1_NONCE_RESERVED_1 = 7,
	_DARWIN_CRYPTEX1_NONCE_CNT,
);
OS_CLOSED_ENUM(image4_cs_trap, uint64_t,
	IMAGE4_CS_TRAP_KMOD_SET_RELEASE_TYPE,
	IMAGE4_CS_TRAP_RESERVED_0,
	IMAGE4_CS_TRAP_RESERVED_1,
	IMAGE4_CS_TRAP_NONCE_SET,
	IMAGE4_CS_TRAP_NONCE_ROLL,
	IMAGE4_CS_TRAP_IMAGE_ACTIVATE,
	IMAGE4_CS_TRAP_KMOD_SET_BOOT_UUID,
	_IMAGE4_CS_TRAP_CNT,
);
#pragma mark Trap Arguments


image4_cs_trap_argv_decl(kmod_set_release_type) {
	char __cs_copy csmx_release_type[64];
};
image4_cs_trap_argv_decl(kmod_set_boot_uuid) {
	uint8_t __cs_copy csmx_uuid[16];
};
image4_cs_trap_argv_decl(nonce_set) {
	uint64_t csmx_handle;
	uint32_t csmx_flags;
	uint8_t __cs_copy csmx_clear[16];
	uint8_t __cs_copy csmx_cipher[16];
};
image4_cs_trap_argv_decl(nonce_roll) {
	uint64_t csmx_handle;
};
image4_cs_trap_argv_decl(image_activate) {
	uint64_t csmx_handle;
	image4_cs_addr_t __cs_xfer csmx_payload;
	uint32_t csmx_payload_len;
	image4_cs_addr_t __cs_xfer csmx_manifest;
	uint32_t csmx_manifest_len;
};
#pragma mark API

OS_EXPORT OS_WARN_RESULT
image4_cs_trap_handler_t _Nullable
image4_cs_trap_resolve_handler(image4_cs_trap_t trap);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_cs_trap_resolve_handler);
OS_EXPORT OS_WARN_RESULT
ssize_t
image4_cs_trap_vector_size(image4_cs_trap_t trap);
IMAGE4_XNU_AVAILABLE_DIRECT(image4_cs_trap_vector_size);
__BEGIN_DECLS
OS_ASSUME_NONNULL_BEGIN
OS_ASSUME_PTR_ABI_SINGLE_BEGIN

#pragma mark Restricted API

OS_EXPORT OS_WARN_RESULT OS_NONNULL2
const void *_Nullable
image4_environment_get_firmware_chip(
	uint32_t v,
	const image4_environment_t *nv);
int      au_open(void)
__AUDIT_API_DEPRECATED;
int      au_write(int d, token_t *m)
__AUDIT_API_DEPRECATED;
int      au_close(int d, int keep, short event)
__AUDIT_API_DEPRECATED;
int      au_close_buffer(int d, short event, u_char *buffer, size_t *buflen)
__AUDIT_API_DEPRECATED;
int      au_close_token(token_t *tok, u_char *buffer, size_t *buflen)
__AUDIT_API_DEPRECATED;
token_t *au_to_file(const char *file, struct timeval tm)
__AUDIT_API_DEPRECATED;
token_t *au_to_header32_tm(int rec_size, au_event_t e_type, au_emod_t e_mod,
    struct timeval tm)
__AUDIT_API_DEPRECATED;
token_t *au_to_header32_ex_tm(int rec_size, au_event_t e_type, au_emod_t e_mod,
    struct timeval tm, struct auditinfo_addr *aia)
__AUDIT_API_DEPRECATED;
token_t *au_to_header64_tm(int rec_size, au_event_t e_type, au_emod_t e_mod,
    struct timeval tm)
__AUDIT_API_DEPRECATED;
token_t *au_to_me(void)
__AUDIT_API_DEPRECATED;
token_t *au_to_arg(char n, const char *text, uint32_t v)
__AUDIT_API_DEPRECATED;
token_t *au_to_arg32(char n, const char *text, uint32_t v)
__AUDIT_API_DEPRECATED;
token_t *au_to_arg64(char n, const char *text, uint64_t v)
__AUDIT_API_DEPRECATED;
token_t *au_to_attr(struct vnode_au_info *vni)
__AUDIT_API_DEPRECATED;
token_t *au_to_attr32(struct vnode_au_info *vni)
__AUDIT_API_DEPRECATED;
token_t *au_to_attr64(struct vnode_au_info *vni)
__AUDIT_API_DEPRECATED;
token_t *au_to_data(char unit_print, char unit_type, char unit_count,
    const char *p)
__AUDIT_API_DEPRECATED;
token_t *au_to_exit(int retval, int err)
__AUDIT_API_DEPRECATED;
token_t *au_to_groups(int *groups)
__AUDIT_API_DEPRECATED;
token_t *au_to_newgroups(uint16_t n, gid_t *groups)
__AUDIT_API_DEPRECATED;
token_t *au_to_in_addr(struct in_addr *internet_addr)
__AUDIT_API_DEPRECATED;
token_t *au_to_in_addr_ex(struct in6_addr *internet_addr)
__AUDIT_API_DEPRECATED;
token_t *au_to_ip(struct ip *ip)
__AUDIT_API_DEPRECATED;
token_t *au_to_ipc(char type, int id)
__AUDIT_API_DEPRECATED;
token_t *au_to_ipc_perm(struct ipc_perm *perm)
__AUDIT_API_DEPRECATED;
token_t *au_to_iport(uint16_t iport)
__AUDIT_API_DEPRECATED;
token_t *au_to_opaque(const char *data, uint16_t bytes)
__AUDIT_API_DEPRECATED;
token_t *au_to_path(const char *path)
__AUDIT_API_DEPRECATED;
token_t *au_to_process(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_process32(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_process64(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_process_ex(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_process32_ex(au_id_t auid, uid_t euid, gid_t egid,
    uid_t ruid, gid_t rgid, pid_t pid, au_asid_t sid,
    au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_process64_ex(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_return(char status, uint32_t ret)
__AUDIT_API_DEPRECATED;
token_t *au_to_return32(char status, uint32_t ret)
__AUDIT_API_DEPRECATED;
token_t *au_to_return64(char status, uint64_t ret)
__AUDIT_API_DEPRECATED;
token_t *au_to_seq(long audit_count)
__AUDIT_API_DEPRECATED;
token_t *au_to_socket_ex(u_short so_domain, u_short so_type,
    struct sockaddr *sa_local, struct sockaddr *sa_remote)
__AUDIT_API_DEPRECATED;
token_t *au_to_sock_inet(struct sockaddr_in *so)
__AUDIT_API_DEPRECATED;
token_t *au_to_sock_inet32(struct sockaddr_in *so)
__AUDIT_API_DEPRECATED;
token_t *au_to_sock_inet128(struct sockaddr_in6 *so)
__AUDIT_API_DEPRECATED;
token_t *au_to_sock_unix(struct sockaddr_un *so)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject32(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject64(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject_ex(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject32_ex(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_subject64_ex(au_id_t auid, uid_t euid, gid_t egid, uid_t ruid,
    gid_t rgid, pid_t pid, au_asid_t sid, au_tid_addr_t *tid)
__AUDIT_API_DEPRECATED;
token_t *au_to_exec_args(char *args, int argc)
__AUDIT_API_DEPRECATED;
token_t *au_to_exec_env(char *envs, int envc)
__AUDIT_API_DEPRECATED;
token_t *au_to_certificate_hash(char *hash, int hashc)
__AUDIT_API_14_DEPRECATED;
token_t *au_to_krb5_principal(char *principal, int princ)
__AUDIT_API_14_DEPRECATED;
token_t *au_to_text(const char *text)
__AUDIT_API_DEPRECATED;
token_t *au_to_kevent(struct kevent *kev)
__AUDIT_API_DEPRECATED;
token_t *au_to_trailer(int rec_size)
__AUDIT_API_DEPRECATED;
token_t *au_to_zonename(const char *zonename)
__AUDIT_API_DEPRECATED;
token_t *au_to_identity(uint32_t signer_type, const char* signing_id,
    u_char signing_id_trunc, const char* team_id, u_char team_id_trunc,
    uint8_t* cdhash, uint16_t cdhash_len)
__AUDIT_API_14_DEPRECATED;
int      au_bsm_to_domain(u_short bsm_domain, int *local_domainp)
__AUDIT_API_DEPRECATED;
int      au_bsm_to_errno(u_char bsm_error, int *errorp)
__AUDIT_API_DEPRECATED;
int      au_bsm_to_fcntl_cmd(u_short bsm_fcntl_cmd, int *local_fcntl_cmdp)
__AUDIT_API_DEPRECATED;
int      au_bsm_to_socket_type(u_short bsm_socket_type,
    int *local_socket_typep)
__AUDIT_API_DEPRECATED;
u_short  au_domain_to_bsm(int local_domain)
__AUDIT_API_DEPRECATED;
u_char   au_errno_to_bsm(int local_errno)
__AUDIT_API_DEPRECATED;
u_short  au_fcntl_cmd_to_bsm(int local_fcntl_command)
__AUDIT_API_DEPRECATED;
u_short  au_socket_type_to_bsm(int local_socket_type)
__AUDIT_API_DEPRECATED;
extern void act_set_astbsd(thread_t);
extern void bsd_ast(thread_t);
extern void kevent_ast(thread_t thread, uint16_t bits);
extern void act_set_astkevent(thread_t thread, uint16_t bits);
extern uint16_t act_clear_astkevent(thread_t thread, uint16_t bits);
extern bool act_set_ast_reset_pcs(struct task *task, thread_t thread);
extern void ast_dtrace_on(void);
extern void act_set_astproc_resource(thread_t);
extern void proc_filedesc_ast(task_t task);
int authenticate_root_with_chunklist(const char *rootdmg_path, boolean_t *out_enforced);
int authenticate_root_version_check(void);
int authenticate_bootkc_uuid(void);
int authenticate_libkern_uuid(void);
extern void
set_proc_name(struct image_params *imgp, proc_t p);
OS_CLOSED_ENUM(memorystatus_action, uint32_t,
    MEMORYSTATUS_KILL_HIWATER,     
    MEMORYSTATUS_KILL_AGGRESSIVE,     
    MEMORYSTATUS_KILL_TOP_PROCESS,     
    MEMORYSTATUS_WAKE_SWAPPER,  
    MEMORYSTATUS_PROCESS_SWAPIN_QUEUE, 
    MEMORYSTATUS_KILL_SUSPENDED_SWAPPABLE, 
    MEMORYSTATUS_KILL_SWAPPABLE, 
    MEMORYSTATUS_KILL_IDLE, 
    MEMORYSTATUS_KILL_LONG_IDLE, 
    MEMORYSTATUS_KILL_NONE,     
    );
bool memorystatus_is_system_healthy(const memorystatus_system_health_t *status);
uint32_t memorystatus_pick_kill_cause(const memorystatus_system_health_t *status);
bool memorystatus_avail_pages_below_pressure(void);
bool memorystatus_avail_pages_below_critical(void);
bool is_reason_thrashing(unsigned cause);
bool is_reason_zone_map_exhaustion(unsigned cause);
memorystatus_action_t memorystatus_pick_action(jetsam_state_t state,
    uint32_t *kill_cause, bool highwater_remaining,
    bool suspended_swappable_apps_remaining,
    bool swappable_apps_remaining, int *jld_idle_kills);
#pragma mark Logging Utilities

__enum_decl(memorystatus_log_level_t, unsigned int, {
	MEMORYSTATUS_LOG_LEVEL_DEFAULT = 0,
	MEMORYSTATUS_LOG_LEVEL_INFO = 1,
	MEMORYSTATUS_LOG_LEVEL_DEBUG = 2,
});
#pragma mark Jetsam Priority Management


void memstat_update_priority_locked(proc_t p, int priority,
    memstat_priority_options_t options);
static inline bool
_memstat_proc_is_aging(proc_t p)
{
	return p->p_memstat_dirty & P_DIRTY_AGING_IN_PROGRESS;
}

static inline bool
_memstat_proc_is_tracked(proc_t p)
{
	return p->p_memstat_dirty & P_DIRTY_TRACK;
}

static inline bool
_memstat_proc_is_dirty(proc_t p)
{
	return p->p_memstat_dirty & P_DIRTY_IS_DIRTY;
}

static inline bool
_memstat_proc_is_internal(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_INTERNAL;
}

static inline bool
_memstat_proc_can_idle_exit(proc_t p)
{
	return _memstat_proc_is_tracked(p) &&
	       (p->p_memstat_dirty & P_DIRTY_ALLOW_IDLE_EXIT);
}

static inline bool
_memstat_proc_has_priority_assertion(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_PRIORITY_ASSERTION;
}

static inline bool
_memstat_proc_is_managed(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_MANAGED;
}

static inline bool
_memstat_proc_is_frozen(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_FROZEN;
}

static inline bool
_memstat_proc_is_suspended(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_SUSPENDED;
}

static inline void
_memstat_proc_set_suspended(proc_t p)
{
	LCK_MTX_ASSERT(&proc_list_mlock, LCK_ASSERT_OWNED);
	if (!_memstat_proc_is_suspended(p)) {
		p->p_memstat_state |= P_MEMSTAT_SUSPENDED;
	}
}

static inline void
_memstat_proc_set_resumed(proc_t p)
{
	LCK_MTX_ASSERT(&proc_list_mlock, LCK_ASSERT_OWNED);
	if (_memstat_proc_is_suspended(p)) {
		p->p_memstat_state &= ~P_MEMSTAT_SUSPENDED;
	}
}


static inline bool
_memstat_proc_is_elevated(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_USE_ELEVATED_INACTIVE_BAND;
}


static inline bool
_memstat_proc_cached_memlimit_is_fatal(proc_t p)
{
	return p->p_memstat_state & P_MEMSTAT_FATAL_MEMLIMIT;
}


static inline bool
_memstat_proc_memlimit_is_fatal(proc_t p, bool is_active)
{
	const uint32_t flag = is_active ?
	    P_MEMSTAT_MEMLIMIT_ACTIVE_FATAL : P_MEMSTAT_MEMLIMIT_INACTIVE_FATAL;
	return p->p_memstat_state & flag;
}

static inline bool
_memstat_proc_active_memlimit_is_fatal(proc_t p)
{
	return _memstat_proc_memlimit_is_fatal(p, true);
}

static inline bool
_memstat_proc_inactive_memlimit_is_fatal(proc_t p)
{
	return _memstat_proc_memlimit_is_fatal(p, false);
}

#pragma mark Jetsam


bool memstat_evaluate_page_shortage(
	bool *should_enforce_memlimits,
	bool *should_idle_exit,
	bool *should_jetsam,
	bool *should_reap);
int memorystatus_ballast_control(bool drain);
bool memorystatus_kill_on_sustained_pressure(void);
bool memstat_kill_idle_process(memorystatus_kill_cause_t cause,
    uint64_t *footprint_out);
bool memstat_kill_with_jetsam_reason_sync(pid_t pid, os_reason_t jetsam_reason);
uint32_t memstat_get_proccnt_upto_priority(uint32_t max_bucket_index);
uint32_t memstat_get_idle_proccnt(void);
extern void mbuf_tag_id_first_last(mbuf_tag_id_t * first, mbuf_tag_id_t * last);
extern errno_t mbuf_tag_id_find_internal(const char *string,
    mbuf_tag_id_t * out_id, int create);
load_return_t fatfile_validate_fatarches(vm_offset_t data_ptr, vm_size_t data_size, off_t file_size);
load_return_t fatfile_getbestarch(vm_offset_t data_ptr, vm_size_t data_size, struct image_params *imgp, struct fat_arch *archret, bool affinity);
load_return_t fatfile_getbestarch_for_cputype(cpu_type_t cputype, cpu_subtype_t cpusubtype,
    vm_offset_t data_ptr, vm_size_t data_size, struct image_params *imgp, struct fat_arch *archret);
load_return_t fatfile_getarch_with_bits(integer_t archbits,
    vm_offset_t data_ptr, vm_size_t data_size, struct fat_arch *archret);
__options_decl(HR_flags_t, uint32_t, {
	BrowserHostEntitlementMask       = 0x01,
	BrowserGPUEntitlementMask        = 0x02,
	BrowserNetworkEntitlementMask    = 0x04,
	BrowserWebContentEntitlementMask = 0x08,
});
load_return_t load_machfile(
	struct image_params     *imgp,
	struct mach_header      *header,
	thread_t                thread,
	vm_map_t                *mapp,
	load_result_t           *result);
load_return_t
validate_potential_simulator_binary(
	cpu_type_t               exectype,
	struct image_params      *imgp,
	off_t                    file_offset,
	off_t                    macho_size);
__BEGIN_DECLS




LIST_HEAD(soflow_hash_head, soflow_hash_entry);
bool soflow_fill_hash_entry_from_address(struct soflow_hash_entry *, bool, struct sockaddr *, bool);
bool soflow_fill_hash_entry_from_inp(struct soflow_hash_entry *, bool, struct inpcb *, bool);
void *soflow_db_get_feature_context(struct soflow_db *, u_int64_t);
u_int64_t soflow_db_get_feature_context_id(struct soflow_db *, struct sockaddr *, struct sockaddr *);
void soflow_feat_set_functions(soflow_feat_gc_needed_func, soflow_feat_gc_perform_func,
    soflow_feat_detach_entry_func, soflow_feat_detach_db_func);
bool soflow_db_apply(struct soflow_db *, soflow_entry_apply_func, void *context);
extern void tty_dev_register(struct tty_dev_t *dev);
extern int ttnread(struct tty *tp);
extern void termios32to64(struct termios32 *in, struct user_termios *out);
extern void termios64to32(struct user_termios *in, struct termios32 *out);
__BEGIN_DECLS

int     copyin(const user_addr_t uaddr, void *__sized_by(len) kaddr, size_t len) OS_WARN_RESULT;
int     copyout(const void *__sized_by(len) kaddr, user_addr_t udaddr, size_t len);
__BEGIN_DECLS
static inline int
imax(int a, int b)
{
	return a > b ? a : b;
}
static inline int
imin(int a, int b)
{
	return a < b ? a : b;
}
static inline long
lmax(long a, long b)
{
	return a > b ? a : b;
}
static inline long
lmin(long a, long b)
{
	return a < b ? a : b;
}
static inline u_int
max(u_int a, u_int b)
{
	return a > b ? a : b;
}
static inline u_int
min(u_int a, u_int b)
{
	return a < b ? a : b;
}
static inline u_int32_t
ulmax(u_int32_t a, u_int32_t b)
{
	return a > b ? a : b;
}
static inline u_int32_t
ulmin(u_int32_t a, u_int32_t b)
{
	return a < b ? a : b;
}




extern int      ffs(unsigned int);
extern int      ffsll(unsigned long long);
extern int      fls(unsigned int);
extern int      flsll(unsigned long long);
extern u_int32_t        random(void);
extern size_t   scanc(size_t, u_char *, const u_char *, u_char);
extern long     strtol(const char*, char **, int);
extern u_long   strtoul(const char *, char **, int);
extern quad_t   strtoq(const char *, char **, int);
extern u_quad_t strtouq(const char *, char **, int);
extern char     *strsep(char **, const char *);
extern void     *memchr(const void *, int, size_t);
extern void     url_decode(char *str);
int     snprintf(char *__counted_by(count), size_t count, const char *, ...) __printflike(3, 4);
int     scnprintf(char *__counted_by(count), size_t count, const char *, ...) __printflike(3, 4);
const char *
    tsnprintf(char *__counted_by(count) dst, size_t count, const char *fmt, ...) __printflike(3, 4);
const char *
    vtsnprintf(char *__counted_by(count) dst, size_t count, const char *fmt, va_list ap) __printflike(3, 0);
int     sprintf(char *bufp, const char *, ...) __deprecated __printflike(2, 3);
int     sscanf(const char *, char const *, ...) __scanflike(2, 3);
int     printf(const char *, ...) __printflike(1, 2);
int     _consume_printf_args(int, ...);
uint32_t        crc32(uint32_t crc, const void *bufp, size_t len);
int     copystr(const void *kfaddr, void *kdaddr, size_t len, size_t *done);
int     copyinstr(const user_addr_t uaddr, void *kaddr, size_t len, size_t *done) OS_WARN_RESULT;
int     copyoutstr(const void *kaddr, user_addr_t udaddr, size_t len, size_t *done);
int     copyin_atomic32(const user_addr_t user_addr, uint32_t *u32);
int     copyin_atomic32_wait_if_equals(const user_addr_t user_addr, uint32_t u32);
int     copyin_atomic64(const user_addr_t user_addr, uint64_t *u64);
int     copyout_atomic32(uint32_t u32, user_addr_t user_addr);
int     copyout_atomic64(uint64_t u64, user_addr_t user_addr);
int     copyoutstr_prevalidate(const void *kaddr, user_addr_t uaddr, size_t len);
int vsscanf(const char *, char const *, va_list);
extern int      vprintf(const char *, va_list) __printflike(1, 0);
extern int      vsnprintf(char *, size_t, const char *, va_list) __printflike(3, 0);
extern int      vscnprintf(char *, size_t, const char *, va_list) __printflike(3, 0);
extern bool     printf_log_locked(bool addcr, const char*, ...) __printflike(2, 3);
extern bool     vprintf_log_locked(const char *, va_list, bool driverkit) __printflike(1, 0);
extern void     osobject_retain(void * object);
extern void     osobject_release(void * object);
extern int      vsprintf(char *bufp, const char *, va_list) __deprecated __printflike(2, 0);
extern void invalidate_icache(vm_offset_t, unsigned, int);
extern void flush_dcache(vm_offset_t, unsigned, int);
extern void invalidate_icache64(addr64_t, unsigned, int);
extern void flush_dcache64(addr64_t, unsigned, int);
extern struct tty       *copy_constty(void);
extern struct tty       *set_constty(struct tty *);
int consopen(dev_t, int, int, struct proc *);
int consclose(dev_t, int, int, struct proc *);
int consread(dev_t, struct uio *, int);
int conswrite(dev_t, struct uio *, int);
int consioctl(dev_t, u_long, caddr_t, int, struct proc *);
int consselect(dev_t, int, void *, struct proc *);
int kmopen(dev_t, int, int, struct proc *);
int kmclose(dev_t, int, int, struct proc *);
int kmread(dev_t, struct uio *, int);
int kmwrite(dev_t, struct uio *, int);
int kmioctl(dev_t, u_long, caddr_t, int, struct proc *);
int kmputc(dev_t, char);
int grade_binary(cpu_type_t, cpu_subtype_t, cpu_subtype_t, bool allow_simulator_binary);
boolean_t binary_match(cpu_type_t mask_bits, cpu_type_t req_cpu,
    cpu_subtype_t req_subcpu, cpu_type_t test_cpu,
    cpu_subtype_t test_subcpu);
size_t
net_bloom_filter_get_size(uint32_t num_bits);
void
net_bloom_filter_destroy(struct net_bloom_filter *filter);
void
net_bloom_filter_insert(struct net_bloom_filter *filter,
    const void * __sized_by(length)buffer,
    uint32_t length);
bool
net_bloom_filter_contains(struct net_bloom_filter *filter,
    const void * __sized_by(length)buffer,
    uint32_t length);
extern void  bpfattach(ifnet_t interface, u_int data_link_type,
    u_int header_length);
extern errno_t  bpf_attach(ifnet_t interface, u_int32_t data_link_type,
    u_int32_t header_length, bpf_send_func send, bpf_tap_func tap);
extern void bpf_tap_in(ifnet_t interface, u_int32_t dlt, mbuf_t packet,
    void *__sized_by(header_len) header, size_t header_len);
extern void bpf_tap_out(ifnet_t interface, u_int32_t dlt, mbuf_t packet,
    void *__sized_by(header_len) header, size_t header_len);
void    bstp_attach(struct bstp_state *, struct bstp_cb_ops *);
void    bstp_detach(struct bstp_state *);
void    bstp_init(struct bstp_state *);
void    bstp_stop(struct bstp_state *);
int     bstp_create(struct bstp_state *, struct bstp_port *, struct ifnet *);
int     bstp_enable(struct bstp_port *);
void    bstp_disable(struct bstp_port *);
void    bstp_destroy(struct bstp_port *);
void    bstp_linkstate(struct ifnet *, int);
int     bstp_set_htime(struct bstp_state *, int);
int     bstp_set_fdelay(struct bstp_state *, int);
int     bstp_set_maxage(struct bstp_state *, int);
int     bstp_set_holdcount(struct bstp_state *, int);
int     bstp_set_protocol(struct bstp_state *, int);
int     bstp_set_priority(struct bstp_state *, int);
int     bstp_set_port_priority(struct bstp_port *, int);
int     bstp_set_path_cost(struct bstp_port *, uint32_t);
int     bstp_set_edge(struct bstp_port *, int);
int     bstp_set_autoedge(struct bstp_port *, int);
int     bstp_set_ptp(struct bstp_port *, int);
int     bstp_set_autoptp(struct bstp_port *, int);
void    bstp_input(struct bstp_port *, struct mbuf *);
void bstp_sys_init(void);
extern void cfil_register_m_tag(void);
extern void cfil_init(void);
extern boolean_t cfil_filter_present(void);
extern boolean_t cfil_sock_connected_pending_verdict(struct socket *so);
extern boolean_t cfil_sock_is_dead(struct socket *so);
extern boolean_t cfil_sock_tcp_add_time_wait(struct socket *so);
extern errno_t cfil_sock_attach(struct socket *so,
    struct sockaddr *local, struct sockaddr *remote, int dir);
extern errno_t cfil_sock_detach(struct socket *so);
extern int cfil_sock_data_out(struct socket *so, struct sockaddr  *to,
    struct mbuf *data, struct mbuf *control,
    uint32_t flags, struct soflow_hash_entry *);
extern int cfil_sock_data_in(struct socket *so, struct sockaddr *from,
    struct mbuf *data, struct mbuf *control,
    uint32_t flags, struct soflow_hash_entry *);
extern int cfil_sock_shutdown(struct socket *so, int *how);
extern void cfil_sock_is_closed(struct socket *so);
extern void cfil_sock_notify_shutdown(struct socket *so, int how);
extern void cfil_sock_close_wait(struct socket *so);
extern boolean_t cfil_sock_data_pending(struct sockbuf *sb);
extern int cfil_sock_data_space(struct sockbuf *sb);
extern void cfil_sock_buf_update(struct sockbuf *sb);
extern cfil_sock_id_t cfil_sock_id_from_socket(struct socket *so);
extern cfil_sock_id_t cfil_sock_id_from_datagram_socket(struct socket *so, struct sockaddr *local, struct sockaddr *remote);
extern struct m_tag *cfil_dgram_get_socket_state(struct mbuf *m, uint32_t *state_change_cnt,
    uint32_t *options, struct sockaddr **faddr, int *inp_flags);
extern boolean_t cfil_dgram_peek_socket_state(struct mbuf *m, int *inp_flags);
extern cfil_crypto_state_t
cfil_crypto_init_client(cfil_crypto_key client_key);
extern void
cfil_crypto_cleanup_state(cfil_crypto_state_t state);
extern int
cfil_crypto_sign_data(cfil_crypto_state_t state, cfil_crypto_data_t data,
    const struct iovec *__counted_by(extra_data_count)extra_data, size_t extra_data_count,
    cfil_crypto_signature signature, u_int32_t *signature_length);
int
devtimer_valid(devtimer_ref timer);
void
devtimer_retain(devtimer_ref timer);
void *
devtimer_arg0(devtimer_ref timer);
devtimer_ref
devtimer_create(devtimer_process_func process_func, void * arg0);
void
devtimer_invalidate(devtimer_ref timer);
void
devtimer_release(devtimer_ref timer);
void
devtimer_set_absolute(devtimer_ref t,
    struct timeval abs_time,
    devtimer_timeout_func func,
    void * arg1, void * arg2);
void
devtimer_set_relative(devtimer_ref t,
    struct timeval rel_time,
    devtimer_timeout_func func,
    void * arg1, void * arg2);
void
devtimer_cancel(devtimer_ref t);
int
devtimer_enabled(devtimer_ref t);
int32_t
devtimer_current_secs(void);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct dlil_threading_info, dlil_threading_info);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct dlil_main_threading_info, dlil_main_threading_info);
extern void dlil_init(void);
extern errno_t ifp_if_ioctl(struct ifnet *, unsigned long, void *);
extern errno_t ifp_if_output(struct ifnet *, struct mbuf *);
extern void ifp_if_start(struct ifnet *);
extern errno_t dlil_set_bpf_tap(ifnet_t, bpf_tap_mode, bpf_packet_func);
extern errno_t dlil_send_arp_internal(ifnet_t, u_int16_t,
    const struct sockaddr_dl *, const struct sockaddr *,
    const struct sockaddr_dl *, const struct sockaddr *);
extern net_thread_marks_t net_thread_marks_push(u_int32_t);
extern net_thread_marks_t net_thread_unmarks_push(u_int32_t);
extern void net_thread_marks_pop(net_thread_marks_t);
extern void net_thread_unmarks_pop(net_thread_marks_t);
extern u_int32_t net_thread_is_marked(u_int32_t);
extern u_int32_t net_thread_is_unmarked(u_int32_t);
extern int dlil_output(ifnet_t, protocol_family_t, mbuf_t, void *,
    const struct sockaddr *, int, struct flowadv *);
extern void dlil_input_packet_list(struct ifnet *, struct mbuf *);
extern void dlil_input_packet_list_extended(struct ifnet *, struct mbuf *,
    u_int32_t, ifnet_model_t);
extern errno_t dlil_resolve_multi(struct ifnet *,
    const struct sockaddr *, struct sockaddr *, size_t);
extern errno_t dlil_send_arp(ifnet_t, u_int16_t, const struct sockaddr_dl *,
    const struct sockaddr *, const struct sockaddr_dl *,
    const struct sockaddr *, u_int32_t);
extern int dlil_attach_filter(ifnet_t, const struct iff_filter *,
    interface_filter_t *, u_int32_t);
extern void dlil_detach_filter(interface_filter_t);
extern boolean_t dlil_has_ip_filter(void);
extern boolean_t dlil_has_if_filter(struct ifnet *);
extern void dlil_proto_unplumb_all(ifnet_t);
extern int dlil_post_msg(struct ifnet *, u_int32_t, u_int32_t,
    struct net_event_data *, u_int32_t, boolean_t);
extern void dlil_post_sifflags_msg(struct ifnet *);
extern int dlil_post_complete_msg(struct ifnet *, struct kev_msg *);
extern int dlil_alloc_local_stats(struct ifnet *);
extern void ifnet_filter_update_tso(struct ifnet *, boolean_t filter_enable);
extern errno_t dlil_rxpoll_validate_params(struct ifnet_poll_params *);
extern void dlil_rxpoll_update_params(struct ifnet *,
    struct ifnet_poll_params *);
extern void ifnet_poll(struct ifnet *);
extern errno_t ifnet_input_poll(struct ifnet *, struct mbuf *,
    struct mbuf *, const struct ifnet_stat_increment_param *);
extern int dlil_if_acquire(u_int32_t, const void * __sized_by(uniqueid_len), size_t uniqueid_len, const char * __null_terminated, struct ifnet **);
extern void dlil_if_release(struct ifnet *ifp);
extern errno_t dlil_if_ref(struct ifnet *);
extern errno_t dlil_if_free(struct ifnet *);
extern int dlil_node_present(struct ifnet *, struct sockaddr *, int32_t, int,
    int, u_int8_t[48]);
extern void dlil_node_absent(struct ifnet *, struct sockaddr *);
extern int dlil_node_present_v2(struct ifnet *, struct sockaddr *, struct sockaddr_dl *, int32_t, int,
    int, u_int8_t[48]);
extern const void *dlil_ifaddr_bytes(const struct sockaddr_dl *, size_t *,
    kauth_cred_t *);
static inline const void *__header_indexable
__attribute__((always_inline))
dlil_ifaddr_bytes_indexable(const struct sockaddr_dl * sdl, size_t *sizep, kauth_cred_t *cred)
{
	const void *__unsafe_indexable raw_bytes = dlil_ifaddr_bytes(sdl, sizep, cred);
	return __unsafe_forge_bidi_indexable(const void*, raw_bytes, *sizep);
}

extern void dlil_report_issues(struct ifnet *, u_int8_t[DLIL_MODIDLEN],
    u_int8_t[DLIL_MODARGLEN]);
extern int proto_hash_value(u_int32_t);
extern const char *dlil_kev_dl_code_str(u_int32_t);
extern errno_t dlil_rxpoll_set_params(struct ifnet *,
    struct ifnet_poll_params *, boolean_t);
extern errno_t dlil_rxpoll_get_params(struct ifnet *,
    struct ifnet_poll_params *);
extern errno_t dlil_output_handler(struct ifnet *, struct mbuf *);
extern errno_t dlil_input_handler(struct ifnet *, struct mbuf *,
    struct mbuf *, const struct ifnet_stat_increment_param *,
    boolean_t, struct thread *);
extern void dlil_ifclassq_setup(struct ifnet *, struct ifclassq *);
__attribute__((always_inline))
static inline void
ifp_inc_traffic_class_in(struct ifnet *ifp, struct mbuf *m)
{
	if (!(m->m_flags & M_PKTHDR)) {
		return;
	}

	switch (m_get_traffic_class(m)) {
	case MBUF_TC_BE:
		ifp->if_tc.ifi_ibepackets++;
		ifp->if_tc.ifi_ibebytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_BK:
		ifp->if_tc.ifi_ibkpackets++;
		ifp->if_tc.ifi_ibkbytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_VI:
		ifp->if_tc.ifi_ivipackets++;
		ifp->if_tc.ifi_ivibytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_VO:
		ifp->if_tc.ifi_ivopackets++;
		ifp->if_tc.ifi_ivobytes += (u_int64_t)m->m_pkthdr.len;
		break;
	default:
		break;
	}

	if (mbuf_is_traffic_class_privileged(m)) {
		ifp->if_tc.ifi_ipvpackets++;
		ifp->if_tc.ifi_ipvbytes += (u_int64_t)m->m_pkthdr.len;
	}
}


__attribute__((always_inline))
static inline void
ifp_inc_traffic_class_out(struct ifnet *ifp, struct mbuf *m)
{
	if (!(m->m_flags & M_PKTHDR)) {
		return;
	}

	switch (m_get_traffic_class(m)) {
	case MBUF_TC_BE:
		ifp->if_tc.ifi_obepackets++;
		ifp->if_tc.ifi_obebytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_BK:
		ifp->if_tc.ifi_obkpackets++;
		ifp->if_tc.ifi_obkbytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_VI:
		ifp->if_tc.ifi_ovipackets++;
		ifp->if_tc.ifi_ovibytes += (u_int64_t)m->m_pkthdr.len;
		break;
	case MBUF_TC_VO:
		ifp->if_tc.ifi_ovopackets++;
		ifp->if_tc.ifi_ovobytes += (u_int64_t)m->m_pkthdr.len;
		break;
	default:
		break;
	}

	if (mbuf_is_traffic_class_privileged(m)) {
		ifp->if_tc.ifi_opvpackets++;
		ifp->if_tc.ifi_opvbytes += (u_int64_t)m->m_pkthdr.len;
	}
}

extern void ifnet_ioctl_async(struct ifnet *, u_long);
extern mbuf_t bridge_early_input(struct ifnet *ifp, mbuf_t m, u_int32_t cnt);
__private_extern__ int ifnet_set_nat64prefix(struct ifnet *,
    struct ipv6_prefix *__counted_by(NAT64_MAX_NUM_PREFIXES));
__private_extern__ int ifnet_get_nat64prefix(struct ifnet *,
    struct ipv6_prefix *__counted_by(NAT64_MAX_NUM_PREFIXES));
extern kern_return_t dlil_affinity_set(struct thread *, u_int32_t);
extern boolean_t packet_has_vlan_tag(struct mbuf * m);
void log_hexdump(void *__sized_by(len) data, size_t len);
extern void if_flt_monitor_busy(struct ifnet *);
extern void if_flt_monitor_unbusy(struct ifnet *);
extern void if_flt_monitor_enter(struct ifnet *);
extern void if_flt_monitor_leave(struct ifnet *);
extern void dlil_allocation_zones_init(void);
extern struct dlil_ifnet * dlif_ifnet_alloc(void);
extern void dlif_ifnet_free(struct dlil_ifnet *);
extern struct ifnet_filter * dlif_filt_alloc(void);
extern void dlif_filt_free(struct ifnet_filter *);
extern struct if_proto * dlif_proto_alloc(void);
extern void dlif_proto_free(struct if_proto * );
extern struct tcpstat_local * dlif_tcpstat_alloc(void);
extern void dlif_tcpstat_free(struct tcpstat_local *);
extern struct udpstat_local * dlif_udpstat_alloc(void);
extern void dlif_udpstat_free(struct udpstat_local *);
extern void if_proto_ref(struct if_proto *);
extern void if_proto_free(struct if_proto *);
extern struct ifaddr * dlil_alloc_lladdr(struct ifnet *ifp, const struct sockaddr_dl *ll_addr);
extern u_int32_t dlil_ifp_protolist(struct ifnet *ifp, protocol_family_t *list __counted_by(list_count),
    u_int32_t list_count);
extern void dlil_if_trace(struct dlil_ifnet *, int);
extern void _dlil_if_release(ifnet_t ifp, bool clear_in_use);
void dlil_input_stats_add(const struct ifnet_stat_increment_param *,
    struct dlil_threading_info *, struct ifnet *, boolean_t);
boolean_t dlil_input_stats_sync(struct ifnet *,
    struct dlil_threading_info *);
extern void dlil_dt_tcall_fn(thread_call_param_t, thread_call_param_t);
extern void dlil_clean_threading_info(struct dlil_threading_info *inp);
int dlil_create_input_thread(ifnet_t, struct dlil_threading_info *,
    thread_continue_t *);
void dlil_terminate_input_thread(struct dlil_threading_info *);
extern boolean_t dlil_is_rxpoll_input(thread_continue_t func);
boolean_t dlil_is_native_netif_nexus(ifnet_t ifp);
void dlil_incr_pending_thread_count(void);
void dlil_decr_pending_thread_count(void);
int dlil_is_clat_needed(protocol_family_t proto_family, mbuf_t m);
errno_t dlil_clat46(ifnet_t ifp, protocol_family_t *proto_family, mbuf_t *m);
errno_t dlil_clat64(ifnet_t ifp, protocol_family_t *proto_family, mbuf_t *m);
extern void dlil_if_lock(void);
extern void dlil_if_unlock(void);
extern void dlil_if_lock_assert(void);
extern void ifnet_head_lock_assert(ifnet_lock_assert_t what);
extern void ifnet_lock_assert(struct ifnet *ifp, ifnet_lock_assert_t what);
extern void ifnet_lock_shared(struct ifnet *ifp);
extern void ifnet_lock_exclusive(struct ifnet *ifp);
extern void ifnet_lock_done(struct ifnet *ifp);
extern void if_inet6data_lock_shared(struct ifnet *ifp);
extern void if_inet6data_lock_exclusive(struct ifnet *ifp);
extern void if_inet6data_lock_done(struct ifnet *ifp);
extern void ifnet_head_lock_shared(void);
extern void ifnet_head_lock_exclusive(void);
extern void ifnet_head_done(void);
extern void ifnet_head_assert_exclusive(void);
errno_t if_mcasts_update_async(struct ifnet *);
__attribute__((always_inline))
static inline const char *
drop_reason_str(drop_reason_t value)
{
	switch (value) {
		DROP_REASON_LIST
	default:
		return NULL;
	}
	;
}



extern uint32_t droptap_total_tap_count;
extern void droptap_init(void);
extern void droptap_input_mbuf(struct mbuf *, drop_reason_t, const char *,
    uint16_t, uint16_t, struct ifnet *, char *);
extern void droptap_output_mbuf(struct mbuf *, drop_reason_t, const char *,
    uint16_t, uint16_t, struct ifnet *);
extern errno_t ether_attach_inet(ifnet_t ifp, protocol_family_t protocol_family);
extern void ether_detach_inet(ifnet_t ifp, protocol_family_t protocol_family);
extern errno_t ether_attach_inet6(ifnet_t ifp, protocol_family_t protocol_family);
extern void ether_detach_inet6(ifnet_t ifp, protocol_family_t protocol_family);
extern errno_t ether_attach_at(struct ifnet *ifp, protocol_family_t proto_family);
extern void ether_detach_at(struct ifnet *ifp, protocol_family_t proto_family);
STAILQ_HEAD(flowadv_fclist, flowadv_fcentry);
__BEGIN_DECLS

extern void flowadv_init(void);
extern struct flowadv_fcentry *flowadv_alloc_entry(int);
extern void flowadv_free_entry(struct flowadv_fcentry *);
extern void flowadv_add(struct flowadv_fclist *);
extern void flowadv_add_entry(struct flowadv_fcentry *);
int bond_family_init(void);
#pragma pack(4)



#pragma pack()












#pragma pack(4)


#pragma pack()



#pragma pack(4)


#pragma pack()





#pragma pack(4)


#pragma pack()



#pragma pack(4)



#pragma pack()







#pragma pack(4)



#pragma pack()



#pragma pack(4)


#pragma pack()




#pragma pack(4)



#pragma pack()





extern u_int8_t bstp_etheraddr[ETHER_ADDR_LEN];
int     bridgeattach(int);
#pragma pack(4)



union ifbrip {
	struct in_addr  ifbrip_addr;
	struct in6_addr ifbrip_addr6;
};
__BEGIN_DECLS

extern int ether_family_init(void);
errno_t ether_demux(ifnet_t interface, mbuf_t packet, char* header,
    protocol_family_t *protocol);
errno_t ether_add_proto(ifnet_t interface, protocol_family_t protocol,
    const struct ifnet_demux_desc *demux_list __counted_by(demux_count), u_int32_t demux_count);
errno_t ether_del_proto(ifnet_t interface, protocol_family_t protocol);
errno_t ether_frameout_extended(ifnet_t interface, mbuf_t *packet,
    const struct sockaddr *dest, IFNET_LLADDR_T dest_lladdr,
    IFNET_FRAME_TYPE_T frame_type,
    u_int32_t *prepend_len, u_int32_t *postpend_len);
errno_t ether_ioctl(ifnet_t interface, u_int32_t command, void* data);
errno_t ether_check_multi(ifnet_t ifp, const struct sockaddr *multicast);
__private_extern__ void
if_fake_init(void);
extern void gif_init(void);
errno_t ipsec_register_control(void);
int ipsec_interface_isvalid(ifnet_t interface);
errno_t ipsec_inject_inbound_packet(ifnet_t     interface, mbuf_t packet);
void ipsec_set_pkthdr_for_interface(ifnet_t interface, mbuf_t packet, int family,
    uint32_t flowid);
void ipsec_set_ipoa_for_interface(ifnet_t interface, struct ip_out_args *ipoa);
void ipsec_set_ip6oa_for_interface(ifnet_t interface, struct ip6_out_args *ip6oa);
RB_PROTOTYPE_SC_PREV(__private_extern__, ll_reach_tree, if_llreach,
    ls_link, ifllr_cmp);
extern void ifnet_llreach_ifattach(struct ifnet *, boolean_t);
extern void ifnet_llreach_ifdetach(struct ifnet *);
extern struct if_llreach *ifnet_llreach_alloc(struct ifnet *, u_int16_t,
    void *__sized_by(alen),
    unsigned int alen, u_int32_t);
extern void ifnet_llreach_free(struct if_llreach *);
extern int ifnet_llreach_reachable(struct if_llreach *);
extern int ifnet_llreach_reachable_delta(struct if_llreach *, u_int64_t);
extern void ifnet_llreach_set_reachable(struct ifnet *, u_int16_t,
    void *__sized_by(alen) addr,
    unsigned int alen);
extern u_int64_t ifnet_llreach_up2calexp(struct if_llreach *, u_int64_t);
extern u_int64_t ifnet_llreach_up2upexp(struct if_llreach *, u_int64_t);
extern int ifnet_llreach_get_defrouter(struct ifnet *, sa_family_t,
    struct ifnet_llreach_info *);
extern void ifnet_lr2ri(struct if_llreach *, struct rt_reach_info *);
extern void ifnet_lr2iflri(struct if_llreach *, struct ifnet_llreach_info *);
extern void ifnet_lr2lri(struct if_llreach *, struct if_llreach_info *);
extern void iflr_addref(struct if_llreach *, int);
extern void iflr_remref(struct if_llreach *);
void if_ports_used_init(void);
void if_ports_used_update_wakeuuid(struct ifnet *);
bool if_ports_used_add_inpcb(const uint32_t ifindex, const struct inpcb *inp);
void if_ports_used_match_mbuf(struct ifnet *ifp, protocol_family_t proto_family,
    struct mbuf *m);
__private_extern__ void if_redirect_init(void);
void* utun_alloc(size_t size);
void utun_free(void *ptr);
errno_t utun_register_control(void);
#pragma pack(4)
















#pragma pack(push, 1)
enum ifnet_interface_advisory_version : uint8_t {
	
	IF_INTERFACE_ADVISORY_VERSION_1 = 1,
	IF_INTERFACE_ADVISORY_VERSION_MIN = IF_INTERFACE_ADVISORY_VERSION_1,
	
	IF_INTERFACE_ADVISORY_VERSION_2 = 2,
	IF_INTERFACE_ADVISORY_VERSION_CURRENT = IF_INTERFACE_ADVISORY_VERSION_2,
	IF_INTERFACE_ADVISORY_VERSION_MAX = IF_INTERFACE_ADVISORY_VERSION_2,
};
int vlan_family_init(void);
errno_t net_init_add(net_init_func_ptr  init_func);
extern void net_init_run(void);
extern void iptap_init(void);
extern int      sflt_permission_check(struct inpcb *inp);
extern void     sflt_initsock(struct socket *so);
extern void     sflt_termsock(struct socket *so);
extern errno_t  sflt_attach_internal(struct socket *so, sflt_handle     handle);
extern void     sflt_notify(struct socket *so, sflt_event_t event, void *param);
extern int      sflt_ioctl(struct socket *so, u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)) data);
extern int      sflt_bind(struct socket *so, const struct sockaddr *nam);
extern int      sflt_listen(struct socket *so);
extern int      sflt_accept(struct socket *head, struct socket *so,
    const struct sockaddr *local,
    const struct sockaddr *remote);
extern int      sflt_getsockname(struct socket *so, struct sockaddr **local);
extern int      sflt_getpeername(struct socket *so, struct sockaddr **remote);
extern int      sflt_connectin(struct socket *head,
    const struct sockaddr *remote);
extern int      sflt_connectout(struct socket *so, const struct sockaddr *nam);
extern int      sflt_setsockopt(struct socket *so, struct sockopt *sopt);
extern int      sflt_getsockopt(struct socket *so, struct sockopt *sopt);
extern int      sflt_data_out(struct socket *so, const struct sockaddr  *to,
    mbuf_t *data, mbuf_t *control, sflt_data_flag_t flags);
extern int      sflt_data_in(struct socket *so, const struct sockaddr *from,
    mbuf_t *data, mbuf_t *control, sflt_data_flag_t flags);
__BEGIN_DECLS




extern errno_t ifnet_allocate_internal(const struct ifnet_init_params *init,
    ifnet_t *interface);
extern errno_t ifnet_allocate_extended(const struct ifnet_init_eparams *init,
    ifnet_t *interface);
extern void ifnet_dispose(ifnet_t interface);
extern void ifnet_purge(ifnet_t interface);
extern errno_t ifnet_enqueue(ifnet_t interface, mbuf_t packet);
extern errno_t ifnet_dequeue(ifnet_t interface, mbuf_t *packet);
extern errno_t ifnet_dequeue_service_class(ifnet_t interface,
    mbuf_svc_class_t sc, mbuf_t *packet);
extern errno_t ifnet_dequeue_multi(ifnet_t interface, u_int32_t max,
    mbuf_t *first_packet, mbuf_t *last_packet, u_int32_t *cnt, u_int32_t *len);
extern errno_t ifnet_dequeue_multi_bytes(ifnet_t interface,
    u_int32_t max_bytes, mbuf_t *first_packet, mbuf_t *last_packet,
    u_int32_t *cnt, u_int32_t *len);
extern errno_t ifnet_dequeue_service_class_multi(ifnet_t interface,
    mbuf_svc_class_t sc, u_int32_t max, mbuf_t *first_packet,
    mbuf_t *last_packet, u_int32_t *cnt, u_int32_t *len);
extern errno_t ifnet_set_output_sched_model(ifnet_t interface,
    u_int32_t model);
extern errno_t ifnet_set_sndq_maxlen(ifnet_t interface, u_int32_t maxqlen);
extern errno_t ifnet_get_sndq_maxlen(ifnet_t interface, u_int32_t *maxqlen);
extern errno_t ifnet_get_sndq_len(ifnet_t interface, u_int32_t *packets);
extern errno_t ifnet_get_service_class_sndq_len(ifnet_t interface,
    mbuf_svc_class_t sc, u_int32_t *packets, u_int32_t *bytes);
extern errno_t ifnet_set_rcvq_maxlen(ifnet_t interface, u_int32_t maxqlen);
extern errno_t ifnet_get_rcvq_maxlen(ifnet_t interface, u_int32_t *maxqlen);
extern errno_t ifnet_set_poll_params(ifnet_t interface,
    ifnet_poll_params_t *poll_params);
extern errno_t ifnet_poll_params(ifnet_t interface,
    ifnet_poll_params_t *poll_params);
extern void ifnet_start(ifnet_t interface);
extern errno_t ifnet_flowid(ifnet_t interface, u_int32_t *flowid);
extern errno_t ifnet_enable_output(ifnet_t interface);
extern errno_t ifnet_disable_output(ifnet_t interface);
extern errno_t ifnet_reference(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_release(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_attach(ifnet_t interface,
    const struct sockaddr_dl *ll_addr)
__NKE_API_DEPRECATED;
extern errno_t ifnet_detach(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_interface_family_find(const char *module_string, ifnet_family_t *family_id)
__NKE_API_DEPRECATED;
extern void *ifnet_softc(ifnet_t interface)
__NKE_API_DEPRECATED;
extern const char *ifnet_name(ifnet_t interface)
__NKE_API_DEPRECATED;
extern ifnet_family_t ifnet_family(ifnet_t interface)
__NKE_API_DEPRECATED;
extern ifnet_subfamily_t ifnet_subfamily(ifnet_t interface);
extern u_int32_t ifnet_unit(ifnet_t interface)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_index(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_flags(ifnet_t interface, u_int16_t new_flags,
    u_int16_t mask)
__NKE_API_DEPRECATED;
extern u_int16_t ifnet_flags(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_eflags(ifnet_t interface, u_int32_t new_flags,
    u_int32_t mask);
extern u_int32_t ifnet_eflags(ifnet_t interface);
extern errno_t ifnet_set_idle_flags(ifnet_t interface, u_int32_t new_flags,
    u_int32_t mask);
extern u_int32_t ifnet_idle_flags(ifnet_t interface);
extern errno_t ifnet_set_link_quality(ifnet_t interface, int quality);
extern int ifnet_link_quality(ifnet_t interface);
extern errno_t ifnet_set_interface_state(ifnet_t interface,
    struct if_interface_state *if_interface_state);
extern int ifnet_get_interface_state(ifnet_t interface,
    struct if_interface_state *if_interface_state);
extern errno_t ifnet_inet_defrouter_llreachinfo(ifnet_t interface,
    struct ifnet_llreach_info *pinfo);
extern errno_t ifnet_inet6_defrouter_llreachinfo(ifnet_t interface,
    struct ifnet_llreach_info *pinfo);
extern errno_t ifnet_set_capabilities_supported(ifnet_t interface, u_int32_t new_caps,
    u_int32_t mask)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_capabilities_supported(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_capabilities_enabled(ifnet_t interface, u_int32_t new_caps,
    u_int32_t mask)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_capabilities_enabled(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_offload(ifnet_t interface, ifnet_offload_t offload)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_offload_enabled(ifnet_t interface, ifnet_offload_t offload)
__NKE_API_DEPRECATED;
extern ifnet_offload_t ifnet_offload(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_tso_mtu(ifnet_t interface, sa_family_t family,
    u_int32_t mtuLen)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_tso_mtu(ifnet_t interface, sa_family_t family,
    u_int32_t *mtuLen)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_wake_flags(ifnet_t interface, u_int32_t properties, u_int32_t mask)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_get_wake_flags(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_link_mib_data(ifnet_t interface, void *__sized_by(mibLen) mibData,
    u_int32_t mibLen)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_link_mib_data(ifnet_t interface, void *__sized_by(*mibLen) mibData,
    u_int32_t *mibLen)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_get_link_mib_data_length(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_attach_protocol(ifnet_t interface,
    protocol_family_t protocol_family,
    const struct ifnet_attach_proto_param *proto_details)
__NKE_API_DEPRECATED;
extern errno_t ifnet_attach_protocol_v2(ifnet_t interface,
    protocol_family_t protocol_family,
    const struct ifnet_attach_proto_param_v2 *proto_details)
__NKE_API_DEPRECATED;
extern errno_t ifnet_detach_protocol(ifnet_t interface,
    protocol_family_t protocol_family)
__NKE_API_DEPRECATED;
extern errno_t ifnet_output(ifnet_t interface,
    protocol_family_t protocol_family, mbuf_t packet, void *route,
    const struct sockaddr *dest)
__NKE_API_DEPRECATED;
extern errno_t ifnet_output_raw(ifnet_t interface,
    protocol_family_t protocol_family, mbuf_t packet)
__NKE_API_DEPRECATED;
extern errno_t ifnet_input(ifnet_t interface, mbuf_t first_packet,
    const struct ifnet_stat_increment_param *stats)
__NKE_API_DEPRECATED;
extern errno_t ifnet_input_extended(ifnet_t interface, mbuf_t first_packet,
    mbuf_t last_packet, const struct ifnet_stat_increment_param *stats);
extern errno_t ifnet_ioctl(ifnet_t interface, protocol_family_t protocol,
    unsigned long ioctl_code, void *ioctl_arg)
__NKE_API_DEPRECATED;
extern errno_t ifnet_event(ifnet_t interface, struct kern_event_msg *event_ptr)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_mtu(ifnet_t interface, u_int32_t mtu)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_mtu(ifnet_t interface)
__NKE_API_DEPRECATED;
extern u_int8_t ifnet_type(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_addrlen(ifnet_t interface, u_int8_t addrlen)
__NKE_API_DEPRECATED;
extern u_int8_t ifnet_addrlen(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_hdrlen(ifnet_t interface, u_int8_t hdrlen)
__NKE_API_DEPRECATED;
extern u_int8_t ifnet_hdrlen(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_metric(ifnet_t interface, u_int32_t metric)
__NKE_API_DEPRECATED;
extern u_int32_t ifnet_metric(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_baudrate(ifnet_t interface, u_int64_t baudrate)
__NKE_API_DEPRECATED;
extern u_int64_t ifnet_baudrate(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_bandwidths(ifnet_t interface,
    if_bandwidths_t *output_bw, if_bandwidths_t *input_bw);
extern errno_t ifnet_bandwidths(ifnet_t interface, if_bandwidths_t *output_bw,
    if_bandwidths_t *input_bw);
extern errno_t ifnet_set_latencies(ifnet_t interface,
    if_latencies_t *output_lt, if_latencies_t *input_lt);
extern errno_t ifnet_latencies(ifnet_t interface, if_latencies_t *output_lt,
    if_latencies_t *input_lt);
extern errno_t ifnet_stat_increment(ifnet_t interface,
    const struct ifnet_stat_increment_param *counts)
__NKE_API_DEPRECATED;
extern errno_t ifnet_stat_increment_in(ifnet_t interface,
    u_int32_t packets_in, u_int32_t bytes_in, u_int32_t errors_in)
__NKE_API_DEPRECATED;
extern errno_t ifnet_stat_increment_out(ifnet_t interface,
    u_int32_t packets_out, u_int32_t bytes_out, u_int32_t errors_out)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_stat(ifnet_t interface,
    const struct ifnet_stats_param *stats)
__NKE_API_DEPRECATED;
extern errno_t ifnet_stat(ifnet_t interface,
    struct ifnet_stats_param *out_stats)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_promiscuous(ifnet_t interface, int on)
__NKE_API_DEPRECATED;
extern errno_t ifnet_touch_lastchange(ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_lastchange(ifnet_t interface, struct timeval *last_change)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_address_list(ifnet_t interface, ifaddr_t *__null_terminated *addresses)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_address_list_with_count(ifnet_t interface,
    ifaddr_t *__counted_by(*addresses_count) * addresses, uint16_t *addresses_count)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_address_list_family(ifnet_t interface,
    ifaddr_t *__null_terminated *addresses, sa_family_t family)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_address_list_family_with_count(ifnet_t interface,
    ifaddr_t *__counted_by(*addresses_count) * addresses, uint16_t *addresses_count,
    sa_family_t family);
extern errno_t ifnet_get_inuse_address_list(ifnet_t interface,
    ifaddr_t *__null_terminated *addresses);
__private_extern__ errno_t ifnet_get_address_list_family_internal(ifnet_t,
    ifaddr_t *__counted_by(*addresses_count) *, uint16_t *addresses_count,
    sa_family_t, int, int, int);
extern void ifnet_address_list_free_counted_by_internal(ifaddr_t * __counted_by(addresses_count) addresses,
    uint16_t addresses_count);
extern void ifnet_free_address_list(ifaddr_t *__null_terminated addresses)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_lladdr(ifnet_t interface, const void *__sized_by(lladdr_len) lladdr,
    size_t lladdr_len)
__NKE_API_DEPRECATED;
extern errno_t ifnet_lladdr_copy_bytes(ifnet_t interface, void *__sized_by(length) lladdr,
    size_t length)
__NKE_API_DEPRECATED;
extern errno_t ifnet_guarded_lladdr_copy_bytes(ifnet_t interface, void *__sized_by(length) lladdr,
    size_t length);
extern void *ifnet_lladdr(ifnet_t interface);
extern errno_t ifnet_llbroadcast_copy_bytes(ifnet_t interface, void *__sized_by(bufferlen) addr,
    size_t bufferlen, size_t *out_len)
__NKE_API_DEPRECATED;
extern errno_t ifnet_set_lladdr_and_type(ifnet_t interface, const void *__sized_by(length) lladdr,
    size_t length, u_char type)
__NKE_API_DEPRECATED;
extern errno_t ifnet_resolve_multicast(ifnet_t ifp,
    const struct sockaddr *proto_addr, struct sockaddr *ll_addr, size_t ll_len)
__NKE_API_DEPRECATED;
extern errno_t ifnet_add_multicast(ifnet_t interface,
    const struct sockaddr *maddr, ifmultiaddr_t *multicast)
__NKE_API_DEPRECATED;
extern errno_t ifnet_remove_multicast(ifmultiaddr_t multicast)
__NKE_API_DEPRECATED;
extern errno_t ifnet_get_multicast_list(ifnet_t interface,
    ifmultiaddr_t *__null_terminated *addresses)
__NKE_API_DEPRECATED;
extern void ifnet_free_multicast_list(ifmultiaddr_t *__null_terminated multicasts)
__NKE_API_DEPRECATED;
extern errno_t ifnet_find_by_name(const char *ifname, ifnet_t *interface)
__NKE_API_DEPRECATED;
extern errno_t ifnet_list_get(ifnet_family_t family,
    ifnet_t *__counted_by(*count) * interfaces,
    uint32_t *count)
__NKE_API_DEPRECATED;
extern errno_t ifnet_list_get_all(ifnet_family_t family,
    ifnet_t *__counted_by(*count) * interfaces, u_int32_t *count);
extern void ifnet_list_free(ifnet_t *__null_terminated interfaces)
__NKE_API_DEPRECATED;
extern void ifnet_list_free_counted_by_internal(ifnet_t * __counted_by(count) interfaces, uint32_t count);
extern errno_t ifaddr_reference(ifaddr_t ifaddr)
__NKE_API_DEPRECATED;
extern errno_t ifaddr_release(ifaddr_t ifaddr)
__NKE_API_DEPRECATED;
extern errno_t ifaddr_address(ifaddr_t ifaddr, struct sockaddr *out_addr,
    u_int32_t addr_size)
__NKE_API_DEPRECATED;
extern sa_family_t ifaddr_address_family(ifaddr_t ifaddr)
__NKE_API_DEPRECATED;
extern errno_t ifaddr_dstaddress(ifaddr_t ifaddr, struct sockaddr *out_dstaddr,
    u_int32_t dstaddr_size)
__NKE_API_DEPRECATED;
extern errno_t ifaddr_netmask(ifaddr_t ifaddr, struct sockaddr *out_netmask,
    u_int32_t netmask_size)
__NKE_API_DEPRECATED;
extern ifnet_t ifaddr_ifnet(ifaddr_t ifaddr)
__NKE_API_DEPRECATED;
extern ifaddr_t ifaddr_withaddr(const struct sockaddr *address)
__NKE_API_DEPRECATED;
extern ifaddr_t ifaddr_withdstaddr(const struct sockaddr *destination)
__NKE_API_DEPRECATED;
extern ifaddr_t ifaddr_withnet(const struct sockaddr *net)
__NKE_API_DEPRECATED;
extern ifaddr_t ifaddr_withroute(int flags, const struct sockaddr *destination,
    const struct sockaddr *gateway)
__NKE_API_DEPRECATED;
extern ifaddr_t ifaddr_findbestforaddr(const struct sockaddr *addr,
    ifnet_t interface)
__NKE_API_DEPRECATED;
extern errno_t ifaddr_get_ia6_flags(ifaddr_t ifaddr, u_int32_t *out_flags);
extern errno_t ifmaddr_reference(ifmultiaddr_t ifmaddr)
__NKE_API_DEPRECATED;
extern errno_t ifmaddr_release(ifmultiaddr_t ifmaddr)
__NKE_API_DEPRECATED;
extern errno_t ifmaddr_address(ifmultiaddr_t ifmaddr,
    struct sockaddr *out_multicast, u_int32_t addr_size)
__NKE_API_DEPRECATED;
extern errno_t ifmaddr_lladdress(ifmultiaddr_t ifmaddr,
    struct sockaddr *out_link_layer_multicast, u_int32_t addr_size)
__NKE_API_DEPRECATED;
extern ifnet_t ifmaddr_ifnet(ifmultiaddr_t ifmaddr)
__NKE_API_DEPRECATED;
extern errno_t ifnet_clone_attach(struct ifnet_clone_params *cloner_params, if_clone_t *ifcloner);
extern errno_t ifnet_clone_detach(if_clone_t ifcloner);
extern errno_t ifnet_get_local_ports(ifnet_t ifp, u_int8_t bitfield[IP_PORTRANGE_BITFIELD_LEN]);
extern errno_t ifnet_get_local_ports_extended(ifnet_t ifp,
    protocol_family_t protocol, u_int32_t flags, u_int8_t bitfield[IP_PORTRANGE_BITFIELD_LEN]);
extern errno_t ifnet_report_issues(ifnet_t ifp, u_int8_t modid[IFNET_MODIDLEN],
    u_int8_t info[IFNET_MODARGLEN]);
extern errno_t ifnet_tx_compl_status(ifnet_t ifp, mbuf_t m, tx_compl_val_t val);
extern errno_t ifnet_tx_compl(ifnet_t ifp, mbuf_t m);
extern errno_t
ifnet_notice_node_presence(ifnet_t ifp, struct sockaddr *sa, int32_t rssi,
    int lqm, int npm, u_int8_t srvinfo[48]);
extern errno_t ifnet_notice_node_absence(ifnet_t ifp, struct sockaddr *sa);
extern errno_t
ifnet_notice_node_presence_v2(ifnet_t ifp, struct sockaddr *sa, struct sockaddr_dl *sdl, int32_t rssi,
    int lqm, int npm, u_int8_t srvinfo[48]);
extern errno_t ifnet_notice_primary_elected(ifnet_t ifp);
extern errno_t ifnet_notice_master_elected(ifnet_t ifp);
extern errno_t
ifnet_set_delegate(ifnet_t ifp, ifnet_t delegated_ifp);
extern errno_t
ifnet_get_delegate(ifnet_t ifp, ifnet_t *pdelegated_ifp);
extern errno_t ifnet_get_keepalive_offload_frames(ifnet_t ifp,
    struct ifnet_keepalive_offload_frame *__counted_by(frames_array_count) frames_array,
    u_int32_t frames_array_count, size_t frame_data_offset,
    u_int32_t *used_frames_count);
extern errno_t ifnet_notify_tcp_keepalive_offload_timeout(ifnet_t ifp,
    struct ifnet_keepalive_offload_frame *frame);
extern errno_t ifnet_link_status_report(ifnet_t ifp, const void *__sized_by(buffer_len) buffer,
    size_t buffer_len);
extern errno_t ifnet_set_fastlane_capable(ifnet_t interface, boolean_t capable);
extern errno_t ifnet_get_fastlane_capable(ifnet_t interface, boolean_t *capable);
extern errno_t ifnet_get_unsent_bytes(ifnet_t interface, int64_t *unsent_bytes);
extern void ifnet_normalise_unsent_data(void);
extern errno_t ifnet_set_low_power_mode(ifnet_t interface, boolean_t on);
extern errno_t ifnet_get_low_power_mode(ifnet_t interface, boolean_t *on);
extern errno_t ifnet_touch_lastupdown(ifnet_t interface);
extern errno_t ifnet_updown_delta(ifnet_t interface, struct timeval *updown_delta);
extern errno_t ifnet_set_management(ifnet_t interface, boolean_t on);
extern errno_t ifnet_set_congested_link(ifnet_t interface, boolean_t on);
extern errno_t ifnet_get_congested_link(ifnet_t interface, boolean_t *on);
extern errno_t iflt_attach_internal(ifnet_t interface, const struct iff_filter *filter,
    interface_filter_t *filter_ref);
extern void iflt_detach(interface_filter_t filter_ref)
__NKE_API_DEPRECATED;
extern errno_t proto_register_input(protocol_family_t protocol,
    proto_input_handler input, proto_input_detached_handler detached,
    int chains);
extern void proto_unregister_input(protocol_family_t protocol);
extern errno_t proto_input(protocol_family_t protocol, mbuf_t packet)
__NKE_API_DEPRECATED;
extern errno_t proto_inject(protocol_family_t protocol, mbuf_t packet)
__NKE_API_DEPRECATED;
extern errno_t proto_register_plumber(protocol_family_t proto_fam,
    ifnet_family_t if_fam, proto_plumb_handler plumb,
    proto_unplumb_handler unplumb)
__NKE_API_DEPRECATED;
extern void proto_unregister_plumber(protocol_family_t proto_fam,
    ifnet_family_t if_fam)
__NKE_API_DEPRECATED;
extern errno_t proto_plumb(protocol_family_t protocol_family, ifnet_t ifp);
extern errno_t proto_unplumb(protocol_family_t protocol_family, ifnet_t ifp);
__private_extern__ void
proto_kpi_init(void);
SLIST_HEAD(multicast_list, multicast_entry);
void
multicast_list_init(struct multicast_list * mc_list);
int
multicast_list_program(struct multicast_list * mc_list,
    struct ifnet * source_ifp,
    struct ifnet * target_ifp);
int
multicast_list_remove(struct multicast_list * mc_list);
int
nat464_translate_icmp(int, void *);
int
    nat464_translate_icmp_ip(pbuf_t *, uint16_t, uint16_t *, uint16_t *,
    uint8_t, uint8_t, uint16_t, struct nat464_addr *,
    struct nat464_addr *, protocol_family_t, protocol_family_t );
int
    nat464_synthesize_ipv6(ifnet_t, const struct in_addr *, struct in6_addr *);
int
    nat464_synthesize_ipv4(ifnet_t, const struct in6_addr *, struct in_addr *);
int
    nat464_translate_64(pbuf_t *, int, uint8_t, uint8_t *, uint8_t, struct in_addr,
    struct in_addr, uint64_t, boolean_t *);
int
    nat464_translate_46(pbuf_t *, uint16_t, uint8_t, uint8_t, uint8_t, struct in6_addr,
    struct in6_addr, uint16_t);
int
    nat464_translate_proto(pbuf_t *, struct nat464_addr *, struct nat464_addr *,
    uint8_t, protocol_family_t, protocol_family_t, int, boolean_t);
int
    nat464_insert_frag46(pbuf_t *, uint16_t, uint16_t, boolean_t);
int
    nat464_remove_frag64(pbuf_t *, uint32_t, uint16_t, boolean_t);
uint16_t
    nat464_cksum_fixup(uint16_t, uint16_t, uint16_t, uint8_t);
extern void necp_client_init(void);
extern int necp_application_find_policy_match_internal(proc_t proc, u_int8_t *parameters __sized_by(parameters_size), u_int32_t parameters_size,
    struct necp_aggregate_result *returned_result,
    u_int32_t *flags, u_int32_t *reason, u_int required_interface_index,
    const union necp_sockaddr_union *override_local_addr,
    const union necp_sockaddr_union *override_remote_addr,
    struct necp_client_endpoint *returned_v4_gateway,
    struct necp_client_endpoint *returned_v6_gateway,
    struct rtentry **returned_route, bool ignore_address,
    bool has_client,
    uuid_t *returned_override_euuid);
extern u_int8_t * __counted_by(0) necp_buffer_write_tlv(u_int8_t * __counted_by(0)cursor_, u_int8_t type, u_int32_t length, const void *value __sized_by(length), u_int8_t * buffer __sized_by(buffer_length), u_int32_t buffer_length);
extern u_int8_t * __counted_by(0) necp_buffer_write_tlv_if_different(u_int8_t * __counted_by(0)cursor_, u_int8_t type,
    u_int32_t length, const void *value __sized_by(length), bool *updated,
    u_int8_t * buffer __sized_by(buffer_length), u_int32_t buffer_length);
extern u_int8_t necp_buffer_get_tlv_type(u_int8_t * __counted_by(buffer_length)buffer, size_t buffer_length, u_int32_t tlv_offset);
extern u_int32_t necp_buffer_get_tlv_length(u_int8_t * __counted_by(buffer_length)buffer, size_t buffer_length, u_int32_t tlv_offset);
extern int necp_buffer_find_tlv(u_int8_t * buffer __sized_by(buffer_length), u_int32_t buffer_length, int offset, u_int8_t type, int *err, int next);
__attribute__((always_inline))
static inline u_int8_t * __header_indexable
necp_buffer_get_tlv_value(u_int8_t * __counted_by(buffer_length)buffer, size_t buffer_length, u_int32_t tlv_offset, u_int32_t *value_size)
{
	u_int32_t ensured_value_size = 0;
	u_int8_t *value = __necp_buffer_get_tlv_value(buffer, buffer_length, tlv_offset, &ensured_value_size);
	if (value_size) {
		*value_size = ensured_value_size;
	}
	return value;
}




typedef u_int32_t necp_kernel_policy_id;
extern void necp_init(void);
extern errno_t necp_set_socket_attributes(struct inp_necp_attributes *attributes, struct sockopt *sopt);
extern errno_t necp_get_socket_attributes(struct inp_necp_attributes *attributes, struct sockopt *sopt);
extern errno_t necp_set_socket_domain_attributes(struct socket *so, const char *domain __null_terminated, const char *domain_owner __null_terminated);
extern int necp_set_socket_resolver_signature(struct inpcb *inp, struct sockopt *sopt);
extern int necp_get_socket_resolver_signature(struct inpcb *inp, struct sockopt *sopt);
extern bool necp_socket_has_resolver_signature(struct inpcb *inp);
extern bool necp_socket_resolver_signature_matches_address(struct inpcb *inp, union necp_sockaddr_union *address);
extern void necp_inpcb_remove_cb(struct inpcb *inp);
extern void necp_inpcb_dispose(struct inpcb *inp);
extern u_int32_t necp_socket_get_content_filter_control_unit(struct socket *so);
extern u_int32_t necp_socket_get_policy_gencount(struct socket *so);
extern bool necp_socket_should_use_flow_divert(struct inpcb *inp);
extern u_int32_t necp_socket_get_flow_divert_control_unit(struct inpcb *inp, uint32_t *aggregate_unit);
extern bool necp_socket_should_rescope(struct inpcb *inp);
extern u_int necp_socket_get_rescope_if_index(struct inpcb *inp);
extern u_int32_t necp_socket_get_effective_mtu(struct inpcb *inp, u_int32_t current_mtu);
extern bool necp_socket_is_allowed_to_recv_on_interface(struct inpcb *inp, ifnet_t interface);
extern bool necp_socket_is_allowed_to_send_recv(struct inpcb *inp, ifnet_t input_interface, u_int16_t pf_tag,
    necp_kernel_policy_id *return_policy_id,
    u_int32_t *return_route_rule_id,
    necp_kernel_policy_id *return_skip_policy_id, u_int32_t *return_pass_flags);
extern bool necp_socket_is_allowed_to_send_recv_v4(struct inpcb *inp, u_int16_t local_port,
    u_int16_t remote_port, struct in_addr *local_addr,
    struct in_addr *remote_addr, ifnet_t input_interface, u_int16_t pf_tag,
    necp_kernel_policy_id *return_policy_id, u_int32_t *return_route_rule_id,
    necp_kernel_policy_id *return_skip_policy_id, u_int32_t *return_pass_flags);
extern bool necp_socket_is_allowed_to_send_recv_v6(struct inpcb *inp, u_int16_t local_port,
    u_int16_t remote_port, struct in6_addr *local_addr,
    struct in6_addr *remote_addr, ifnet_t input_interface, u_int16_t pf_tag,
    necp_kernel_policy_id *return_policy_id, u_int32_t *return_route_rule_id,
    necp_kernel_policy_id *return_skip_policy_id, u_int32_t *return_pass_flags);
extern void necp_socket_update_qos_marking(struct inpcb *inp, struct rtentry *route, u_int32_t route_rule_id);
extern bool necp_lookup_current_qos_marking(int32_t *qos_marking_gencount, struct rtentry *route, struct ifnet *interface, u_int32_t route_rule_id, bool old_qos_marking);
extern int necp_mark_packet_from_socket(struct mbuf *packet, struct inpcb *inp, necp_kernel_policy_id policy_id,
    u_int32_t route_rule_id, necp_kernel_policy_id skip_policy_id, u_int32_t pass_flags);
extern necp_kernel_policy_id necp_get_policy_id_from_packet(struct mbuf *packet);
extern necp_kernel_policy_id necp_get_skip_policy_id_from_packet(struct mbuf *packet);
extern u_int16_t necp_get_packet_filter_tags_from_packet(struct mbuf *packet);
extern bool necp_packet_should_skip_filters(struct mbuf *packet);
extern u_int32_t necp_get_last_interface_index_from_packet(struct mbuf *packet);
extern u_int32_t necp_get_route_rule_id_from_packet(struct mbuf *packet);
extern int necp_get_app_uuid_from_packet(struct mbuf *packet,
    uuid_t app_uuid);
extern necp_kernel_policy_id necp_socket_find_policy_match(struct inpcb *inp, struct sockaddr *override_local_addr,
    struct sockaddr *override_remote_addr, u_int32_t override_bound_interface);
extern necp_kernel_policy_id necp_ip_output_find_policy_match(struct mbuf *packet, int flags, struct ip_out_args *ipoa,
    struct rtentry *rt,
    necp_kernel_policy_result *result,
    necp_kernel_policy_result_parameter *result_parameter);
extern necp_kernel_policy_id necp_ip6_output_find_policy_match(struct mbuf *packet, int flags, struct ip6_out_args *ip6oa,
    struct rtentry *rt,
    necp_kernel_policy_result *result,
    necp_kernel_policy_result_parameter *result_parameter);
extern int necp_mark_packet_from_ip(struct mbuf *packet, necp_kernel_policy_id policy_id);
extern int necp_mark_packet_from_ip_with_skip(struct mbuf *packet, necp_kernel_policy_id policy_id, necp_kernel_policy_id skip_policy_id);
extern int necp_mark_packet_from_interface(struct mbuf *packet, ifnet_t interface);
extern ifnet_t necp_get_ifnet_from_result_parameter(necp_kernel_policy_result_parameter *result_parameter);
extern bool necp_packet_can_rebind_to_ifnet(struct mbuf *packet, struct ifnet *interface, struct route *new_route, int family);
extern bool necp_packet_is_allowed_over_interface(struct mbuf *packet, struct ifnet *interface);
extern int necp_mark_packet_as_keepalive(struct mbuf *packet, bool is_keepalive);
extern bool necp_get_is_keepalive_from_packet(struct mbuf *packet);
extern int necp_sign_resolver_answer(uuid_t client_id, u_int32_t sign_type,
    u_int8_t *data, size_t data_length,
    u_int8_t *tag, size_t *out_tag_length);
extern bool necp_validate_resolver_answer(uuid_t client_id, u_int32_t sign_type,
    u_int8_t *data __sized_by(data_length), size_t data_length,
    u_int8_t *tag __sized_by(tag_length), size_t tag_length);
extern int necp_sign_application_id(uuid_t client_id, u_int32_t sign_type,
    u_int8_t *__counted_by(*out_tag_length) tag, size_t *out_tag_length);
extern bool necp_validate_application_id(uuid_t client_id, u_int32_t sign_type,
    u_int8_t *tag __sized_by(tag_length), size_t tag_length);
extern void necp_update_all_clients(void);
extern void necp_update_all_clients_immediately_if_needed(bool should_update_immediately);
extern void necp_force_update_client(uuid_t client_id, uuid_t remove_netagent_uuid, u_int32_t agent_generation);
extern bool necp_set_client_as_background(proc_t proc, struct fileproc *fp, bool background);
extern void necp_fd_memstatus(proc_t proc, uint32_t status, struct necp_fd_data *client_fd);
extern void necp_fd_defunct(proc_t proc, struct necp_fd_data *client_fd);
extern void necp_client_request_in_process_flow_divert(pid_t pid);
extern int necp_client_register_socket_flow(pid_t pid, uuid_t client_id, struct inpcb *inp);
extern int necp_client_register_socket_listener(pid_t pid, uuid_t client_id, struct inpcb *inp);
extern int necp_client_assert_bb_radio_manager(uuid_t client_id, bool assert);
extern int necp_client_assign_from_socket(pid_t pid, uuid_t client_id, struct inpcb *inp);
extern int necp_assign_client_result(uuid_t netagent_uuid, uuid_t client_id,
    u_int8_t *assigned_results __sized_by(assigned_results_length), size_t assigned_results_length);
extern int necp_assign_client_group_members(uuid_t netagent_uuid, uuid_t client_id,
    u_int8_t *__counted_by(assigned_group_members_length) assigned_group_members,
    size_t assigned_group_members_length);
extern int necp_stats_ctor(struct skmem_obj_info *oi, struct skmem_obj_info *oim, void *arg, uint32_t skmflag);
extern int necp_stats_dtor(void *addr, void *arg);
extern int
necp_update_flow_protoctl_event(uuid_t netagent_uuid, uuid_t client_id,
    uint32_t protoctl_event_code, uint32_t protoctl_event_val,
    uint32_t protoctl_event_tcp_seq_num);
extern void necp_copy_inp_domain_info(struct inpcb *, struct socket *, struct nstat_domain_info *);
extern void necp_with_inp_domain_name(struct socket *so, void *ctx, void (*with_func)(char *domain_name __null_terminated, void *ctx));
extern bool net_domain_contains_hostname(char *hostname_string __null_terminated, char *domain_string __null_terminated);
__private_extern__ void netsrc_init(void);
errno_t netagent_init(void);
int netagent_ioctl(u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)) data);
extern void netagent_post_updated_interfaces(uuid_t uuid);
extern u_int32_t netagent_get_flags(uuid_t uuid);
extern errno_t netagent_set_flags(uuid_t uuid, u_int32_t flags);
extern u_int32_t netagent_get_generation(uuid_t uuid);
extern bool netagent_get_agent_domain_and_type(uuid_t uuid, char *domain __sized_by(NETAGENT_DOMAINSIZE), char *type __sized_by(NETAGENT_TYPESIZE));
extern int netagent_kernel_trigger(uuid_t uuid);
extern int netagent_client_message(uuid_t agent_uuid, uuid_t necp_client_uuid, pid_t pid, void *handle, u_int8_t message_type);
extern int netagent_client_message_with_params(uuid_t agent_uuid,
    uuid_t necp_client_uuid,
    pid_t pid,
    void *handle,
    u_int8_t message_type,
    struct necp_client_agent_parameters *parameters,
    void * __sized_by(*assigned_results_length) * assigned_results,
    size_t *assigned_results_length);
extern int netagent_copyout(uuid_t uuid, user_addr_t user_addr, u_int32_t user_size);
extern int netagent_acquire_token(uuid_t uuid, user_addr_t user_addr, u_int32_t user_size, int *retval);
extern netagent_session_t netagent_create(netagent_event_f event_handler, void *handle);
extern void netagent_destroy(netagent_session_t session);
extern errno_t netagent_register(netagent_session_t session, struct netagent *agent);
extern errno_t netagent_update(netagent_session_t session, struct netagent *agent);
extern errno_t netagent_unregister(netagent_session_t session);
extern errno_t netagent_assign_nexus(netagent_session_t _session,
    uuid_t necp_client_uuid,
    void *assign_message __sized_by(assigned_results_length),
    size_t assigned_results_length);
extern errno_t netagent_update_flow_protoctl_event(netagent_session_t _session,
    uuid_t client_id,
    uint32_t protoctl_event_code,
    uint32_t protoctl_event_val,
    uint32_t protoctl_event_tcp_seq_number);
extern int netagent_use(uuid_t agent_uuid, uint64_t *out_use_count);
void net_perf_start_time(net_perf_t *npp, struct timeval *tv);
void net_perf_measure_time(net_perf_t *npp, struct timeval *start, uint64_t num_pkts);
void net_perf_histogram(net_perf_t *npp, uint64_t num_pkts);
boolean_t net_perf_validate_bins(uint64_t bins);
extern void net_str_id_first_last(u_int32_t *, u_int32_t *, u_int32_t);
extern errno_t net_str_id_find_internal(const char *, u_int32_t *, u_int32_t, int);
extern void net_str_id_init(void);
void nstat_init(void);
void nstat_route_connect_attempt(struct rtentry *rte);
void nstat_route_connect_success(struct rtentry *rte);
void nstat_route_tx(struct rtentry *rte, u_int32_t packets, u_int32_t bytes, u_int32_t flags);
void nstat_route_rx(struct rtentry *rte, u_int32_t packets, u_int32_t bytes, u_int32_t flags);
void nstat_route_rtt(struct rtentry *rte, u_int32_t rtt, u_int32_t rtt_var);
void nstat_route_update(struct rtentry *rte, uint32_t connect_attempts, uint32_t connect_successes,
    uint32_t rx_packets, uint32_t rx_bytes, uint32_t rx_duplicatebytes, uint32_t rx_outoforderbytes,
    uint32_t tx_packets, uint32_t tx_bytes, uint32_t tx_retransmit,
    uint32_t rtt, uint32_t rtt_var);
void nstat_route_detach(struct rtentry *rte);
void nstat_tcp_new_pcb(struct inpcb *inp);
void nstat_udp_new_pcb(struct inpcb *inp);
void nstat_route_new_entry(struct rtentry *rt);
void nstat_pcb_detach(struct inpcb *inp);
void nstat_pcb_event(struct inpcb *inp, u_int64_t event);
void nstat_pcb_cache(struct inpcb *inp);
void nstat_pcb_invalidate_cache(struct inpcb *inp);
void nstat_ifnet_threshold_reached(unsigned int ifindex);
void nstat_sysinfo_send_data(struct nstat_sysinfo_data *);
int ntstat_tcp_progress_enable(struct sysctl_req *req);
u_int32_t nstat_ifnet_to_flags(struct ifnet *ifp);
nstat_context nstat_provider_stats_open(nstat_provider_context ctx,
    int provider_id,
    u_int64_t properties,                   
    nstat_provider_request_vals_fn req_fn,
    nstat_provider_request_extensions_fn req_extensions_fn);
void nstat_provider_stats_close(nstat_context nstat_ctx);
void nstat_provider_stats_event(nstat_context nstat_ctx, uint64_t event);
void nwk_wq_init(void);
void nwk_wq_enqueue(struct nwk_wq_entry *nwk_item);
extern void pkt_mnglr_init(void);
void            pbuf_init_memory(pbuf_t *, const struct pbuf_memory *,
    struct ifnet *);
void            pbuf_destroy(pbuf_t *);
void            pbuf_sync(pbuf_t *);
void *          pbuf_ensure_contig(pbuf_t *, size_t);
void *          pbuf_ensure_writable(pbuf_t *, size_t);
void *          pbuf_resize_segment(pbuf_t *, int off, int olen, int nlen);
void *          pbuf_contig_segment(pbuf_t *, int off, int len);
void            pbuf_copy_data(pbuf_t *, int, int, void *__sized_by(buflen), size_t buflen);
void            pbuf_copy_back(pbuf_t *, int, int, void *__sized_by(buflen), size_t buflen);
uint16_t        pbuf_inet_cksum(const pbuf_t *, uint32_t, uint32_t, uint32_t);
uint16_t        pbuf_inet6_cksum(const pbuf_t *, uint32_t, uint32_t, uint32_t);
mbuf_svc_class_t pbuf_get_service_class(const pbuf_t *);
void *          pbuf_get_packet_buffer_address(const pbuf_t *);
extern void pktap_init(void);
extern void pktap_input(struct ifnet *, protocol_family_t, struct mbuf *, char *);
extern void pktap_output(struct ifnet *, protocol_family_t, struct mbuf *,
    u_int32_t, u_int32_t);
extern void pktap_fill_proc_info(struct pktap_header *, protocol_family_t,
    struct mbuf *, u_int32_t, int, struct ifnet *);
extern void pktap_finalize_proc_info(struct pktap_header *);
extern void pktap_v2_finalize_proc_info(struct pktap_v2_hdr *);
extern void convert_to_pktap_header_to_v2(struct bpf_packet *bpf_pkt, bool truncate);
extern LIST_HEAD(rawcb_list_head, rawcb) rawcb_list;
__BEGIN_DECLS
extern int raw_attach(struct socket *, int);
extern void raw_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
extern void raw_detach_nofree(struct rawcb *);
extern void raw_disconnect(struct rawcb *);
extern void raw_input(struct mbuf *, struct sockproto *, struct sockaddr *,
    struct sockaddr *);
int rvi_init(void);
extern bitmap_t *__sized_by_or_null(BITMAP_SIZE(UINT16_MAX)) restricted_port_bitmap;
extern void restricted_in_port_init(void);
extern bool current_task_can_use_restricted_in_port(in_port_t port, uint8_t protocol, uint32_t port_flags);
static inline struct rtentry *
__attribute__((always_inline)) __stateful_pure
rn_rtentry(struct radix_node *rn)
{
	return __container_of(rn, struct rtentry, rt_nodes[0]);
}



enum {
	ROUTE_STATUS_UPDATE = 1,
	ROUTE_ENTRY_REFRESH,
	ROUTE_ENTRY_DELETED,
	ROUTE_LLENTRY_RESOLVED,
	ROUTE_LLENTRY_UNREACH,
	ROUTE_LLENTRY_CHANGED,
	ROUTE_LLENTRY_STALE,
	ROUTE_LLENTRY_TIMEDOUT,
	ROUTE_LLENTRY_DELETED,
	ROUTE_LLENTRY_EXPIRED,
	ROUTE_LLENTRY_PROBED,
	ROUTE_EVHDLR_DEREGISTER,
};
extern const char * route_event2str(int route_event);
EVENTHANDLER_DECLARE(route_event, route_event_fn);
extern int route_op_entitlement_check(struct socket *, kauth_cred_t, int, boolean_t);
extern void route_init(void);
extern void routegenid_update(void);
extern void routegenid_inet_update(void);
extern void routegenid_inet6_update(void);
extern void rt_ifmsg(struct ifnet *);
extern void rt_missmsg(u_char, struct rt_addrinfo *, int, int);
extern void rt_newaddrmsg(u_char, struct ifaddr *, int, struct rtentry *);
extern void rt_newmaddrmsg(u_char, struct ifmultiaddr *);
extern int rt_setgate(struct rtentry *, struct sockaddr *, struct sockaddr *);
extern void set_primary_ifscope(int, unsigned int);
extern unsigned int get_primary_ifscope(int);
extern boolean_t rt_primary_default(struct rtentry *, struct sockaddr *);
extern struct rtentry *rt_lookup(boolean_t, struct sockaddr *,
    struct sockaddr *, struct radix_node_head *, unsigned int);
extern struct rtentry *rt_lookup_coarse(boolean_t, struct sockaddr *,
    struct sockaddr *, struct radix_node_head *);
extern void rtalloc(struct route *);
extern void rtalloc_scoped(struct route *, unsigned int);
extern void rtalloc_ign(struct route *, uint32_t);
extern void rtalloc_scoped_ign(struct route *, uint32_t, unsigned int);
extern struct rtentry *rtalloc1(struct sockaddr *, int, uint32_t);
extern struct rtentry *rtalloc1_scoped(struct sockaddr *, int, uint32_t,
    unsigned int);
extern struct rtentry *rtalloc1_scoped_locked(struct sockaddr *, int,
    uint32_t, unsigned int);
extern void rtfree_locked(struct rtentry *);
extern void rtfree(struct rtentry *);
extern void rtref(struct rtentry *);
extern int rtunref(struct rtentry *);
extern void rtsetifa(struct rtentry *, struct ifaddr *);
extern int rtinit(struct ifaddr *, uint8_t, int);
extern int rtinit_locked(struct ifaddr *, uint8_t, int);
extern int rtioctl(unsigned long req, caddr_t __sized_by(IOCPARM_LEN(req)), struct proc *);
extern void rtredirect(struct ifnet *, struct sockaddr *, struct sockaddr *,
    struct sockaddr *, int, struct sockaddr *, struct rtentry **);
extern int rtrequest(int, struct sockaddr *,
    struct sockaddr *, struct sockaddr *, int, struct rtentry **);
extern int rtrequest_scoped(int, struct sockaddr *, struct sockaddr *,
    struct sockaddr *, int, struct rtentry **, unsigned int);
extern int rtrequest_locked(int, struct sockaddr *,
    struct sockaddr *, struct sockaddr *, int, struct rtentry **);
extern int rtrequest_scoped_locked(int, struct sockaddr *, struct sockaddr *,
    struct sockaddr *, int, struct rtentry **, unsigned int);
extern void sin_set_ifscope(struct sockaddr *, unsigned int);
extern unsigned int sin_get_ifscope(struct sockaddr *);
extern unsigned int sin6_get_ifscope(struct sockaddr *);
extern void rt_lock(struct rtentry *, boolean_t);
extern void rt_unlock(struct rtentry *);
extern struct sockaddr *rtm_scrub(int, int, struct sockaddr *,
    struct sockaddr *, void *buf __sized_by(buflen), uint32_t buflen, kauth_cred_t *);
extern boolean_t rt_validate(struct rtentry *);
extern void rt_set_proxy(struct rtentry *, boolean_t);
extern void rt_set_gwroute(struct rtentry *, struct sockaddr *,
    struct rtentry *);
extern void rt_revalidate_gwroute(struct rtentry *, struct rtentry *);
extern errno_t route_to_gwroute(const struct sockaddr *, struct rtentry *,
    struct rtentry **);
extern void rt_setexpire(struct rtentry *, uint64_t);
extern void rt_str(struct rtentry *, char *ds __sized_by(dslen), uint32_t dslen, char *gs __sized_by(gslen), uint32_t gslen);
extern const char *rtm2str(int);
extern void route_clear(struct route *);
extern void route_copyin(struct route *, struct route *, size_t);
extern void route_copyout(struct route *, const struct route *, size_t);
extern boolean_t rt_ifa_is_dst(struct sockaddr *, struct ifaddr *);
extern struct sockaddr *sa_copy(struct sockaddr *, struct sockaddr_storage *,
    unsigned int *);
extern void route_event_init(struct route_event *p_route_ev, struct rtentry *rt,
    struct rtentry *gwrt, int route_ev_code);
extern int route_event_walktree(struct radix_node *rn, void *arg);
extern void route_event_enqueue_nwk_wq_entry(struct rtentry *, struct rtentry *,
    uint32_t, eventhandler_tag, boolean_t);
__STC_DEFINE_SELF_CONVERTERS(struct, sockaddr);
__STC_DEFINE_OBJECT_CONVERTERS(struct, sockaddr, struct, sockaddr_storage);
__STC_DEFINE_OBJECT_CONVERTERS(struct, sockaddr, union, sockaddr_in_4_6);
__STC_DEFINE_OBJECT_CONVERTERS(struct, sockaddr, union, necp_sockaddr_union);
__STC_DEFINE_BYTE_TO_OBJ_CNVS(struct, sockaddr, 2, 255);
__STC_DEFINE_SELF_CONVERTERS(struct, sockaddr_storage);
__STC_DEFINE_BYTE_TO_OBJ_CNVS(struct, sockaddr_storage,
    sizeof(struct sockaddr_storage), sizeof(struct sockaddr_storage));
__STC_DEFINE_BYTE_TO_OBJ_CNVS(union, sockaddr_in_4_6,
    sizeof(union sockaddr_in_4_6), sizeof(union sockaddr_in_4_6));
__STC_DEFINE_BYTE_TO_OBJ_CNVS(union, necp_sockaddr_union,
    sizeof(union necp_sockaddr_union), sizeof(union necp_sockaddr_union));
boolean_t net_trie_init(struct net_trie *new_trie, size_t prefix_count, size_t leaf_count, size_t bytes_count);
boolean_t net_trie_init_with_mem(struct net_trie *new_trie, uint8_t * __sized_by(trie_memory_size) memory, size_t trie_memory_size,
    size_t nodes_mem_size, size_t child_maps_mem_size, size_t bytes_mem_size,
    uint16_t nodes_count, uint16_t child_maps_count, uint16_t bytes_count);
uint16_t net_trie_insert(struct net_trie *trie,
    const uint8_t * __sized_by(string_length) string, size_t string_length,
    const uint8_t * __sized_by(metadata_length) metadata, size_t metadata_length,
    boolean_t reverse);
uint16_t net_trie_search(struct net_trie *trie,
    const uint8_t * __sized_by(string_length) string, size_t string_length,
    const uint8_t * __sized_by(*metadata_length) * metadata, size_t *metadata_length,
    boolean_t reverse,
    boolean_t partial_match_allowed,
    char partial_match_terminator,
    boolean_t *high_ascii_detected,
    check_metadata_func check_metadata);
void net_trie_free(struct net_trie *new_trie);
void                    dhcpol_init(dhcpol_t * list);
void                    dhcpol_free(dhcpol_t * list);
int                     dhcpol_count(dhcpol_t * list);
boolean_t               dhcpol_add(dhcpol_t * list, const void * element);
const void *            dhcpol_element(dhcpol_t * list, int i);
boolean_t               dhcpol_concat(dhcpol_t * list, dhcpol_t * extra);
boolean_t               dhcpol_parse_buffer(dhcpol_t * list,
    const void * buffer,
    int length);
const void *            dhcpol_find(dhcpol_t * list, int tag, int * len_p,
    int * start);
boolean_t               dhcpol_parse_packet(dhcpol_t * options,
    const struct dhcp * pkt, int len);
RB_HEAD(fd_pcb_tree, flow_divert_pcb);
void            flow_divert_init(void);
void            flow_divert_detach(struct socket *so);
errno_t         flow_divert_token_set(struct socket *so, struct sockopt *sopt);
errno_t         flow_divert_token_get(struct socket *so, struct sockopt *sopt);
errno_t         flow_divert_pcb_init(struct socket *so);
errno_t         flow_divert_connect_out(struct socket *so, struct sockaddr *to, proc_t p);
errno_t         flow_divert_implicit_data_out(struct socket *so, int flags, mbuf_t data, struct sockaddr *to, mbuf_t control, struct proc *p);
void    icmp6_init(struct ip6protosw *, struct domain *);
void    icmp6_paramerror(struct mbuf *, int);
void    icmp6_error_flag(struct mbuf *, int, int, int, int);
void    icmp6_error(struct mbuf *, int, int, int);
int     icmp6_input(struct mbuf **, int *, int);
void    icmp6_reflect(struct mbuf *, size_t);
void    icmp6_prepare(struct mbuf *);
void    icmp6_redirect_input(struct mbuf *, int, int);
void    icmp6_redirect_output(struct mbuf *, struct rtentry *);
void    icmp6_mtudisc_update(struct ip6ctlparam *, int);
SYSCTL_DECL(_net_inet_icmp);
int     arpresolve(struct ifnet *, struct rtentry *, struct mbuf *,
    struct sockaddr *, u_char *, struct rtentry *);
void    arp_ifinit(struct ifnet *, struct ifaddr *);
extern void igmp_init(struct protosw *, struct domain *);
extern int igmp_change_state(struct in_multi *, struct igmp_tparams *);
extern struct igmp_ifinfo *igmp_domifattach(struct ifnet *, zalloc_flags_t);
extern void igmp_domifreattach(struct igmp_ifinfo *);
extern void igmp_domifdetach(struct ifnet *);
extern void igmp_input(struct mbuf *, int);
extern int igmp_joingroup(struct in_multi *);
extern void igmp_leavegroup(struct in_multi *);
extern void igmp_set_timeout(struct igmp_tparams *);
extern void igmp_set_fast_timeout(struct igmp_tparams *);
extern void igi_addref(struct igmp_ifinfo *, int);
extern void igi_remref(struct igmp_ifinfo *);
__private_extern__ void igmp_initsilent(struct ifnet *, struct igmp_ifinfo *);
SYSCTL_DECL(_net_inet_igmp);
extern int       inet_aton(const char *, struct in_addr *);
extern const char *inet_ntop(int, const void *, char *, socklen_t);
void inp_log_addresses(struct inpcb *inp, char *__sized_by(lbuflen) lbuf,
    socklen_t lbuflen, char *__sized_by(fbuflen) fbuf,
    socklen_t fbuflen);
extern boolean_t arp_is_entry_probing(route_t p_route);
extern errno_t arp_lookup_ip(ifnet_t interface,
    const struct sockaddr_in *ip_dest,
    struct sockaddr_dl *__sized_by(ll_dest_len)ll_dest,
    size_t ll_dest_len, route_t hint, mbuf_t packet);
extern errno_t arp_ip_handle_input(ifnet_t ifp, u_int16_t arpop,
    const struct sockaddr_dl *sender_hw, const struct sockaddr_in *sender_ip,
    const struct sockaddr_in *target_ip);
extern void in_arpdrain(void *);
extern void arp_llreach_set_reachable(struct ifnet *,
    void *__sized_by(alen) addr, unsigned int alen);
void in_gif_input(struct mbuf *, int);
int in_gif_output(struct ifnet *, int, struct mbuf *, struct rtentry *);
int gif_encapcheck4(const struct mbuf *, int, int, void *);
LIST_HEAD(inpcbhead, inpcb);
LIST_HEAD(inpcbporthead, inpcbport);
#pragma pack(4)















#pragma pack()








typedef void (*inpcb_timer_func_t)(struct inpcbinfo *);
extern void in_pcbinit(void);
extern void in_pcbinfo_attach(struct inpcbinfo *);
extern int in_pcbinfo_detach(struct inpcbinfo *);
extern void inpcb_gc_sched(struct inpcbinfo *, u_int32_t type);
extern void inpcb_timer_sched(struct inpcbinfo *, u_int32_t type);
extern void in_losing(struct inpcb *);
extern void in_rtchange(struct inpcb *, int);
extern int in_pcballoc(struct socket *, struct inpcbinfo *, struct proc *);
extern int in_pcbbind(struct inpcb *, struct sockaddr *, struct sockaddr *, struct proc *);
extern int in_pcbconnect(struct inpcb *, struct sockaddr *, struct proc *,
    unsigned int, struct ifnet **);
extern void in_pcbdetach(struct inpcb *);
extern void in_pcbdispose(struct inpcb *);
extern void in_pcbdisconnect(struct inpcb *);
extern int in_pcbinshash(struct inpcb *, struct sockaddr *, int);
extern int in_pcbladdr(struct inpcb *, struct sockaddr *, struct in_addr *,
    unsigned int, struct ifnet **, int);
extern struct inpcb *in_pcblookup_local(struct inpcbinfo *, struct in_addr,
    u_int, int);
extern struct inpcb *in_pcblookup_local_and_cleanup(struct inpcbinfo *,
    struct in_addr, u_int, int);
extern struct inpcb *in_pcblookup_hash(struct inpcbinfo *, struct in_addr,
    u_int, struct in_addr, u_int, int, struct ifnet *);
extern struct inpcb *in_pcblookup_hash_try(struct inpcbinfo *pcbinfo,
    struct in_addr faddr, u_int fport_arg, struct in_addr laddr,
    u_int lport_arg, int wildcard, struct ifnet *ifp);
extern int in_pcblookup_hash_exists(struct inpcbinfo *, struct in_addr,
    u_int, struct in_addr, u_int, int, uid_t *, gid_t *, struct ifnet *);
extern void in_pcbnotifyall(struct inpcbinfo *, struct in_addr, int,
    void (*)(struct inpcb *, int));
extern void in_pcbrehash(struct inpcb *);
extern int in_getpeeraddr(struct socket *, struct sockaddr **);
extern int in_getsockaddr(struct socket *, struct sockaddr **);
extern int in_getsockaddr_s(struct socket *, struct sockaddr_in *);
extern int in_pcb_checkstate(struct inpcb *, int, int);
extern void in_pcbremlists(struct inpcb *);
extern void inpcb_to_compat(struct inpcb *, struct inpcb_compat *);
extern void inpcb_to_xinpcb64(struct inpcb *, struct xinpcb64 *);
extern int get_pcblist_n(short, struct sysctl_req *, struct inpcbinfo *);
extern void inpcb_get_ports_used(ifnet_t, int, u_int32_t,
    bitstr_t *__counted_by(bitstr_size(IP_PORTRANGE_SIZE)), struct inpcbinfo *);
extern uint32_t inpcb_count_opportunistic(unsigned int, struct inpcbinfo *,
    u_int32_t);
extern uint32_t inpcb_find_anypcb_byaddr(struct ifaddr *, struct inpcbinfo *);
extern void inp_route_copyout(struct inpcb *, struct route *);
extern void inp_route_copyin(struct inpcb *, struct route *);
extern int inp_bindif(struct inpcb *, unsigned int, struct ifnet **);
extern int inp_bindtodevice(struct inpcb *, const char *);
extern void inp_set_nocellular(struct inpcb *);
extern void inp_clear_nocellular(struct inpcb *);
extern void inp_set_noexpensive(struct inpcb *);
extern void inp_set_noconstrained(struct inpcb *);
extern void inp_set_awdl_unrestricted(struct inpcb *);
extern boolean_t inp_get_awdl_unrestricted(struct inpcb *);
extern void inp_clear_awdl_unrestricted(struct inpcb *);
extern void inp_set_intcoproc_allowed(struct inpcb *);
extern boolean_t inp_get_intcoproc_allowed(struct inpcb *);
extern void inp_clear_intcoproc_allowed(struct inpcb *);
extern void inp_set_management_allowed(struct inpcb *);
extern boolean_t inp_get_management_allowed(struct inpcb *);
extern void inp_clear_management_allowed(struct inpcb *);
extern void inp_set_ultra_constrained_allowed(struct inpcb *);
extern void inp_update_necp_policy(struct inpcb *, struct sockaddr *, struct sockaddr *, u_int);
extern void inp_set_want_app_policy(struct inpcb *);
extern void inp_clear_want_app_policy(struct inpcb *);
extern u_int32_t inp_calc_flowhash(struct inpcb *);
extern void inp_reset_fc_state(struct inpcb *);
extern int inp_set_fc_state(struct inpcb *, int advcode);
extern void inp_fc_unthrottle_tcp(struct inpcb *);
extern void inp_fc_throttle_tcp(struct inpcb *inp);
extern void inp_flowadv(uint32_t);
extern int inp_flush(struct inpcb *, int);
extern int inp_findinpcb_procinfo(struct inpcbinfo *, uint32_t, struct so_procinfo *);
extern void inp_get_soprocinfo(struct inpcb *, struct so_procinfo *);
extern int inp_update_policy(struct inpcb *);
extern boolean_t inp_restricted_recv(struct inpcb *, struct ifnet *);
extern boolean_t inp_restricted_send(struct inpcb *, struct ifnet *);
extern void inp_incr_sndbytes_total(struct socket *, int);
extern void inp_decr_sndbytes_total(struct socket *, int);
extern void inp_count_sndbytes(struct inpcb *, u_int32_t);
extern void inp_incr_sndbytes_unsent(struct socket *, int32_t);
extern void inp_decr_sndbytes_unsent(struct socket *, int32_t);
extern int32_t inp_get_sndbytes_allunsent(struct socket *, u_int32_t);
extern void inp_decr_sndbytes_allunsent(struct socket *, u_int32_t);
extern void inp_set_activity_bitmap(struct inpcb *inp);
extern void inp_get_activity_bitmap(struct inpcb *inp, activity_bitmap_t *b);
extern void inp_update_last_owner(struct socket *so, struct proc *p, struct proc *ep);
extern void inp_copy_last_owner(struct socket *so, struct socket *head);
extern void inp_enter_bind_in_progress(struct socket *so);
extern void inp_exit_bind_in_progress(struct socket *so);
extern void inp_clear_INP_INADDR_ANY(struct socket *);
extern int inp_limit_companion_link(struct inpcbinfo *pcbinfo, u_int32_t limit);
extern int inp_recover_companion_link(struct inpcbinfo *pcbinfo);
extern void in_management_interface_check(void);
extern void in_pcb_check_management_entitled(struct inpcb *inp);
extern void in_pcb_check_ultra_constrained_entitled(struct inpcb *inp);
extern char *inp_snprintf_tuple(struct inpcb *, char *__sized_by(buflen) buf, size_t buflen);
extern boolean_t in_broadcast(struct in_addr, struct ifnet *);
extern boolean_t in_canforward(struct in_addr);
extern u_int32_t in_netof(struct in_addr);
extern uint32_t os_cpu_in_cksum_mbuf(struct mbuf *m, int len, int off,
    uint32_t initial_sum);
extern uint16_t inet_cksum(struct mbuf *, uint32_t, uint32_t, uint32_t);
extern uint16_t inet_cksum_buffer(const void *__sized_by(__len), uint32_t, uint32_t, uint32_t __len);
extern uint16_t in_addword(uint16_t, uint16_t);
extern uint16_t in_pseudo(uint32_t, uint32_t, uint32_t);
extern uint16_t in_pseudo64(uint64_t, uint64_t, uint64_t);
extern uint16_t in_cksum_hdr_opt(const struct ip *);
extern uint16_t ip_cksum_hdr_dir(struct mbuf *, uint32_t, int);
extern uint16_t ip_cksum_hdr_dir_buffer(const void *__sized_by(__len), uint32_t, uint32_t __len, int);
extern uint32_t in_finalize_cksum(struct mbuf *, uint32_t, uint32_t);
extern uint16_t b_sum16(const void *__sized_by(len)buf, int len);
extern int in_getconninfo(struct socket *, sae_connid_t, uint32_t *,
    uint32_t *, int32_t *, user_addr_t, socklen_t *, user_addr_t, socklen_t *,
    uint32_t *, user_addr_t, uint32_t *);
extern struct in_ifaddr * inifa_ifpwithflag(struct ifnet *, uint32_t);
extern struct in_ifaddr * inifa_ifpclatv4(struct ifnet *);
extern int in_localaddr(struct in_addr);
extern int inaddr_local(struct in_addr);
extern char     *inet_ntoa(struct in_addr);
extern char     *inet_ntoa_r(struct in_addr ina, char *buf,
    size_t buflen);
extern int      inet_pton(int af, const char *, void *);
extern void net_qos_map_init(void);
extern void net_qos_map_change(uint32_t mode);
extern errno_t set_packet_qos(struct mbuf *, struct ifnet *, boolean_t, int,
    int, u_int8_t *);
extern int so_get_netsvc_marking_level(struct socket *);
extern uint8_t fastlane_sc_to_dscp(uint32_t svc_class);
extern uint8_t rfc4594_sc_to_dscp(uint32_t svc_class);
extern uint8_t custom_sc_to_dscp(uint32_t svc_class);
extern mbuf_traffic_class_t rfc4594_dscp_to_tc(uint8_t dscp);
static inline const struct in_ifaddr *
__attribute__((overloadable)) __pure2
__ifatoia_const(const struct ifaddr *ifa)
{
	return __container_of(ifa, const struct in_ifaddr, ia_ifa);
}
static inline struct in_ifaddr *
__attribute__((overloadable)) __pure2
__ifatoia(struct ifaddr *ifa)
{
	return __container_of(ifa, struct in_ifaddr, ia_ifa);
}


static inline const struct in_ifaddr *
__attribute__((overloadable)) __pure2
__iatoia_const(const struct in_ifaddr *ia)
{
	return ia;
}
static inline struct in_ifaddr *
__attribute__((overloadable)) __pure2
__iatoia(struct in_ifaddr *ia)
{
	return ia;
}



















extern void socket_post_kev_msg(uint32_t, struct kev_socket_event_data *,
    uint32_t);
extern void socket_post_kev_msg_closed(struct socket *);
extern TAILQ_HEAD(in_ifaddrhead, in_ifaddr) in_ifaddrhead;
TAILQ_HEAD(in_ifaddrhashhead, in_ifaddr);
RB_HEAD(ip_msource_tree, ip_msource);
RB_PROTOTYPE_SC_PREV(__private_extern__, ip_msource_tree, ip_msource,
    ims_link, ip_msource_cmp);
extern LIST_HEAD(in_multihead, in_multi) in_multihead;
extern int imo_multi_filter(const struct ip_moptions *,
    const struct ifnet *, const struct sockaddr_in *,
    const struct sockaddr_in *);
extern int imo_clone(struct inpcb *, struct inpcb *);
extern void inm_commit(struct in_multi *);
extern void inm_clear_recorded(struct in_multi *);
extern void inm_print(const struct in_multi *);
extern int inm_record_source(struct in_multi *inm, const in_addr_t);
extern void inm_release(struct in_multi *);
extern struct in_multi *in_addmulti(struct in_addr *, struct ifnet *);
extern void in_delmulti(struct in_multi *);
extern int in_leavegroup(struct in_multi *, struct in_mfilter *);
extern int in_multi_detach(struct in_multi *);
extern void inm_addref(struct in_multi *, int);
extern void inm_remref(struct in_multi *, int);
extern void inm_purge(struct in_multi *);
extern uint8_t ims_get_mode(const struct in_multi *,
    const struct ip_msource *, uint8_t);
extern int in_control(struct socket *, u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)),
    struct ifnet *, struct proc *);
extern int in_inithead(void **, int);
extern void in_rtqdrain(void);
extern struct radix_node *in_validate(struct radix_node *);
extern void ip_input(struct mbuf *);
extern void ip_input_process_list(struct mbuf *);
extern int in_ifadown(struct ifaddr *ifa, int);
extern void in_ifscrub(struct ifnet *, struct in_ifaddr *, int);
extern uint32_t inaddr_hashval(uint32_t);
extern struct in_ifaddrhashhead *inaddr_hashlookup(uint32_t);
extern void in_purgeaddrs(struct ifnet *);
extern void gre_input(struct mbuf *, int);
extern void imf_leave(struct in_mfilter *);
extern void imf_purge(struct in_mfilter *);
extern int inp_join_group(struct inpcb *, struct sockopt *);
extern int inp_leave_group(struct inpcb *, struct sockopt *);
extern void in_multihead_lock_exclusive(void);
extern void in_multihead_lock_shared(void);
extern void in_multihead_lock_assert(int);
extern void in_multihead_lock_done(void);
SLIST_HEAD(dn_flow_set_head, dn_flow_set);
SLIST_HEAD(dn_pipe_head, dn_pipe);
extern uint32_t my_random(void);
void ip_dn_init(void);
#pragma pack(4)








#pragma pack()












extern struct eventhandler_lists_ctxt dummynet_evhdlr_ctxt;
extern void dummynet_init(void);
extern void dummynet_register_m_tag(void);
extern void dummynet_event_enqueue_nwk_wq_entry(struct dummynet_event *);
extern const char *dummynet_event2str(int);
EVENTHANDLER_DECLARE(dummynet_event, dummynet_event_fn);
extern void ip_ecn_ingress(int, u_int8_t *, const u_int8_t *);
extern int ip_ecn_egress(int, const u_int8_t *, u_int8_t *);
__BEGIN_DECLS
void    encap4_input(struct mbuf *, int);
int     encap6_input(struct mbuf **, int *, int);
const struct encaptab *encap_attach(int, int, const struct sockaddr *,
    const struct sockaddr *, const struct sockaddr *,
    const struct sockaddr *, const struct protosw *, void *);
const struct encaptab *encap_attach_func(int, int,
    int (*)(const struct mbuf *, int, int, void *),
    const struct protosw *, void *);
int     encap_detach(const struct encaptab *);
void    *encap_getarg(struct mbuf *);
void    encap_init(void);
void encap_register_m_tag(void);
void    icmp_error(struct mbuf *, int, int, n_long, u_int32_t);
void    icmp_input(struct mbuf *, int);
int     ip_next_mtu(int, int);
extern void ip_moptions_init(void);
extern struct ip_moptions *ip_allocmoptions(zalloc_flags_t);
extern int inp_getmoptions(struct inpcb *, struct sockopt *);
extern int inp_setmoptions(struct inpcb *, struct sockopt *);
extern void imo_addref(struct ip_moptions *, int);
extern void imo_remref(struct ip_moptions *);
extern int ip_checkrouteralert(struct mbuf *);
extern int ip_ctloutput(struct socket *, struct sockopt *sopt);
extern void ip_drain(void);
extern void ip_init(struct protosw *, struct domain *);
extern int ip_output(struct mbuf *, struct mbuf *, struct route *, int,
    struct ip_moptions *, struct ip_out_args *);
extern int ip_output_list(struct mbuf *, int, struct mbuf *, struct route *,
    int, struct ip_moptions *, struct ip_out_args *);
extern void ip_output_checksum(struct ifnet *, struct mbuf *, int, int,
    uint32_t *);
extern struct in_ifaddr *ip_rtaddr(struct in_addr);
extern int ip_savecontrol(struct inpcb *, struct mbuf **, struct ip *,
    struct mbuf *);
extern struct mbuf *ip_srcroute(void);
extern void  ip_stripoptions(struct mbuf *);
extern u_int16_t ip_randomid(uint64_t);
extern int ip_fragment(struct mbuf *, struct ifnet *, uint32_t, int);
extern void ip_setsrcifaddr_info(struct mbuf *, uint16_t, struct in_ifaddr *);
extern void ip_setdstifaddr_info(struct mbuf *, uint16_t, struct in_ifaddr *);
extern int ip_getsrcifaddr_info(struct mbuf *, uint32_t *, uint32_t *);
extern int ip_getdstifaddr_info(struct mbuf *, uint32_t *, uint32_t *);
extern int rip_ctloutput(struct socket *, struct sockopt *);
extern void rip_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
extern void rip_init(struct protosw *, struct domain *);
extern void rip_input(struct mbuf *, int);
extern int rip_output(struct mbuf *, struct socket *, u_int32_t, struct mbuf *);
extern int rip_unlock(struct socket *, int, void *);
extern int rip_send(struct socket *, int, struct mbuf *, struct sockaddr *,
    struct mbuf *, struct proc *);
extern void tcp_in_cksum_stats(u_int32_t);
extern void tcp_out_cksum_stats(u_int32_t);
extern void udp_in_cksum_stats(u_int32_t);
extern void udp_out_cksum_stats(u_int32_t);
extern void tcp_in6_cksum_stats(u_int32_t);
extern void tcp_out6_cksum_stats(u_int32_t);
extern void udp_in6_cksum_stats(u_int32_t);
extern void udp_out6_cksum_stats(u_int32_t);
extern int ip_gre_output(struct mbuf *);
extern int ip_gre_register_input(gre_input_func_t);
extern errno_t ipf_addv4_internal(const struct ipf_filter *filter,
    ipfilter_t *filter_ref);
extern errno_t ipf_addv6_internal(const struct ipf_filter *filter,
    ipfilter_t *filter_ref);
extern errno_t ipf_remove(ipfilter_t filter_ref)
__NKE_API_DEPRECATED;
extern errno_t ipf_inject_input(mbuf_t data, ipfilter_t filter_ref)
__NKE_API_DEPRECATED;
extern errno_t ipf_inject_output(mbuf_t data, ipfilter_t filter_ref,
    ipf_pktopts_t options)
__NKE_API_DEPRECATED;
TAILQ_HEAD(ipfilter_list, ipfilter);
extern ipfilter_t ipf_get_inject_filter(struct mbuf *m);
extern void ipf_ref(void);
extern void ipf_unref(void);
extern void ip_proto_dispatch_in(struct mbuf *m, int hlen, u_int8_t proto,
    ipfilter_t ipfref);
extern void ipfilter_register_m_tag(void);
__BEGIN_DECLS
extern void mptcp_data_ack_rcvd(struct mptcb *mp_tp, struct tcpcb *tp, u_int64_t full_dack);
extern void mptcp_update_window_wakeup(struct tcpcb *tp);
extern void tcp_do_mptcp_options(struct tcpcb *, u_char * opt __ended_by(optend), u_char *optend, struct tcphdr *,
    struct tcpopt *, uint8_t);
extern unsigned mptcp_setup_syn_opts(struct socket *, u_char* __ended_by(optend), u_char *optend, unsigned);
extern unsigned mptcp_setup_join_ack_opts(struct tcpcb *, u_char* __ended_by(optend), u_char *optend, unsigned);
extern unsigned int mptcp_setup_opts(struct tcpcb *tp, int32_t off, u_char *opt __ended_by(optend), u_char *optend,
    unsigned int optlen, int flags, int len,
    boolean_t *p_mptcp_acknow, boolean_t *do_not_compress);
extern void mptcp_update_dss_rcv_state(struct mptcp_dsn_opt *, struct tcpcb *,
    uint16_t);
extern void mptcp_update_rcv_state_meat(struct mptcb *, struct tcpcb *,
    u_int64_t, u_int32_t, u_int16_t, uint16_t);
__BEGIN_DECLS
extern uint32_t mptcp_timer(struct mppcbinfo *mppi);
extern void mptcp_start_timer(struct mptses *mpte, int timer_type);
extern void mptcp_cancel_timer(struct mptcb *mp_tp, int timer_type);
extern void mptcp_cancel_all_timers(struct mptcb *mp_tp);
extern void mptcp_init_urgency_timer(struct mptses *mpte);
extern void mptcp_set_urgency_timer(struct mptses *mpte);
static inline struct socket *
mptetoso(struct mptses *mpte)
{
	return mpte->mpte_mppcb->mpp_socket;
}

static inline struct mptses *
mptompte(struct mppcb *mp)
{
	return (struct mptses *)mp->mpp_pcbe;
}

static inline struct mptses *
mpsotompte(struct socket *so)
{
	return mptompte(mpsotomppcb(so));
}

static inline boolean_t
mpp_try_lock(struct mppcb *mp)
{
	if (!lck_mtx_try_lock(&mp->mpp_lock)) {
		return false;
	}

	VERIFY(!(mp->mpp_flags & MPP_INSIDE_OUTPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_INPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_SETGETOPT));

	return true;
}

static inline void
mpp_lock(struct mppcb *mp)
{
	lck_mtx_lock(&mp->mpp_lock);
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_OUTPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_INPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_SETGETOPT));
}

static inline void
mpp_unlock(struct mppcb *mp)
{
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_OUTPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_INPUT));
	VERIFY(!(mp->mpp_flags & MPP_INSIDE_SETGETOPT));
	lck_mtx_unlock(&mp->mpp_lock);
}

static inline lck_mtx_t *
mpp_getlock(struct mppcb *mp, int flags)
{
	if (flags & PR_F_WILLUNLOCK) {
		VERIFY(!(mp->mpp_flags & MPP_INSIDE_OUTPUT));
		VERIFY(!(mp->mpp_flags & MPP_INSIDE_INPUT));
		VERIFY(!(mp->mpp_flags & MPP_INSIDE_SETGETOPT));
	}

	return &mp->mpp_lock;
}

static inline int
mptcp_subflow_cwnd_space(struct socket *so)
{
	struct tcpcb *tp = sototcpcb(so);
	int cwnd = (int)(MIN(tp->snd_wnd, tp->snd_cwnd) - (so->so_snd.sb_cc));

	return MIN(cwnd, sbspace(&so->so_snd));
}

static inline bool
mptcp_subflows_need_backup_flag(struct mptses *mpte)
{
	return mpte->mpte_svctype < MPTCP_SVCTYPE_AGGREGATE ||
	       mpte->mpte_svctype == MPTCP_SVCTYPE_PURE_HANDOVER;
}











typedef enum mptcp_state {
	MPTCPS_CLOSED           = 0,    
	MPTCPS_LISTEN           = 1,    
	MPTCPS_ESTABLISHED      = 2,    
	MPTCPS_CLOSE_WAIT       = 3,    
	MPTCPS_FIN_WAIT_1       = 4,    
	MPTCPS_CLOSING          = 5,    
	MPTCPS_LAST_ACK         = 6,    
	MPTCPS_FIN_WAIT_2       = 7,    
	MPTCPS_TIME_WAIT        = 8,    
	MPTCPS_TERMINATE        = 9,    
} mptcp_state_t;
static inline struct mptcb *
tptomptp(struct tcpcb *tp)
{
	return tp->t_mptcb;
}





extern struct mppcbinfo mtcbinfo;
__BEGIN_DECLS
extern void mptcp_init(struct protosw *, struct domain *);
extern int mptcp_ctloutput(struct socket *, struct sockopt *);
extern int mptcp_session_create(struct mppcb *);
extern boolean_t mptcp_ok_to_create_subflows(struct mptcb *mp_tp);
extern void mptcp_check_subflows_and_add(struct mptses *mpte);
extern void mptcp_check_subflows_and_remove(struct mptses *mpte);
extern void mptcpstats_inc_switch(struct mptses *mpte, const struct mptsub *mpts);
extern void mptcpstats_update(struct mptcp_itf_stats *stats __counted_by(stats_count), uint16_t stats_count, const struct mptsub *mpts);
extern int mptcpstats_get_index_by_ifindex(struct mptcp_itf_stats *stats __counted_by(stats_count), uint16_t stats_count, u_short ifindex, boolean_t create);
extern struct mptses *mptcp_drop(struct mptses *mpte, struct mptcb *mp_tp, u_short errno);
extern struct mptses *mptcp_close(struct mptses *, struct mptcb *);
extern int mptcp_lock(struct socket *, int, void *);
extern int mptcp_unlock(struct socket *, int, void *);
extern lck_mtx_t *mptcp_getlock(struct socket *, int);
extern void mptcp_subflow_workloop(struct mptses *);
extern void mptcp_sched_create_subflows(struct mptses *);
extern void mptcp_finish_usrclosed(struct mptses *mpte);
extern struct mptopt *mptcp_sopt_alloc(void);
extern const char *mptcp_sopt2str(int, int);
extern void mptcp_sopt_free(struct mptopt *);
extern void mptcp_sopt_insert(struct mptses *, struct mptopt *);
extern void mptcp_sopt_remove(struct mptses *, struct mptopt *);
extern struct mptopt *mptcp_sopt_find(struct mptses *, struct sockopt *);
extern int mptcp_subflow_add(struct mptses *, struct sockaddr *,
    struct sockaddr *, uint32_t, sae_connid_t *);
extern void mptcp_subflow_del(struct mptses *, struct mptsub *);
extern void mptcp_handle_input(struct socket *so);
extern int mptcp_subflow_output(struct mptses *mpte, struct mptsub *mpts, int flags);
extern void mptcp_clean_reinjectq(struct mptses *mpte);
extern void mptcp_subflow_shutdown(struct mptses *, struct mptsub *);
extern void mptcp_subflow_disconnect(struct mptses *, struct mptsub *);
extern int mptcp_subflow_sosetopt(struct mptses *, struct mptsub *,
    struct mptopt *);
extern int mptcp_subflow_sogetopt(struct mptses *, struct socket *,
    struct mptopt *);
extern void mptcp_input(struct mptses *, struct mbuf *);
extern boolean_t mptcp_can_send_more(struct mptcb *mp_tp, boolean_t ignore_reinject);
extern int mptcp_output(struct mptses *);
extern void mptcp_close_fsm(struct mptcb *, uint32_t);
extern void mptcp_hmac_sha1(mptcp_key_t, mptcp_key_t, u_int32_t, u_int32_t,
    u_char sha_digest[SHA1_RESULTLEN]);
extern void mptcp_hmac_sha256(mptcp_key_t, mptcp_key_t, u_char* __sized_by(msglen), uint16_t msglen,
    u_char sha_digest[SHA256_DIGEST_LENGTH]);
extern void mptcp_get_mpjoin_hmac(mptcp_addr_id, struct mptcb *, u_char * __sized_by(digest_len), uint8_t digest_len);
extern void mptcp_get_rands(mptcp_addr_id, struct mptcb *, u_int32_t *,
    u_int32_t *);
extern void mptcp_set_raddr_rand(mptcp_addr_id, struct mptcb *, mptcp_addr_id,
    u_int32_t);
extern int mptcp_init_remote_parms(struct mptcb *);
extern boolean_t mptcp_ok_to_keepalive(struct mptcb *);
extern void mptcp_insert_dsn(struct mppcb *, struct mbuf *);
extern void mptcp_output_getm_dsnmap32(struct socket *so, int off,
    uint32_t *dsn, uint32_t *relseq,
    uint16_t *data_len, uint16_t *dss_csum);
extern void mptcp_output_getm_dsnmap64(struct socket *so, int off,
    uint64_t *dsn, uint32_t *relseq,
    uint16_t *data_len, uint16_t *dss_csum);
extern void mptcp_output_getm_data_level_details(struct socket *so, int off,
    uint16_t *data_len, uint16_t *dss_csum);
extern void mptcp_act_on_txfail(struct socket *);
extern struct mptsub *mptcp_get_subflow(struct mptses *mpte, struct mptsub **preferred);
extern int mptcp_get_map_for_dsn(struct socket *so, uint64_t dsn_fail, uint32_t *tcp_seq);
extern int32_t mptcp_adj_sendlen(struct socket *so, int32_t off);
extern void mptcp_sbrcv_grow(struct mptcb *mp_tp);
extern int32_t mptcp_sbspace(struct mptcb *);
extern void mptcp_notify_mpready(struct socket *);
extern void mptcp_notify_mpfail(struct socket *);
extern void mptcp_notify_close(struct socket *);
extern boolean_t mptcp_no_rto_spike(struct socket*);
extern int mptcp_set_notsent_lowat(struct mptses *mpte, int optval);
extern u_int32_t mptcp_get_notsent_lowat(struct mptses *mpte);
extern int mptcp_notsent_lowat_check(struct socket *so);
extern void mptcp_ask_symptoms(struct mptses *mpte);
extern void mptcp_control_register(void);
extern mptcp_wifi_quality_t mptcp_wifi_quality_for_session(struct mptses *mpte);
extern boolean_t symptoms_is_wifi_lossy(void);
extern void mptcp_session_necp_cb(void *, int, uint32_t, uint32_t, bool *);
extern struct sockaddr *mptcp_get_session_dst(struct mptses *mpte,
    boolean_t has_v6, boolean_t has_v4);
extern void mptcp_set_restrictions(struct socket *mp_so);
extern void mptcp_clear_cellicon(void);
extern void mptcp_unset_cellicon(struct mptses *mpte, struct mptsub *mpts, uint32_t val);
extern void mptcp_reset_rexmit_state(struct tcpcb *tp);
extern void mptcp_reset_keepalive(struct tcpcb *tp);
extern int mptcp_validate_csum(struct tcpcb *tp, struct mbuf *m, uint64_t dsn,
    uint32_t sseq, uint16_t dlen, uint16_t csum, int dfin);
static inline struct mppcb *
mpsotomppcb(struct socket *mp_so)
{
	VERIFY(SOCK_DOM(mp_so) == PF_MULTIPATH);
	return (struct mppcb *)mp_so->so_pcb;
}



static inline boolean_t
mptcp_should_defer_upcall(struct mppcb *mpp)
{
	return !!(mpp->mpp_flags & (MPP_INSIDE_OUTPUT | MPP_INSIDE_INPUT | MPP_INPUT_HANDLE | MPP_WUPCALL));
}




__BEGIN_DECLS
extern void mp_pcbinfo_attach(struct mppcbinfo *);
extern int mp_pcbinfo_detach(struct mppcbinfo *);
extern int mp_pcballoc(struct socket *, struct mppcbinfo *);
extern void mp_pcbdetach(struct socket *);
extern void mptcp_pcbdispose(struct mppcb *);
extern void mp_gc_sched(void);
extern void mptcp_timer_sched(void);
extern void mptcp_handle_deferred_upcalls(struct mppcb *mpp, uint32_t flag);
extern int mp_getsockaddr(struct socket *mp_so, struct sockaddr **nam);
extern int mp_getpeeraddr(struct socket *mp_so, struct sockaddr **nam);
extern int necp_client_register_multipath_cb(pid_t pid, uuid_t client_id, struct mppcb *mpp);
extern void necp_mppcb_dispose(struct mppcb *mpp);
extern void tcp_cache_set_cookie(struct tcpcb *tp, u_char *__counted_by(len) cookie, u_int8_t len);
extern int tcp_cache_get_cookie(struct tcpcb *tp, u_char *__counted_by(buflen) cookie, uint8_t buflen, u_int8_t *len);
extern unsigned int tcp_cache_get_cookie_len(struct tcpcb *tp);
extern uint8_t tcp_cache_get_mptcp_version(struct sockaddr* dst);
extern void tcp_cache_update_mptcp_version(struct tcpcb *tp, boolean_t succeeded);
extern void tcp_heuristic_tfo_loss(struct tcpcb *tp);
extern void tcp_heuristic_tfo_rst(struct tcpcb *tp);
extern void tcp_heuristic_mptcp_loss(struct tcpcb *tp);
extern void tcp_heuristic_ecn_loss(struct tcpcb *tp);
extern void tcp_heuristic_tfo_middlebox(struct tcpcb *tp);
extern void tcp_heuristic_ecn_aggressive(struct tcpcb *tp);
extern void tcp_heuristic_tfo_success(struct tcpcb *tp);
extern void tcp_heuristic_mptcp_success(struct tcpcb *tp);
extern void tcp_heuristic_ecn_success(struct tcpcb *tp);
extern boolean_t tcp_heuristic_do_tfo(struct tcpcb *tp);
extern int tcp_heuristic_do_mptcp(struct tcpcb *tp);
extern boolean_t tcp_heuristic_do_ecn(struct tcpcb *tp);
extern void tcp_heuristic_ecn_droprst(struct tcpcb *tp);
extern void tcp_heuristic_ecn_droprxmt(struct tcpcb *tp);
extern void tcp_heuristic_ecn_synrst(struct tcpcb *tp);
extern boolean_t tcp_heuristic_do_ecn_with_address(struct ifnet *ifp,
    union sockaddr_in_4_6 *local_address);
extern void tcp_heuristics_ecn_update(struct necp_tcp_ecn_cache *necp_buffer,
    struct ifnet *ifp, union sockaddr_in_4_6 *local_address);
extern boolean_t tcp_heuristic_do_tfo_with_address(struct ifnet *ifp,
    union sockaddr_in_4_6 *local_address, union sockaddr_in_4_6 *remote_address,
    u_int8_t *__counted_by(maxlen) cookie, u_int8_t maxlen, u_int8_t *cookie_len);
extern void tcp_heuristics_tfo_update(struct necp_tcp_tfo_cache *necp_buffer,
    struct ifnet *ifp, union sockaddr_in_4_6 *local_address,
    union sockaddr_in_4_6 *remote_address);
extern void tcp_cache_init(void);
extern void tcp_cc_init(void);
extern void tcp_cc_resize_sndbuf(struct tcpcb *tp);
extern void tcp_bad_rexmt_fix_sndbuf(struct tcpcb *tp);
extern void tcp_cc_cwnd_init_or_reset(struct tcpcb *tp);
extern int tcp_cc_delay_ack(struct tcpcb *tp, struct tcphdr *th);
extern void tcp_cc_allocate_state(struct tcpcb *tp);
extern void tcp_cc_after_idle_stretchack(struct tcpcb *tp);
extern uint32_t tcp_cc_is_cwnd_nonvalidated(struct tcpcb *tp);
extern void tcp_cc_adjust_nonvalidated_cwnd(struct tcpcb *tp);
extern u_int32_t tcp_get_max_pipeack(struct tcpcb *tp);
extern void tcp_clear_pipeack_state(struct tcpcb *tp);
extern void tcp_log_connection_summary(const char *func_name, int line_no, struct tcpcb *tp);
extern void tcp_log_th_flags(void *hdr, struct tcphdr *th, struct tcpcb *tp, bool outgoing, struct ifnet *ifp);
extern void tcp_log_connection(struct tcpcb *tp, const char *event, int error);
extern void tcp_log_listen(struct tcpcb *tp, int error);
extern void tcp_log_drop_pcb(void *hdr, struct tcphdr *th, struct tcpcb *tp, bool outgoing, const char *reason);
extern void tcp_log_drop_pkt(void *hdr, struct tcphdr *th, struct ifnet *ifp, const char *reason);
extern void tcp_log_rtt_info(const char *func_name, int line_no, struct tcpcb *tp);
extern void tcp_log_rt_rtt(const char *func_name, int line_no, struct tcpcb *tp, struct rtentry *rt);
extern void tcp_log_rtt_change(const char *func_name, int line_no, struct tcpcb *tp, int old_srtt, int old_rttvar);
extern void tcp_log_keepalive(const char *func_name, int line_no, struct tcpcb *tp, int32_t idle_time);
extern void tcp_log_message(const char *func_name, int line_no, struct tcpcb *tp, const char *format, ...) __printflike(4, 5);
extern void tcp_log_fsw_flow(const char *func_name, int line_no, struct tcpcb *tp, const char *format, ...) __printflike(4, 5);
extern void tcp_log_state_change(const char *func_name, int line_no, struct tcpcb *tp, int new_state);
extern void tcp_log_output(const char *func_name, int line_no, struct tcpcb *tp, const char *format, ...) __printflike(4, 5);
extern void tcp_log_bind(struct inpcb *inp, const char *event, int error);
LIST_HEAD(timerlisthead, tcptimerentry);
TAILQ_HEAD(tcptailq, tcpcb);
static inline struct tcp_globals *
tcp_get_globals(struct tcpcb *tp)
{
#pragma unused(tp)
	return NULL;
}

static inline uint32_t
tcp_globals_now(struct tcp_globals *globals)
{
#pragma unused(globals)
	return tcp_now;
}

extern void tcp_ccdbg_control_register(void);
extern void tcp_ccdbg_trace(struct tcpcb *tp, struct tcphdr *th, int32_t event);
LIST_HEAD(tsegqe_head, tseg_qent);
RB_HEAD(tcp_seg_sent_tree_head, tcp_seg_sent);
RB_PROTOTYPE(tcp_seg_sent_tree_head, tcp_seg_sent, seg_link, tcp_seg_cmp)





















__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct tcpcb, tcpcb);
static inline bool
tcp_sent_tlp_retrans(const struct tcpcb *tp)
{
	return (tp->t_flagsext & (TF_SENT_TLPROBE | TF_TLP_IS_RETRANS)) == (TF_SENT_TLPROBE | TF_TLP_IS_RETRANS);
}





extern uint8_t tcprexmtthresh;
#pragma pack(4)














#pragma pack()





extern int tcp_TCPTV_MIN;
KALLOC_TYPE_DECLARE(tcp_reass_zone);
extern struct tseg_qent * tcp_reass_qent_alloc(void);
extern void tcp_reass_qent_free(struct tseg_qent *te);
KALLOC_TYPE_DECLARE(tcp_rxt_seg_zone);
extern struct tcp_rxt_seg * tcp_rxt_seg_qent_alloc(void);
extern void tcp_rxt_seg_qent_free(struct tcp_rxt_seg *te);
KALLOC_TYPE_DECLARE(tcp_seg_sent_zone);
extern struct tcp_seg_sent * tcp_seg_sent_qent_alloc(void);
extern void tcp_seg_sent_qent_free(struct tcp_seg_sent *te);
void     tcp_canceltimers(struct tcpcb *);
void     tcp_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
int      tcp_ctloutput(struct socket *, struct sockopt *);
void     tcp_drain(void);
void     tcp_getrt_rtt(struct tcpcb *tp, struct rtentry *rt);
void     tcp_init(struct protosw *, struct domain *);
void     tcp_input(struct mbuf *, int);
void     tcp_mss(struct tcpcb *, int, unsigned int);
uint32_t tcp_ceil(double a);
uint32_t tcp_round_to(uint32_t val, uint32_t round);
uint32_t tcp_round_up(uint32_t val, uint32_t base);
uint32_t ntoh24(u_char * p __sized_by(3));
uint32_t tcp_packets_this_ack(struct tcpcb *tp, uint32_t acked);
int      tcp_mssopt(struct tcpcb *);
void     tcp_drop_syn_sent(struct inpcb *, int);
uint32_t tcp_get_effective_mtu(struct rtentry *, uint32_t);
void     tcp_mtudisc(struct inpcb *, int);
int      tcp_output(struct tcpcb *);
void     tcp_respond(struct tcpcb *, void *ipgen __sized_by(ipgen_size), size_t ipgen_size, struct tcphdr *, struct mbuf *,
    tcp_seq, tcp_seq, uint8_t, struct tcp_respond_args *);
void     tcp_setpersist(struct tcpcb *);
void     tcp_gc(struct inpcbinfo *);
void     tcp_itimer(struct inpcbinfo *ipi);
void     tcp_check_timer_state(struct tcpcb *tp);
void     tcp_run_timerlist(void *arg1, void *arg2);
void     tcp_sched_timers(struct tcpcb *tp);
void     tcp_fillheaders(struct mbuf *, struct tcpcb *, void *, void *);
void     tcp_trace(int, int, struct tcpcb *, void *, struct tcphdr *, int);
void tcp_fill_info(struct tcpcb *, struct tcp_info *);
void tcp_sack_doack(struct tcpcb *, struct tcpopt *, struct tcphdr *,
    uint32_t *, uint32_t *);
extern boolean_t tcp_sack_process_dsack(struct tcpcb *, struct tcpopt *,
    struct tcphdr *, boolean_t *);
int tcp_detect_bad_rexmt(struct tcpcb *, struct tcphdr *, struct tcpopt *,
    u_int32_t rxtime);
void     tcp_update_sack_list(struct tcpcb *tp, tcp_seq rcv_laststart, tcp_seq rcv_lastend);
void     tcp_clean_sackreport(struct tcpcb *tp);
uint32_t tcp_sack_adjust(struct tcpcb *tp);
void     tcp_sack_partialack(struct tcpcb *, struct tcphdr *);
void     tcp_free_sackholes(struct tcpcb *tp);
int32_t  tcp_sbspace(struct tcpcb *tp);
void     tcp_set_tso(struct tcpcb *tp, struct ifnet *ifp);
void     tcp_set_ecn(struct tcpcb *tp, struct ifnet *ifp);
uint8_t  tcp_get_ace(struct tcphdr *th);
uint32_t tcp_flight_size(struct tcpcb *tp);
void     tcp_reset_stretch_ack(struct tcpcb *tp);
extern void tcp_get_ports_used(ifnet_t ifp, int, u_int32_t, bitstr_t *__counted_by(bitstr_size(IP_PORTRANGE_SIZE)));
uint32_t tcp_count_opportunistic(unsigned int ifindex, u_int32_t flags);
uint32_t tcp_find_anypcb_byaddr(struct ifaddr *ifa);
void tcp_set_max_rwinscale(struct tcpcb *tp, struct socket *so);
void tcp_bwmeas_free(struct tcpcb *tp);
extern int32_t timer_diff(uint32_t t1, uint32_t toff1, uint32_t t2, uint32_t toff2);
void tcp_rack_transmit_seg(struct tcpcb *tp, struct tcp_seg_sent *seg, tcp_seq start, tcp_seq end, uint32_t xmit_ts, uint8_t flags);
void tcp_rack_update_reordering_window(struct tcpcb *tp, tcp_seq highest_acked_sacked);
void tcp_rack_update_reordering_win_persist(struct tcpcb *tp);
void tcp_rack_bad_rexmt_restore(struct tcpcb *tp);
void tcp_rack_reset_segs_retransmitted(struct tcpcb *tp);
void tcp_rack_update_segment_acked(struct tcpcb *tp, uint32_t tsecr, uint32_t xmit_ts, uint32_t end_seq, bool retransmitted);
bool tcp_rack_detect_loss_and_arm_timer(struct tcpcb *tp, uint32_t dup_acks);
void tcp_rack_reordering_timeout(struct tcpcb *tp, uint32_t dup_acks);
void tcp_rack_loss_on_rto(struct tcpcb *tp, bool in_rto);
uint32_t tcp_rack_adjust(struct tcpcb *tp, uint32_t cwin);
void tcp_rack_detect_reordering_dsack(struct tcpcb *tp, tcp_seq start, tcp_seq end);
void tcp_rack_detect_reordering_acked(struct tcpcb *tp, struct tcp_seg_sent *seg);
uint32_t tcp_seg_len(struct tcp_seg_sent *seg);
void tcp_seg_sent_insert(struct tcpcb *tp, struct tcp_seg_sent *seg, tcp_seq start, tcp_seq end, uint32_t xmit_ts, uint8_t flags);
void tcp_segs_doack(struct tcpcb *tp, tcp_seq th_ack, struct tcpopt *to);
void tcp_segs_dosack(struct tcpcb *tp, tcp_seq sblk_start, tcp_seq sblk_end, uint32_t tsecr, uint32_t *newbytes_sacked);
void tcp_segs_clear_sacked(struct tcpcb *tp);
void tcp_mark_seg_lost(struct tcpcb *tp, struct tcp_seg_sent *seg);
void tcp_seg_delete(struct tcpcb *tp, struct tcp_seg_sent *seg);
void tcp_segs_sent_clean(struct tcpcb *tp, bool free_segs);
extern void tcp_set_background_cc(struct socket *);
extern void tcp_set_foreground_cc(struct socket *);
extern void tcp_set_recv_bg(struct socket *);
extern void tcp_clear_recv_bg(struct socket *);
extern boolean_t tcp_sack_byte_islost(struct tcpcb *tp);
int      tcp_lock(struct socket *, int, void *);
int      tcp_unlock(struct socket *, int, void *);
void     calculate_tcp_clock(void);
uint64_t microuptime_ns(void);
uint64_t tcp_pacer_get_packet_tx_time(struct tcpcb *tp, uint16_t pkt_len);
void tcp_set_mbuf_tx_time(struct mbuf *m, uint64_t tx_time);
extern void tcp_keepalive_reset(struct tcpcb *);
extern uint32_t get_base_rtt(struct tcpcb *tp);
extern tcp_seq tcp_new_isn(struct tcpcb *tp);
extern int tcp_input_checksum(int, struct mbuf *, struct tcphdr *, int, int);
extern void tcp_getconninfo(struct socket *, struct conninfo_tcp *);
extern void add_to_time_wait(struct tcpcb *, uint32_t delay);
extern void add_to_time_wait_now(struct tcpcb *tp, uint32_t delay);
extern void tcp_pmtud_revert_segment_size(struct tcpcb *tp);
extern void tcp_rxtseg_insert(struct tcpcb *, tcp_seq, tcp_seq);
extern struct tcp_rxt_seg *tcp_rxtseg_find(struct tcpcb *, tcp_seq, tcp_seq);
extern void tcp_rxtseg_set_spurious(struct tcpcb *tp, tcp_seq start, tcp_seq end);
extern void tcp_rxtseg_clean(struct tcpcb *);
extern boolean_t tcp_rxtseg_detect_bad_rexmt(struct tcpcb *, tcp_seq);
extern boolean_t tcp_rxtseg_dsack_for_tlp(struct tcpcb *);
extern u_int32_t tcp_rxtseg_total_size(struct tcpcb *tp);
extern void tcp_rexmt_save_state(struct tcpcb *tp);
void tcp_enter_fast_recovery(struct tcpcb *tp);
extern void tcp_interface_send_probe(u_int16_t if_index_available);
extern void tcp_probe_connectivity(struct ifnet *ifp, u_int32_t enable);
extern void tcp_get_connectivity_status(struct tcpcb *,
    struct tcp_conn_status *);
extern void tcp_clear_keep_alive_offload(struct socket *so);
extern void tcp_fill_keepalive_offload_frames(struct ifnet *,
    struct ifnet_keepalive_offload_frame * frames_array __counted_by(frames_array_count), u_int32_t frames_array_count, size_t, u_int32_t *);
extern int tcp_notify_kao_timeout(ifnet_t ifp,
    struct ifnet_keepalive_offload_frame *frame);
extern void tcp_disable_tfo(struct tcpcb *tp);
extern void tcp_tfo_gen_cookie(struct inpcb *inp, u_char *out __sized_by(blk_size), size_t blk_size);
extern int tcp_freeq(struct tcpcb *tp);
extern errno_t tcp_notify_ack_id_valid(struct tcpcb *, struct socket *, u_int32_t);
extern errno_t tcp_add_notify_ack_marker(struct tcpcb *, u_int32_t);
extern void tcp_notify_ack_free(struct tcpcb *);
extern void tcp_notify_acknowledgement(struct tcpcb *, struct socket *);
extern void tcp_get_notify_ack_count(struct tcpcb *,
    struct tcp_notify_ack_complete *);
extern void tcp_get_notify_ack_ids(struct tcpcb *tp,
    struct tcp_notify_ack_complete *);
extern void tcp_update_mss_locked(struct socket *, struct ifnet *);
extern int get_tcp_inp_list(struct inpcb * __single * __counted_by(n), size_t n, inp_gen_t);
extern bool tcp_notify_ack_active(struct socket *so);
extern void tcp_set_finwait_timeout(struct tcpcb *);
extern int dump_tcp_reass_qlen(char * str __sized_by(str_len), int str_len);
extern uint32_t tcp_reass_qlen_space(struct socket *);
__private_extern__ void tcp_update_stats_per_flow(
	struct ifnet_stats_per_flow *, struct ifnet *);
extern void tcp_set_link_heur_rtomin(struct tcpcb *tp, ifnet_t ifp);
extern void udp_log_bind(struct inpcb *inp, int error);
extern void udp_log_connect(struct inpcb *inp, int error);
extern void udp_log_connection_summary(struct inpcb *inp);
extern void udp_log_message(const char *func_name, int line_no, struct inpcb *inp, const char *format, ...) __printflike(4, 5);
extern void udp_log_drop_pcb(void *hdr, struct udphdr *uh, struct inpcb *inp, bool outgoing, const char *format);
SYSCTL_DECL(_net_inet_udp);
__BEGIN_DECLS
extern void udp_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
extern int udp_ctloutput(struct socket *, struct sockopt *);
extern void udp_init(struct protosw *, struct domain *);
extern void udp_input(struct mbuf *, int);
extern int udp_connectx_common(struct socket *, int, struct sockaddr *,
    struct sockaddr *, struct proc *, uint32_t, sae_associd_t,
    sae_connid_t *, uint32_t, void *, uint32_t, struct uio*, user_ssize_t *);
extern void udp_notify(struct inpcb *inp, int errno);
extern int udp_shutdown(struct socket *so);
extern int udp_lock(struct socket *, int, void *);
extern int udp_unlock(struct socket *, int, void *);
extern lck_mtx_t *udp_getlock(struct socket *, int);
extern void udp_get_ports_used(ifnet_t ifp, int, u_int32_t, bitstr_t *__counted_by(bitstr_size(IP_PORTRANGE_SIZE)));
extern uint32_t udp_count_opportunistic(unsigned int, u_int32_t);
extern uint32_t udp_find_anypcb_byaddr(struct ifaddr *);
extern void udp_fill_keepalive_offload_frames(struct ifnet *,
    struct ifnet_keepalive_offload_frame *__counted_by(frames_count) frames_array,
    uint32_t frames_count, size_t, u_int32_t *);
extern const struct ah_algorithm *ah_algorithm_lookup(int);
extern size_t ah_hdrlen(struct secasvar *);
extern size_t ah_hdrsiz(struct ipsecrequest *);
extern int ah_schedule(const struct ah_algorithm *, struct secasvar *);
extern void ah4_input(struct mbuf *, int);
extern int ah4_output(struct mbuf *, struct secasvar *);
extern int ah4_calccksum(struct mbuf *, caddr_t __sized_by(len), size_t len,
    const struct ah_algorithm *, struct secasvar *);
extern int ah6_input(struct mbuf **, int *, int);
extern int ah6_output(struct mbuf *, u_char *, struct mbuf *,
    struct secasvar *);
extern int ah6_calccksum(struct mbuf *, caddr_t __sized_by(len),
    size_t len, const struct ah_algorithm *, struct secasvar *);
extern void ah6_ctlinput(int, struct sockaddr *, void *);
extern const struct esp_algorithm *esp_algorithm_lookup(int);
extern int esp_max_ivlen(void);
extern int esp4_output(struct mbuf *, struct secasvar *);
extern void esp4_input(struct mbuf *, int off);
extern struct mbuf *esp4_input_extended(struct mbuf *, int off, ifnet_t interface);
extern size_t esp_hdrsiz(struct ipsecrequest *);
extern int esp_kpipe_output(struct secasvar *, kern_packet_t, kern_packet_t);
extern int esp_kpipe_input(ifnet_t, kern_packet_t, kern_packet_t);
extern int esp_schedule(const struct esp_algorithm *, struct secasvar *);
extern int esp_auth(struct mbuf *, size_t, size_t,
    struct secasvar *, u_char *__sized_by(ESP_AUTH_MAXSUMSIZE));
extern int esp_auth_data(struct secasvar *, uint8_t *, size_t, uint8_t *, size_t);
extern void esp_init(void);
extern int esp6_output(struct mbuf *, u_char *, struct mbuf *,
    struct secasvar *);
extern int esp6_input(struct mbuf **, int *, int);
extern int esp6_input_extended(struct mbuf **mp, int *offp, int proto, ifnet_t interface);
extern void esp6_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
size_t esp_chachapoly_schedlen(const struct esp_algorithm *);
int esp_chachapoly_schedule(const struct esp_algorithm *,
    struct secasvar *);
int esp_chachapoly_encrypt(struct mbuf *, size_t, size_t, struct secasvar *,
    const struct esp_algorithm *, int);
int esp_chachapoly_decrypt(struct mbuf *, size_t, struct secasvar *,
    const struct esp_algorithm *, int);
int esp_chachapoly_encrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(out_ivlen), size_t out_ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
int esp_chachapoly_decrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(ivlen), size_t ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
int esp_chachapoly_encrypt_finalize(struct secasvar *, unsigned char *, size_t);
int esp_chachapoly_decrypt_finalize(struct secasvar *, unsigned char *, size_t);
int esp_chachapoly_mature(struct secasvar *);
int esp_chachapoly_ivlen(const struct esp_algorithm *, struct secasvar *);
size_t esp_aes_schedlen(const struct esp_algorithm *);
int esp_aes_schedule(const struct esp_algorithm *, struct secasvar *);
int esp_cbc_decrypt_aes(struct mbuf *, size_t, struct secasvar *,
    const struct esp_algorithm *, int);
int
    esp_cbc_encrypt_aes(struct mbuf *, size_t, size_t, struct secasvar *,
    const struct esp_algorithm *, int);
int esp_aes_cbc_encrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(out_ivlen), size_t out_ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
int esp_aes_cbc_decrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(ivlen), size_t ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
size_t esp_gcm_schedlen(const struct esp_algorithm *);
int esp_gcm_schedule(const struct esp_algorithm *, struct secasvar *);
int esp_gcm_ivlen(const struct esp_algorithm *, struct secasvar *);
int esp_gcm_encrypt_aes(struct mbuf *, size_t, size_t, struct secasvar *, const struct esp_algorithm *, int);
int esp_gcm_decrypt_aes(struct mbuf *, size_t, struct secasvar *, const struct esp_algorithm *, int);
int esp_gcm_encrypt_finalize(struct secasvar *, unsigned char *, size_t);
int esp_gcm_decrypt_finalize(struct secasvar *, unsigned char *, size_t);
int esp_aes_gcm_encrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(ivlen), size_t ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
int esp_aes_gcm_decrypt_data(struct secasvar *,
    uint8_t *__sized_by(input_data_len), size_t input_data_len,
    struct newesp *,
    uint8_t *__sized_by(ivlen), size_t ivlen,
    uint8_t *__sized_by(output_data_len), size_t output_data_len);
int in6_gif_input(struct mbuf **, int *, int);
int in6_gif_output(struct ifnet *, int, struct mbuf *, struct rtentry *);
int gif_encapcheck6(const struct mbuf *, int, int, void *);
extern int in6_domifattach(struct ifnet *);
extern int in6_ifattach_prelim(struct ifnet *);
extern int in6_ifattach_aliasreq(struct ifnet *, struct ifnet *,
    struct in6_aliasreq *);
extern int in6_ifattach_llcgareq(struct ifnet *, struct in6_cgareq *);
extern void in6_ifdetach(struct ifnet *);
extern int in6_iid_from_hw(struct ifnet *, struct in6_addr *);
extern void in6_iid_mktmp(struct ifnet *, u_int8_t *__sized_by(8), const u_int8_t *__sized_by(8), int);
extern void in6_tmpaddrtimer(void *);
extern int in6_nigroup(struct ifnet *, const char *__counted_by(hostlen), size_t hostlen, struct in6_addr *, uint32_t*);
extern void in6_losing(struct inpcb *);
extern int in6_pcbbind(struct inpcb *, struct sockaddr *, struct sockaddr *, struct proc *);
extern int in6_pcbconnect(struct inpcb *, struct sockaddr *, struct proc *);
extern void in6_pcbdetach(struct inpcb *);
extern void in6_pcbdisconnect(struct inpcb *);
extern int in6_pcbladdr(struct inpcb *, struct sockaddr *,
    struct in6_addr *, struct ifnet **);
extern struct inpcb *in6_pcblookup_local(struct inpcbinfo *, struct in6_addr *,
    u_int, uint32_t, int);
extern struct inpcb *in6_pcblookup_hash(struct inpcbinfo *, struct in6_addr *,
    u_int, uint32_t, struct in6_addr *, u_int, uint32_t, int, struct ifnet *);
extern struct inpcb *in6_pcblookup_hash_try(struct inpcbinfo *pcbinfo,
    struct in6_addr *faddr, u_int fport_arg, uint32_t fifscope,
    struct in6_addr *laddr, u_int lport_arg, uint32_t lifscope, int wildcard,
    struct ifnet *ifp);
extern int in6_pcblookup_hash_exists(struct inpcbinfo *, struct in6_addr *,
    u_int, uint32_t, struct in6_addr *, u_int, uint32_t, int, uid_t *, gid_t *, struct ifnet *, bool);
extern void in6_pcbnotify(struct inpcbinfo *, struct sockaddr *, u_int,
    const struct sockaddr *, u_int, int, void *, void (*)(struct inpcb *, int));
extern void in6_rtchange(struct inpcb *, int);
extern struct sockaddr *in6_sockaddr(in_port_t port, struct in6_addr *addr_p, uint32_t ifscope);
extern void in6_sockaddr_s(in_port_t, struct in6_addr *, struct sockaddr_in6 *, uint32_t);
extern int in6_getpeeraddr(struct socket *, struct sockaddr **);
extern int in6_getsockaddr(struct socket *, struct sockaddr **);
extern int in6_getsockaddr_s(struct socket *, struct sockaddr_in6 *);
extern int in6_mapped_sockaddr(struct socket *so, struct sockaddr **nam);
extern int in6_mapped_peeraddr(struct socket *so, struct sockaddr **nam);
extern uint8_t in6_selecthlim(struct in6pcb *, struct ifnet *);
extern int in6_pcbsetport(struct in6_addr *, struct sockaddr *, struct inpcb *,
    struct proc *, int);
extern void init_sin6(struct sockaddr_in6 *sin6, struct mbuf *m);
extern void in6p_route_copyout(struct inpcb *, struct route_in6 *);
extern void in6p_route_copyin(struct inpcb *, struct route_in6 *);
static inline const struct in6_ifaddr *
__attribute__((overloadable)) __pure2
__ifatoia6_const(const struct ifaddr *ifa)
{
	return __container_of(ifa, const struct in6_ifaddr, ia_ifa);
}
static inline struct in6_ifaddr *
__attribute__((overloadable)) __pure2
__ifatoia6(struct ifaddr *ifa)
{
	return __container_of(ifa, struct in6_ifaddr, ia_ifa);
}


extern TAILQ_HEAD(in6_ifaddrhead, in6_ifaddr) in6_ifaddrhead;
extern TAILQ_HEAD(in6_ifaddrhashhead, in6_ifaddr) * __counted_by(in6addr_nhash) in6_ifaddrhashtbl;
static __inline uint32_t
in6addr_hashval(const struct in6_addr *in6)
{
	
	if (in6addr_nhash > 1) {
		uint32_t x;

		x = in6->s6_addr32[0] ^ in6->s6_addr32[1] ^ in6->s6_addr32[2] ^
		    in6->s6_addr32[3];

		return (x * in6addr_hashp) % in6addr_nhash;
	} else {
		return 0;
	}
}


extern lck_mtx_t nd6_mutex_data;
void in6_post_msg(struct ifnet *, u_int32_t, struct in6_ifaddr *,
    uint8_t *__sized_by(maclen) mac, size_t maclen);
RB_HEAD(ip6_msource_tree, ip6_msource);
RB_PROTOTYPE_SC_PREV(__private_extern__, ip6_msource_tree, ip6_msource,
    im6s_link, ip6_msource_cmp);
extern LIST_HEAD(in6_multihead, in6_multi) in6_multihead;
extern int im6o_mc_filter(const struct ip6_moptions *, struct ifnet *,
    const struct sockaddr_in6 *, const struct sockaddr_in6 *);
extern int in6_mc_join(struct ifnet *, const struct in6_addr *,
    struct in6_mfilter *, struct in6_multi **, int);
extern int in6_mc_leave(struct in6_multi *, struct in6_mfilter *);
extern void in6m_clear_recorded(struct in6_multi *);
extern void in6m_commit(struct in6_multi *);
extern void in6m_purge(struct in6_multi *);
extern void in6m_print(const struct in6_multi *);
extern int in6m_record_source(struct in6_multi *, const struct in6_addr *);
extern int ip6_getmoptions(struct inpcb *, struct sockopt *);
extern int ip6_setmoptions(struct inpcb *, struct sockopt *);
extern struct in6_multi_mship *in6_joingroup(struct ifnet *,
    struct in6_addr *, int *, int);
extern int in6_leavegroup(struct in6_multi_mship *);
extern void in6m_addref(struct in6_multi *, int);
extern void in6m_remref(struct in6_multi *, int);
extern int in6_multi_detach(struct in6_multi *);
extern int in6_ifindex2scopeid(int);
extern int in6_mask2len(struct in6_addr *, u_char *__counted_by(0) lim0);
extern void in6_len2mask(struct in6_addr *, int);
extern int in6_control(struct socket *, u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)),
    struct ifnet *, struct proc *);
extern int in6_update_ifa(struct ifnet *, struct in6_aliasreq *, int,
    struct in6_ifaddr **);
extern void in6_purgeaddr(struct ifaddr *);
extern int in6if_do_dad(struct ifnet *);
extern void in6_purgeif(struct ifnet *);
extern void in6_savemkludge(struct in6_ifaddr *);
extern void in6_setmaxmtu(void);
extern void in6_restoremkludge(struct in6_ifaddr *, struct ifnet *);
extern void in6_purgemkludge(struct ifnet *);
extern struct in6_ifaddr *in6ifa_ifpforlinklocal(struct ifnet *, int);
extern struct in6_ifaddr *in6ifa_ifpwithflag(struct ifnet *, int);
extern struct in6_ifaddr *in6ifa_ifpwithaddr(struct ifnet *, struct in6_addr *);
extern struct in6_ifaddr *in6ifa_prproxyaddr(struct in6_addr *, uint32_t);
extern void in6ifa_getlifetime(struct in6_ifaddr *,
    struct in6_addrlifetime *, int);
extern void in6ifa_setlifetime(struct in6_ifaddr *, struct in6_addrlifetime *);
extern const char *ip6_sprintf(const struct in6_addr *);
extern int in6_addr2scopeid(struct ifnet *, struct in6_addr *);
extern int in6_matchlen(struct in6_addr *, struct in6_addr *);
extern int in6_are_prefix_equal(struct in6_addr *p1, uint32_t ifscope1, struct in6_addr *p2, uint32_t ifscope2,
    int len);
extern void in6_prefixlen2mask(struct in6_addr *maskp, int len);
extern int in6_prefix_add_ifid(int iilen, struct in6_ifaddr *ia);
extern void in6_prefix_remove_ifid(int iilen, struct in6_ifaddr *ia);
extern void in6_purgeprefix(struct ifnet *);
extern void in6_purgeaddrs(struct ifnet *);
extern uint8_t im6s_get_mode(const struct in6_multi *,
    const struct ip6_msource *, uint8_t);
extern void im6f_leave(struct in6_mfilter *);
extern void im6f_purge(struct in6_mfilter *);
extern int in6_embedscope(struct in6_addr *, const struct sockaddr_in6 *,
    struct inpcb *, struct ifnet **, struct ip6_pktopts *, uint32_t *);
extern int in6_recoverscope(struct sockaddr_in6 *, const struct in6_addr *,
    struct ifnet *);
extern void in6_aliasreq_64_to_32(struct in6_aliasreq_64 *,
    struct in6_aliasreq_32 *);
extern void in6_aliasreq_32_to_64(struct in6_aliasreq_32 *,
    struct in6_aliasreq_64 *);
extern void in6_ifaddr_init(void);
extern int in6_inithead(void **, int);
extern void in6_rtqdrain(void);
extern struct radix_node *in6_validate(struct radix_node *);
extern int in6_if2idlen(struct ifnet *);
extern int in6_src_ioctl(u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)));
extern void in6_multihead_lock_exclusive(void);
extern void in6_multihead_lock_shared(void);
extern void in6_multihead_lock_assert(int);
extern void in6_multihead_lock_done(void);
extern void in6_cga_init(void);
extern void in6_cga_node_lock(void);
extern void in6_cga_node_unlock(void);
extern void in6_cga_query(struct in6_cga_nodecfg *);
extern int in6_cga_start(const struct in6_cga_nodecfg *);
extern int in6_cga_stop(void);
extern int in6_cga_generate(struct in6_cga_prepare *, u_int8_t,
    struct in6_addr *, struct ifnet *);
extern int in6_getconninfo(struct socket *, sae_connid_t, uint32_t *,
    uint32_t *, int32_t *, user_addr_t, socklen_t *,
    user_addr_t, socklen_t *, uint32_t *, user_addr_t, uint32_t *);
extern void in6_ip6_to_sockaddr(const struct in6_addr *ip6, u_int16_t port, uint32_t ifscope,
    struct sockaddr_in6 *sin6, u_int32_t maxlen);
extern void in6_cgareq_copy_from_user32(const void *__sized_by(sizeof(struct in6_cgareq_32)), struct in6_cgareq*);
extern void in6_cgareq_copy_from_user64(const void *__sized_by(sizeof(struct in6_cgareq_64)), struct in6_cgareq*);
extern void ip6_ecn_ingress(int, u_int32_t *, const u_int32_t *);
extern int ip6_ecn_egress(int, const u_int32_t *, u_int32_t *);
extern void ip46_ecn_ingress(int, u_int32_t *, const u_int8_t *);
extern int ip46_ecn_egress(int, const u_int32_t *, u_int8_t *);
extern void ip64_ecn_ingress(int, u_int8_t *, const u_int32_t *);
extern int ip64_ecn_egress(int, const u_int8_t *, u_int32_t *);
extern int icmp6_ctloutput(struct socket *, struct sockopt *);
extern int icmp6_dgram_ctloutput(struct socket *, struct sockopt *);
extern int icmp6_dgram_send(struct socket *, int, struct mbuf *,
    struct sockaddr *, struct mbuf *, struct proc *);
extern int icmp6_dgram_attach(struct socket *, int, struct proc *);
extern void ip6_register_m_tag(void);
extern void ip6_init(struct ip6protosw *, struct domain *);
extern void ip6_input(struct mbuf *);
extern void ip6_setsrcifaddr_info(struct mbuf *, uint32_t, struct in6_ifaddr *);
extern void ip6_setdstifaddr_info(struct mbuf *, uint32_t, struct in6_ifaddr *);
extern int ip6_getsrcifaddr_info(struct mbuf *, uint32_t *, uint32_t *);
extern int ip6_getdstifaddr_info(struct mbuf *, uint32_t *, uint32_t *);
extern uint32_t ip6_input_getsrcifscope(struct mbuf *);
extern uint32_t ip6_input_getdstifscope(struct mbuf *);
extern void ip6_output_setsrcifscope(struct mbuf *, uint32_t, struct in6_ifaddr *);
extern void ip6_output_setdstifscope(struct mbuf *, uint32_t, struct in6_ifaddr *);
extern uint32_t ip6_output_getsrcifscope(struct mbuf *);
extern uint32_t ip6_output_getdstifscope(struct mbuf *);
extern void ip6_freepcbopts(struct ip6_pktopts *);
extern int ip6_unknown_opt(uint8_t * __counted_by(optplen) optp, size_t optplen, struct mbuf *, size_t);
extern char *ip6_get_prevhdr(struct mbuf *, int);
extern int ip6_nexthdr(struct mbuf *, int, int, int *);
extern int ip6_lasthdr(struct mbuf *, int, int, int *);
extern boolean_t ip6_pkt_has_ulp(struct mbuf *m);
extern void ip6_moptions_init(void);
extern struct ip6_moptions *ip6_allocmoptions(zalloc_flags_t);
extern void im6o_addref(struct ip6_moptions *, int);
extern void im6o_remref(struct ip6_moptions *);
extern struct ip6aux *ip6_addaux(struct mbuf *);
extern struct ip6aux *ip6_findaux(struct mbuf *);
extern void ip6_delaux(struct mbuf *);
extern int ip6_process_hopopts(struct mbuf *, u_int8_t *__sized_by(hbhlen) opthead, int hbhlen,
    u_int32_t *, u_int32_t *);
extern struct mbuf **ip6_savecontrol_v4(struct inpcb *, struct mbuf *,
    struct mbuf **, int *);
extern int ip6_savecontrol(struct inpcb *, struct mbuf *, struct mbuf **);
extern struct mbuf *ip6_forward(struct mbuf *, struct route_in6 *, int);
extern void ip6_notify_pmtu(struct inpcb *, struct sockaddr_in6 *, u_int32_t *);
extern void ip6_mloopback(struct ifnet *, struct ifnet *, struct mbuf *,
    struct sockaddr_in6 *, uint32_t, int32_t);
extern int ip6_output(struct mbuf *, struct ip6_pktopts *, struct route_in6 *,
    int, struct ip6_moptions *, struct ifnet **, struct ip6_out_args *);
extern int ip6_output_list(struct mbuf *, int, struct ip6_pktopts *,
    struct route_in6 *, int, struct ip6_moptions *, struct ifnet **,
    struct ip6_out_args *);
extern int ip6_ctloutput(struct socket *, struct sockopt *);
extern int ip6_raw_ctloutput(struct socket *, struct sockopt *);
extern void ip6_initpktopts(struct ip6_pktopts *);
extern int ip6_setpktoptions(struct mbuf *, struct ip6_pktopts *, int, int);
extern void ip6_clearpktopts(struct ip6_pktopts *, int);
extern struct ip6_pktopts *ip6_copypktopts(struct ip6_pktopts *, zalloc_flags_t);
extern int ip6_optlen(struct inpcb *);
extern void ip6_drain(void);
extern int ip6_do_fragmentation(struct mbuf **, uint32_t, struct ifnet *, uint32_t,
    struct ip6_hdr *, uint8_t *, uint32_t, int, uint32_t);
extern int route6_input(struct mbuf **, int *, int);
extern void frag6_init(void);
extern int frag6_input(struct mbuf **, int *, int);
extern void frag6_drain(void);
extern int rip6_input(struct mbuf **, int *, int);
extern void rip6_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
extern int rip6_ctloutput(struct socket *so, struct sockopt *sopt);
extern int rip6_output(struct mbuf *, struct socket *, struct sockaddr_in6 *,
    struct mbuf *, int);
extern int dest6_input(struct mbuf **, int *, int);
extern struct ifaddr * in6_selectsrc_core_ifa(struct sockaddr_in6 *, struct ifnet *);
extern struct in6_addr * in6_selectsrc_core(struct sockaddr_in6 *,
    uint32_t, struct ifnet *, int, struct in6_addr *,
    struct ifnet **, int *, struct ifaddr **, struct route_in6 *, boolean_t);
extern struct in6_addr *in6_selectsrc(struct sockaddr_in6 *,
    struct ip6_pktopts *, struct inpcb *, struct route_in6 *,
    struct ifnet **, struct in6_addr *, unsigned int, int *);
extern struct in6_addrpolicy *in6_addrsel_lookup_policy(struct sockaddr_in6 *);
extern int in6_selectroute(struct sockaddr_in6 *, struct sockaddr_in6 *,
    struct ip6_pktopts *, struct ip6_moptions *, struct in6_ifaddr **,
    struct route_in6 *, struct ifnet **, struct rtentry **, int,
    struct ip6_out_args *);
extern int ip6_setpktopts(struct mbuf *control, struct ip6_pktopts *opt,
    struct ip6_pktopts *stickyopt, int uproto);
extern uint32_t ip6_randomid(uint64_t);
extern uint32_t ip6_randomflowlabel(void);
extern struct secpolicy *ipsec4_getpolicybysock(struct mbuf *, u_int8_t,
    struct socket *, int *);
extern struct secpolicy *ipsec4_getpolicybyaddr(struct mbuf *, u_int8_t, int,
    int *);
extern int ipsec4_getpolicybyinterface(struct mbuf *, u_int8_t, int *,
    struct ip_out_args *, struct secpolicy **);
extern u_int ipsec_get_reqlevel(struct ipsecrequest *);
extern int ipsec_init_policy(struct socket *so, struct inpcbpolicy **);
extern int ipsec_copy_policy(struct inpcbpolicy *, struct inpcbpolicy *);
extern u_int ipsec_get_reqlevel(struct ipsecrequest *);
extern int ipsec4_set_policy(struct inpcb *inp, int optname,
    caddr_t __sized_by(len)request, size_t len, int priv);
extern int ipsec4_delete_pcbpolicy(struct inpcb *);
extern int ipsec4_in_reject_so(struct mbuf *, struct socket *);
extern int ipsec4_in_reject(struct mbuf *, struct inpcb *);
extern int ipsec_chkreplay(u_int32_t, struct secasvar *, u_int8_t);
extern int ipsec_updatereplay(u_int32_t, struct secasvar *, u_int8_t);
extern size_t ipsec4_hdrsiz(struct mbuf *, u_int8_t, struct inpcb *);
extern size_t ipsec_hdrsiz_tcp(struct tcpcb *);
extern size_t ipsec_hdrsiz(struct secpolicy *);
extern const char *ipsec4_logpacketstr(struct ip *, u_int32_t);
extern const char *ipsec_logsastr(struct secasvar *);
extern void ipsec_dumpmbuf(struct mbuf *);
extern int ipsec4_interface_output(struct ipsec_output_state *state, ifnet_t interface);
extern int ipsec4_output(struct ipsec_output_state *, struct secpolicy *, int);
extern struct mbuf * ipsec6_splithdr(struct mbuf *);
extern int ipsec6_encapsulate(struct mbuf *, struct secasvar *);
extern int ipsec4_tunnel_validate(struct mbuf *, int, u_int, struct secasvar *, sa_family_t *);
extern struct mbuf *ipsec_copypkt(struct mbuf *);
extern void ipsec_delaux(struct mbuf *);
extern int ipsec_setsocket(struct mbuf *, struct socket *);
extern struct socket *ipsec_getsocket(struct mbuf *);
extern int ipsec_incr_history_count(struct mbuf *, int, u_int32_t);
extern u_int32_t ipsec_get_history_count(struct mbuf *);
extern void ipsec_monitor_sleep_wake(void);
extern void ipsec_get_local_ports(void);
extern void ipsec_init(void);
extern void ipsec_register_m_tag(void);
extern int ipsec4_interface_kpipe_output(ifnet_t, kern_packet_t, kern_packet_t);
extern int ipsec6_interface_kpipe_output(ifnet_t, kern_packet_t, kern_packet_t);
extern void ipsec_fill_ip6_sockaddr_4_6_with_ifscope(union sockaddr_in_4_6 *sin46,
    struct in6_addr *ip6, u_int16_t port, uint32_t ifscope);
extern void ipsec_fill_ip6_sockaddr_4_6(union sockaddr_in_4_6 *sin46,
    struct in6_addr *ip6, u_int16_t port);
extern void ipsec_fill_ip_sockaddr_4_6(union sockaddr_in_4_6 *sin46,
    struct in_addr ip, u_int16_t port);
extern struct secpolicy *ipsec6_getpolicybysock(struct mbuf *, u_int8_t,
    struct socket *, int *);
extern struct secpolicy *ipsec6_getpolicybyaddr(struct mbuf *, u_int8_t, int,
    int *);
extern int ipsec6_getpolicybyinterface(struct mbuf *,
    u_int8_t, int, struct ip6_out_args *, int *, struct secpolicy **);
extern int ipsec6_in_reject_so(struct mbuf *, struct socket *);
extern int ipsec6_delete_pcbpolicy(struct inpcb *);
extern int ipsec6_set_policy(struct inpcb *inp, int optname,
    caddr_t __sized_by(len)request, size_t len, int priv);
extern int ipsec6_in_reject(struct mbuf *, struct inpcb *);
extern size_t ipsec6_hdrsiz(struct mbuf *, u_int8_t, struct inpcb *);
extern const char *ipsec6_logpacketstr(struct ip6_hdr *, u_int32_t);
extern int ipsec6_interface_output(struct ipsec_output_state *, ifnet_t, u_char *, struct mbuf *);
extern int ipsec6_output_trans(struct ipsec_output_state *, u_char *,
    struct mbuf *, struct secpolicy *, int, int *);
extern int ipsec6_output_tunnel(struct ipsec_output_state *,
    struct secpolicy *, int);
extern int ipsec6_tunnel_validate(struct mbuf *, int, u_int,
    struct secasvar *, sa_family_t *);
extern int mld_change_state(struct in6_multi *, struct mld_tparams *,
    const int);
extern struct mld_ifinfo *mld_domifattach(struct ifnet *, zalloc_flags_t);
extern void mld_domifreattach(struct mld_ifinfo *);
extern void mld_domifdetach(struct ifnet *);
extern void mld_fasttimo(void);
extern void mld_ifdetach(struct ifnet *);
extern int mld_input(struct mbuf *, int, int);
extern void mld_init(void);
extern void mld_set_timeout(struct mld_tparams *);
extern void mld_set_fast_timeout(struct mld_tparams *);
extern void mli_addref(struct mld_ifinfo *, int);
extern void mli_remref(struct mld_ifinfo *);
__private_extern__ void mld6_initsilent(struct ifnet *, struct mld_ifinfo *);
TAILQ_HEAD(nd_drhead, nd_defrouter);
TAILQ_HEAD(nd_rtihead, nd_route_info);
void nd6_rti_list_wait(const char *);
void nd6_rti_list_signal_done(void);
void ndrti_free(struct nd_route_info *rti);
void nd6_rtilist_remove(struct nd_route_info *);
void nd6_rtilist_update(struct nd_route_info *, struct nd_defrouter *);
int nd6_rtilist_add(struct nd_route_info *, struct nd_defrouter *,
    struct nd_route_info **);
void nd6_rti_purge(struct nd_route_info *);
void nd6_rti_delreq(struct nd_route_info *);
void nd6_rti_select(struct nd_route_info *, struct ifnet *);
RB_HEAD(prproxy_sols_tree, nd6_prproxy_soltgt);
LIST_HEAD(nd_prhead, nd_prefix);
extern void nd6_sched_timeout(struct timeval *, struct timeval *);
extern void nd6_init(void);
extern void nd6_ifreset(struct ifnet *ifp);
extern void nd6_ifattach(struct ifnet *);
extern int nd6_is_addr_neighbor(struct sockaddr_in6 *, struct ifnet *, int);
extern void nd6_option_init(void *__sized_by(icmp6len), size_t icmp6len, union nd_opts *);
extern struct nd_opt_hdr *nd6_option(union nd_opts *);
extern int nd6_options(union nd_opts *);
extern struct rtentry *nd6_lookup(struct in6_addr *, int, struct ifnet *, int);
extern void nd6_setmtu(struct ifnet *);
extern void nd6_purge(struct ifnet *);
extern void nd6_free(struct rtentry *);
extern void nd6_nud_hint(struct rtentry *, struct in6_addr *, int);
extern int nd6_resolve(struct ifnet *, struct rtentry *,
    struct mbuf *, struct sockaddr *, u_char *);
extern void nd6_rtrequest(int, struct rtentry *, struct sockaddr *);
extern int nd6_ioctl(u_long cmd, caddr_t __counted_by(IOCPARM_LEN(cmd)), struct ifnet *);
extern void nd6_cache_lladdr(struct ifnet *, struct in6_addr *,
    char *lladdr __sized_by(lladdrlen), int lladdrlen,
    int, int, int *);
extern int nd6_output_list(struct ifnet *, struct ifnet *, struct mbuf *,
    struct sockaddr_in6 *, struct rtentry *, struct flowadv *);
extern int nd6_output(struct ifnet *, struct ifnet *, struct mbuf *,
    struct sockaddr_in6 *, struct rtentry *, struct flowadv *);
extern int nd6_need_cache(struct ifnet *);
extern void nd6_drain(void *);
extern void nd6_post_msg(u_int32_t, struct nd_prefix_list *, u_int32_t,
    u_int32_t);
extern int nd6_setifinfo(struct ifnet *, u_int32_t, u_int32_t);
extern const char *ndcache_state2str(short);
extern void ln_setexpire(struct llinfo_nd6 *, uint64_t);
extern void nd6_nbr_init(void);
extern void nd6_na_input(struct mbuf *, int, int);
extern void nd6_na_output(struct ifnet *, const struct in6_addr *,
    const struct in6_addr *, u_int32_t, int, struct sockaddr *);
extern void nd6_ns_input(struct mbuf *, int, int);
extern void nd6_ns_output(struct ifnet *, const struct in6_addr *,
    const struct in6_addr *, struct llinfo_nd6 *,
    uint8_t *__counted_by(noncelen) nonce, size_t noncelen);
static inline caddr_t __header_indexable __stateful_pure
nd6_ifptomac(struct ifnet *ifp)
{
	switch (ifp->if_type) {
	case IFT_ARCNET:
	case IFT_ETHER:
	case IFT_IEEE8023ADLAG:
	case IFT_FDDI:
	case IFT_IEEE1394:
	case IFT_BRIDGE:
	case IFT_ISO88025:
		return (caddr_t)IF_LLADDR(ifp);
	default:
		return NULL;
	}
}

extern void nd6_dad_start(struct ifaddr *, int *);
extern void nd6_dad_stop(struct ifaddr *);
extern void nd6_llreach_alloc(struct rtentry *, struct ifnet *,
    void * __sized_by(alen), unsigned int alen, boolean_t);
extern void nd6_llreach_set_reachable(struct ifnet *, void *__sized_by(alen) addr, unsigned int alen);
extern void nd6_llreach_use(struct llinfo_nd6 *);
extern void nd6_alt_node_addr_decompose(struct ifnet *, struct sockaddr *,
    struct sockaddr_dl *, struct sockaddr_in6 *);
extern int nd6_alt_node_present(struct ifnet *, struct sockaddr_in6 *,
    struct sockaddr_dl *, int32_t, int, int);
extern int nd6_alt_node_absent(struct ifnet *, struct sockaddr_in6 *, struct sockaddr_dl *);
extern struct in6_ifaddr *in6_pfx_newpersistaddr(struct nd_prefix *, int,
    int *, boolean_t, uint8_t);
extern void nd6_rtr_init(void);
extern void nd6_rs_input(struct mbuf *, int, int);
extern void nd6_ra_input(struct mbuf *, int, int);
extern void prelist_del(struct nd_prefix *);
extern struct nd_defrouter *defrtrlist_update(struct nd_defrouter *,
    struct nd_drhead *);
extern void defrouter_select(struct ifnet *, struct nd_drhead *);
extern void defrouter_reset(void);
extern int defrtrlist_ioctl(u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)));
extern void defrtrlist_del(struct nd_defrouter *, struct nd_drhead *);
extern int defrtrlist_add_static(struct nd_defrouter *);
extern int defrtrlist_del_static(struct nd_defrouter *);
extern void prelist_remove(struct nd_prefix *);
extern int prelist_update(struct nd_prefix *, struct nd_defrouter *,
    struct mbuf *, int);
extern int nd6_prelist_add(struct nd_prefix *, struct nd_defrouter *,
    struct nd_prefix **, boolean_t);
extern int nd6_prefix_onlink(struct nd_prefix *);
extern int nd6_prefix_onlink_scoped(struct nd_prefix *, unsigned int);
extern int nd6_prefix_offlink(struct nd_prefix *);
extern void pfxlist_onlink_check(void);
extern void defrouter_set_reachability(struct in6_addr *, struct ifnet *, boolean_t);
extern struct nd_defrouter *defrouter_lookup(struct nd_drhead *,
    struct in6_addr *, struct ifnet *);
extern struct nd_pfxrouter *pfxrtr_lookup(struct nd_prefix *, struct nd_defrouter *);
extern struct nd_prefix *nd6_prefix_lookup(struct nd_prefix *, int);
extern int in6_init_prefix_ltimes(struct nd_prefix *ndpr);
extern void rt6_flush(struct in6_addr *, struct ifnet *);
extern int nd6_setdefaultiface(int);
extern int in6_tmpifadd(const struct in6_ifaddr *, int);
extern void nddr_addref(struct nd_defrouter *);
extern struct nd_defrouter *nddr_remref(struct nd_defrouter *);
extern uint64_t nddr_getexpire(struct nd_defrouter *);
extern void ndpr_addref(struct nd_prefix *);
extern struct nd_prefix *ndpr_remref(struct nd_prefix *);
extern uint64_t ndpr_getexpire(struct nd_prefix *);
extern void defrouter_delreq(struct nd_defrouter *, struct nd_route_info *);
extern int nd6_if_prproxy(struct ifnet *, boolean_t);
extern void nd6_prproxy_prelist_update(struct nd_prefix *, struct nd_prefix *);
extern boolean_t nd6_prproxy_ifaddr(struct in6_ifaddr *);
extern void nd6_proxy_find_fwdroute(struct ifnet *, struct route_in6 *);
extern boolean_t nd6_prproxy_isours(struct mbuf *, struct ip6_hdr *,
    struct route_in6 *, unsigned int);
extern void nd6_prproxy_ns_output(struct ifnet *, struct ifnet *,
    struct in6_addr *, struct in6_addr *, struct llinfo_nd6 *);
extern void nd6_prproxy_ns_input(struct ifnet *, struct in6_addr *,
    char *__sized_by(lladdrlen) lladdr, int lladdrlen, struct in6_addr *,
    struct in6_addr *, uint8_t *__counted_by(noncelen) nonce, size_t noncelen);
extern void nd6_prproxy_na_input(struct ifnet *, struct in6_addr *,
    struct in6_addr *, struct in6_addr *, int);
extern void nd6_prproxy_sols_reap(struct nd_prefix *);
extern void nd6_prproxy_sols_prune(struct nd_prefix *, u_int32_t);
extern int nd6_if_disable(struct ifnet *, boolean_t);
void in6_ifaddr_set_dadprogress(struct in6_ifaddr *ia);
extern errno_t nd6_lookup_ipv6(ifnet_t interface,
    const struct sockaddr_in6 *ip6_dest, struct sockaddr_dl *ll_dest,
    size_t ll_dest_len, route_t hint, mbuf_t packet);
extern void scope6_ifattach(struct ifnet *);
extern void scope6_setdefault(struct ifnet *);
extern u_int32_t scope6_in6_addrscope(struct in6_addr *);
extern u_int32_t scope6_addr2default(struct in6_addr *);
extern int sa6_embedscope(struct sockaddr_in6 *, int, uint32_t *);
extern int sa6_recoverscope(struct sockaddr_in6 *, boolean_t);
extern int in6_setscope(struct in6_addr *, struct ifnet *, u_int32_t *);
extern int in6_clearscope(struct in6_addr *);
extern void rtkey_to_sa6(struct rtentry *, struct sockaddr_in6 *);
extern void rtgw_to_sa6(struct rtentry *, struct sockaddr_in6 *);
extern bool in6_are_addr_equal_scoped(const struct in6_addr *, const struct in6_addr *,
    uint32_t, uint32_t);
extern bool in6_are_masked_addr_scope_equal(const struct in6_addr *, uint32_t, const struct in6_addr *, uint32_t, const struct in6_addr *);
extern void in6_verify_ifscope(const struct in6_addr *, uint32_t);
void    tcp6_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
void    tcp6_init(void);
int     tcp6_input(struct mbuf **, int *, int);
SYSCTL_DECL(_net_inet6_udp6);
extern void udp6_ctlinput(int, struct sockaddr *, void *, struct ifnet *);
extern int udp6_input(struct mbuf **, int *, int);
extern int udp6_output(struct inpcb *, struct mbuf *, struct sockaddr *,
    struct mbuf *, struct proc *);
extern int udp6_connect(struct socket *, struct sockaddr *, struct proc *);
extern struct secpolicy *key_allocsp(struct secpolicyindex *, u_int);
extern struct secasvar *key_allocsa_policy(struct secasindex *);
extern struct secpolicy *key_gettunnel(struct sockaddr *,
    struct sockaddr *, struct sockaddr *, struct sockaddr *);
extern struct secasvar *key_alloc_outbound_sav_for_interface(ifnet_t interface, int family,
    struct sockaddr *src,
    struct sockaddr *dst);
extern int key_checkrequest(struct ipsecrequest *isr, struct secasindex *,
    struct secasvar **sav);
extern bool key_checksa_present(union sockaddr_in_4_6 *src, union sockaddr_in_4_6 *dst);
extern u_int16_t key_natt_get_translated_port(struct secasvar *);
extern void key_freesp(struct secpolicy *, int);
extern void key_freesav(struct secasvar *, int);
extern struct secpolicy *key_newsp(void);
extern struct secpolicy *key_msg2sp(struct sadb_x_policy *__sized_by(len), size_t len, int *);
extern struct mbuf *key_sp2msg(struct secpolicy *);
extern int key_ismyaddr(struct sockaddr *);
extern int key_spdacquire(struct secpolicy *);
extern void key_timehandler(void);
extern u_int32_t key_random(void);
extern void key_randomfill(void *, size_t);
extern void key_freereg(struct socket *);
extern int key_parse(struct mbuf *, struct socket *);
extern int key_checktunnelsanity(struct secasvar *, u_int, caddr_t, caddr_t);
extern void key_sa_recordxfer(struct secasvar *, size_t);
extern void key_sa_routechange(struct sockaddr *);
extern void key_sa_chgstate(struct secasvar *, u_int8_t);
extern void key_sa_stir_iv(struct secasvar *);
extern void key_delsah(struct secashead *sah);
extern struct secashead *key_newsah2(struct secasindex *saidx, u_int8_t dir);
extern void key_delsav(struct secasvar *sav);
extern struct secpolicy *key_getspbyid(u_int32_t);
extern void key_delsp_for_ipsec_if(ifnet_t ipsec_if);
extern u_int32_t key_fill_offload_frames_for_savs(struct ifnet *,
    struct ifnet_keepalive_offload_frame *__counted_by(frames_count)frames_array, u_int32_t frames_count, size_t);
extern bool key_custom_ipsec_token_is_valid(void *);
extern int key_reserve_custom_ipsec(void **, union sockaddr_in_4_6 *, union sockaddr_in_4_6 *, u_int8_t proto);
extern void key_release_custom_ipsec(void **);
extern struct secpolicy *keydb_newsecpolicy(void);
extern void keydb_delsecpolicy(struct secpolicy *);
extern struct secashead *keydb_newsecashead(void);
extern struct secreplay *keydb_newsecreplay(u_int8_t);
extern void keydb_delsecreplay(struct secreplay *);
extern int key_output(struct mbuf *, struct socket* so);
extern int key_usrreq(struct socket *,
    int, struct mbuf *, struct mbuf *, struct mbuf *);
extern int key_sendup(struct socket *, struct sadb_msg *, u_int, int);
extern int key_sendup_mbuf(struct socket *, struct mbuf *, int);
extern void kdebug_sadb(struct sadb_msg *);
extern void kdebug_sadb_x_policy(struct sadb_ext *);
extern void kdebug_secpolicy(struct secpolicy *);
extern void kdebug_secpolicyindex(struct secpolicyindex *);
extern void kdebug_secasindex(struct secasindex *);
extern void kdebug_secasv(struct secasvar *);
extern void kdebug_mbufhdr(struct mbuf *);
extern void kdebug_mbuf(struct mbuf *);
extern void kdebug_sockaddr(struct sockaddr *);
extern void ipsec_hexdump(caddr_t, int);
extern void ipsec_bindump(caddr_t, int);
LIST_HEAD(nfs_gss_svc_ctx_hashhead, nfs_gss_svc_ctx);
__BEGIN_DECLS

void    nfs_gss_svc_init(void);
int     nfs_gss_svc_cred_get(struct nfsrv_descript *, struct nfsm_chain *);
int     nfs_gss_svc_verf_put(struct nfsrv_descript *, struct nfsm_chain *);
int     nfs_gss_svc_ctx_init(struct nfsrv_descript *, struct nfsrv_sock *, mbuf_t *);
int     nfs_gss_svc_prepare_reply(struct nfsrv_descript *, struct nfsm_chain *);
int     nfs_gss_svc_protect_reply(struct nfsrv_descript *, mbuf_t);
void    nfs_gss_svc_ctx_deref(struct nfs_gss_svc_ctx *);
void    nfs_gss_svc_cleanup(void);
__enum_decl(workq_tr_state_t, uint8_t, {
	WORKQ_TR_STATE_IDLE               = 0, 
	WORKQ_TR_STATE_NEW                = 1, 
	WORKQ_TR_STATE_QUEUED             = 2, 
	WORKQ_TR_STATE_CANCELED           = 3, 
	WORKQ_TR_STATE_BINDING            = 4, 
	WORKQ_TR_STATE_BOUND              = 5, 
});
__options_decl(workq_tr_flags_t, uint8_t, {
	WORKQ_TR_FLAG_KEVENT            = 0x01,
	WORKQ_TR_FLAG_WORKLOOP          = 0x02,
	WORKQ_TR_FLAG_OVERCOMMIT        = 0x04,
	WORKQ_TR_FLAG_WL_PARAMS         = 0x08,
	WORKQ_TR_FLAG_WL_OUTSIDE_QOS    = 0x10,
	WORKQ_TR_FLAG_COOPERATIVE       = 0x20,
	
	WORKQ_TR_FLAG_PERMANENT_BIND    = 0x40,
});
__options_decl(workq_state_flags_t, uint32_t, {
	WQ_EXITING                  = 0x0001,
	WQ_PROC_SUSPENDED           = 0x0002,
	WQ_DEATH_CALL_SCHEDULED     = 0x0004,

	WQ_DELAYED_CALL_SCHEDULED   = 0x0010,
	WQ_DELAYED_CALL_PENDED      = 0x0020,
	WQ_IMMEDIATE_CALL_SCHEDULED = 0x0040,
	WQ_IMMEDIATE_CALL_PENDED    = 0x0080,
});
TAILQ_HEAD(workq_uthread_head, uthread);
uint32_t _get_pwq_state_kdp(proc_t p);
void workq_exit(struct proc *p);
void workq_mark_exiting(struct proc *p);
bool workq_is_exiting(struct proc *p);
void workq_thread_set_max_qos(struct proc *p, struct workq_threadreq_s *kqr);
void workq_thread_terminate(struct proc *p, struct uthread *uth);
__options_decl(workq_kern_threadreq_flags_t, uint32_t, {
	WORKQ_THREADREQ_NONE                = 0x00,
	WORKQ_THREADREQ_SET_AST_ON_FAILURE  = 0x01,
	WORKQ_THREADREQ_ATTEMPT_REBIND      = 0x02,
	WORKQ_THREADREQ_CAN_CREATE_THREADS  = 0x04,
	WORKQ_THREADREQ_MAKE_OVERCOMMIT     = 0x08,
});
bool workq_kern_threadreq_initiate(struct proc *p, struct workq_threadreq_s *kqr,
    struct turnstile *ts, thread_qos_t qos, workq_kern_threadreq_flags_t flags);
void workq_kern_threadreq_modify(struct proc *p, struct workq_threadreq_s *kqr,
    thread_qos_t qos, workq_kern_threadreq_flags_t flags);
void workq_kern_threadreq_update_inheritor(struct proc *p, struct workq_threadreq_s *kqr,
    thread_t owner, struct turnstile *ts, turnstile_update_flags_t flags);
kern_return_t workq_kern_threadreq_permanent_bind(struct proc *p, struct workq_threadreq_s *kqr);
void workq_kern_bound_thread_wakeup(struct workq_threadreq_s *kqr);
void workq_kern_bound_thread_park(struct workq_threadreq_s *kqr);
void workq_kern_bound_thread_terminate(struct workq_threadreq_s *kqr);
bool workq_thread_is_permanently_bound(struct uthread *uth);
void workq_kern_bound_thread_reset_pri(struct workq_threadreq_s *kqr,
    struct uthread *uth);
void workq_kern_threadreq_lock(struct proc *p);
void workq_kern_threadreq_unlock(struct proc *p);
void workq_kern_threadreq_redrive(struct proc *p, workq_kern_threadreq_flags_t flags);
void workq_kern_quantum_expiry_reevaluate(struct proc *p, thread_t thread);
bool bsdthread_part_of_cooperative_workqueue(struct uthread *uth);
void workq_proc_suspended(struct proc *p);
void workq_proc_resumed(struct proc *p);
void workq_init(void);
static const char *sk_verb_flags_string[] = {
	SK_VERB_FLAGS_TABLE(EXPAND_TO_STRING)
};
static inline void __attribute__((always_inline))
__stats_fold(uint64_t *__counted_by(len)dst, uint64_t *__counted_by(len)src, size_t len)
{
	
	size_t i;
	for (i = 0; i < len; i++) {
		dst[i] += src[i];
	}
}


static inline void __attribute__((always_inline))
__stats_reset(uint64_t *__counted_by(len)_arr, size_t len)
{
	
	size_t i;
	for (i = 0; i < len; i++) {
		_arr[i] = 0;
	}
}





STATS_REGISTER(ip_stats, IP_STATS);
STATS_REGISTER(ip6_stats, IP6_STATS);
STATS_REGISTER(tcp_stats, TCP_STATS);
STATS_REGISTER(udp_stats, UDP_STATS);
STATS_REGISTER(quic_stats, QUIC_STATS);
STATS_REGISTER(fsw_stats, FSW_STATS);
STATS_REGISTER(netif_stats, NETIF_STATS);
extern void flow_stats_free(struct flow_stats *fs);
__attribute__((always_inline))
static inline void
flow_stats_retain(struct flow_stats *fs)
{
	os_ref_retain(&fs->fs_refcnt);
}

__attribute__((always_inline))
static inline void
flow_stats_release(struct flow_stats *fs)
{
	if (__improbable(os_ref_release(&fs->fs_refcnt) == 0)) {
		flow_stats_free(fs);
	}
}

__attribute__((always_inline))
static inline os_ref_count_t
flow_stats_refcnt(struct flow_stats *fs)
{
	return os_ref_get_count(&fs->fs_refcnt);
}








































typedef enum {
	
	SREG_GUARD_HEAD = 0,    
	SREG_SCHEMA,            
	SREG_RING,              
	SREG_BUF_DEF,           
	SREG_BUF_LARGE,         
	SREG_RXBUF_DEF,         
	SREG_RXBUF_LARGE,       
	SREG_TXBUF_DEF,         
	SREG_TXBUF_LARGE,       
	SREG_UMD,               
	SREG_TXAUSD,            
	SREG_RXFUSD,            
	SREG_UBFT,              
	SREG_USTATS,            
	SREG_FLOWADV,           
	SREG_NEXUSADV,          
	SREG_SYSCTLS,           
	SREG_GUARD_TAIL,        

	
	SREG_KMD,               
	SREG_RXKMD,             
	SREG_TXKMD,             
	SREG_KBFT,              
	SREG_RXKBFT,            
	SREG_TXKBFT,            
	SREG_TXAKSD,            
	SREG_RXFKSD,            
	SREG_KSTATS,            
	SREG_INSTRINSIC,        

	SREG_MAX                
} sk_stats_region_id_t;
extern void skmem_sysctl_init(void);
extern void *__sized_by(sk_sys_objsize) skmem_get_sysctls_obj(size_t *);
extern int skmem_sysctl_handle_int(struct sysctl_oid *oidp, void *arg1,
    int arg2, struct sysctl_req *req);
__private_extern__ void
_aio_close(struct proc *p, int fd);
__private_extern__ void
_aio_exit(struct proc *p);
__private_extern__ void
_aio_exec(struct proc *p);
__private_extern__ void
_aio_create_worker_threads(int num);
__private_extern__ void
aio_init(void);
task_t
get_aiotask(void);
extern uint32_t vnode_vid(void *vp);
extern boolean_t vnode_isonexternalstorage(void *vp);
extern int fill_procregioninfo(task_t t, uint64_t arg, struct proc_regioninfo_internal *pinfo, uintptr_t *vp, uint32_t *vid);
extern int fill_procregioninfo_onlymappedvnodes(task_t t, uint64_t arg, struct proc_regioninfo_internal *pinfo, uintptr_t *vp, uint32_t *vid);
void fill_taskprocinfo(task_t task, struct proc_taskinfo_internal * ptinfo);
int fill_taskthreadinfo(task_t task, uint64_t thaddr, bool thuniqueid, struct proc_threadinfo_internal * ptinfo, void *, int *);
int fill_taskthreadlist(task_t task, void * buffer, int thcount, bool thuniqueid);
int fill_taskthreadschedinfo(task_t task, uint64_t thaddr, struct proc_threadschedinfo_internal *thread_sched_info);
int get_numthreads(task_t);
boolean_t bsd_hasthreadname(void *uth);
void bsd_getthreadname(void *uth, char* buffer);
void bsd_setthreadname(void *uth, uint64_t tid, const char* buffer);
void bsd_threadcdir(void * uth, void *vptr, int *vidp);
extern void bsd_copythreadname(void *dst_uth, void *src_uth);
int fill_taskipctableinfo(task_t task, uint32_t *table_size, uint32_t *table_free);
__BEGIN_DECLS


void    buf_markaged(buf_t bp);
void    buf_markinvalid(buf_t bp);
void    buf_markdelayed(buf_t bp);
void    buf_markclean(buf_t);
void    buf_markeintr(buf_t bp);
void    buf_markfua(buf_t bp);
int     buf_fua(buf_t bp);
int     buf_valid(buf_t bp);
int     buf_fromcache(buf_t bp);
void *  buf_upl(buf_t bp);
uint32_t buf_uploffset(buf_t bp);
kauth_cred_t buf_rcred(buf_t bp);
kauth_cred_t buf_wcred(buf_t bp);
proc_t  buf_proc(buf_t bp);
uint32_t buf_dirtyoff(buf_t bp);
uint32_t buf_dirtyend(buf_t bp);
void    buf_setdirtyoff(buf_t bp, uint32_t);
void    buf_setdirtyend(buf_t bp, uint32_t);
errno_t buf_error(buf_t bp);
void    buf_seterror(buf_t bp, errno_t);
void    buf_setflags(buf_t bp, int32_t flags);
void    buf_clearflags(buf_t bp, int32_t flags);
int32_t buf_flags(buf_t bp);
void    buf_reset(buf_t bp, int32_t flags);
errno_t buf_map(buf_t bp, caddr_t *io_addr);
errno_t buf_map_range(buf_t bp, caddr_t *io_addr);
errno_t buf_map_range_with_prot(buf_t bp, caddr_t *io_addr, vm_prot_t prot);
errno_t buf_unmap(buf_t bp);
errno_t buf_unmap_range(buf_t bp);
void    buf_setdrvdata(buf_t bp, void *drvdata);
void *  buf_drvdata(buf_t bp);
void    buf_setfsprivate(buf_t bp, void *fsprivate);
void *  buf_fsprivate(buf_t bp);
daddr64_t buf_blkno(buf_t bp);
daddr64_t buf_lblkno(buf_t bp);
uint32_t buf_lblksize(buf_t bp);
void    buf_setblkno(buf_t bp, daddr64_t blkno);
void    buf_setlblkno(buf_t bp, daddr64_t lblkno);
void    buf_setlblksize(buf_t bp, uint32_t lblksize);
uint32_t buf_count(buf_t bp);
uint32_t buf_size(buf_t bp);
uint32_t buf_resid(buf_t bp);
void    buf_setcount(buf_t bp, uint32_t bcount);
void    buf_setsize(buf_t bp, uint32_t);
void    buf_setresid(buf_t bp, uint32_t resid);
void    buf_setdataptr(buf_t bp, uintptr_t data);
uintptr_t buf_dataptr(buf_t bp);
vnode_t buf_vnode(buf_t bp);
void    buf_setvnode(buf_t bp, vnode_t vp);
dev_t   buf_device(buf_t bp);
errno_t buf_setdevice(buf_t bp, vnode_t vp);
errno_t buf_strategy(vnode_t devvp, void *ap);
errno_t buf_invalblkno(vnode_t vp, daddr64_t lblkno, int flags);
void * buf_callback(buf_t bp);
errno_t buf_setcallback(buf_t bp, void (*callback)(buf_t, void *), void *transaction);
errno_t buf_setupl(buf_t bp, upl_t upl, uint32_t offset);
buf_t   buf_clone(buf_t bp, int io_offset, int io_size, void (*iodone)(buf_t, void *), void *arg);
buf_t   buf_create_shadow(buf_t bp, boolean_t force_copy, uintptr_t external_storage, void (*iodone)(buf_t, void *), void *arg);
int     buf_shadow(buf_t bp);
buf_t   buf_alloc(vnode_t vp);
void    buf_free(buf_t bp);
int     buf_invalidateblks(vnode_t vp, int flags, int slpflag, int slptimeo);
void    buf_flushdirtyblks(vnode_t vp, int wait, int flags, const char *msg);
void    buf_iterate(vnode_t vp, int (*callout)(buf_t, void *), int flags, void *arg);
void    buf_clear(buf_t bp);
errno_t buf_bawrite(buf_t bp);
errno_t buf_bdwrite(buf_t bp);
errno_t buf_bwrite(buf_t bp);
void    buf_biodone(buf_t bp);
errno_t buf_biowait(buf_t bp);
void    buf_brelse(buf_t bp);
errno_t buf_bread(vnode_t vp, daddr64_t blkno, int size, kauth_cred_t cred, buf_t *bpp);
errno_t buf_breadn(vnode_t vp, daddr64_t blkno, int size, daddr64_t *rablks, int *rasizes, int nrablks, kauth_cred_t cred, buf_t *bpp);
errno_t buf_meta_bread(vnode_t vp, daddr64_t blkno, int size, kauth_cred_t cred, buf_t *bpp);
errno_t buf_meta_breadn(vnode_t vp, daddr64_t blkno, int size, daddr64_t *rablks, int *rasizes, int nrablks, kauth_cred_t cred, buf_t *bpp);
u_int   minphys(buf_t bp);
int     physio(void (*f_strategy)(buf_t), buf_t bp, dev_t dev, int flags, u_int (*f_minphys)(buf_t), struct uio *uio, int blocksize);
buf_t   buf_getblk(vnode_t vp, daddr64_t blkno, int size, int slpflag, int slptimeo, int operation);
buf_t   buf_geteblk(int size);
void    buf_clear_redundancy_flags(buf_t bp, uint32_t flags);
uint32_t        buf_redundancy_flags(buf_t bp);
void    buf_set_redundancy_flags(buf_t bp, uint32_t flags);
bufattr_t buf_attr(buf_t bp);
void buf_markstatic(buf_t bp);
int     buf_static(buf_t bp);
void bufattr_markioscheduled(bufattr_t bap);
int bufattr_ioscheduled(bufattr_t bap);
void bufattr_markexpeditedmeta(bufattr_t bap);
int bufattr_expeditedmeta(bufattr_t bap);
void    buf_setfilter(buf_t, void (*)(buf_t, void *), void *, void(**)(buf_t, void *), void **);
bufattr_t bufattr_alloc(void);
bufattr_t bufattr_dup(bufattr_t bap);
void bufattr_free(bufattr_t bap);
void bufattr_setcpx(bufattr_t bap, struct cpx *cpx);
uint64_t bufattr_cpoff(bufattr_t bap);
void bufattr_setcpoff(bufattr_t bap, uint64_t);
int bufattr_rawencrypted(bufattr_t bap);
void bufattr_markgreedymode(bufattr_t bap);
int     bufattr_greedymode(bufattr_t bap);
void bufattr_markisochronous(bufattr_t bap);
int     bufattr_isochronous(bufattr_t bap);
int bufattr_throttled(bufattr_t bap);
int bufattr_willverify(bufattr_t bap);
int bufattr_passive(bufattr_t bap);
int bufattr_nocache(bufattr_t bap);
int bufattr_meta(bufattr_t bap);
void bufattr_markmeta(bufattr_t bap);
int bufattr_delayidlesleep(bufattr_t bap);
vm_offset_t buf_kernel_addrperm_addr(void * addr);
void bufattr_markquickcomplete(bufattr_t bap);
int bufattr_quickcomplete(bufattr_t bap);
int     count_lock_queue(void);
errno_t buf_acquire(buf_t, int, int, int);
buf_t   buf_create_shadow_priv(buf_t bp, boolean_t force_copy, uintptr_t external_storage, void (*iodone)(buf_t, void *), void *arg);
void    buf_drop(buf_t);
__BEGIN_DECLS

buf_t   alloc_io_buf(vnode_t, int);
void    free_io_buf(buf_t);
int     allocbuf(struct buf *, int);
void    bufinit(void);
void    buf_list_lock(void);
void    buf_list_unlock(void);
void    cluster_init(void);
uint32_t     count_busy_buffers(void);
int buf_flushdirtyblks_skipinfo(vnode_t, int, int, const char *);
void buf_wait_for_shadow_io(vnode_t, daddr64_t);
vnode_t buf_vnop_vnode(buf_t);
__BEGIN_DECLS


__enum_decl(coalition_policy_flavor_t, uint32_t, {
	COALITION_POLICY_SUPPRESS = 1,
});
__enum_decl(coalition_policy_suppress_t, uint32_t, {
	COALITION_POLICY_SUPPRESS_NONE = 0,
	COALITION_POLICY_SUPPRESS_DARWIN_BG = 1,
});
vm_address_t
code_signing_allocate(
	size_t alloc_size);
void
code_signing_deallocate(
	vm_address_t *alloc_addr,
	size_t alloc_size);
static inline const img4_runtime_object_spec_t*
image4_get_object_spec_from_index(
	img4_runtime_object_spec_index_t obj_spec_index)
{
	const img4_runtime_object_spec_t *__single obj_spec = NULL;

	switch (obj_spec_index) {
	case IMG4_RUNTIME_OBJECT_SPEC_INDEX_SUPPLEMENTAL_ROOT:
		obj_spec = IMG4_RUNTIME_OBJECT_SPEC_SUPPLEMENTAL_ROOT;
		break;

	case IMG4_RUNTIME_OBJECT_SPEC_INDEX_LOCAL_POLICY:
		obj_spec = IMG4_RUNTIME_OBJECT_SPEC_LOCAL_POLICY;
		break;

	default:
		break;
	}

	return obj_spec;
}


void
code_signing_init(void);
void
code_signing_configuration(
	code_signing_monitor_type_t *monitor_type,
	code_signing_config_t *config);
void
disable_code_signing_feature(
	code_signing_config_t feature);
kern_return_t
secure_channel_shared_page(
	uint64_t *secure_channel_phys,
	size_t *secure_channel_size);
void
enable_developer_mode(void);
void
disable_developer_mode(void);
bool
developer_mode_state(void);
kern_return_t
restricted_execution_mode_enable(void);
kern_return_t
restricted_execution_mode_state(void);
void
update_csm_device_state(void);
void
complete_security_boot_mode(
	uint32_t security_boot_mode);
int
csblob_register_profile(
	struct cs_blob *csblob,
	cs_profile_register_t *profile);
void
garbage_collect_provisioning_profiles(void);
void
set_compilation_service_cdhash(
	const uint8_t *cdhash);
bool
match_compilation_service_cdhash(
	const uint8_t *cdhash);
void
set_local_signing_public_key(
	const uint8_t public_key[XNU_LOCAL_SIGNING_KEY_SIZE]);
uint8_t*
get_local_signing_public_key(void);
void
unrestrict_local_signing_cdhash(
	const uint8_t *cdhash);
void*
kernel_image4_storage_data(
	size_t *allocated_size);
void
kernel_image4_set_nonce(
	const img4_nonce_domain_index_t ndi,
	const img4_nonce_t *nonce);
void
kernel_image4_roll_nonce(
	const img4_nonce_domain_index_t ndi);
errno_t
kernel_image4_copy_nonce(
	const img4_nonce_domain_index_t ndi,
	img4_nonce_t *nonce_out);
errno_t
kernel_image4_execute_object(
	img4_runtime_object_spec_index_t obj_spec_index,
	const img4_buff_t *payload,
	const img4_buff_t *manifest);
errno_t
kernel_image4_copy_object(
	img4_runtime_object_spec_index_t obj_spec_index,
	vm_address_t object_out,
	size_t *object_length);
const void*
kernel_image4_get_monitor_exports(void);
errno_t
kernel_image4_set_release_type(
	const char *release_type);
errno_t
kernel_image4_set_bnch_shadow(
	const img4_nonce_domain_index_t ndi);
errno_t
kernel_image4_monitor_trap(
	image4_cs_trap_t selector,
	const void *input_data,
	size_t input_size,
	void *output_data,
	size_t *output_size);
kern_return_t
csm_resolve_os_entitlements_from_proc(
	const proc_t process,
	const void **os_entitlements);
kern_return_t
get_trust_level_kdp(
	pmap_t pmap,
	uint32_t *trust_level);
kern_return_t
get_jit_address_range_kdp(
	pmap_t pmap,
	uintptr_t *jit_region_start,
	uintptr_t *jit_region_end);
kern_return_t
address_space_debugged(
	const proc_t process);
__BEGIN_DECLS

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wnullability-completeness"
#pragma GCC diagnostic ignored "-Wnullability-completeness-on-arrays"






void CSM_PREFIX(toggle_developer_mode)(
	bool state);
kern_return_t CSM_PREFIX(rem_enable)(void);
kern_return_t CSM_PREFIX(rem_state)(void);
kern_return_t CSM_PREFIX(secure_channel_shared_page)(
	uint64_t * secure_channel_phys,
	size_t *secure_channel_size);
void CSM_PREFIX(update_device_state)(void);
void CSM_PREFIX(complete_security_boot_mode)(
	uint32_t security_boot_mode);
void CSM_PREFIX(set_compilation_service_cdhash)(
	const uint8_t cdhash[CS_CDHASH_LEN]);
bool CSM_PREFIX(match_compilation_service_cdhash)(
	const uint8_t cdhash[CS_CDHASH_LEN]);
void CSM_PREFIX(set_local_signing_public_key)(
	const uint8_t * public_key);
uint8_t* CSM_PREFIX(get_local_signing_public_key)(void);
void* CSM_PREFIX(image4_storage_data)(
	size_t * allocated_size);
void CSM_PREFIX(image4_set_nonce)(
	const img4_nonce_domain_index_t ndi,
	const img4_nonce_t *nonce);
void CSM_PREFIX(image4_roll_nonce)(
	const img4_nonce_domain_index_t ndi);
errno_t CSM_PREFIX(image4_copy_nonce)(
	const img4_nonce_domain_index_t ndi,
	img4_nonce_t *nonce_out);
errno_t CSM_PREFIX(image4_execute_object)(
	img4_runtime_object_spec_index_t obj_spec_index,
	const img4_buff_t *payload,
	const img4_buff_t *manifest);
errno_t CSM_PREFIX(image4_copy_object)(
	img4_runtime_object_spec_index_t obj_spec_index,
	vm_address_t object_out,
	size_t *object_length);
const void* CSM_PREFIX(image4_get_monitor_exports)(void);
errno_t CSM_PREFIX(image4_set_release_type)(
	const char *release_type);
errno_t CSM_PREFIX(image4_set_bnch_shadow)(
	const img4_nonce_domain_index_t ndi);
kern_return_t CSM_PREFIX(image4_transfer_region)(
	image4_cs_trap_t selector,
	vm_address_t region_addr,
	vm_size_t region_size);
kern_return_t CSM_PREFIX(image4_reclaim_region)(
	image4_cs_trap_t selector,
	vm_address_t region_addr,
	vm_size_t region_size);
errno_t CSM_PREFIX(image4_monitor_trap)(
	image4_cs_trap_t selector,
	const void *input_data,
	size_t input_size);
__BEGIN_DECLS
int             enodev(void);
void    enodev_strat(void);
int ldisc_register(int, struct linesw *);
void ldisc_deregister(int);
extern int cdevsw_setkqueueok(int, const struct cdevsw*, int);
extern void devsw_lock(dev_t, int);
extern void devsw_unlock(dev_t, int);
int  bdevsw_isfree(int);
int  bdevsw_add(int, const struct bdevsw *);
int  bdevsw_remove(int, const struct bdevsw *);
int  cdevsw_isfree(int);
int  cdevsw_add(int, const struct cdevsw *);
int  cdevsw_add_with_bdev(int index, const struct cdevsw * csw, int bdev);
int  cdevsw_remove(int, const struct cdevsw *);
int  isdisk(dev_t, int);
__BEGIN_DECLS



enum {
	CPDBG_OFFSET_IO = CP_CODE(0),   
};
cpx_t cpx_alloc(size_t key_size, bool needs_ctx);
int cpx_alloc_ctx(cpx_t cpx);
void cpx_free_ctx(cpx_t cpx);
void cpx_init(cpx_t, size_t key_len);
void cpx_init_ctx_ptr(cpx_t cpx);
void cpx_free(cpx_t);
void cpx_writeprotect(cpx_t cpx);
__attribute__((const)) size_t cpx_size(size_t key_len);
__attribute__((pure)) bool cpx_is_sep_wrapped_key(const struct cpx *);
void cpx_set_is_sep_wrapped_key(struct cpx *, bool);
__attribute__((pure)) bool cpx_is_composite_key(const struct cpx *);
void cpx_set_is_composite_key(struct cpx *, bool);
__attribute__((pure)) bool cpx_use_offset_for_iv(const struct cpx *);
void cpx_set_use_offset_for_iv(struct cpx *, bool);
__attribute__((pure)) bool cpx_synthetic_offset_for_iv(const struct cpx *);
void cpx_set_synthetic_offset_for_iv(struct cpx *, bool);
__attribute__((pure)) uint16_t cpx_key_len(const struct cpx *);
void cpx_set_key_len(struct cpx *, uint16_t key_len);
__attribute__((pure)) void *cpx_key(const struct cpx *);
aes_encrypt_ctx *cpx_iv_aes_ctx(struct cpx *);
void cpx_flush(cpx_t cpx);
bool cpx_can_copy(const struct cpx *src, const struct cpx *dst);
void cpx_copy(const struct cpx *src, cpx_t dst);
uint16_t cpx_max_key_len(const struct cpx *cpx);
bool cpx_has_key(const struct cpx *cpx);
size_t cpx_sizex(const struct cpx *cpx);
void cpx_set_aes_iv_key(struct cpx *cpx, void *iv_key);
int cp_key_store_action(cp_key_store_action_t);
int cp_key_store_action_for_volume(uuid_t volume_uuid, cp_key_store_action_t action);
cp_key_os_version_t cp_os_version(void);
int cp_is_valid_class(int isdir, int32_t protectionclass);
__BEGIN_DECLS




enum {
	DECMPDBG_DECOMPRESS_FILE            = DECMPDBG_CODE(0),
	DECMPDBG_FETCH_COMPRESSED_HEADER    = DECMPDBG_CODE(1),
	DECMPDBG_FETCH_UNCOMPRESSED_DATA    = DECMPDBG_CODE(2),
	DECMPDBG_FREE_COMPRESSED_DATA       = DECMPDBG_CODE(4),
	DECMPDBG_FILE_IS_COMPRESSED         = DECMPDBG_CODE(5),
};
static inline bool
decmpfs_type_is_dataless(uint32_t cmp_type)
{
	return cmp_type == DATALESS_CMPFS_TYPE || cmp_type == DATALESS_PKG_CMPFS_TYPE;
}

typedef struct __attribute__((packed)) {
	
	uint32_t compression_magic;
	uint32_t compression_type;   
	union {
		uint64_t uncompressed_size;  
		decmpfs_raw_item_size _size;
	};
	unsigned char attr_bytes[0]; 
} decmpfs_disk_header;
static inline uint64_t
decmpfs_get_uncompressed_size(const decmpfs_header *hdr)
{
	if (hdr->compression_magic == DECMPFS_MAGIC && hdr->compression_type == DATALESS_PKG_CMPFS_TYPE) {
		return DECMPFS_PKG_SIZE(hdr->_size);
	}

	return hdr->uncompressed_size;
}

static inline uint32_t
decmpfs_get_directory_entries(const decmpfs_header *hdr)
{
	if (hdr->compression_magic == DECMPFS_MAGIC && hdr->compression_type == DATALESS_PKG_CMPFS_TYPE) {
		return DECMPFS_PKG_CHLD_COUNT(hdr->_size);
	}

	return (uint32_t)hdr->uncompressed_size;
}


enum {
	CMP_Type1       = 1,

	

	CMP_MAX         = 255
};
void decmpfs_init(void);
decmpfs_cnode *decmpfs_cnode_alloc(void);
void decmpfs_cnode_free(decmpfs_cnode *dp);
void decmpfs_cnode_init(decmpfs_cnode *cp);
void decmpfs_cnode_destroy(decmpfs_cnode *cp);
int decmpfs_hides_rsrc(vfs_context_t ctx, decmpfs_cnode *cp);
int decmpfs_hides_xattr(vfs_context_t ctx, decmpfs_cnode *cp, const char *xattr);
bool decmpfs_trylock_compressed_data(decmpfs_cnode *cp, int exclusive);
void decmpfs_lock_compressed_data(decmpfs_cnode *cp, int exclusive);
void decmpfs_unlock_compressed_data(decmpfs_cnode *cp, int exclusive);
uint32_t decmpfs_cnode_get_vnode_state(decmpfs_cnode *cp);
void decmpfs_cnode_set_vnode_state(decmpfs_cnode *cp, uint32_t state, int skiplock);
uint64_t decmpfs_cnode_get_vnode_cached_size(decmpfs_cnode *cp);
uint64_t decmpfs_cnode_get_vnode_cached_nchildren(decmpfs_cnode *cp);
uint64_t decmpfs_cnode_get_vnode_cached_total_size(decmpfs_cnode *cp);
void decmpfs_cnode_set_vnode_cached_size(decmpfs_cnode *cp, uint64_t size);
void decmpfs_cnode_set_vnode_cached_nchildren(decmpfs_cnode *cp, uint64_t nchildren);
void decmpfs_cnode_set_vnode_cached_total_size(decmpfs_cnode *cp, uint64_t total_sz);
uint32_t decmpfs_cnode_cmp_type(decmpfs_cnode *cp);
int decmpfs_file_is_compressed(vnode_t vp, decmpfs_cnode *cp);
errno_t decmpfs_validate_compressed_file(vnode_t vp, decmpfs_cnode *cp);
int decmpfs_decompress_file(vnode_t vp, decmpfs_cnode *cp, off_t toSize, int truncate_okay, int skiplock);
int decmpfs_free_compressed_data(vnode_t vp, decmpfs_cnode *cp);
int decmpfs_update_attributes(vnode_t vp, struct vnode_attr *vap);
errno_t decmpfs_pagein_compressed(struct vnop_pagein_args *ap, int *is_compressed, decmpfs_cnode *cp);
errno_t decmpfs_read_compressed(struct vnop_read_args *ap, int *is_compressed, decmpfs_cnode *cp);
errno_t unregister_decmpfs_decompressor(uint32_t compression_type, decmpfs_registration *registration);
#pragma pack(4)


#pragma pack()





struct direntry __DARWIN_STRUCT_DIRENTRY;
__options_decl(getdirentries64_flags_t, unsigned, {
	
	GETDIRENTRIES64_EOF = 1U << 0,
});
void doc_tombstone_clear(struct doc_tombstone *ut, struct vnode **old_vpp);
void doc_tombstone_save(struct vnode *dvp, struct vnode *vp,
    struct componentname *cnp, uint64_t doc_id,
    ino64_t file_id);
bool doc_tombstone_should_ignore_name(const char *nameptr, int len);
bool doc_tombstone_should_save(struct doc_tombstone *ut, struct vnode *vp,
    struct componentname *cnp);
#pragma pack(4)




#pragma pack()






extern TAILQ_HEAD(domains_head, domain) domains;
__BEGIN_DECLS
extern void net_add_domain_old(struct domain_old *dp);
extern int net_del_domain_old(struct domain_old *);
extern void net_drain_domains(void);
extern void domain_proto_mtx_lock_assert_held(void);
extern void domain_proto_mtx_lock_assert_notheld(void);
extern domain_guard_t domain_guard_deploy(void);
extern void domain_guard_release(domain_guard_t);
extern domain_unguard_t domain_unguard_deploy(void);
extern void domain_unguard_release(domain_unguard_t);
extern struct domain_old *pffinddomain_old(int);
extern struct domain *pffinddomain(int) __XNU_INTERNAL(pffinddomain);
extern int dtrace_getipl(void);
extern uintptr_t dtrace_caller(int);
extern uint32_t dtrace_cas32(uint32_t *, uint32_t, uint32_t);
extern void *dtrace_casptr(void *, void *, void *);
extern void dtrace_copyin(user_addr_t, uintptr_t, size_t, volatile uint16_t *);
extern void dtrace_copyinstr(user_addr_t, uintptr_t, size_t, volatile uint16_t *);
extern void dtrace_copyout(uintptr_t, user_addr_t, size_t, volatile uint16_t *);
extern void dtrace_copyoutstr(uintptr_t, user_addr_t, size_t, volatile uint16_t *);
extern void dtrace_getpcstack(pc_t *, int, int, uint32_t *);
extern uint64_t dtrace_load64(uintptr_t);
extern int dtrace_canload(uint64_t, size_t, dtrace_mstate_t*, dtrace_vstate_t*);
extern uint64_t dtrace_getreg(struct regs *, uint_t);
extern uint64_t dtrace_getvmreg(uint_t);
extern int dtrace_getstackdepth(int);
extern void dtrace_getupcstack(uint64_t *, int);
extern void dtrace_getufpstack(uint64_t *, uint64_t *, int);
extern int dtrace_getustackdepth(void);
extern uintptr_t dtrace_fulword(void *);
extern uint8_t dtrace_fuword8(user_addr_t);
extern uint16_t dtrace_fuword16(user_addr_t);
extern uint32_t dtrace_fuword32(user_addr_t);
extern uint64_t dtrace_fuword64(user_addr_t);
extern int dtrace_proc_waitfor(dtrace_procdesc_t*);
extern void dtrace_probe_error(dtrace_state_t *, dtrace_epid_t, int, int,
    int, uint64_t);
extern int dtrace_assfail(const char *, const char *, int);
extern int dtrace_attached(void);
extern hrtime_t dtrace_gethrestime(void);
extern void dtrace_flush_caches(void);
extern void dtrace_copy(uintptr_t, uintptr_t, size_t);
extern void dtrace_copystr(uintptr_t, uintptr_t, size_t, volatile uint16_t *);
extern void* dtrace_ptrauth_strip(void*, uint64_t);
extern int dtrace_is_valid_ptrauth_key(uint64_t);
extern uint64_t dtrace_physmem_read(uint64_t, size_t);
extern void dtrace_physmem_write(uint64_t, uint64_t, size_t);
extern void dtrace_livedump(char *, size_t);
extern minor_t dtrace_state_reserve(void);
extern dtrace_state_t* dtrace_state_allocate(minor_t minor);
extern dtrace_state_t* dtrace_state_get(minor_t minor);
extern void dtrace_state_free(minor_t minor);
extern void dtrace_restriction_policy_load(void);
extern boolean_t dtrace_is_restricted(void);
extern boolean_t dtrace_are_restrictions_relaxed(void);
extern boolean_t dtrace_fbt_probes_restricted(void);
extern boolean_t dtrace_sdt_probes_restricted(void);
extern boolean_t dtrace_can_attach_to_proc(proc_t);
void                            dtrace_ptss_release_entry(struct proc* p, struct dtrace_ptss_page_entry* e);
void                            dtrace_ptss_free_page(struct proc* p, struct dtrace_ptss_page* ptss_page);
void                            dtrace_ptss_enable(struct proc* p);
void                            dtrace_ptss_exec_exit(struct proc* p);
void                            dtrace_ptss_fork(struct proc* parent, struct proc* child);
#pragma pack(4)



#pragma pack()











                                        
                                        
                                        







                                        



















enum {
	eNoteReapDeprecated __deprecated_enum_msg("This kqueue(2) EVFILT_PROC flag is deprecated") = 0x10000000
};
void eventhandler_init(void);
eventhandler_tag eventhandler_register(struct eventhandler_lists_ctxt *evthdlr_lists_ctxt,
    struct eventhandler_list *list, const char *name, void *func, struct eventhandler_entry_arg arg, int priority);
void eventhandler_deregister(struct eventhandler_list *list,
    eventhandler_tag tag);
void eventhandler_prune_list(struct eventhandler_list *list);
void eventhandler_lists_ctxt_init(struct eventhandler_lists_ctxt *evthdlr_lists_ctxt);
void eventhandler_lists_ctxt_destroy(struct eventhandler_lists_ctxt *evthdlr_lists_ctxt);
LIST_HEAD(knote_locks, knote_lock_ctx);
__options_decl(kq_state_t, uint16_t, {
	KQ_SLEEP          = 0x0002, 
	KQ_PROCWAIT       = 0x0004, 
	KQ_KEV32          = 0x0008, 
	KQ_KEV64          = 0x0010, 
	KQ_KEV_QOS        = 0x0020, 
	KQ_WORKQ          = 0x0040, 
	KQ_WORKLOOP       = 0x0080, 
	KQ_PROCESSING     = 0x0100, 
	KQ_DRAIN          = 0x0200, 
	KQ_DYNAMIC        = 0x0800, 
	KQ_R2K_ARMED      = 0x1000, 
	KQ_HAS_TURNSTILE  = 0x2000, 
});
LIST_HEAD(kqwllist, kqworkloop);
extern void kqueue_threadreq_unbind(struct proc *p, workq_threadreq_t);
extern void kqueue_threadreq_bind(struct proc *p, workq_threadreq_t req,
    thread_t thread, unsigned int flags);
extern void
kqueue_threadreq_bind_prepost(struct proc *p, workq_threadreq_t req,
    struct uthread *uth);
extern void kqueue_threadreq_bind_commit(struct proc *p, thread_t thread);
extern void kqueue_threadreq_cancel(struct proc *p, workq_threadreq_t req);
extern workq_threadreq_param_t kqueue_threadreq_workloop_param(workq_threadreq_t req);
extern struct kqueue *kqueue_alloc(struct proc *);
extern void kqueue_dealloc(struct kqueue *);
extern void kqworkq_dealloc(struct kqworkq *kqwq);
extern void knotes_dealloc(struct proc *);
extern void kqworkloops_dealloc(struct proc *);
extern int kevent_register(struct kqueue *, struct kevent_qos_s *,
    struct knote **);
extern int kqueue_scan(struct kqueue *, int flags,
    struct kevent_ctx_s *, kevent_callback_t);
extern int kqueue_stat(struct kqueue *, void *, int, proc_t);
extern void kevent_set_workq_quantum_expiry_user_tsd(proc_t p, thread_t t,
    uint64_t flags);
extern void kqworkloop_bound_thread_park_prepost(workq_threadreq_t req);
extern void kqworkloop_bound_thread_park_commit(workq_threadreq_t req,
    event_t event, thread_continue_t continuation);
extern void kqworkloop_bound_thread_terminate(workq_threadreq_t req,
    uint16_t *uu_workq_flags_orig);
__BEGIN_DECLS





__enum_decl(system_event_type, uint8_t, {
	SYSTEM_EVENT_TYPE_FIRST = 0,
	SYSTEM_EVENT_TYPE_INFO,
	SYSTEM_EVENT_TYPE_ERROR,
	SYSTEM_EVENT_TYPE_LAST
});
__enum_decl(system_event_subsystem, uint8_t, {
	SYSTEM_EVENT_SUBSYSTEM_FIRST = 0,
	SYSTEM_EVENT_SUBSYSTEM_LAUNCHD,
	SYSTEM_EVENT_SUBSYSTEM_TEST,
	SYSTEM_EVENT_SUBSYSTEM_NVRAM,
	SYSTEM_EVENT_SUBSYSTEM_PROCESS,
	SYSTEM_EVENT_SUBSYSTEM_PMRD,
	SYSTEM_EVENT_SUBSYSTEM_LAST
});
#pragma pack(4)





#pragma pack()





typedef uint64_t kqueue_id_t;
LIST_HEAD(knote_list, knote);
TAILQ_HEAD(kqtailq, knote);
__options_decl(kn_status_t, uint16_t , {
	KN_ACTIVE         = 0x001,  
	KN_QUEUED         = 0x002,  
	KN_DISABLED       = 0x004,  
	KN_DROPPING       = 0x008,  
	KN_LOCKED         = 0x010,  
	KN_POSTING        = 0x020,  
	
	KN_DEFERDELETE    = 0x080,  
	KN_MERGE_QOS      = 0x100,  
	KN_REQVANISH      = 0x200,  
	KN_VANISHED       = 0x400,  
	KN_SUPPRESSED     = 0x800,  
});
_Static_assert(!VM_PACKING_IS_BASE_RELATIVE(KNOTE_KQ_PACKED),
    "Make sure the knote pointer packing is based on arithmetic shifts");
static inline struct kqueue *
knote_get_kq(struct knote *kn)
{
	vm_offset_t ptr = VM_UNPACK_POINTER(kn->kn_kq_packed, KNOTE_KQ_PACKED);
	return __unsafe_forge_single(struct kqueue *, ptr);
}

static inline int
knote_get_seltype(struct knote *kn)
{
	switch (kn->kn_filter) {
	case EVFILT_READ:
		return FREAD;
	case EVFILT_WRITE:
		return FWRITE;
	default:
		panic("%s(%p): invalid filter %d\n",
		    __func__, kn, kn->kn_filter);
		return 0;
	}
}


typedef struct kevent_ctx_s *kevent_ctx_t;
kevent_ctx_t
kevent_get_context(thread_t thread);
SLIST_HEAD(klist, knote);
extern void     knote_init(void);
extern void     klist_init(struct klist *list);
extern void knote(struct klist *list, long hint, bool autodetach);
extern int knote_attach(struct klist *list, struct knote *kn);
extern int knote_detach(struct klist *list, struct knote *kn);
extern void knote_vanish(struct klist *list, bool make_active);
extern void knote_set_error(struct knote *kn, int error);
extern int64_t knote_low_watermark(const struct knote *kn) __pure2;
extern void knote_fill_kevent_with_sdata(struct knote *kn, struct kevent_qos_s *kev);
extern void knote_fill_kevent(struct knote *kn, struct kevent_qos_s *kev, int64_t data);
extern void *knote_kn_hook_get_raw(struct knote *kn);
extern void knote_kn_hook_set_raw(struct knote *kn, void *kn_hook);
extern void knote_fdclose(struct proc *p, int fd);
extern const struct filterops *knote_fops(struct knote *kn);
extern struct turnstile *kqueue_turnstile(struct kqueue *);
extern struct turnstile *kqueue_alloc_turnstile(struct kqueue *);
extern void kqueue_set_iotier_override(struct kqueue *kqu, uint8_t iotier_override);
extern uint8_t kqueue_get_iotier_override(struct kqueue *kqu);
int kevent_proc_copy_uptrs(void *proc, uint64_t *buf, uint32_t bufsize);
int kevent_copyout_proc_dynkqids(void *proc, user_addr_t ubuf,
    uint32_t ubufsize, int32_t *nkqueues_out);
int kevent_copyout_dynkqinfo(void *proc, kqueue_id_t kq_id, user_addr_t ubuf,
    uint32_t ubufsize, int32_t *size_out);
int kevent_copyout_dynkqextinfo(void *proc, kqueue_id_t kq_id, user_addr_t ubuf,
    uint32_t ubufsize, int32_t *nknotes_out);
extern int filt_wlattach_sync_ipc(struct knote *kn);
extern void filt_wldetach_sync_ipc(struct knote *kn);
extern int kevent_workq_internal(struct proc *p,
    user_addr_t changelist, int nchanges,
    user_addr_t eventlist, int nevents,
    user_addr_t data_out, user_size_t *data_available,
    unsigned int flags, int32_t *retval);
extern void fasttrap_tracepoint_retire(proc_t *p, fasttrap_tracepoint_t *tp);
extern int fasttrap_tracepoint_init(proc_t *, fasttrap_tracepoint_t *,
    user_addr_t, fasttrap_probe_type_t);
extern int fasttrap_tracepoint_install(proc_t *, fasttrap_tracepoint_t *);
extern int fasttrap_tracepoint_remove(proc_t *, fasttrap_tracepoint_t *);
extern uint64_t fasttrap_pid_getarg(void *, dtrace_id_t, void *, int, int);
extern uint64_t fasttrap_usdt_getarg(void *, dtrace_id_t, void *, int, int);
extern int fbt_invop(uintptr_t, uintptr_t *, uintptr_t);
extern void fbt_provide_module(void *, struct modctl *);
extern int fbt_enable (void *arg, dtrace_id_t id, void *parg);
extern bool fbt_module_excluded(struct modctl*);
extern bool fbt_excluded(const char *);
extern void fbt_blacklist_init(void);
extern void fbt_provide_probe(struct modctl *ctl, const char *modname, const char *name, machine_inst_t *instr, machine_inst_t *limit);
__BEGIN_DECLS
int file_socket(int, socket_t *);
int file_vnode(int, vnode_t *);
int file_vnode_withvid(int, vnode_t *, uint32_t *);
int file_flags(int, int *);
int file_drop(int);
int fp_getfvp(struct proc *p, int fd, struct fileproc **resultfp, struct vnode  **resultvp);
int fp_get_pipe_id(proc_t p, int fd, uint64_t *result_pipe_id);
void generate_file_permissions_guard_exception(unsigned int code_target, int64_t subcode);
__options_decl(filedesc_flags_t, uint8_t, {
	
	FD_CHROOT                     = 0x01,

	
	FD_WORKLOOP                   = 0x02,

});
extern bool
fdt_available_locked(proc_t p, int n);
extern struct fdt_iterator
fdt_next(proc_t p, int fd, bool only_settled);
extern struct fdt_iterator
fdt_prev(proc_t p, int fd, bool only_settled);
extern void
fdt_init(proc_t p);
extern void
fdt_destroy(proc_t p);
extern int
fdt_fork(struct filedesc *child_fdt, proc_t parent_p, struct vnode *uth_cdir, bool in_exec);
extern void
fdt_exec(proc_t p, struct ucred *p_cred, short posix_spawn_flags, thread_t thread, bool in_exec);
extern void
fdt_invalidate(proc_t p);
extern int      dupfdopen(proc_t p, int indx, int dfd, int mode, int error);
extern int      fdalloc(proc_t p, int want, int *result);
extern void     fdrelse(struct proc * p, int fd);
extern int      falloc_withinit(
	proc_t                  p,
	struct ucred           *p_cred,
	struct vfs_context     *ctx,
	struct fileproc       **resultfp,
	int                    *resultfd,
	fp_initfn_t             fp_init,
	void                   *initarg);
__BEGIN_DECLS

#pragma GCC visibility push(hidden)

struct proc;
__options_decl(fileproc_vflags_t, unsigned int, {
	FPV_NONE        = 0,
	FPV_DRAIN       = 0x01,
});
__options_decl(fileproc_flags_t, uint16_t, {
	FP_NONE         = 0,
	FP_CLOEXEC      = 0x01,
	FP_CLOFORK      = 0x02,
	FP_INSELECT     = 0x04,
	FP_AIOISSUED    = 0x08,
	FP_SELCONFLICT  = 0x10,  
});
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct fileproc, fileproc);
__pure2
static inline caddr_t __unsafe_indexable
ofd_to_id(const struct fileglob *fg)
{
	return (caddr_t __unsafe_indexable)~(uintptr_t)fg;
}

extern int maxfiles;
os_refgrp_decl_extern(f_refgrp);
#pragma mark files (struct fileglob)


struct fileglob *
fg_alloc_init(vfs_context_t ctx);
void
fg_ref(proc_t proc, struct fileglob *fg);
void
fg_drop_live(struct fileglob *fg);
int
fg_drop(proc_t p, struct fileglob *fg);
bool
fg_sendable(struct fileglob *fg);
void *
fg_get_data_volatile(struct fileglob *fg);
__pure2
static inline void *
fg_get_data(struct fileglob *fg)
{
	return fg_get_data_volatile(fg);
}


void
fg_set_data(struct fileglob *fg, void *fg_data);
#pragma mark file descriptor entries (struct fileproc)


__pure2
static inline void *
fp_get_data(struct fileproc *fp)
{
	return fg_get_data(fp->fp_glob);
}


static inline void *
fp_get_data_volatile(struct fileproc *fp)
{
	return fg_get_data_volatile(fp->fp_glob);
}


static inline void
fp_set_data(struct fileproc *fp, void *fg_data)
{
	fg_set_data(fp->fp_glob, fg_data);
}


extern int
fp_get_ftype(proc_t p, int fd, file_type_t ftype, int err, struct fileproc **fpp);
extern struct fileproc *
fp_get_noref_locked(proc_t p, int fd);
extern struct fileproc *
fp_get_noref_locked_with_iocount(proc_t p, int fd);
extern int
fp_close_and_unlock(proc_t p, kauth_cred_t p_cred, int fd, struct fileproc *fp, int flags);
int fo_read(struct fileproc *fp, struct uio *uio, int flags, vfs_context_t ctx);
int fo_write(struct fileproc *fp, struct uio *uio, int flags,
    vfs_context_t ctx);
int fo_ioctl(struct fileproc *fp, u_long com, caddr_t data, vfs_context_t ctx);
int fo_select(struct fileproc *fp, int which, void *wql, vfs_context_t ctx);
int fo_close(struct fileglob *fg, vfs_context_t ctx);
int fo_drain(struct fileproc *fp, vfs_context_t ctx);
int fo_kqfilter(struct fileproc *fp, struct knote *kn, struct kevent_qos_s *kev);
int fo_no_read(struct fileproc *fp, struct uio *uio, int flags, vfs_context_t ctx);
int fo_no_write(struct fileproc *fp, struct uio *uio, int flags,
    vfs_context_t ctx);
int fo_no_ioctl(struct fileproc *fp, u_long com, caddr_t data, vfs_context_t ctx);
int fo_no_select(struct fileproc *fp, int which, void *wql, vfs_context_t ctx);
int fo_no_drain(struct fileproc *fp, vfs_context_t ctx);
int fo_no_kqfilter(struct fileproc *, struct knote *, struct kevent_qos_s *kev);
int fp_tryswap(proc_t, int fd, struct fileproc *nfp);
int fp_drop(struct proc *p, int fd, struct fileproc *fp, int locked);
void fp_free(struct proc * p, int fd, struct fileproc * fp);
int fp_lookup(struct proc *p, int fd, struct fileproc **resultfp, int locked);
int fp_lookup_guarded(struct proc *p, int fd, guardid_t guard, struct fileproc **resultfp, int locked);
int fp_isguarded(struct fileproc *fp, u_int attribs);
int fp_guard_exception(proc_t p, int fd, struct fileproc *fp, u_int attribs);
int open1(vfs_context_t ctx, struct nameidata *ndp, int uflags,
    struct vnode_attr *vap, fp_initfn_t fp_init, void *initarg,
    int32_t *retval, int authfd);
int chdir_internal(proc_t p, vfs_context_t ctx, struct nameidata *ndp, int per_thread);
int kqueue_internal(struct proc *p, fp_initfn_t, void *initarg, int32_t *retval);
void procfdtbl_releasefd(struct proc * p, int fd, struct fileproc * fp);
extern struct fileproc *fileproc_alloc_init(void);
extern void fileproc_free(struct fileproc *fp);
extern void guarded_fileproc_copy_guard(struct fileproc *ofp, struct fileproc *nfp);
extern void guarded_fileproc_unguard(struct fileproc *fp);
extern void fg_vn_data_free(void *fgvndata);
extern int nameiat(struct nameidata *ndp, int dirfd);
extern void vn_offset_lock(struct fileglob *fg);
extern void vn_offset_unlock(struct fileglob *fg);
extern int falloc_guarded(struct proc *p, struct fileproc **fp, int *fd,
    vfs_context_t ctx, const guardid_t *guard, u_int attrs);
extern void fileproc_modify_vflags(struct fileproc *fp, fileproc_vflags_t vflags, boolean_t clearflags);
fileproc_vflags_t fileproc_get_vflags(struct fileproc *fp);
#pragma mark internal version of syscalls

int fileport_makefd(proc_t p, ipc_port_t port, fileproc_flags_t fp_flags, int *fd);
int dup2(proc_t p, kauth_cred_t p_cred, int from, int to, int *fd);
int close_nocancel(proc_t p, kauth_cred_t p_cred, int fd);
int fchdir(proc_t p, vfs_context_t ctx, int fd, bool per_thread);
int vfs_materialize_dir(struct vnode *vp, uint64_t op, char *file_name, size_t namelen);
int vfs_materialize_reparent(struct vnode *vp, struct vnode *tdvp);
void fsevent_unmount(struct mount *mp, vfs_context_t ctx);
void create_fsevent_from_kevent(vnode_t vp, uint32_t kevents, struct vnode_attr *vap);
int   vnode_get_fse_info_from_vap(vnode_t vp, fse_info *fse, struct vnode_attr *vap);
char *get_pathbuff(void);
void  release_pathbuff(char *path);
int  need_fsevent(int type, vnode_t vp);
int  add_fsevent(int type, vfs_context_t, ...);
int  test_fse_access_granted(vnode_t vp, unsigned long type, vfs_context_t);
void fslog_extmod_msgtracer(proc_t caller, proc_t target);
imageboot_type_t        imageboot_needed(void);
bool    imageboot_desired(void);
void    imageboot_setup(imageboot_type_t type);
int     imageboot_format_is_valid(const char *root_path);
int     imageboot_mount_image(const char *root_path, int height, imageboot_type_t type);
int     imageboot_pivot_image(const char *image_path, imageboot_type_t type, const char *mount_path,
    const char *outgoing_root_path, const bool rooted_dmg, const bool skip_signature_check);
int     imageboot_read_file_pageable(const char *path, void **bufp, size_t *bufszp);
int     imageboot_read_file(const char *path, void **bufp, size_t *bufszp, off_t *fsizep);
int     imageboot_read_file_from_offset(const char *path, off_t offset, void **bufp, size_t *bufszp);
#pragma pack(4)






#pragma pack()
















struct ucred;
int     ipcperm(struct ucred *, struct ipc_perm *, int);
__enum_decl(kdebug_emit_filter_t, uint32_t, {
	KDEMIT_DISABLE,
	KDEMIT_ALL,
	KDEMIT_TYPEFILTER,
	KDEMIT_RANGE,
	KDEMIT_EXACT,
});
static_assert(N_STORAGE_UNITS_PER_BUFFER <= 0x7ff,
    "shoudn't overflow kds_ptr.offset");
uint32_t kdbg_cpu_count(void);
void kdebug_lck_init(void);
int kdebug_storage_lock(struct kd_control *ctl);
void kdebug_storage_unlock(struct kd_control *ctl, int intrs_en);
bool kdebug_disable_wrap(struct kd_control *ctl, kdebug_emit_filter_t *old_emit,
    kdebug_live_flags_t *old_live_flags);
int create_buffers_triage(void);
int create_buffers(struct kd_control *ctl, struct kd_buffer *buf, vm_tag_t tag);
void delete_buffers(struct kd_control *ctl, struct kd_buffer *buf);
void kernel_debug_write(struct kd_control *ctl, struct kd_buffer *buf,
    struct kd_record kd_rec);
int kernel_debug_read(struct kd_control *ctl, struct kd_buffer *buf,
    user_addr_t buffer, size_t *number, vnode_t vp, vfs_context_t ctx,
    uint32_t file_version);
void commpage_update_kdebug_state(void);
__BEGIN_DECLS




#pragma mark - kernel tracepoints















#pragma mark - kernel API



int kernel_debug_string(uint32_t debugid, uint64_t *str_id, const char *str);
void kernel_debug_disable(void);
bool kdebug_using_continuous_time(void);
extern uint64_t kdebug_timestamp_from_absolute(uint64_t abstime);
extern uint64_t kdebug_timestamp_from_continuous(uint64_t conttime);
extern uint64_t kdebug_timestamp(void);
bool kdebug_debugid_enabled(uint32_t debugid);
bool kdebug_debugid_explicitly_enabled(uint32_t debugid);
uint32_t kdebug_commpage_state(void);
__options_decl(kdebug_coproc_flags_t, uint32_t, {
	
	KDCP_CONTINUOUS_TIME = 0x001,
});
int kdebug_register_coproc(const char *name, kdebug_coproc_flags_t flags,
    kd_callback_fn callback, void *context);
void kernel_debug_enter(uint32_t coreid, uint32_t debugid, uint64_t timestamp,
    uintptr_t arg1, uintptr_t arg2, uintptr_t arg3, uintptr_t arg4,
    uintptr_t threadid);
__kpi_deprecated("use kdebug_register_coproc instead")
int kernel_debug_register_callback(kd_callback_t callback);
void kernel_debug(uint32_t debugid, uintptr_t arg1, uintptr_t arg2,
    uintptr_t arg3, uintptr_t arg4, uintptr_t arg5);
void kernel_debug1(uint32_t debugid, uintptr_t arg1, uintptr_t arg2,
    uintptr_t arg3, uintptr_t arg4, uintptr_t arg5);
void kernel_debug_flags(uint32_t debugid, uintptr_t arg1, uintptr_t arg2,
    uintptr_t arg3, uintptr_t arg4, uint64_t flags);
void kernel_debug_filtered(uint32_t debugid, uintptr_t arg1, uintptr_t arg2,
    uintptr_t arg3, uintptr_t arg4);
#pragma mark - xnu API


void kdebug_startup(void);
void kernel_debug_early(uint32_t  debugid, uintptr_t arg1, uintptr_t arg2,
    uintptr_t arg3, uintptr_t arg4);
void kernel_debug_string_early(const char *message);
void kernel_debug_string_simple(uint32_t eventid, const char *str);
extern void kdebug_reset(void);
void kdbg_dump_trace_to_file(const char *, bool reenable);
int kdbg_bootstrap(bool early_trace, int mode);
void kdebug_init(unsigned int n_events, char *filterdesc,
    enum kdebug_opts opts);
void kdebug_trace_start(unsigned int n_events, const char *filterdesc,
    enum kdebug_opts opts);
uint64_t kdebug_wake(void);
void kdebug_free_early_buf(void);
void kdbg_trace_data(struct proc *proc, long *arg_pid, long *arg_uniqueid);
__options_decl(kdebug_vfs_lookup_flags_t, uint32_t, {
	KDBG_VFSLKUP_LOOKUP = 0x01,
	KDBG_VFSLKUP_NOPROCFILT = 0x02,
});
void kdebug_vfs_lookup(const char *path_words, size_t path_len, void *vnp,
    kdebug_vfs_lookup_flags_t flags);
void ktriage_extract(uint64_t thread_id, void *buf, uint32_t bufsz);
int ktriage_unregister_subsystem_strings(uint8_t subsystem);
void ktriage_record(uint64_t thread_id, uint64_t debugid, uintptr_t arg);
void kdebug_lookup_gen_events(long *path_words, int path_len, void *vnp,
    bool lookup);
void delete_buffers_triage(void);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct buf, buf);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct file, file);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct mount, mount);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct vnode, vnode);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct proc, proc);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct proc_ident, proc_ident);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct uio, uio);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct vfs_context, vfs_context);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct vfstable, vfstable);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __ifnet, ifnet);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __mbuf, mbuf);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __pkthdr, pkthdr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __socket, socket);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __sockopt, sockopt);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __ifaddr, ifaddr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __ifmultiaddr, ifmultiaddr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __ifnet_filter, ifnet_filter);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __rtentry, rtentry);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __if_clone, if_clone);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct __bufattr, bufattr);
errno_t
ctl_register(struct kern_ctl_reg *userkctl, kern_ctl_ref *kctlref);
errno_t
ctl_deregister(kern_ctl_ref kctlref);
errno_t
    ctl_enqueuedata(kern_ctl_ref kctlref, u_int32_t unit, void *__sized_by(len) data,
    size_t len, u_int32_t flags);
errno_t
ctl_enqueuembuf(kern_ctl_ref kctlref, u_int32_t unit, mbuf_t m, u_int32_t flags);
errno_t
ctl_enqueuembuf_list(kern_ctl_ref kctlref, u_int32_t unit, mbuf_t m_list,
    u_int32_t flags, mbuf_t *m_remain);
errno_t
ctl_getenqueuepacketcount(kern_ctl_ref kctlref, u_int32_t unit, u_int32_t *pcnt);
errno_t
ctl_getenqueuespace(kern_ctl_ref kctlref, u_int32_t unit, size_t *space);
errno_t
ctl_getenqueuereadable(kern_ctl_ref kctlref, u_int32_t unit, u_int32_t *difference);
void kctl_fill_socketinfo(struct socket *, struct socket_info *);
u_int32_t ctl_id_by_name(const char *);
errno_t ctl_name_by_id(u_int32_t, char *__counted_by(maxsize), size_t maxsize);
#pragma pack(4)

#pragma pack()






















errno_t kev_vendor_code_find(const char *vendor_string, u_int32_t *vendor_code);
errno_t kev_msg_post(struct kev_msg *event_msg);
int     kev_post_msg(struct kev_msg *event);
int     kev_post_msg_nowait(struct kev_msg *event);
LIST_HEAD(kern_event_head, kern_event_pcb);
extern void memorystatus_post_snapshot(void);
extern void memorystatus_init(void);
extern void memorystatus_init_at_boot_snapshot(void);
extern int memorystatus_add(proc_t p, boolean_t locked);
__options_closed_decl(memstat_priority_options_t, uint8_t, {
	MEMSTAT_PRIORITY_OPTIONS_NONE   = 0x00,
	
	MEMSTAT_PRIORITY_IS_ASSERTION   = 0x01,
	MEMSTAT_PRIORITY_IS_EFFECTIVE   = 0x02,
	
	MEMSTAT_PRIORITY_INSERT_HEAD    = 0x04,
	
	MEMSTAT_PRIORITY_NO_AGING = 0x08,
});
extern int memorystatus_set_priority(proc_t p, int priority, uint64_t user_data,
    memstat_priority_options_t options);
__options_decl(memlimit_options_t, uint8_t, {
	MEMLIMIT_OPTIONS_NONE = 0x00,
	MEMLIMIT_ACTIVE_FATAL = 0x01,
	MEMLIMIT_INACTIVE_FATAL = 0x02,
});
extern int memorystatus_set_memlimits(proc_t p, int32_t active_limit, int32_t inactive_limit, memlimit_options_t options);
extern int memorystatus_remove(proc_t p);
int memorystatus_update_inactive_jetsam_priority_band(pid_t pid, uint32_t opflags, int priority, boolean_t effective_now);
int memorystatus_relaunch_flags_update(proc_t p, int relaunch_flags);
extern int memorystatus_dirty_track(proc_t p, uint32_t pcontrol);
extern int memorystatus_dirty_set(proc_t p, boolean_t self, uint32_t pcontrol);
extern int memorystatus_dirty_get(proc_t p, boolean_t locked);
extern int memorystatus_dirty_clear(proc_t p, uint32_t pcontrol);
extern int memorystatus_on_terminate(proc_t p);
extern void memorystatus_on_suspend(proc_t p);
extern void memorystatus_on_resume(proc_t p);
extern void memorystatus_on_inactivity(proc_t p);
extern void memorystatus_on_pageout_scan_end(void);
void memorystatus_kevent_init(lck_grp_t *grp, lck_attr_t *attr);
int memorystatus_knote_register(struct knote *kn);
void memorystatus_knote_unregister(struct knote *kn);
int memorystatus_get_pressure_status_kdp(void);
int  memorystatus_get_proccnt_upto_priority(int32_t max_bucket_index);
proc_t memorystatus_get_first_proc_locked(unsigned int *bucket_index, boolean_t search);
proc_t memorystatus_get_next_proc_locked(unsigned int *bucket_index, proc_t p, boolean_t search);
void memorystatus_get_task_page_counts(task_t task, uint32_t *footprint, uint32_t *max_footprint_lifetime, uint32_t *purgeable_pages);
bool memorystatus_task_has_increased_memory_limit_entitlement(task_t task);
bool memorystatus_task_has_increased_debugging_memory_limit_entitlement(task_t task);
bool memorystatus_task_has_legacy_footprint_entitlement(task_t task);
bool memorystatus_task_has_ios13extended_footprint_limit(task_t task);
void memorystatus_freeze_init(void);
extern int  memorystatus_freeze_process_sync(proc_t p);
#pragma once


__BEGIN_DECLS




extern uint32_t memorystatus_get_critical_page_shortage_threshold(void);
extern uint32_t memorystatus_get_idle_exit_page_shortage_threshold(void);
extern uint32_t memorystatus_get_soft_memlimit_page_shortage_threshold(void);
extern uint32_t memorystatus_get_reaper_page_shortage_threshold(void);
extern uint32_t memorystatus_get_available_page_count(void);
extern void memorystatus_update_available_page_count(uint32_t available_pages);
extern void memorystatus_fast_jetsam_override(bool enable_override);
extern bool memorystatus_kill_on_zone_map_exhaustion(pid_t pid);
extern bool memorystatus_kill_on_VM_compressor_space_shortage(bool async);
extern void memorystatus_kill_on_vps_starvation(void);
extern bool memorystatus_kill_on_vnode_exhaustion(void);
extern void memorystatus_thread_wake(void);
extern void memorystatus_respond_to_compressor_exhaustion(void);
extern void memorystatus_respond_to_swap_exhaustion(void);
__BEGIN_DECLS






void init_system_override(void);
__BEGIN_DECLS


extern void *mbuf_data(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void *mbuf_datastart(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_setdata(mbuf_t mbuf, void *data, size_t len)
__NKE_API_DEPRECATED;
extern errno_t mbuf_align_32(mbuf_t mbuf, size_t len)
__NKE_API_DEPRECATED;
extern addr64_t mbuf_data_to_physical(void *ptr)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get(mbuf_how_t how, mbuf_type_t type, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_gethdr(mbuf_how_t how, mbuf_type_t type, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_attachcluster(mbuf_how_t how, mbuf_type_t type,
    mbuf_t *mbuf, caddr_t extbuf __sized_by_or_null(extsize), void (*extfree)(caddr_t, u_int, caddr_t),
    size_t extsize, caddr_t extarg)
__NKE_API_DEPRECATED;
extern errno_t mbuf_alloccluster(mbuf_how_t how, size_t *size, char * __sized_by_or_null(*size) * addr)
__NKE_API_DEPRECATED;
extern void mbuf_freecluster(caddr_t addr, size_t size)
__NKE_API_DEPRECATED;
extern errno_t mbuf_ring_cluster_alloc(mbuf_how_t how, mbuf_type_t type,
    mbuf_t *mbuf, void (*extfree)(caddr_t, u_int, caddr_t), size_t *size);
extern int mbuf_ring_cluster_is_active(mbuf_t mbuf);
extern errno_t mbuf_ring_cluster_activate(mbuf_t mbuf);
extern errno_t mbuf_cluster_set_prop(mbuf_t mbuf, u_int32_t oldprop,
    u_int32_t newprop);
extern errno_t mbuf_cluster_get_prop(mbuf_t mbuf, u_int32_t *prop);
extern errno_t mbuf_getcluster(mbuf_how_t how, mbuf_type_t type, size_t size,
    mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_mclget(mbuf_how_t how, mbuf_type_t type, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_allocpacket(mbuf_how_t how, size_t packetlen,
    unsigned int * maxchunks, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_allocpacket_list(unsigned int numpkts, mbuf_how_t how,
    size_t packetlen, unsigned int * maxchunks, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_getpacket(mbuf_how_t how, mbuf_t *mbuf)
__NKE_API_DEPRECATED;
extern mbuf_t mbuf_free(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_freem(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern int mbuf_freem_list(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern size_t mbuf_leadingspace(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern size_t mbuf_trailingspace(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_copym(const mbuf_t src, size_t offset, size_t len,
    mbuf_how_t how, mbuf_t *new_mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_dup(const mbuf_t src, mbuf_how_t how, mbuf_t *new_mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_prepend(mbuf_t *mbuf, size_t len, mbuf_how_t how)
__NKE_API_DEPRECATED;
extern errno_t mbuf_split(mbuf_t src, size_t offset, mbuf_how_t how,
    mbuf_t *new_mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_pullup(mbuf_t *mbuf, size_t len)
__NKE_API_DEPRECATED;
extern errno_t mbuf_pulldown(mbuf_t src, size_t *offset, size_t length,
    mbuf_t *location)
__NKE_API_DEPRECATED;
extern void mbuf_adj(mbuf_t mbuf, int len)
__NKE_API_DEPRECATED;
extern errno_t mbuf_adjustlen(mbuf_t mbuf, int amount)
__NKE_API_DEPRECATED;
extern mbuf_t mbuf_concatenate(mbuf_t dst, mbuf_t src)
__NKE_API_DEPRECATED;
extern errno_t mbuf_copydata(const mbuf_t mbuf, size_t offset, size_t length,
    void *out_data __sized_by_or_null(length))
__NKE_API_DEPRECATED;
extern errno_t mbuf_copyback(mbuf_t mbuf, size_t offset, size_t length,
    const void *data __sized_by_or_null(length), mbuf_how_t how)
__NKE_API_DEPRECATED;
extern int mbuf_mclhasreference(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern mbuf_t mbuf_next(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_setnext(mbuf_t mbuf, mbuf_t next)
__NKE_API_DEPRECATED;
extern mbuf_t mbuf_nextpkt(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_setnextpkt(mbuf_t mbuf, mbuf_t nextpkt)
__NKE_API_DEPRECATED;
extern size_t mbuf_len(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_setlen(mbuf_t mbuf, size_t len)
__NKE_API_DEPRECATED;
extern size_t mbuf_maxlen(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern mbuf_type_t mbuf_type(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_settype(mbuf_t mbuf, mbuf_type_t new_type)
__NKE_API_DEPRECATED;
extern mbuf_flags_t mbuf_flags(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_setflags(mbuf_t mbuf, mbuf_flags_t flags)
__NKE_API_DEPRECATED;
extern errno_t mbuf_setflags_mask(mbuf_t mbuf, mbuf_flags_t flags,
    mbuf_flags_t mask)
__NKE_API_DEPRECATED;
extern errno_t mbuf_copy_pkthdr(mbuf_t dest, const mbuf_t src)
__NKE_API_DEPRECATED;
extern size_t mbuf_pkthdr_len(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_pkthdr_setlen(mbuf_t mbuf, size_t len)
__NKE_API_DEPRECATED;
extern size_t mbuf_pkthdr_maxlen(const mbuf_t mbuf);
extern void mbuf_pkthdr_adjustlen(mbuf_t mbuf, int amount)
__NKE_API_DEPRECATED;
extern ifnet_t mbuf_pkthdr_rcvif(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_pkthdr_setrcvif(mbuf_t mbuf, ifnet_t ifp)
__NKE_API_DEPRECATED;
extern void *mbuf_pkthdr_header(const mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_pkthdr_setheader(mbuf_t mbuf, void *header)
__NKE_API_DEPRECATED;
extern void mbuf_inbound_modified(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern void mbuf_outbound_finalize(mbuf_t mbuf, u_int32_t protocol_family,
    size_t protocol_offset)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_vlan_tag(mbuf_t mbuf, u_int16_t vlan)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get_vlan_tag(mbuf_t mbuf, u_int16_t *vlan)
__NKE_API_DEPRECATED;
extern errno_t mbuf_clear_vlan_tag(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_csum_requested(mbuf_t mbuf,
    mbuf_csum_request_flags_t request, u_int32_t value);
extern errno_t mbuf_get_csum_requested(mbuf_t mbuf,
    mbuf_csum_request_flags_t *request, u_int32_t *value)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get_tso_requested(mbuf_t mbuf,
    mbuf_tso_request_flags_t *request, u_int32_t *mss)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get_gso_info(mbuf_t mbuf, mbuf_gso_type_t *type,
    uint16_t *seg_size, uint16_t *hdr_len)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_gso_info(mbuf_t mbuf,
    mbuf_gso_type_t type, uint16_t seg_size, uint16_t hdr_len)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get_lro_info(mbuf_t mbuf, uint8_t *seg_cnt,
    uint8_t *dup_ack_cnt)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_lro_info(mbuf_t mbuf, uint8_t seg_cnt,
    uint8_t dup_ack_cnt)
__NKE_API_DEPRECATED;
extern errno_t mbuf_clear_csum_requested(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_csum_performed(mbuf_t mbuf,
    mbuf_csum_performed_flags_t flags, u_int32_t value)
__NKE_API_DEPRECATED;
extern errno_t mbuf_get_csum_performed(mbuf_t mbuf,
    mbuf_csum_performed_flags_t *flags, u_int32_t *value);
extern u_int32_t mbuf_get_mlen(void)
__NKE_API_DEPRECATED;
extern u_int32_t mbuf_get_mhlen(void)
__NKE_API_DEPRECATED;
extern u_int32_t mbuf_get_minclsize(void)
__NKE_API_DEPRECATED;
extern u_int32_t mbuf_get_msize(void);
extern errno_t mbuf_clear_csum_performed(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_inet_cksum(mbuf_t mbuf, int protocol, u_int32_t offset,
    u_int32_t length, u_int16_t *csum)
__NKE_API_DEPRECATED;
extern errno_t mbuf_inet6_cksum(mbuf_t mbuf, int protocol, u_int32_t offset,
    u_int32_t length, u_int16_t *csum)
__NKE_API_DEPRECATED;
extern errno_t mbuf_tag_id_find(const char *module_string,
    mbuf_tag_id_t *module_id)
__NKE_API_DEPRECATED;
extern errno_t mbuf_tag_allocate(mbuf_t mbuf, mbuf_tag_id_t module_id,
    mbuf_tag_type_t type, size_t length, mbuf_how_t how, void **data_p)
__NKE_API_DEPRECATED;
extern errno_t mbuf_tag_find(mbuf_t mbuf, mbuf_tag_id_t module_id,
    mbuf_tag_type_t type, size_t *length, void **data_p)
__NKE_API_DEPRECATED;
extern void mbuf_tag_free(mbuf_t mbuf, mbuf_tag_id_t module_id,
    mbuf_tag_type_t type)
__NKE_API_DEPRECATED;
extern errno_t mbuf_add_drvaux(mbuf_t mbuf, mbuf_how_t how,
    u_int32_t family, u_int32_t subfamily, size_t length, void **data_p);
extern errno_t mbuf_find_drvaux(mbuf_t mbuf, u_int32_t *family_p,
    u_int32_t *subfamily_p, u_int32_t *length_p, void **data_p);
extern void mbuf_del_drvaux(mbuf_t mbuf);
extern void mbuf_stats(struct mbuf_stat *stats)
__NKE_API_DEPRECATED;
extern mbuf_traffic_class_t mbuf_get_traffic_class(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern errno_t mbuf_set_traffic_class(mbuf_t mbuf, mbuf_traffic_class_t tc)
__NKE_API_DEPRECATED;
extern int mbuf_is_traffic_class_privileged(mbuf_t mbuf)
__NKE_API_DEPRECATED;
extern u_int32_t mbuf_get_traffic_class_max_count(void);
extern errno_t mbuf_get_traffic_class_index(mbuf_traffic_class_t tc,
    u_int32_t *index);
extern u_int32_t mbuf_get_service_class_max_count(void);
extern errno_t mbuf_get_service_class_index(mbuf_svc_class_t sc,
    u_int32_t *index);
extern mbuf_svc_class_t mbuf_get_service_class(mbuf_t mbuf);
extern errno_t mbuf_set_service_class(mbuf_t mbuf, mbuf_svc_class_t sc);
extern int mbuf_is_service_class_privileged(mbuf_t mbuf);
extern errno_t mbuf_pkthdr_aux_flags(mbuf_t mbuf,
    mbuf_pkthdr_aux_flags_t *paux_flags);
extern errno_t mbuf_get_driver_scratch(mbuf_t m, u_int8_t **area,
    size_t *area_ln);
extern errno_t mbuf_get_unsent_data_bytes(const mbuf_t m,
    u_int32_t *unsent_data);
extern errno_t mbuf_pkt_new_flow(const mbuf_t m, u_int32_t *retval);
extern errno_t mbuf_last_pkt(const mbuf_t m, u_int32_t *retval);
extern size_t mbuf_pkt_list_len(const mbuf_t mbuf);
extern size_t mbuf_pkt_list_maxlen(const mbuf_t mbuf);
extern errno_t mbuf_get_timestamp(mbuf_t mbuf, u_int64_t *ts, boolean_t *valid);
extern errno_t mbuf_set_timestamp(mbuf_t mbuf, u_int64_t ts, boolean_t valid);
extern errno_t mbuf_register_tx_compl_callback(
	mbuf_tx_compl_func callback);
extern errno_t mbuf_unregister_tx_compl_callback(
	mbuf_tx_compl_func callback);
extern errno_t mbuf_get_timestamp_requested(mbuf_t mbuf, boolean_t *requested);
extern errno_t mbuf_set_timestamp_requested(mbuf_t mbuf,
    uintptr_t *pktid, mbuf_tx_compl_func callback);
extern errno_t mbuf_get_status(mbuf_t mbuf, kern_return_t *status);
extern errno_t mbuf_set_status(mbuf_t mbuf, kern_return_t status);
extern errno_t mbuf_get_tx_compl_data(mbuf_t m, uintptr_t *arg,
    uintptr_t *data);
extern errno_t mbuf_set_tx_compl_data(mbuf_t m, uintptr_t arg,
    uintptr_t data);
extern errno_t mbuf_get_flowid(mbuf_t mbuf, u_int16_t *flowid);
extern errno_t mbuf_set_flowid(mbuf_t mbuf, u_int16_t flowid);
extern errno_t mbuf_get_keepalive_flag(mbuf_t mbuf, boolean_t *is_keepalive);
extern errno_t mbuf_set_keepalive_flag(mbuf_t mbuf, boolean_t is_keepalive);
extern errno_t mbuf_get_wake_packet_flag(mbuf_t mbuf, boolean_t *is_wake_packet);
extern errno_t mbuf_set_wake_packet_flag(mbuf_t mbuf, boolean_t is_wake_packet);
__BEGIN_DECLS



void kx_qsort(void* array, size_t nm, size_t member_size, int (*)(const void *, const void *));
extern errno_t sock_accept_internal(socket_t so, struct sockaddr *__sized_by(fromlen) from, int fromlen,
    int flags, sock_upcall callback, void *cookie, socket_t *new_so);
extern errno_t sock_bind(socket_t so, const struct sockaddr *to)
__NKE_API_DEPRECATED;
extern errno_t sock_connect(socket_t so, const struct sockaddr *to, int flags)
__NKE_API_DEPRECATED;
extern errno_t sock_connectwait(socket_t so, const struct timeval *tv);
extern errno_t sock_getpeername(socket_t so, struct sockaddr *__sized_by(peernamelen) peername,
    int peernamelen)
__NKE_API_DEPRECATED;
extern errno_t sock_getsockname(socket_t so, struct sockaddr *__sized_by(socknamelen) sockname,
    int socknamelen)
__NKE_API_DEPRECATED;
extern errno_t sock_getsockopt(socket_t so, int level, int optname,
    void *optval, int *optlen)
__NKE_API_DEPRECATED;
extern errno_t sock_ioctl(socket_t so, unsigned long request, void *__sized_by(IOCPARM_LEN(request)) argp)
__NKE_API_DEPRECATED;
extern errno_t sock_setsockopt(socket_t so, int level, int optname,
    const void *optval, int optlen)
__NKE_API_DEPRECATED;
extern errno_t sock_settclassopt(socket_t so, const void* optval, size_t optlen);
extern errno_t sock_gettclassopt(socket_t so, void* optval, size_t* optlen);
extern void socket_set_traffic_mgt_flags_locked(socket_t so, u_int8_t flags);
extern void socket_clear_traffic_mgt_flags_locked(socket_t so, u_int8_t flags);
extern void socket_set_traffic_mgt_flags(socket_t so, u_int8_t flags);
extern void socket_clear_traffic_mgt_flags(socket_t so, u_int8_t flags);
extern errno_t socket_defunct(struct proc *, socket_t so, int);
extern errno_t sock_receive_internal(socket_t, struct msghdr *, mbuf_t *,
    int, size_t *);
extern errno_t sock_listen(socket_t so, int backlog)
__NKE_API_DEPRECATED;
extern errno_t sock_receive(socket_t so, struct msghdr *msg, int flags,
    size_t *recvdlen)
__NKE_API_DEPRECATED;
extern errno_t sock_receivembuf(socket_t so, struct msghdr *msg, mbuf_t *data,
    int flags, size_t *recvlen)
__NKE_API_DEPRECATED;
extern errno_t sock_send(socket_t so, const struct msghdr *msg, int flags,
    size_t *sentlen)
__NKE_API_DEPRECATED;
extern errno_t sock_sendmbuf(socket_t so, const struct msghdr *msg, mbuf_t data,
    int flags, size_t *sentlen)
__NKE_API_DEPRECATED;
extern errno_t sock_sendmbuf_can_wait(socket_t so, const struct msghdr *msg, mbuf_t data,
    int flags, size_t *sentlen);
extern errno_t sock_shutdown(socket_t so, int how)
__NKE_API_DEPRECATED;
extern errno_t sock_socket_internal(int domain, int type, int protocol,
    sock_upcall callback, void *cookie, socket_t *new_so);
extern void sock_close(socket_t so)
__NKE_API_DEPRECATED;
extern void sock_retain(socket_t so)
__NKE_API_DEPRECATED;
extern void sock_release(socket_t so)
__NKE_API_DEPRECATED;
extern errno_t sock_setpriv(socket_t so, int on)
__NKE_API_DEPRECATED;
extern int sock_isconnected(socket_t so)
__NKE_API_DEPRECATED;
extern int sock_isnonblocking(socket_t so)
__NKE_API_DEPRECATED;
extern errno_t sock_gettype(socket_t so, int *domain, int *type, int *protocol)
__NKE_API_DEPRECATED;
extern errno_t sock_nointerrupt(socket_t so, int on);
extern socket_t sock_getlistener(socket_t so);
extern errno_t sock_getaddr(socket_t so, struct sockaddr **psockname,
    int peername);
extern void sock_freeaddr(struct sockaddr *sockname);
extern errno_t sock_setupcall(socket_t sock, sock_upcall callback,
    void *context);
extern errno_t sock_setupcalls(socket_t sock, sock_upcall read_callback,
    void *read_context, sock_upcall write_callback, void *write_context);
extern void sock_setupcalls_locked(socket_t sock,
    sock_upcall rcallback, void *rcontext,
    sock_upcall wcallback, void *wcontext, int locked);
extern errno_t sock_catchevents(socket_t sock, sock_evupcall event_callback,
    void *event_context, uint32_t event_mask);
extern void sock_catchevents_locked(socket_t sock, sock_evupcall ecallback,
    void *econtext, uint32_t emask);
extern int sock_iskernel(socket_t);
extern errno_t sflt_register_internal(const struct sflt_filter *filter,
    int domain, int type, int protocol);
extern errno_t sflt_unregister(sflt_handle handle)
__NKE_API_DEPRECATED;
extern errno_t sflt_attach(socket_t socket, sflt_handle handle)
__NKE_API_DEPRECATED;
extern errno_t sflt_detach(socket_t socket, sflt_handle handle)
__NKE_API_DEPRECATED;
extern errno_t sock_inject_data_in(socket_t so, const struct sockaddr *from,
    mbuf_t data, mbuf_t control, sflt_data_flag_t flags)
__NKE_API_DEPRECATED;
extern errno_t sock_inject_data_out(socket_t so, const struct sockaddr *to,
    mbuf_t data, mbuf_t control, sflt_data_flag_t flags)
__NKE_API_DEPRECATED;
extern sockopt_dir sockopt_direction(sockopt_t sopt)
__NKE_API_DEPRECATED;
extern int sockopt_level(sockopt_t sopt)
__NKE_API_DEPRECATED;
extern int sockopt_name(sockopt_t sopt)
__NKE_API_DEPRECATED;
extern size_t sockopt_valsize(sockopt_t sopt)
__NKE_API_DEPRECATED;
extern errno_t sockopt_copyin(sockopt_t sopt, void *__sized_by(length) data, size_t length)
__NKE_API_DEPRECATED;
extern errno_t sockopt_copyout(sockopt_t sopt, void *__sized_by(length) data, size_t length)
__NKE_API_DEPRECATED;
__enum_decl(ktrace_state_t, unsigned int, {
	
	KTRACE_STATE_OFF = 0,
	
	KTRACE_STATE_FG,
	
	KTRACE_STATE_BG,
});
void ktrace_lock(void);
void ktrace_unlock(void);
void ktrace_assert_lock_held(void);
void ktrace_start_single_threaded(void);
void ktrace_end_single_threaded(void);
int ktrace_configure(uint32_t config_mask);
void ktrace_reset(uint32_t reset_mask);
int ktrace_read_check(void);
void ktrace_kernel_configure(uint32_t config_mask);
void ktrace_disable(ktrace_state_t state_to_match);
int ktrace_get_owning_pid(void);
bool ktrace_background_active(void);
int ktrace_set_owning_pid(int pid);
void lockdown_mode_init(void);
int get_lockdown_mode_state(void);
void enable_lockdown_mode(void);
void disable_lockdown_mode(void);
TAILQ_HEAD(locklist, lockf);
__BEGIN_DECLS

void    lf_init(void);
int     lf_advlock(struct vnop_advlock_args *);
int     lf_assert(struct vnop_advlock_args *, void **);
void    lf_commit(void *, int);
void    lf_abort_advlocks(vnode_t);
int log_data_as_kernel(unsigned int tag, unsigned int flags, void *buffer, unsigned int size);
__BEGIN_DECLS






ZONE_VIEW_DECLARE(ZV_NAMEI);
__private_extern__ void mcache_init(void);
__private_extern__ unsigned int mcache_getflags(void);
__private_extern__ unsigned int mcache_cache_line_size(void);
__private_extern__ mcache_t *mcache_create(const char *, size_t,
    size_t, u_int32_t, int);
__private_extern__ void *mcache_alloc(mcache_t *, int);
__private_extern__ void mcache_free(mcache_t *, void *);
__private_extern__ mcache_t *mcache_create_ext(const char *, size_t,
    mcache_allocfn_t, mcache_freefn_t, mcache_auditfn_t, mcache_logfn_t,
    mcache_notifyfn_t, void *__unsafe_indexable, u_int32_t, int);
__private_extern__ void mcache_destroy(mcache_t *);
__private_extern__ unsigned int mcache_alloc_ext(mcache_t *, mcache_obj_t **,
    unsigned int, int);
__private_extern__ void mcache_free_ext(mcache_t *, mcache_obj_t *);
__private_extern__ void mcache_reap(void);
__private_extern__ void mcache_reap_now(mcache_t *, boolean_t);
__private_extern__ boolean_t mcache_purge_cache(mcache_t *, boolean_t);
__private_extern__ void mcache_waiter_inc(mcache_t *);
__private_extern__ void mcache_waiter_dec(mcache_t *);
__private_extern__ boolean_t mcache_bkt_isempty(mcache_t *);
__private_extern__ void mcache_buffer_log(mcache_audit_t *, void *, mcache_t *,
    struct timeval *);
__private_extern__ void mcache_set_pattern(u_int64_t, void *, size_t);
__private_extern__ void *mcache_verify_pattern(u_int64_t, void *, size_t);
__private_extern__ void mcache_audit_free_verify(mcache_audit_t *,
    void *, size_t, size_t);
__private_extern__ void mcache_audit_free_verify_set(mcache_audit_t *,
    void *, size_t, size_t);
__private_extern__ char *mcache_dump_mca(char buf[DUMP_MCA_BUF_SIZE], mcache_audit_t *);
#pragma pack(4)



#pragma pack()







#pragma pack(4)



#pragma pack()











































union union_vfsidctl { 
	struct user32_vfsidctl vc32;
	struct user_vfsidctl vc64;
};
__BEGIN_DECLS
extern int VFS_MOUNT(mount_t, vnode_t, user_addr_t, vfs_context_t);
extern int VFS_START(mount_t, int, vfs_context_t);
extern int VFS_UNMOUNT(mount_t, int, vfs_context_t);
extern int VFS_ROOT(mount_t, vnode_t *, vfs_context_t);
extern int VFS_QUOTACTL(mount_t, int, uid_t, caddr_t, vfs_context_t);
extern int VFS_GETATTR(mount_t, struct vfs_attr *, vfs_context_t);
extern int VFS_SETATTR(mount_t, struct vfs_attr *, vfs_context_t);
extern int VFS_SYNC(mount_t, int, vfs_context_t);
extern int VFS_VGET(mount_t, ino64_t, vnode_t *, vfs_context_t);
extern int VFS_FHTOVP(mount_t, int, unsigned char *, vnode_t *, vfs_context_t);
extern int VFS_VPTOFH(vnode_t, int *, unsigned char *, vfs_context_t);
extern int VFS_IOCTL(mount_t mp, u_long command, caddr_t data,
    int flags, vfs_context_t context);
extern int VFS_VGET_SNAPDIR(mount_t, vnode_t *, vfs_context_t);
int vfs_fsadd(struct vfs_fsentry *vfe, vfstable_t *handle);
int vfs_fsremove(vfstable_t handle);
int     vfs_iterate(int flags, int (*callout)(struct mount *, void *), void *arg);
int     vfs_init_io_attributes(vnode_t devvp, mount_t mp);
uint64_t vfs_flags(mount_t mp);
void    vfs_setflags(mount_t mp, uint64_t flags);
void    vfs_clearflags(mount_t mp, uint64_t flags);
int     vfs_issynchronous(mount_t mp);
int     vfs_iswriteupgrade(mount_t mp);
int     vfs_isupdate(mount_t mp);
int     vfs_isreload(mount_t mp);
int     vfs_isforce(mount_t mp);
int     vfs_isunmount(mount_t mp);
int     vfs_isrdonly(mount_t mp);
int     vfs_isrdwr(mount_t mp);
int     vfs_authopaque(mount_t mp);
int     vfs_authopaqueaccess(mount_t mp);
void    vfs_setauthopaque(mount_t mp);
void    vfs_setauthopaqueaccess(mount_t mp);
void    vfs_clearauthopaque(mount_t mp);
void    vfs_clearauthopaqueaccess(mount_t mp);
void    vfs_setextendedsecurity(mount_t mp);
void    vfs_clearextendedsecurity(mount_t mp);
void    vfs_setnoswap(mount_t mp);
void    vfs_clearnoswap(mount_t mp);
void    vfs_setlocklocal(mount_t mp);
int     vfs_authcache_ttl(mount_t mp);
void    vfs_setauthcache_ttl(mount_t mp, int ttl);
void    vfs_clearauthcache_ttl(mount_t mp);
uint32_t vfs_maxsymlen(mount_t mp);
void    vfs_setmaxsymlen(mount_t mp, uint32_t symlen);
void *  vfs_fsprivate(mount_t mp);
void    vfs_setfsprivate(mount_t mp, void *mntdata);
int     vfs_update_vfsstat(mount_t mp, vfs_context_t ctx, int eventtype);
int     vfs_typenum(mount_t mp);
void    vfs_name(mount_t mp, char *buffer);
int     vfs_devblocksize(mount_t mp);
void    vfs_ioattr(mount_t mp, struct vfsioattr *ioattrp);
void    vfs_setioattr(mount_t mp, struct vfsioattr *ioattrp);
int     vfs_64bitready(mount_t mp);
int     vfs_busy(mount_t mp, int flags);
void    vfs_unbusy(mount_t mp);
void    vfs_getnewfsid(struct mount *mp);
mount_t vfs_getvfs(fsid_t *fsid);
mount_t vfs_getvfs_with_vfsops(fsid_t *fsid, const struct vfsops *ops);
int     vfs_mountedon(struct vnode *vp);
int     vfs_unmountbyfsid(fsid_t *fsid, int flags, vfs_context_t ctx);
void    vfs_event_signal(fsid_t *fsid, u_int32_t event, intptr_t data);
void    vfs_event_init(void);
void vfs_set_root_unmounted_cleanly(void);
int     vfs_getbyid(fsid_t *fsid, ino64_t ino, vnode_t *vpp, vfs_context_t ctx);
int     vfs_getattr(mount_t mp, struct vfs_attr *vfa, vfs_context_t ctx);
int     vfs_setattr(mount_t mp, struct vfs_attr *vfa, vfs_context_t ctx);
int     vfs_extendedsecurity(mount_t);
mount_t vfs_getvfs_by_mntonname(char *);
vnode_t vfs_vnodecovered(mount_t mp);
vnode_t vfs_vnodecovered_noblock(mount_t mp);
int vfs_setdevvp(mount_t mp, vnode_t vp);
vnode_t vfs_devvp(mount_t mp);
int vfs_nativexattrs(mount_t mp);
void *  vfs_mntlabel(mount_t mp);
void    vfs_setcompoundopen(mount_t mp);
void    vfs_setfskit(mount_t mp);
uint32_t vfs_getextflags(mount_t mp);
char *  vfs_getfstypenameref_locked(mount_t mp, size_t *lenp);
void    vfs_getfstypename(mount_t mp, char *buf, size_t buflen);
void    vfs_setfstypename_locked(mount_t mp, const char *name);
void    vfs_setfstypename(mount_t mp, const char *name);
uint64_t vfs_throttle_mask(mount_t mp);
int vfs_isswapmount(mount_t mp);
int     vfs_context_dataless_materialization_is_prevented(vfs_context_t);
boolean_t vfs_context_is_dataless_manipulator(vfs_context_t);
boolean_t vfs_context_can_resolve_triggers(vfs_context_t);
boolean_t vfs_context_can_break_leases(vfs_context_t);
boolean_t vfs_context_skip_mtime_update(vfs_context_t ctx);
void    vfs_setmntsystem(mount_t mp);
void    vfs_setmntsystemdata(mount_t mp);
void    vfs_setmntswap(mount_t mp);
boolean_t vfs_is_basesystem(mount_t mp);
boolean_t vfs_iskernelmount(mount_t mp);
boolean_t vfs_shutdown_in_progress(void);
boolean_t vfs_shutdown_finished(void);
void    vfs_update_last_completion_time(void);
uint64_t vfs_last_completion_time(void);
OS_ENUM(bsd_bootfail_mode, uint32_t,
    BSD_BOOTFAIL_SEAL_BROKEN = 1,
    BSD_BOOTFAIL_MEDIA_MISSING = 2);
boolean_t bsd_boot_to_recovery(bsd_bootfail_mode_t mode, uuid_t volume_uuid, boolean_t reboot);
int     vfs_addtrigger(mount_t mp, const char *relpath, struct vnode_trigger_info *vtip, vfs_context_t ctx);
int     vfs_settriggercallback(fsid_t *fsid, vfs_trigger_callback_t vtc, void *data, uint32_t flags, vfs_context_t ctx);
void mount_set_noreaddirext(mount_t);
void vfs_get_statfs64(struct mount *mp, struct statfs64 *sfs);
uint64_t vfs_mount_id(mount_t mp);
int vfs_mount_at_path(const char *fstype, const char *path,
    vnode_t pvp, vnode_t vp, void *data, size_t datalen, int mnt_flags,
    int flags);
OS_ENUM(graftdmg_type, uint32_t,
    GRAFTDMG_CRYPTEX_BOOT = 1,
    GRAFTDMG_CRYPTEX_PREBOOT = 2,
    GRAFTDMG_CRYPTEX_DOWNLEVEL = 3,
    
    
    GRAFTDMG_CRYPTEX_PDI_NONCE = 6,
    GRAFTDMG_CRYPTEX_EFFECTIVE_AP = 7,
    GRAFTDMG_CRYPTEX_MOBILE_ASSET = 8,
    
    GRAFTDMG_CRYPTEX_MAX = 8);
OS_ENUM(cryptex_auth_type, uint32_t,
    
    
    
    CRYPTEX1_AUTH_ENV_GENERIC = 4,
    CRYPTEX1_AUTH_ENV_GENERIC_SUPPLEMENTAL = 5,
    CRYPTEX_AUTH_PDI_NONCE = 6,
    
    CRYPTEX_AUTH_MOBILE_ASSET = 8,
    
    CRYPTEX_AUTH_MAX = 8);
int     statfs_ext(const char *path, struct statfs *buf, int flags);
int     fstatfs_ext(int fd, struct statfs *buf, int flags);
TAILQ_HEAD(vnodelst, vnode);
int     vfstable_del(struct vfstable *);
extern TAILQ_HEAD(mntlist, mount) mountlist;
void mount_list_lock(void);
void mount_list_unlock(void);
void mount_lock_init(mount_t);
void mount_lock_destroy(mount_t);
void mount_lock(mount_t);
void mount_lock_spin(mount_t);
void mount_unlock(mount_t);
void mount_iterate_lock(mount_t);
void mount_iterate_unlock(mount_t);
void mount_lock_renames(mount_t);
void mount_unlock_renames(mount_t);
void mount_ref(mount_t, int);
void mount_drop(mount_t, int);
int  mount_refdrain(mount_t);
errno_t vfs_rootmountalloc(const char *, const char *, mount_t *mpp);
int vfs_mount_recovery(void);
int vfs_switch_root(const char *, const char *, vfs_switch_root_flags_t);
int     vfs_mountroot(void);
void    vfs_unmountall(int only_non_system);
int     safedounmount(struct mount *, int, vfs_context_t);
int     dounmount(struct mount *, int, int, vfs_context_t);
void    dounmount_submounts(struct mount *, int, vfs_context_t);
int     vfs_setmounting(vnode_t);
void    vfs_clearmounting(vnode_t);
void    vfs_setmountedon(vnode_t);
void  mount_dropcrossref(mount_t, vnode_t, int);
mount_t mount_lookupby_volfsid(int, int);
mount_t mount_list_lookupby_fsid(fsid_t *, int, int);
int  mount_list_add(mount_t);
void mount_list_remove(mount_t);
int  mount_iterref(mount_t, int);
int  mount_isdrained(mount_t, int);
void mount_iterdrop(mount_t);
void mount_iterdrain(mount_t);
void mount_iterreset(mount_t);
int kernel_mount(const char *, vnode_t, vnode_t, const char *, void *, size_t, int, uint32_t, vfs_context_t);
int  throttle_get_io_policy(struct uthread **ut);
int  throttle_get_passive_io_policy(struct uthread **ut);
void *throttle_info_update_by_mount(mount_t mp);
void rethrottle_thread(uthread_t ut);
extern int num_trailing_0(uint64_t n);
KALLOC_TYPE_DECLARE(mount_zone);
#pragma pack(4)

{
	struct __ipc_perm_new   msg_perm; 
	__int32_t       msg_first;      
	__int32_t       msg_last;       
	msglen_t        msg_cbytes;     
	msgqnum_t       msg_qnum;       
	msglen_t        msg_qbytes;     
	pid_t           msg_lspid;      
	pid_t           msg_lrpid;      
	time_t          msg_stime;      
	__int32_t       msg_pad1;       
	time_t          msg_rtime;      
	__int32_t       msg_pad2;       
	time_t          msg_ctime;      
	__int32_t       msg_pad3;       
	__int32_t       msg_pad4[4];    
};
extern void log_putc(char);
extern void log_putc_locked(struct msgbuf *, char);
extern int log_dmesg(user_addr_t, uint32_t, int32_t *);
int     namei(struct nameidata *ndp);
void    nameidone(struct nameidata *);
int     lookup(struct nameidata *ndp);
int     relookup(struct vnode *dvp, struct vnode **vpp,
    struct componentname *cnp);
void    lookup_compound_vnop_post_hook(int error, vnode_t dvp, vnode_t vp, struct nameidata *ndp, int did_create);
void    kdebug_lookup(struct vnode *dp, struct componentname *cnp);
void    cache_purgevfs(mount_t mp);
int             cache_lookup_path(struct nameidata *ndp, struct componentname *cnp, vnode_t dp,
    vfs_context_t context, int *dp_authorized, vnode_t last_dp);
void            vnode_cache_authorized_action(vnode_t vp, vfs_context_t context, kauth_action_t action);
void            vnode_uncache_authorized_action(vnode_t vp, kauth_action_t action);
boolean_t       vnode_cache_is_stale(vnode_t vp);
boolean_t       vnode_cache_is_authorized(vnode_t vp, vfs_context_t context, kauth_action_t action);
int             lookup_validate_creation_path(struct nameidata *ndp);
int             namei_compound_available(vnode_t dp, struct nameidata *ndp);
int             netboot_setup(void);
int             netboot_mountroot(void);
int             netboot_root(void);
boolean_t       netboot_iaddr(struct in_addr * iaddr_p);
boolean_t       netboot_rootpath(struct in_addr * server_ip,
    char * name, size_t name_len,
    char * path, size_t path_len);
int os_log_coprocessor_as_kernel(void *buff, uint64_t buff_len, os_log_type_t type, const char *uuid, uint64_t timestamp, uint32_t offset, bool stream_log);
int os_log_coprocessor_register_as_kernel(const char *uuid, const char *file_path, size_t file_path_len);
__enum_decl(persona_type_t, int, {
	PERSONA_INVALID      = 0,
	PERSONA_GUEST        = 1,
	PERSONA_MANAGED      = 2,
	PERSONA_PRIV         = 3,
	PERSONA_SYSTEM       = 4,
	PERSONA_DEFAULT      = 5,
	PERSONA_SYSTEM_PROXY = 6,
	PERSONA_SYS_EXT      = 7,
	PERSONA_ENTERPRISE   = 8,

	PERSONA_TYPE_MAX     = PERSONA_ENTERPRISE,
});
_Static_assert(sizeof(struct kpersona_info) == 348, "sizeof(kpersona_info) == 348");
uid_t persona_get_id(struct persona *persona);
uid_t persona_get_uid(struct persona *persona);
int persona_get_type(struct persona *persona);
int persona_find(const char *login, uid_t uid,
    struct persona **persona, size_t *plen);
uid_t current_persona_get_id(void);
void persona_put(struct persona *persona);
int persona_find_by_type(persona_type_t persona_type, struct persona **persona,
    size_t *plen);
boolean_t persona_is_adoption_allowed(struct persona *persona);
kern_return_t do_pgo_reset_counters(void);
__BEGIN_DECLS
extern int pipe_stat(struct pipe *, void *, int);
extern uint64_t pipe_id(struct pipe *);
__BEGIN_DECLS
int     priv_check_cred(kauth_cred_t cred, int priv, int flags);
extern int proc_is_classic(proc_t p);
extern bool proc_is_exotic(proc_t p);
extern bool proc_is_alien(proc_t p);
proc_t current_proc_EXTERNAL(void);
extern bool proc_is_driver(proc_t p);
extern bool proc_is_third_party_debuggable_driver(proc_t p);
extern struct proc_ident proc_ident(proc_t p);
extern int      msleep(void *chan, lck_mtx_t *mtx, int pri, const char *__unsafe_indexable wmesg, struct timespec * ts );
extern int      msleep0(void *chan, lck_mtx_t *mtx, int pri, const char *__unsafe_indexable wmesg, int timo, int (*continuation)(int));
extern void     wakeup(void *chan);
extern void wakeup_one(caddr_t chan);
extern int proc_selfpid(void);
extern int proc_selfppid(void);
extern uint64_t proc_selfcsflags(void);
extern int proc_csflags(proc_t p, uint64_t* flags);
extern void proc_signal(int pid, int signum);
extern int proc_issignal(int pid, sigset_t mask);
extern int proc_isinferior(int pid1, int pid2);
void proc_name(int pid, char * buf, int size);
extern const char *proc_best_name(proc_t p);
void proc_selfname(char * buf, int size);
extern proc_t proc_find(int pid);
extern proc_t proc_find_ident(struct proc_ident const *i);
extern proc_t proc_find_audit_token(const audit_token_t token);
extern proc_t proc_self(void);
extern int proc_rele(proc_t p);
extern int proc_pid(proc_t);
extern int proc_ppid(proc_t);
extern int proc_original_ppid(proc_t);
extern int proc_orig_ppidversion(proc_t);
extern int proc_starttime(proc_t, struct timeval *);
extern boolean_t proc_is_simulated(const proc_t);
extern uint32_t proc_platform(const proc_t);
extern uint32_t proc_min_sdk(proc_t);
extern uint32_t proc_sdk(proc_t);
extern int proc_noremotehang(proc_t);
extern int proc_forcequota(proc_t);
extern int proc_chrooted(proc_t);
extern boolean_t proc_send_synchronous_EXC_RESOURCE(proc_t p);
extern int proc_is64bit(proc_t);
extern int proc_is64bit_data(proc_t);
extern int proc_isinitproc(proc_t);
extern int proc_exiting(proc_t);
extern int proc_in_teardown(proc_t);
extern int proc_suser(proc_t p);
extern int proc_issetugid(proc_t p);
extern int proc_tbe(proc_t);
extern int proc_gettty(proc_t p, vnode_t *vp);
extern int proc_gettty_dev(proc_t p, dev_t *dev);
pid_t proc_selfpgrpid(void);
pid_t proc_pgrpid(proc_t p);
pid_t proc_sessionid(proc_t p);
void bsd_set_dependency_capable(task_t task);
extern int      tsleep(void *chan, int pri, const char *wmesg, int timo);
extern int      msleep1(void *chan, lck_mtx_t *mtx, int pri, const char *wmesg, u_int64_t timo);
task_t proc_task(proc_t);
extern int proc_pidversion(proc_t);
extern proc_t proc_parent(proc_t);
extern void proc_parent_audit_token(proc_t, audit_token_t *);
extern uint32_t proc_persona_id(proc_t);
extern uint32_t proc_getuid(proc_t);
extern uint32_t proc_getgid(proc_t);
extern int proc_getcdhash(proc_t, unsigned char *);
extern int proc_pidbackgrounded(pid_t pid, uint32_t* state);
extern uint64_t proc_uniqueid(proc_t);
extern uint64_t proc_puniqueid(proc_t);
extern void proc_set_responsible_pid(proc_t target_proc, pid_t responsible_pid);
extern int proc_is_forcing_hfs_case_sensitivity(proc_t);
extern int proc_lvfork(proc_t);
extern int proc_increment_ru_oublock(proc_t, long *);
extern int proc_isabortedsignal(proc_t);
extern boolean_t proc_is_translated(proc_t);
extern bool proc_is_x86_64_compat(proc_t);
extern bool proc_ignores_content_protection(proc_t proc);
extern bool proc_skip_mtime_update(proc_t proc);
extern bool proc_support_long_paths(proc_t proc);
extern bool proc_allow_low_space_writes(proc_t p);
bool proc_use_alternative_symlink_ea(proc_t p);
bool proc_is_rsr(proc_t p);
extern bool proc_disallow_rw_for_o_evtonly(proc_t p);
extern int proc_exitstatus(proc_t p);
extern bool   proc_is_zombie(proc_t p);
extern void proc_getexecutableuuid(proc_t, unsigned char *, unsigned long);
extern int proc_get_originatorbgstate(uint32_t *is_backgrounded);
extern int proc_pidoriginatoruuid(uuid_t uuid_buf, uint32_t buffersize);
extern uint64_t proc_was_throttled(proc_t);
extern uint64_t proc_did_throttle(proc_t);
extern bool proc_is_traced(proc_t p);
extern void proc_coalitionids(proc_t, uint64_t[COALITION_NUM_TYPES]);
extern uint64_t get_current_unique_pid(void);
extern int proc_selfexecutableargs(uint8_t *buf, size_t *buflen);
extern off_t proc_getexecutableoffset(proc_t p);
extern vnode_t proc_getexecutablevnode(proc_t);
extern vnode_t proc_getexecutablevnode_noblock(proc_t);
extern int proc_set_syscall_filter_callbacks(syscall_filter_cbs_t callback);
extern int proc_set_syscall_filter_index(int which, int num, int index);
extern size_t proc_get_syscall_filter_mask_size(int which);
extern unsigned char *proc_get_syscall_filter_mask(proc_t p, int which);
extern int proc_set_syscall_filter_mask(proc_t p, int which, unsigned char *maskptr, size_t masklen);
extern int proc_set_filter_message_flag(proc_t p, boolean_t flag);
extern int proc_get_filter_message_flag(proc_t p, boolean_t *flag);
__BEGIN_DECLS




_Static_assert(sizeof(struct proc_uniqidentifierinfo) == 56, "sizeof(struct proc_uniqidentifierinfo) == 56");
extern int proc_fdlist(proc_t p, struct proc_fdinfo *buf, size_t *count);
extern int proc_pidoriginatorpid_uuid(uuid_t uuid, uint32_t buffersize, pid_t *pid);
extern int fill_socketinfo(socket_t so, struct socket_info *si);
extern int fill_pshminfo(struct pshmnode * pshm, struct pshm_info * pinfo);
extern int fill_pseminfo(struct psemnode * psem, struct psem_info * pinfo);
extern int fill_pipeinfo(struct pipe * cpipe, struct pipe_info * pinfo);
extern int fill_kqueueinfo(struct kqueue * kq, struct kqueue_info * kinfo);
extern int pid_kqueue_extinfo(proc_t, struct kqueue * kq, user_addr_t buffer,
    uint32_t buffersize, int32_t * retval);
extern int pid_kqueue_udatainfo(proc_t p, struct kqueue *kq, uint64_t *buf,
    uint32_t bufsize);
extern int pid_kqueue_listdynamickqueues(proc_t p, user_addr_t ubuf,
    uint32_t bufsize, int32_t *retval);
extern int pid_dynamickqueue_extinfo(proc_t p, kqueue_id_t kq_id,
    user_addr_t ubuf, uint32_t bufsize, int32_t *retval);
extern int fill_channelinfo(struct kern_channel * chan,
    struct proc_channel_info *chan_info);
extern int fill_procworkqueue(proc_t, struct proc_workqueueinfo *);
extern boolean_t workqueue_get_pwq_exceeded(void *v, boolean_t *exceeded_total,
    boolean_t *exceeded_constrained);
extern uint64_t workqueue_get_task_ss_flags_from_pwq_state_kdp(void *proc);
__options_decl(session_ref_bits_t, uint32_t, {
	S_DEFAULT        = 0x00,
	S_NOCTTY         = 0x01,      
	S_CTTYREF        = 0x02,      
});
__options_decl(pggrp_ref_bits_t, uint32_t, {
	PGRP_REF_NONE    = 0x00,
	PGRP_REF_EMPTY   = 0x01, 
});
__options_decl(proc_ref_bits_t, uint32_t, {
	P_REF_NONE       = 0x00u,
	P_REF_NEW        = 0x01u, 
	P_REF_DEAD       = 0x02u, 
	P_REF_WILL_EXEC  = 0x04u, 
	P_REF_IN_EXEC    = 0x08u, 
	P_REF_DRAINING   = 0x10u, 
	P_REF_SHADOW     = 0x20u, 
	P_REF_PROC_HOLD  = 0x40u, 
	P_REF_TASK_HOLD  = 0x80u, 
});
#pragma pack()


#pragma GCC visibility push(hidden)

extern struct vfs_context vfs_context0;
extern LIST_HEAD(sesshashhead, session) * sesshashtbl;
LIST_HEAD(proclist, proc);
__options_decl(cloneproc_flags_t, uint32_t, {
	CLONEPROC_SPAWN     = 0,
	CLONEPROC_FORK      = 0x0001,
	CLONEPROC_INITPROC  = 0x0002,
	CLONEPROC_EXEC      = 0x0004,
});
extern thread_t cloneproc(task_t, coalition_t *, proc_t, cloneproc_flags_t);
extern struct proc * XNU_PTRAUTH_SIGNED_PTR("initproc") initproc;
extern void proc_lock(struct proc *);
extern void proc_unlock(struct proc *);
extern void proc_spinlock(struct proc *);
extern void proc_spinunlock(struct proc *);
extern void proc_list_lock(void);
extern void proc_list_unlock(void);
extern void proc_klist_lock(void);
extern void proc_klist_unlock(void);
extern void proc_fdlock(struct proc *);
extern void proc_fdlock_spin(struct proc *);
extern void proc_fdunlock(struct proc *);
extern void proc_fdlock_assert(proc_t p, int assertflags);
extern void proc_dirs_lock_shared(struct proc *);
extern void proc_dirs_unlock_shared(struct proc *);
extern void proc_dirs_lock_exclusive(struct proc *);
extern void proc_dirs_unlock_exclusive(struct proc *);
extern void proc_ucred_lock(struct proc *);
extern void proc_ucred_unlock(struct proc *);
extern void proc_update_creds_onproc(struct proc *, kauth_cred_t cred);
extern kauth_cred_t proc_ucred_locked(proc_t p);
extern kauth_cred_t proc_ucred_smr(proc_t p);
extern kauth_cred_t proc_ucred_unsafe(proc_t p) __exported;
extern void proc_best_name_for_pid(int pid, char * buf, int size);
extern int isinferior(struct proc *, struct proc *);
__private_extern__ struct proc *pzfind(pid_t);
__private_extern__ struct proc *proc_find_zombref(pid_t);
__private_extern__ struct proc *proc_find_zombref_locked(pid_t);
__private_extern__ void proc_drop_zombref(struct proc * p);
extern size_t   chgproccnt(uid_t uid, int diff);
extern void     pinsertchild(struct proc *parent, struct proc *child, bool in_exec);
extern void     p_reparentallchildren(proc_t old_proc, proc_t new_proc);
extern int      setsid_internal(struct proc *p);
extern void     setlogin_internal(proc_t p, const char login[static MAXLOGNAME]);
extern int      setgroups_internal(proc_t p, u_int gidsetsize, gid_t *gidset, uid_t gmuid);
extern int      enterpgrp(struct proc *p, pid_t pgid, int mksess);
extern void     fixjobc(struct proc *p, struct pgrp *pgrp, int entering);
extern int      inferior(struct proc *p);
extern void     resetpriority(struct proc *);
extern void     setrunnable(struct proc *);
extern void     setrunqueue(struct proc *);
extern int      sleep(void *chan, int pri) __exported;
extern int      tsleep0(void *chan, int pri, const char *wmesg, int timo, int (*continuation)(int));
extern int      tsleep1(void *chan, int pri, const char *wmesg, u_int64_t abstime, int (*continuation)(int));
extern int      exit1(struct proc *, int, int *);
extern int      exit1_internal(struct proc *, int, int *, boolean_t, boolean_t, int);
extern int      exit_with_reason(struct proc *, int, int *, boolean_t, boolean_t, int, struct os_reason *);
extern int      fork1(proc_t, thread_t *, int, coalition_t *);
extern void proc_reparentlocked(struct proc *child, struct proc * newparent, int cansignal, int locked);
extern bool   proc_list_exited(proc_t p);
extern proc_t proc_find_locked(int pid);
extern proc_t proc_find_noref_smr(int pid);
extern bool proc_is_shadow(proc_t p);
extern proc_t proc_findthread(thread_t thread);
extern void proc_refdrain(proc_t);
extern proc_t proc_refdrain_will_exec(proc_t p);
extern void proc_refwake_did_exec(proc_t p);
extern void proc_childdrainlocked(proc_t);
extern void proc_childdrainstart(proc_t);
extern void proc_childdrainend(proc_t);
extern void  proc_checkdeadrefs(proc_t);
extern void phash_insert_locked(struct proc *);
extern void phash_remove_locked(struct proc *);
extern void phash_replace_locked(struct proc *old_proc, struct proc *new_proc);
extern bool pghash_exists_locked(pid_t);
extern void pghash_insert_locked(struct pgrp *);
extern struct pgrp *pgrp_find(pid_t);
extern void pgrp_rele(struct pgrp * pgrp);
extern struct session * session_find_internal(pid_t sessid);
extern struct pgrp *proc_pgrp(proc_t, struct session **);
extern struct pgrp *pgrp_leave_locked(struct proc *p);
extern struct pgrp *pgrp_enter_locked(struct proc *parent, struct proc *p);
extern struct pgrp *tty_pgrp_locked(struct tty * tp);
extern void pgrp_lock(struct pgrp * pgrp);
extern void pgrp_unlock(struct pgrp * pgrp);
extern struct session *session_find_locked(pid_t sessid);
extern void session_replace_leader(struct proc *old_proc, struct proc *new_proc);
extern struct session *session_alloc(struct proc *leader);
extern void session_lock(struct session * sess);
extern void session_unlock(struct session * sess);
extern struct session *session_ref(struct session *sess);
extern void session_rele(struct session *sess);
extern struct tty *session_set_tty_locked(struct session *sessp, struct tty *);
extern struct tty *session_clear_tty_locked(struct session *sess);
extern struct tty *session_tty(struct session *sess);
extern proc_t proc_parentholdref(proc_t);
extern int proc_parentdropref(proc_t, int);
int  itimerfix(struct timeval *tv);
int  itimerdecr(struct proc * p, struct itimerval *itp, int usec);
void proc_free_realitimer(proc_t proc);
void proc_inherit_itimers(struct proc *old_proc, struct proc *new_proc);
int  timespec_is_valid(const struct timespec *);
void proc_signalstart(struct proc *, int locked);
void proc_signalend(struct proc *, int locked);
int  proc_transstart(struct proc *, int locked, int non_blocking);
void proc_transcommit(struct proc *, int locked);
void proc_transend(struct proc *, int locked);
int  proc_transwait(struct proc *, int locked);
void proc_wait_release(struct proc *p);
void proc_knote(struct proc * p, long hint);
void proc_transfer_knotes(struct proc *old_proc, struct proc *new_proc);
void proc_knote_drain(struct proc *p);
void proc_setregister(proc_t p);
void proc_resetregister(proc_t p);
bool proc_get_pthread_jit_allowlist(proc_t p, bool *late_out);
void proc_set_pthread_jit_allowlist(proc_t p, bool late);
thread_t proc_thread(proc_t);
extern int proc_pendingsignals(proc_t, sigset_t);
int proc_getpcontrol(int pid, int * pcontrolp);
int proc_resetpcontrol(int pid);
extern void proc_set_task(proc_t, task_t);
extern task_t proc_get_task_raw(proc_t proc);
extern proc_t task_get_proc_raw(task_t task);
extern void proc_ref_hold_proc_task_struct(proc_t proc);
extern void proc_release_proc_task_struct(proc_t proc);
extern void task_ref_hold_proc_task_struct(task_t task);
extern void task_release_proc_task_struct(task_t task, proc_ro_t proc_ro);
extern void proc_setpidversion(proc_t, int);
extern uint64_t proc_getcsflags(proc_t);
extern void proc_csflags_update(proc_t, uint64_t);
extern void proc_csflags_set(proc_t, uint64_t);
extern void proc_csflags_clear(proc_t, uint64_t);
extern uint8_t *proc_syscall_filter_mask(proc_t);
extern void proc_syscall_filter_mask_set(proc_t, uint8_t *);
extern pid_t proc_getpid(proc_t);
extern void proc_setplatformdata(proc_t, uint32_t, uint32_t, uint32_t);
extern void proc_set_sigact(proc_t, int, user_addr_t);
extern void proc_set_trampact(proc_t, int, user_addr_t);
extern void proc_set_sigact_trampact(proc_t, int, user_addr_t, user_addr_t);
extern void proc_reset_sigact(proc_t, sigset_t);
extern void proc_setexecutableuuid(proc_t, const uuid_t);
extern const unsigned char *__counted_by(sizeof(uuid_t)) proc_executableuuid_addr(proc_t);
extern void proc_getresponsibleuuid(proc_t target_proc, unsigned char *__counted_by(size)responsible_uuid, unsigned long size);
extern void proc_setresponsibleuuid(proc_t target_proc, unsigned char *__counted_by(size)responsible_uuid, unsigned long size);
extern void pgrp_iterate(struct pgrp *pgrp, proc_iterate_fn_t callout,
    void *arg, bool (^filterfn)(proc_t));
extern void proc_iterate(unsigned int flags, proc_iterate_fn_t callout,
    void *arg, proc_iterate_fn_t filterfn, void *filterarg);
extern void proc_childrenwalk(proc_t p, proc_iterate_fn_t callout, void *arg);
extern void proc_rebootscan(proc_iterate_fn_t callout, void *arg,
    proc_iterate_fn_t filterfn, void *filterarg);
pid_t dtrace_proc_selfpid(void);
pid_t dtrace_proc_selfppid(void);
uid_t dtrace_proc_selfruid(void);
os_refgrp_decl_extern(p_refgrp);
KALLOC_TYPE_DECLARE(proc_stats_zone);
ZONE_DECLARE_ID(ZONE_ID_PROC_TASK, struct proc);
bool proc_ignores_node_permissions(proc_t proc);
extern bool no_paging_space_action(void);
__options_closed_decl(proc_require_flags_t, unsigned int, {
	PROC_REQUIRE_ALLOW_ALL = 0x0, 
	PROC_REQUIRE_ALLOW_NULL = 0x1,
});
void proc_require(proc_t proc, proc_require_flags_t flags);
__BEGIN_DECLS __ASSUME_PTR_ABI_SINGLE_BEGIN
#pragma GCC visibility push(hidden)

struct proc;
extern proc_ro_t proc_ro_alloc(struct proc *p, proc_ro_data_t p_data, struct task *t, task_ro_data_t t_data);
extern proc_ro_t proc_ro_ref_task(proc_ro_t pr, struct task *t, task_ro_data_t t_data);
extern void proc_ro_erase_task(proc_ro_t pr);
extern proc_ro_t proc_get_ro(struct proc *p) __pure2;
extern proc_ro_t task_get_ro(struct task *t) __pure2;
extern struct task *proc_ro_task(proc_ro_t pr) __pure2;
__BEGIN_DECLS









extern int proc_uuid_policy_lookup(uuid_t uuid, uint32_t *flags, int32_t *gencount);
extern void proc_uuid_policy_init(void);
extern int proc_uuid_policy_kernel(uint32_t operation, uuid_t uuid, uint32_t flags);
#pragma pack(4)




#pragma pack()





















struct stat;
__BEGIN_DECLS
extern int pru_abort_notsupp(struct socket *so);
extern int pru_accept_notsupp(struct socket *so, struct sockaddr **nam);
extern int pru_attach_notsupp(struct socket *so, int proto, struct proc *p);
extern int pru_bind_notsupp(struct socket *so, struct sockaddr *nam,
    struct proc *p);
extern int pru_connect_notsupp(struct socket *so, struct sockaddr *nam,
    struct proc *p);
extern int pru_connect2_notsupp(struct socket *so1, struct socket *so2);
extern int pru_connectx_notsupp(struct socket *, struct sockaddr *,
    struct sockaddr *, struct proc *, uint32_t, sae_associd_t,
    sae_connid_t *, uint32_t, void *, uint32_t, struct uio *, user_ssize_t *);
extern int pru_disconnectx_notsupp(struct socket *, sae_associd_t,
    sae_connid_t);
extern int pru_socheckopt_null(struct socket *, struct sockopt *);
extern int pru_control_notsupp(struct socket *so,
    u_long cmd, caddr_t __sized_by(IOCPARM_LEN(cmd)) data,
    struct ifnet *ifp, struct proc *p);
extern int pru_detach_notsupp(struct socket *so);
extern int pru_disconnect_notsupp(struct socket *so);
extern int pru_listen_notsupp(struct socket *so, struct proc *p);
extern int pru_peeraddr_notsupp(struct socket *so, struct sockaddr **nam);
extern int pru_rcvd_notsupp(struct socket *so, int flags);
extern int pru_rcvoob_notsupp(struct socket *so, struct mbuf *m, int flags);
extern int pru_send_notsupp(struct socket *so, int flags, struct mbuf *m,
    struct sockaddr *addr, struct mbuf *control, struct proc *p);
extern int pru_send_list_notsupp(struct socket *, struct mbuf *, u_int *, int);
extern int pru_sense_null(struct socket *so, void * sb, int isstat64);
extern int pru_shutdown_notsupp(struct socket *so);
extern int pru_sockaddr_notsupp(struct socket *so, struct sockaddr **nam);
extern int pru_sosend_notsupp(struct socket *so, struct sockaddr *addr,
    struct uio *uio, struct mbuf *top, struct mbuf *control, int flags);
extern int pru_sosend_list_notsupp(struct socket *, struct mbuf *, size_t, u_int *, int);
extern int pru_soreceive_notsupp(struct socket *so,
    struct sockaddr **paddr, struct uio *uio, struct mbuf **mp0,
    struct mbuf **controlp, int *flagsp);
extern int pru_sopoll_notsupp(struct socket *so, int events,
    struct ucred *cred, void *);
extern void pru_sanitize(struct pr_usrreqs *);
extern void domaininit(void);
extern void domainfin(void);
extern void pfctlinput(int, struct sockaddr *);
extern void pfctlinput2(int, struct sockaddr *, void *);
extern struct protosw *pffindproto_locked(int, int, int);
extern struct protosw *pffindprotonotype(int, int);
extern struct protosw *pffindtype(int, int);
extern struct protosw_old *pffindproto_old(int, int, int);
extern int net_add_proto(struct protosw *, struct domain *, int)
__XNU_INTERNAL(net_add_proto);
extern void net_init_proto(struct protosw *, struct domain *);
extern int net_del_proto(int, int, struct domain *)
__XNU_INTERNAL(net_del_proto);
extern int net_add_proto_old(struct protosw_old *, struct domain_old *);
extern int net_del_proto_old(int, int, struct domain_old *);
extern void net_update_uptime(void);
extern void net_update_uptime_with_time(const struct timeval *);
extern uint64_t net_uptime(void);
extern uint64_t net_uptime_ms(void);
extern uint64_t net_uptime_us(void);
extern void net_uptime2timeval(struct timeval *);
extern struct protosw *pffindproto(int family, int protocol, int type)
__XNU_INTERNAL(pffindproto);
void workq_mark_exiting(struct proc *);
void workq_exit(struct proc *);
void pthread_init(void);
void thread_will_park_or_terminate(thread_t thread);
void pthread_init(void);
__BEGIN_DECLS




typedef 




void os_reason_init(void);
os_reason_t build_userspace_exit_reason(uint32_t reason_namespace, uint64_t reason_code, user_addr_t payload, uint32_t payload_size,
    user_addr_t reason_string, uint64_t reason_flags);
char *exit_reason_get_string_desc(os_reason_t exit_reason);
int os_reason_alloc_buffer(os_reason_t cur_reason, uint32_t osr_bufsize);
void exit_with_mach_exception_using_ast(exception_info_t exception, uint32_t flags);
os_reason_t os_reason_create(uint32_t osr_namespace, uint64_t osr_code);
int os_reason_alloc_buffer_noblock(os_reason_t cur_reason, uint32_t osr_bufsize);
void os_reason_ref(os_reason_t cur_reason);
void os_reason_free(os_reason_t cur_reason);
void os_reason_set_flags(os_reason_t cur_reason, uint64_t flags);
void os_reason_set_description_data(os_reason_t cur_reason, uint32_t type, void *reason_data, uint32_t reason_data_len);
__BEGIN_DECLS
int     reboot_kernel(int, char *);
__END_DECLS



__BEGIN_DECLS
int get_system_inshutdown(void);
#pragma GCC visibility push(hidden)


struct proc;
void calcru(struct proc *p, struct timeval *up, struct timeval *sp, struct timeval *ip);
void ruadd(struct rusage *ru, struct rusage *ru2);
void update_rusage_info_child(struct rusage_info_child *ru, rusage_info_current *ru_current);
void proc_limitfork(struct proc *parent, struct proc *child);
void proc_limitdrop(struct proc *p);
rlim_t proc_limitgetcur(struct proc *p, int which);
void proc_limitsetcur_fsize(struct proc *p, rlim_t value);
int proc_limitgetcur_nofile(struct proc *p);
void gather_rusage_info(struct proc *p, rusage_info_current *ru, int flavor);
int proc_get_rusage(struct proc *proc, int flavor, user_addr_t buffer, int is_zombie);
int iopolicysys_vfs_materialize_dataless_files(struct proc *p, int cmd, int scope,
    int policy, struct _iopol_param_t *iop_param);
__enum_decl(thread_selfcounts_kind_t, uint32_t, {
	
	THSC_CPI = 1,
	
	THSC_CPI_PER_PERF_LEVEL = 2,
	
	THSC_TIME_CPI = 3,
	
	THSC_TIME_CPI_PER_PERF_LEVEL = 4,
	
	THSC_TIME_ENERGY_CPI = 5,
	
	THSC_TIME_ENERGY_CPI_PER_PERF_LEVEL = 6,
});
__BEGIN_DECLS
struct sbuf     *sbuf_new(struct sbuf *, char *, int, int);
void             sbuf_clear(struct sbuf *);
int              sbuf_setpos(struct sbuf *, int);
int              sbuf_bcat(struct sbuf *, const void *, size_t);
int              sbuf_bcpy(struct sbuf *, const void *, size_t);
int              sbuf_cat(struct sbuf *, const char *);
int              sbuf_cpy(struct sbuf *, const char *);
int              sbuf_printf(struct sbuf *, const char *, ...) __printflike(2, 3);
int              sbuf_vprintf(struct sbuf *, const char *, va_list) __printflike(2, 0);
int              sbuf_putc(struct sbuf *, int);
int              sbuf_trim(struct sbuf *);
int              sbuf_overflowed(struct sbuf *);
void             sbuf_finish(struct sbuf *);
char            *sbuf_data(struct sbuf *);
int              sbuf_len(struct sbuf *);
int              sbuf_done(struct sbuf *);
void             sbuf_delete(struct sbuf *);
extern uint64_t sdt_getarg(void *, dtrace_id_t, void *, int, int);
void sdt_provide_module(void *, struct modctl *);
void sdt_early_init(void);
void sdt_load_machsect(struct modctl *ctl);
void sdt_init(void);
void    selrecord(proc_t selector, struct selinfo *, void *);
void    selwakeup(struct selinfo *);
void    selthreadclear(struct selinfo *);
void    select_cleanup_uthread(struct _select *);
void selspec_attach(struct knote *, struct selinfo *);
void selspec_detach(struct knote *);
#pragma pack(4)

{
	struct __ipc_perm_new sem_perm; 
	__int32_t       sem_base;       
	unsigned short  sem_nsems;      
	time_t          sem_otime;      
	__int32_t       sem_pad1;       
	time_t          sem_ctime;      
	                                
	                                
	__int32_t       sem_pad2;       
	__int32_t       sem_pad3[4];    
};
#pragma pack(4)



#pragma pack()

union user_semun {
	user_addr_t     buf;            
	user_addr_t     array;          
};
void    semexit(struct proc *p);
int __sfi_ctl(uint32_t operation, uint32_t sfi_class, uint64_t time, uint64_t *out_time);
int __sfi_pidctl(uint32_t operation, pid_t pid, uint32_t sfi_flags, uint32_t *out_sfi_flags);
#pragma pack(4)



{
	struct __ipc_perm_new shm_perm; 
	size_t          shm_segsz;      
	pid_t           shm_lpid;       
	pid_t           shm_cpid;       
	shmatt_t        shm_nattch;     
	time_t          shm_atime;      
	time_t          shm_dtime;      
	time_t          shm_ctime;      
	void            *shm_internal;  
};
#pragma pack(4)





#pragma pack()





struct label;
__BEGIN_DECLS

void    shmexit(struct proc *);
int     shmfork(struct proc *, struct proc *);
__private_extern__ void shmexec(struct proc *);
void siginfo_user_to_user64(user_siginfo_t *, user64_siginfo_t *);
void    execsigs(struct proc *p, thread_t thread);
void    gsignal(int pgid, int sig);
int     issignal_locked(struct proc *p);
int     CURSIG(struct proc *p);
int clear_procsiglist(struct proc *p, int bit, int in_signalstart);
int set_procsigmask(struct proc *p, int bit);
void    postsig_locked(int sig);
void    siginit(struct proc *p);
void    trapsignal(struct proc *p, int sig, unsigned code);
void    pt_setrunnable(struct proc *p);
int     hassigprop(int sig, int prop);
int setsigvec(proc_t, thread_t, int signum, struct __kern_sigaction *, boolean_t in_sigstart);
void    sendsig(struct proc *,  user_addr_t  action, int sig,
    int returnmask, uint32_t code, sigset_t siginfo);
void    psignal(struct proc *p, int sig);
void    psignal_with_reason(struct proc *p, int sig, struct os_reason *signal_reason);
void    psignal_locked(struct proc *, int);
void    psignal_try_thread(proc_t, thread_t, int signum);
void    psignal_try_thread_with_reason(proc_t, thread_t, int, struct os_reason*);
void    psignal_thread_with_reason(proc_t, thread_t, int, struct os_reason*);
void    psignal_uthread(thread_t, int);
void    pgsignal(struct pgrp *pgrp, int sig, int checkctty);
void    tty_pgsignal_locked(struct tty * tp, int sig, int checkctty);
void    threadsignal(thread_t sig_actthread, int signum,
    mach_exception_code_t code, boolean_t set_exitreason);
int     thread_issignal(proc_t p, thread_t th, sigset_t mask);
void    psignal_vfork(struct proc *p, task_t new_task, thread_t thread,
    int signum);
void    psignal_vfork_with_reason(proc_t p, task_t new_task, thread_t thread,
    int signum, struct os_reason *signal_reason);
void    signal_setast(thread_t sig_actthread);
void    pgsigio(pid_t pgid, int signalnum);
void sig_lock_to_exit(struct proc *p);
int sig_try_locked(struct proc *p);
void    psignal_sigkill_with_reason(struct proc *p, struct os_reason *signal_reason);
void    psignal_sigkill_try_thread_with_reason(struct proc *p, struct thread *thread, struct os_reason *signal_reason);
cpu_type_t process_cpu_type(struct proc * core_proc);
cpu_type_t process_cpu_subtype(struct proc * core_proc);
int     coredump(struct proc *p, uint32_t reserve_mb, int coredump_flags);
void set_thread_exit_reason(void *th, void *reason, boolean_t proc_locked);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct sockaddr, sockaddr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct sockaddr_storage, sockaddr_storage);
#pragma pack(4)



















#pragma pack()



enum sopt_dir { SOPT_GET, SOPT_SET };
extern boolean_t is_cmsg_valid(struct mbuf *control, struct cmsghdr *cmsg);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct recv_msg_elem, recv_msg_elem);
__BEGIN_DECLS
__ASSUME_PTR_ABI_SINGLE_BEGIN

extern int sbappendaddr(struct sockbuf *sb, struct sockaddr *asa,
    struct mbuf *m0, struct mbuf *control, int *error_out);
extern int sbappendchain(struct sockbuf *sb, struct mbuf *m);
extern int sbappendrecord(struct sockbuf *sb, struct mbuf *m0);
extern int sbappendrecord_nodrop(struct sockbuf *sb, struct mbuf *m0);
extern void sbflush(struct sockbuf *sb);
extern int sbspace(struct sockbuf *sb);
extern int soabort(struct socket *so);
extern void socantrcvmore(struct socket *so);
extern void socantsendmore(struct socket *so);
extern int sodisconnect(struct socket *so);
extern void sofree(struct socket *so);
extern void sofreelastref(struct socket *, int);
extern void soisconnected(struct socket *so);
extern boolean_t socanwrite(struct socket *so);
extern void soisconnecting(struct socket *so);
extern void soisdisconnected(struct socket *so);
extern void soisdisconnecting(struct socket *so);
extern struct socket *sonewconn(struct socket *head, int connstatus,
    const struct sockaddr *from);
extern int sopoll(struct socket *so, int events, struct ucred *cred, void *wql);
extern int sooptcopyin(struct sockopt *sopt, void * __sized_by(len), size_t len,
    size_t minlen)
__attribute__ ((warn_unused_result));
extern int sooptcopyout(struct sockopt *sopt, void *__sized_by(len) data, size_t len)
__attribute__ ((warn_unused_result));
extern int sooptcopyin_bindtodevice(struct sockopt *sopt, char * __sized_by(bufsize) buf, size_t bufsize);
extern int soopt_cred_check(struct socket *so, int priv, boolean_t allow_root,
    boolean_t ignore_delegate);
extern int soreceive(struct socket *so, struct sockaddr **paddr,
    struct uio *uio, struct mbuf **mp0, struct mbuf **controlp, int *flagsp);
extern int soreserve(struct socket *so, uint32_t sndcc, uint32_t rcvcc);
extern void soreserve_preconnect(struct socket *so, unsigned int pre_cc);
extern void sorwakeup(struct socket *so);
extern int sosend(struct socket *so, struct sockaddr *addr, struct uio *uio,
    struct mbuf *top, struct mbuf *control, int flags);
extern int sosend_reinject(struct socket *so, struct sockaddr *addr, struct mbuf *top,
    struct mbuf *control, uint32_t sendflags);
extern int sosend_list(struct socket *so, struct mbuf *pktlist, size_t total_pkt_len, u_int *pktcnt, int flags);
extern int soreceive_list(struct socket *so, struct recv_msg_elem *msgarray,
    u_int msgcnt, int *flags);
extern int soreceive_m_list(struct socket *, u_int *, struct mbuf **madrp,
    struct mbuf **, struct mbuf **, int *);
extern void sonullevent(struct socket *so, void *arg, uint32_t hint);
extern struct mbuf *sbconcat_mbufs(struct sockbuf *sb, struct sockaddr *asa, struct mbuf *m0,
    struct mbuf *control);
__BEGIN_DECLS
__ASSUME_PTR_ABI_SINGLE_BEGIN

extern void socketinit(void);
extern struct sockaddr *dup_sockaddr(struct sockaddr *sa, int canwait);
extern int getsock(struct filedesc *fdp, int fd, struct file **fpp);
extern int sockargs(struct mbuf **mp, user_addr_t data, socklen_t buflen, int type);
extern void get_sockev_state(struct socket *, u_int32_t *);
extern void so_update_last_owner_locked(struct socket *, struct proc *);
extern void so_update_policy(struct socket *);
extern void so_acquire_accept_list(struct socket *, struct socket *);
extern void so_release_accept_list(struct socket *);
extern int sbappend(struct sockbuf *sb, struct mbuf *m);
extern int sbappend_nodrop(struct sockbuf *sb, struct mbuf *m);
extern int sbappendstream(struct sockbuf *sb, struct mbuf *m);
extern int sbappendcontrol(struct sockbuf *sb, struct mbuf *m0,
    struct mbuf *control, int *error_out);
extern int sbappendstream_rcvdemux(struct socket *so, struct mbuf *m);
extern void sbcheck(struct sockbuf *sb);
extern void sblastmbufchk(struct sockbuf *, const char *);
extern void sblastrecordchk(struct sockbuf *, const char *);
extern struct mbuf *sbcreatecontrol(caddr_t __sized_by(size) p, int size, int type, int level);
extern struct mbuf **sbcreatecontrol_mbuf(caddr_t __sized_by(size) p, int size, int type,
    int level, struct mbuf **m);
extern void sbdrop(struct sockbuf *sb, int len);
extern void sbdroprecord(struct sockbuf *sb);
extern void sbrelease(struct sockbuf *sb);
extern int sbreserve(struct sockbuf *sb, uint32_t cc);
extern void sbtoxsockbuf(struct sockbuf *sb, struct xsockbuf *xsb);
extern int sbwait(struct sockbuf *sb);
extern void sbwakeup(struct sockbuf *sb);
extern void sb_empty_assert(struct sockbuf *, const char *);
extern int sb_notify(struct sockbuf *sb);
extern void sballoc(struct sockbuf *sb, struct mbuf *m);
extern void sbfree(struct sockbuf *sb, struct mbuf *m);
static inline void *
__sized_by_or_null(size)
alloc_sockaddr(size_t size, zalloc_flags_t flags)
{
	if (__improbable(size > UINT8_MAX)) {
		panic("invalid size");
	}
	__typed_allocators_ignore_push
	void * buf = kheap_alloc(KHEAP_SONAME, size, flags | Z_ZERO);
	__typed_allocators_ignore_pop
	if (buf != NULL) {
		struct sockaddr *sa = __unsafe_forge_bidi_indexable(struct sockaddr *,
		    buf, sizeof(struct sockaddr));
		sa->sa_len = (uint8_t)size;
	}

	return buf;
}



extern int sblock(struct sockbuf *sb, uint32_t flags);
extern void sbunlock(struct sockbuf *sb, boolean_t keeplocked);
extern int soaccept(struct socket *so, struct sockaddr **nam);
extern int soacceptlock(struct socket *so, struct sockaddr **nam, int dolock);
extern int soacceptfilter(struct socket *so, struct socket *head);
extern struct socket *soalloc(int waitok, int dom, int type);
extern int sobindlock(struct socket *so, struct sockaddr *nam, int dolock);
extern int soclose(struct socket *so);
extern int soclose_locked(struct socket *so);
extern void soclose_wait_locked(struct socket *so);
extern int soconnect(struct socket *so, struct sockaddr *nam);
extern int soconnectlock(struct socket *so, struct sockaddr *nam, int dolock);
extern int soconnect2(struct socket *so1, struct socket *so2);
extern int soconnectxlocked(struct socket *so, struct sockaddr *src,
    struct sockaddr *dst, struct proc *, uint32_t, sae_associd_t,
    sae_connid_t *, uint32_t, void *, u_int32_t, uio_t, user_ssize_t *);
extern int sodisconnectx(struct socket *so, sae_associd_t, sae_connid_t);
extern int sodisconnectxlocked(struct socket *so, sae_associd_t, sae_connid_t);
extern int socreate_internal(int dom, struct socket **aso, int type, int proto,
    struct proc *, uint32_t, struct proc *);
extern int socreate(int dom, struct socket **aso, int type, int proto);
extern int socreate_delegate(int dom, struct socket **aso, int type, int proto,
    pid_t epid);
extern void sodealloc(struct socket *so);
extern int sodisconnectlocked(struct socket *so);
extern void soreference(struct socket *so);
extern void sodereference(struct socket *so);
extern void somultipages(struct socket *, boolean_t);
extern void soif2kcl(struct socket *, boolean_t);
extern int sosetdefunct(struct proc *, struct socket *, int level, boolean_t);
extern int sodefunct(struct proc *, struct socket *, int level);
extern int soresume(struct proc *, struct socket *, int);
extern void resume_proc_sockets(proc_t);
extern int so_check_extended_bk_idle_time(struct socket *);
extern void so_drain_extended_bk_idle(struct socket *);
extern void sohasoutofband(struct socket *so);
extern void sodisconnectwakeup(struct socket *so);
extern int soisthrottled(struct socket *so);
extern int soisprivilegedtraffic(struct socket *so);
extern int soissrcbackground(struct socket *so);
extern int soissrcrealtime(struct socket *so);
extern int soissrcbesteffort(struct socket *so);
extern void soclearfastopen(struct socket *so);
extern int solisten(struct socket *so, int backlog);
extern struct socket *sodropablereq(struct socket *head);
extern lck_mtx_t *socket_getlock(struct socket *so, int flags);
extern void socket_lock(struct socket *so, int refcount);
extern void socket_lock_assert_owned(struct socket *so);
extern int socket_try_lock(struct socket *so);
extern void socket_unlock(struct socket *so, int refcount);
extern int sogetaddr_locked(struct socket *, struct sockaddr **, int);
extern const char *solockhistory_nr(struct socket *);
extern void soevent(struct socket *so, uint32_t hint);
extern void sorflush(struct socket *so);
extern void sowflush(struct socket *so);
extern void sowakeup(struct socket *so, struct sockbuf *sb, struct socket *so2);
extern int soioctl(struct socket *so, u_long cmd,
    caddr_t __sized_by(IOCPARM_LEN(cmd)) data, struct proc *p);
extern int sogetoptlock(struct socket *so, struct sockopt *sopt, int);
extern int sosetoptlock(struct socket *so, struct sockopt *sopt, int);
extern int soshutdown(struct socket *so, int how);
extern int soshutdownlock(struct socket *so, int how);
extern int soshutdownlock_final(struct socket *so, int how);
extern void sotoxsocket(struct socket *so, struct xsocket *xso);
extern void sotoxsocket64(struct socket *so, struct xsocket64 *xso);
extern int sosendallatonce(struct socket *so);
extern int soreadable(struct socket *so);
extern int sowriteable(struct socket *so);
extern void sowwakeup(struct socket *so);
extern int sosendcheck(struct socket *, struct sockaddr *, user_ssize_t,
    int32_t, int32_t, int, int *);
extern int soo_ioctl(struct fileproc *, u_long cmd,
    caddr_t __sized_by(IOCPARM_LEN(cmd)), vfs_context_t);
extern int soo_stat(struct socket *, void *, int);
extern int soo_select(struct fileproc *, int, void *, vfs_context_t);
extern int soo_kqfilter(struct fileproc *, struct knote *, struct kevent_qos_s *);
extern struct soflow_hash_entry *soflow_get_flow(struct socket *, struct sockaddr *, struct sockaddr *, struct mbuf *, size_t, soflow_direction_t, u_short);
extern void soflow_free_flow(struct soflow_hash_entry *);
extern void soflow_detach(struct socket *);
extern void so_update_tx_data_stats(struct socket *, uint32_t, uint32_t);
extern void set_packet_service_class(struct mbuf *, struct socket *,
    mbuf_svc_class_t, u_int32_t);
extern int so_tos_from_control(struct mbuf *);
extern int so_tc_from_control(struct mbuf *, int *);
extern mbuf_svc_class_t so_tc2msc(int);
extern int so_svc2tc(mbuf_svc_class_t);
extern void set_tcp_stream_priority(struct socket *so);
extern int so_set_net_service_type(struct socket *, int);
extern int so_set_traffic_class(struct socket *, int);
extern void so_set_default_traffic_class(struct socket *);
extern int so_set_opportunistic(struct socket *, int);
extern int so_get_opportunistic(struct socket *);
extern int so_set_recv_anyif(struct socket *, int);
extern int so_get_recv_anyif(struct socket *);
extern int so_set_effective_pid(struct socket *so, int epid, struct proc *p, boolean_t check_cred);
extern int so_set_effective_uuid(struct socket *so, uuid_t euuid, struct proc *p, boolean_t check_cred);
extern int so_set_restrictions(struct socket *, uint32_t);
extern uint32_t so_get_restrictions(struct socket *);
extern int so_isdstlocal(struct socket *);
extern void so_recv_data_stat(struct socket *, struct mbuf *, size_t);
extern void so_inc_recv_data_stat(struct socket *, size_t, size_t);
extern int so_wait_for_if_feedback(struct socket *);
extern int soopt_getm(struct sockopt *sopt, struct mbuf **mp);
extern int soopt_mcopyin(struct sockopt *sopt, struct mbuf *m);
extern int soopt_mcopyout(struct sockopt *sopt, struct mbuf *m);
extern boolean_t so_cache_timer(void);
extern void mptcp_fallback_sbdrop(struct socket *so, struct mbuf *m, int len);
extern void mptcp_preproc_sbdrop(struct socket *, struct mbuf *, unsigned int);
extern void mptcp_postproc_sbdrop(struct mbuf *, u_int64_t, u_int32_t,
    u_int32_t);
extern void netpolicy_post_msg(uint32_t, struct netpolicy_event_data *,
    uint32_t);
extern int tcp_notsent_lowat_check(struct socket *so);
extern user_ssize_t recv_msg_array_resid(struct recv_msg_elem * __counted_by(count), u_int count);
void sotoxsocket_n(struct socket *, struct xsocket_n *);
void sbtoxsockbuf_n(struct sockbuf *, struct xsockbuf_n *);
void sbtoxsockstat_n(struct socket *, struct xsockstat_n *);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user_msghdr, user_msghdr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user64_msghdr, user64_msghdr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user32_msghdr, user32_msghdr);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user_msghdr_x, user_msghdr_x);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user64_msghdr_x, user64_msghdr_x);
__CCT_DECLARE_CONSTRAINED_PTR_TYPES(struct user32_msghdr_x, user32_msghdr_x);
__options_decl(posix_spawn_options, uint32_t, {
	PSA_OPTION_NONE                         = 0,
	PSA_OPTION_PLUGIN_HOST_DISABLE_A_KEYS   = 0x1,
	PSA_OPTION_ALT_ROSETTA                  = 0x2,
	PSA_OPTION_DATALESS_IOPOLICY            = 0x4,
});
extern void munge_user64_stat(struct stat *sbp, struct user64_stat *usbp);
extern void munge_user32_stat(struct stat *sbp, struct user32_stat *usbp);
SLIST_HEAD(sysctl_oid_list, sysctl_oid);
int sysctl_io_number(struct sysctl_req *req, long long bigValue, size_t valueSize, void *pValue, int *changed);
int sysctl_io_string(struct sysctl_req *req, char *pValue, size_t valueSize, int trunc, int *changed);
int sysctl_io_opaque(struct sysctl_req *req, void *pValue, size_t valueSize, int *changed);
void sysctl_register_oid(struct sysctl_oid *oidp);
void sysctl_unregister_oid(struct sysctl_oid *oidp);
void sysctl_set_osenvironment(unsigned int size, const void* value);
void sysctl_unblock_osenvironment(void);
void sysctl_register_fixed(void) __deprecated;
__END_DECLS












__BEGIN_DECLS
void sysctl_register_oid_early(struct sysctl_oid *oidp);
SYSCTL_DECL(_kern);
SYSCTL_DECL(_sysctl);
SYSCTL_DECL(_vm);
SYSCTL_DECL(_vfs);
SYSCTL_DECL(_net);
SYSCTL_DECL(_debug);
SYSCTL_DECL(_hw);
SYSCTL_DECL(_machdep);
SYSCTL_DECL(_user);
SYSCTL_DECL(_kern_bridge);
SYSCTL_DECL(_hw_features);
static inline bool
kern_osreleasetype_matches(const char *variant)
{
	const size_t len = sizeof(osreleasetype);

	return strnstr(__unsafe_null_terminated_from_indexable(osreleasetype, &osreleasetype[len - 1]),
	           variant, len);
}


void    sysctl_mib_init(void);
__BEGIN_DECLS

int     einval(void);
void    nullsys(void);
int     errsys(void);
int     seltrue(dev_t dev, int which, struct proc *p);
void    ttyprintf(struct tty *, const char *, ...) __printflike(2, 3);
void    realitexpire(struct proc *, void*);
int     hzto(struct timeval *tv);
void    tablefull(const char *);
void    uprintf(const char *, ...) __printflike(1, 2);
int     copywithin(void *saddr, void *daddr, size_t len);
int64_t fulong(user_addr_t addr);
int     sulong(user_addr_t addr, int64_t longword);
uint64_t fuulong(user_addr_t addr);
int     suulong(user_addr_t addr, uint64_t ulongword);
int     clone_system_shared_regions(int shared_regions_active,
    int chain_regions,
    int base_vnode);
extern kern_return_t bsd_exception(int, mach_exception_data_t codes, int);
extern void     bsdinit_task(void);
extern void unix_syscall_return(int) __dead2;
void    initclocks(void);
void    startprofclock(struct proc *);
void    stopprofclock(struct proc *);
void    setstatclockrate(int hzrate);
void    get_procrustime(struct time_value *tv);
void    load_init_program(struct proc *p);
void __pthread_testcancel(int presyscall);
void throttle_info_get_last_io_time(mount_t mp, struct timeval *tv);
void update_last_io_time(mount_t mp);
void throttle_info_end_io(buf_t bp);
void    timeout(void (*)(void *), void *arg, int ticks);
void    timeout_with_leeway(void (*)(void *), void *arg, int ticks, int leeway_ticks);
void    untimeout(void (*)(void *), void *arg);
int     bsd_hostname(char *, size_t, size_t*);
int     vslock(user_addr_ut addr, user_size_ut len);
int     vsunlock(user_addr_ut addr, user_size_ut len, int dirtied);
int     nullop(void);
int     nulldev(void);
int     enoioctl(void);
int     enosys(void);
int     enxio(void);
int     eopnotsupp(void);
void    *hashinit(int count, int type, u_long *hashmask);
LIST_HEAD(generic_hash_head, generic);
void    hashinit_generic(int elements,
    struct generic_hash_head *__counted_by(*out_count) *out_ptr, size_t *out_count);
void    hashdestroy(void *, int type, u_long hashmask);
void    ovbcopy(const void *from, void *to, size_t len);
int     fubyte(user_addr_t addr);
int     fuibyte(user_addr_t addr);
int     subyte(user_addr_t addr, int byte);
int     suibyte(user_addr_t addr, int byte);
long   fuword(user_addr_t addr);
long   fuiword(user_addr_t addr);
int    suword(user_addr_t addr, long word);
int    suiword(user_addr_t addr, long word);
int     useracc(user_addr_ut addr, user_size_ut len, int prot);
void    bsd_timeout(void (*)(void *), void *arg, struct timespec * ts);
void    bsd_untimeout(void (*)(void *), void *arg);
void    set_fsblocksize(struct vnode *);
uint64_t tvtoabstime(struct timeval *);
uint64_t tstoabstime(struct timespec *);
void    *throttle_info_create(void);
void    throttle_info_mount_ref(mount_t mp, void * throttle_info);
void    throttle_info_mount_rel(mount_t mp);
void    throttle_info_release(void *throttle_info);
void    throttle_info_update(void *throttle_info, int flags);
uint32_t throttle_lowpri_io(int sleep_amount);
int     throttle_lowpri_io_will_be_throttled(int sleep_amount);
void    throttle_set_thread_io_policy(int policy);
int     throttle_get_thread_effective_io_policy(void);
int     throttle_thread_io_tier_above_metadata(void);
int     throttle_info_ref_by_mask(uint64_t throttle_mask, throttle_info_handle_t *throttle_info_handle);
void    throttle_info_rel_by_mask(throttle_info_handle_t throttle_info_handle);
void    throttle_info_update_by_mask(void *throttle_info_handle, int flags);
void    throttle_info_disable_throttle(int devno, boolean_t isfusion);
int     throttle_info_io_will_be_throttled(void *throttle_info_handle, int policy);
int  throttle_io_will_be_throttled(int lowpri_window_msecs, mount_t mp);
int throttle_lowpri_window(void) __attribute__((pure));
void throttle_info_reset_window(struct uthread *ut);
void throttle_info_update_with_type(void *throttle_info, int flags, boolean_t isssd);
void *exec_spawnattr_getmacpolicyinfo(const void *macextensions, const char *policyname, size_t *lenp);
void sys_override_io_throttle(boolean_t enable_override);
void systrace_args(int sysnum, void *params, uint64_t *uarg);
void systrace_entry_setargdesc(int sysnum, int ndx, char *desc, size_t descsz);
void systrace_return_setargdesc(int sysnum, int ndx, char *desc, size_t descsz);
SYSCTL_DECL(_net_systm);
__BEGIN_DECLS
void kern_event_init(struct domain *);
void kern_control_init(struct domain *);
int64_t ntp_get_freq(void);
void    ntp_update_second(int64_t *adjustment, clock_sec_t secs);
void    ntp_init(void);
void
load_static_trust_cache(void);
kern_return_t
static_trust_cache_capabilities(
	uint32_t *num_static_trust_caches_ret,
	TCCapabilities_t *capabilities0_ret,
	TCCapabilities_t *capabilities1_ret);
kern_return_t
check_trust_cache_runtime_for_uuid(
	const uint8_t check_uuid[kUUIDSize]);
kern_return_t
unload_trust_cache(uuid_t uuid);
kern_return_t
load_trust_cache(
	const uint8_t *img4_object, const size_t img4_object_len,
	const uint8_t *img4_ext_manifest, const size_t img4_ext_manifest_len);
kern_return_t
load_trust_cache_with_type(
	TCType_t type,
	const uint8_t *img4_object, const size_t img4_object_len,
	const uint8_t *img4_ext_manifest, const size_t img4_ext_manifest_len,
	const uint8_t *img4_aux_manifest, const size_t img4_aux_manifest_len);
kern_return_t
load_legacy_trust_cache(
	const uint8_t *module_data, const size_t module_size);
kern_return_t
query_trust_cache(
	TCQueryType_t query_type,
	const uint8_t cdhash[kTCEntryHashSize],
	TrustCacheQueryToken_t *query_token);
kern_return_t
query_trust_cache_for_rem(
	const uint8_t cdhash[kTCEntryHashSize],
	uint8_t *rem_perms);
__BEGIN_DECLS
__END_DECLS






































__BEGIN_DECLS

int      b_to_q(const u_char *cp, int cc, struct clist *q);
void     catq(struct clist *from, struct clist *to);
void     clist_init(void);
int      getc(struct clist *q);
void     ndflush(struct clist *q, int cc);
int      ndqb(struct clist *q, int flag);
u_char  *firstc(struct clist *clp, int *c);
u_char  *nextc(struct clist *q, u_char *cp, int *c);
int      putc(int c, struct clist *q);
int      q_to_b(struct clist *q, u_char *cp, int cc);
int      unputc(struct clist *q);
int      clalloc(struct clist *clp, int size, int quot);
void     clfree(struct clist *clp);
void    cinit(void);
void    clrbits(u_char *cp, int off, int len);
void    tty_init(void);
int     ttioctl_locked(struct tty *tp, u_long com, caddr_t data, int flag,
    struct proc *p);
int     ttcompat(struct tty *tp, u_long com, caddr_t data, int flag,
    struct proc *p);
void tty_lock(struct tty *tp);
bool tty_trylock(struct tty *tp);
void tty_unlock(struct tty *tp);
bool tty_islocked(struct tty *tp);
void     termioschars(struct termios *t);
int      tputchar(int c, struct tty *tp);
int      ttioctl(struct tty *tp, u_long com, caddr_t data, int flag,
    struct proc *p);
int      ttread(struct tty *tp, struct uio *uio, int flag);
int      ttyselect(struct tty *tp, int rw, void * wql, struct proc *p);
int      ttselect(dev_t dev, int rw, void * wql, struct proc *p);
void     ttsetwater(struct tty *tp);
int      ttspeedtab(int speed, struct speedtab *table);
int      ttstart(struct tty *tp);
void     ttwakeup(struct tty *tp);
int      ttwrite(struct tty *tp, struct uio *uio, int flag);
void     ttwwakeup(struct tty *tp);
void     ttyblock(struct tty *tp);
int      ttycheckoutq(struct tty *tp, int wait);
int      ttyclose(struct tty *tp);
void     ttyflush(struct tty *tp, int rw);
void     ttyinfo(struct tty *tp);
void     ttyinfo_locked(struct tty *tp);
int      ttyinput(int c, struct tty *tp);
int      ttylclose(struct tty *tp, int flag);
int      ttymodem(struct tty *tp, int flag);
int      ttyopen(dev_t device, struct tty *tp);
int      ttysleep(struct tty *tp,
    void *chan, int pri, const char *wmesg, int timeout);
int      ttywait(struct tty *tp);
void     ttyfree(struct tty *);
void     ttyfree_locked(struct tty *);
extern void ttyhold(struct tty *tp);
__BEGIN_DECLS

off_t           ubc_blktooff(struct vnode *, daddr64_t);
daddr64_t       ubc_offtoblk(struct vnode *, off_t);
off_t   ubc_getsize(struct vnode *);
int     ubc_setsize(struct vnode *, off_t);
errno_t ubc_setsize_ex(vnode_t vp, off_t nsize, ubc_setsize_opts_t opts);
kauth_cred_t ubc_getcred(struct vnode *);
int     ubc_setthreadcred(struct vnode *, struct proc *, struct thread *);
errno_t ubc_msync(vnode_t, off_t, off_t, off_t *, int);
int     ubc_pages_resident(vnode_t);
int     ubc_page_op(vnode_t, off_t, int, ppnum_t *, int *);
int     ubc_range_op(vnode_t, off_t, off_t, int, int *);
int     ubc_setcred(struct vnode *, struct ucred *);
void cs_blob_reset_cache(void);
int ubc_cs_blob_revalidate(vnode_t, struct cs_blob *, struct image_params *, int, uint32_t);
int ubc_cs_generation_check(vnode_t);
int cs_entitlements_blob_get(proc_t, void **, size_t *);
int cs_blob_get(proc_t, void **, size_t *);
const char *cs_identity_get(proc_t);
void ubc_cs_free_and_vnode_unlock(struct vnode *);
int UBCINFOEXISTS(const struct vnode *);
void    cluster_update_state(vnode_t, vm_object_offset_t, vm_object_offset_t, boolean_t);
int     advisory_read(vnode_t, off_t, off_t, int);
int     advisory_read_ext(vnode_t, off_t, off_t, int, int (*)(buf_t, void *), void *, int);
int     cluster_read(vnode_t, struct uio *, off_t, int);
int     cluster_read_ext(vnode_t, struct uio *, off_t, int, int (*)(buf_t, void *), void *);
int     cluster_write(vnode_t, struct uio *, off_t, off_t, off_t, off_t, int);
int     cluster_write_ext(vnode_t, struct uio *, off_t, off_t, off_t, off_t, int, int (*)(buf_t, void *), void *);
int     cluster_pageout(vnode_t, upl_t, upl_offset_t, off_t, int, off_t, int);
int     cluster_pageout_ext(vnode_t, upl_t, upl_offset_t, off_t, int, off_t, int, int (*)(buf_t, void *), void *);
int     cluster_pagein(vnode_t, upl_t, upl_offset_t, off_t, int, off_t, int);
int     cluster_pagein_ext(vnode_t, upl_t, upl_offset_t, off_t, int, off_t, int, int (*)(buf_t, void *), void *);
int     cluster_push(vnode_t, int);
int     cluster_push_ext(vnode_t, int, int (*)(buf_t, void *), void *);
int     cluster_push_err(vnode_t, int, int (*)(buf_t, void *), void *, int *);
int     cluster_bp(buf_t);
int     cluster_bp_ext(buf_t, int (*)(buf_t, void *), void *);
void    cluster_zero(upl_t, upl_offset_t, int, buf_t);
int     cluster_copy_upl_data(uio_t, upl_t, int, int *);
int     cluster_copy_ubc_data(vnode_t, uio_t, int *, int);
cl_direct_read_lock_t *cluster_lock_direct_read(vnode_t vp, lck_rw_type_t exclusive);
void cluster_unlock_direct_read(cl_direct_read_lock_t *lck);
int     ubc_upl_map(upl_t, vm_offset_t *);
int     ubc_upl_unmap(upl_t);
int     ubc_upl_map_range(upl_t, vm_offset_t, vm_size_t, vm_prot_t, vm_offset_t *);
int     ubc_upl_unmap_range(upl_t, vm_offset_t, vm_size_t);
int     ubc_upl_commit(upl_t);
int     ubc_upl_commit_range(upl_t, upl_offset_t, upl_size_t, int);
int     ubc_upl_abort(upl_t, int);
int     ubc_upl_abort_range(upl_t, upl_offset_t, upl_size_t, int);
void    ubc_upl_range_needed(upl_t, int, int);
upl_page_info_t *ubc_upl_pageinfo(upl_t);
upl_size_t ubc_upl_maxbufsize(void);
int     is_file_clean(vnode_t, off_t);
errno_t mach_to_bsd_errno(kern_return_t mach_err);
int     ubc_create_upl_external(vnode_t, off_t, int, upl_t *, upl_page_info_t **, int);
int     ubc_create_upl_kernel(vnode_t, off_t, int, upl_t *, upl_page_info_t **, int, vm_tag_t);
boolean_t ubc_is_mapped(const struct vnode *, boolean_t *writable);
__attribute__((pure)) boolean_t ubc_is_mapped_writable(const struct vnode *);
boolean_t ubc_was_mapped(const struct vnode *, boolean_t *writable);
__attribute__((pure)) boolean_t ubc_was_mapped_writable(const struct vnode *);
uint32_t cluster_max_io_size(mount_t, int);
uint8_t cs_hash_type(struct cs_hash const *);
__BEGIN_DECLS

__private_extern__ int  ubc_umount(mount_t mp);
__private_extern__ void ubc_unmountall(void);
__private_extern__ memory_object_t ubc_getpager(vnode_t);
__private_extern__ void ubc_destroy_named(vnode_t vp, vm_object_destroy_reason_t reason);
__private_extern__ void cluster_release(struct ubc_info *);
__private_extern__ uint32_t cluster_throttle_io_limit(vnode_t, uint32_t *);
memory_object_control_t ubc_getobject(vnode_t, int);
int     ubc_info_init(vnode_t);
int     ubc_info_init_withsize(vnode_t, off_t);
void    ubc_info_deallocate(struct ubc_info *);
int     ubc_isinuse(vnode_t, int);
int     ubc_isinuse_locked(vnode_t, int, int);
int     ubc_getcdhash(vnode_t, off_t, unsigned char *);
void    cs_blob_require(struct cs_blob *, vnode_t);
int     ubc_cs_blob_add(
	vnode_t, uint32_t, cpu_type_t, cpu_subtype_t, off_t,
	vm_address_t *, vm_size_t, struct image_params *,
	int, struct cs_blob **, cs_blob_add_flags_t);
void    ubc_get_cs_mtime(vnode_t, struct timespec *);
int     ubc_cs_getcdhash(vnode_t, off_t, unsigned char *);
kern_return_t ubc_cs_blob_allocate(vm_offset_t *, vm_size_t *);
void ubc_cs_blob_deallocate(vm_offset_t, vm_size_t);
boolean_t ubc_cs_is_range_codesigned(vnode_t, mach_vm_offset_t, mach_vm_size_t);
kern_return_t   ubc_cs_validation_bitmap_allocate( vnode_t );
void            ubc_cs_validation_bitmap_deallocate( struct ubc_info * );
__BEGIN_DECLS


uio_t uio_create( int a_iovcount,               
    off_t a_offset,                                             
    int a_spacetype,                                            
    int a_iodirection );
void uio_reset( uio_t a_uio,
    off_t a_offset,                                             
    int a_spacetype,                                            
    int a_iodirection );
uio_t uio_duplicate( uio_t a_uio );
int uio_restore(uio_t uio, uio_t snapshot_uio);
void uio_free( uio_t a_uio );
int uio_addiov( uio_t a_uio, user_addr_t a_baseaddr, user_size_t a_length );
int uio_getiov( uio_t a_uio,
    int a_index,
    user_addr_t * a_baseaddr_p,
    user_size_t * a_length_p );
void uio_update( uio_t a_uio, user_size_t a_count );
user_ssize_t uio_resid( uio_t a_uio );
void uio_setresid( uio_t a_uio, user_ssize_t a_value );
int uio_iovcnt( uio_t a_uio );
off_t uio_offset( uio_t a_uio );
void uio_setoffset( uio_t a_uio, off_t a_offset );
int uio_rw( uio_t a_uio );
void uio_setrw( uio_t a_uio, int a_value );
int uio_isuserspace( uio_t a_uio );
user_addr_t uio_curriovbase( uio_t a_uio );
user_size_t uio_curriovlen( uio_t a_uio );
extern int uiomove(const char *__sized_by(n) cp, int n, struct uio *uio);
extern int uiomove64(const __uint64_t cp, int n, struct uio *uio);
__private_extern__ struct user_iovec * uio_iovsaddr_user( uio_t a_uio );
__private_extern__ int uio_calculateresid_user(uio_t __attribute((nonnull)) a_uio);
__private_extern__ uio_t  uio_createwithbuffer( int a_iovcount, off_t a_offset, int a_spacetype, int a_iodirection, void *a_buf_p, size_t a_buffer_size );
__private_extern__ int copyin_user_iovec_array(user_addr_t uaddr, int spacetype, int count, struct user_iovec *dst);
__private_extern__ void uio_reset_fast( uio_t a_uio,
    off_t a_offset,                                             
    int a_spacetype,                                            
    int a_iodirection );
__private_extern__ int uio_copyout_user(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyin_user(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyout_sys(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyin_sys(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyout_phys_user(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyin_phys_user(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyout_phys_sys(const char *__sized_by(n) c_cp, int n, uio_t uio);
__private_extern__ int uio_copyin_phys_sys(const char *__sized_by(n) c_cp, int n, uio_t uio);
__END_DECLS







extern int ureadc(int c, struct uio *uio);
__BEGIN_DECLS


extern mach_port_name_t ipc_entry_name_mask(mach_port_name_t name);
static __inline mach_port_name_t
ulock_owner_value_to_port_name(uint32_t uval)
{
	
	return ipc_entry_name_mask((mach_port_name_t)uval);
}

extern int ulock_wake(struct task *task, uint32_t operation, user_addr_t addr, uint64_t wake_value);
int     uipc_usrreq(struct socket *so, int req, struct mbuf *m,
    struct mbuf *nam, struct mbuf *control);
int     uipc_ctloutput(struct socket *so, struct sockopt *sopt);
int     unp_connect2(struct socket *so, struct socket *so2);
void    unp_dispose(struct mbuf *m);
int     unp_externalize(struct mbuf *rights);
void    unp_init(void);
int     unp_lock(struct socket *, int, void *);
int     unp_unlock(struct socket *, int, void *);
lck_mtx_t* unp_getlock(struct socket *, int);
int utf8_normalizeOptCaseFoldAndHash(const char *str,
    size_t      str_len,
    bool        case_sens,
    void      (*hash_func)(void *buf, size_t buf_len, void *ctx),
    void       *hash_ctx);
int utf8_normalizeOptCaseFoldAndCompare(const char *strA,
    size_t      strA_len,
    const char *strB,
    size_t      strB_len,
    bool        case_sens,
    bool       *are_equal);
int utf8_normalizeOptCaseFold(const char *str,
    size_t      str_len,
    bool        case_sens,
    int32_t    *ustr,
    int32_t     ustr_size,
    int32_t    *ustr_len);
int utf8_normalizeOptCaseFoldToUTF8(const char *str,
    size_t      str_len,
    bool        case_sens,
    char       *ustr,
    size_t      ustr_size,
    size_t     *ustr_len);
int utf8_normalizeOptCaseFoldToUTF8ForPath(const char *str,
    size_t      str_len,
    bool        case_sens,
    char       *ustr,
    size_t      ustr_size,
    size_t     *ustr_len);
int utf8_normalizeOptCaseFoldAndMatchSubstring(const char    *strA,
    size_t         strA_len,
    const int32_t *ustrB,
    int32_t        ustrB_len,
    bool           case_sens,
    void          *buf,
    size_t         buf_size,
    bool          *has_match);
void utf8_normalizeOptCaseFoldGetUVersion(unsigned char version[4]);
__BEGIN_DECLS
int uname(struct utsname *);
extern int
machine_exception(int exception, mach_exception_code_t code,
    mach_exception_subcode_t subcode);
extern kern_return_t
handle_ux_exception(thread_t thread, int exception,
    mach_exception_code_t code,
    mach_exception_subcode_t subcode);
__END_DECLS


__BEGIN_DECLS

bool os_variant_has_internal_diagnostics(const char *subsystem);
__BEGIN_DECLS
struct proc *current_proc(void);
extern resolver_result_t vfs_resolver_result(uint32_t seq, enum resolver_status stat, int aux);
extern enum resolver_status vfs_resolver_status(resolver_result_t);
extern uint32_t vfs_resolver_sequence(resolver_result_t);
extern int vfs_resolver_auxiliary(resolver_result_t);
extern int vnode_trigger_update(vnode_t vp, resolver_result_t result);
int vn_default_error(void);
__BEGIN_DECLS


errno_t vnode_create(uint32_t flavor, uint32_t size, void  *data, vnode_t *vpp);
__options_decl(vnode_create_options_t, uint32_t, {
	VNODE_CREATE_DEFAULT = 0,
	VNODE_CREATE_EMPTY   = 1,
	VNODE_CREATE_NODEALLOC  = 2
});
errno_t vnode_create_ext(uint32_t flavor, uint32_t size, void  *data, vnode_t *vpp, vnode_create_options_t vc_options);
errno_t vnode_create_empty(vnode_t *vpp);
errno_t vnode_initialize(uint32_t flavor, uint32_t size, void *data, vnode_t *vpp);
int     vnode_addfsref(vnode_t vp);
int     vnode_removefsref(vnode_t vp);
int     vnode_hasdirtyblks(vnode_t vp);
int     vnode_hascleanblks(vnode_t vp);
int     vnode_waitforwrites(vnode_t vp, int output_target, int slpflag, int slptimeout, const char *msg);
void    vnode_startwrite(vnode_t vp);
void    vnode_writedone(vnode_t vp);
uint32_t        vnode_vid(vnode_t vp);
boolean_t vnode_isonexternalstorage(vnode_t vp);
mount_t vnode_mountedhere(vnode_t vp);
mount_t vnode_mount(vnode_t vp);
dev_t   vnode_specrdev(vnode_t vp);
void *  vnode_fsnode(vnode_t vp);
void    vnode_clearfsnode(vnode_t vp);
int     vnode_isvroot(vnode_t vp);
int     vnode_issystem(vnode_t vp);
int     vnode_ismount(vnode_t vp);
int     vnode_isreg(vnode_t vp);
int     vnode_isdir(vnode_t vp);
int     vnode_islnk(vnode_t vp);
int     vnode_isfifo(vnode_t vp);
int     vnode_isblk(vnode_t vp);
int     vnode_ischr(vnode_t vp);
int     vnode_isswap(vnode_t vp);
int     vnode_isnamedstream(vnode_t vp);
errno_t vnode_setasnamedstream(vnode_t vp, vnode_t svp);
errno_t vnode_setasfirmlink(vnode_t vp, vnode_t target_vp);
errno_t vnode_getfirmlink(vnode_t vp, vnode_t *target_vp);
int     vnode_ismountedon(vnode_t vp);
void    vnode_setmountedon(vnode_t vp);
void    vnode_clearmountedon(vnode_t vp);
int     vnode_isrecycled(vnode_t vp);
int     vnode_willberecycled(vnode_t vp);
int     vnode_isnocache(vnode_t vp);
int     vnode_israge(vnode_t vp);
int     vnode_needssnapshots(vnode_t vp);
void    vnode_setnocache(vnode_t vp);
void    vnode_clearnocache(vnode_t vp);
int     vnode_isnoreadahead(vnode_t vp);
void    vnode_setnoreadahead(vnode_t vp);
void    vnode_clearnoreadahead(vnode_t vp);
int     vnode_isfastdevicecandidate(vnode_t vp);
void    vnode_setfastdevicecandidate(vnode_t vp);
void    vnode_clearfastdevicecandidate(vnode_t vp);
int     vnode_isautocandidate(vnode_t vp);
void    vnode_setautocandidate(vnode_t vp);
void    vnode_clearautocandidate(vnode_t vp);
void    vnode_settag(vnode_t vp, int tag);
int     vnode_tag(vnode_t vp);
int     vnode_getattr(vnode_t vp, struct vnode_attr *vap, vfs_context_t ctx);
extern uint64_t vnode_get_va_fsid(struct vnode_attr *vap);
int     vnode_setattr(vnode_t vp, struct vnode_attr *vap, vfs_context_t ctx);
vnode_t vfs_rootvnode(void);
void    vnode_uncache_credentials(vnode_t vp);
void    vnode_setmultipath(vnode_t vp);
uint32_t  vnode_vfsmaxsymlen(vnode_t vp);
int     vnode_vfsisrdonly(vnode_t vp);
int     vnode_vfstypenum(vnode_t vp);
void    vnode_vfsname(vnode_t vp, char *buf);
int     vnode_vfs64bitready(vnode_t vp);
int     vfs_context_get_special_port(vfs_context_t, int, ipc_port_t *);
int     vfs_context_set_special_port(vfs_context_t, int, ipc_port_t);
proc_t  vfs_context_proc(vfs_context_t ctx);
kauth_cred_t    vfs_context_ucred(vfs_context_t ctx);
int     vfs_context_pid(vfs_context_t ctx);
int     vfs_context_copy_audit_token(vfs_context_t ctx, audit_token_t *token);
int     vfs_context_issignal(vfs_context_t ctx, sigset_t mask);
int     vfs_context_suser(vfs_context_t ctx);
int     vfs_context_is64bit(vfs_context_t ctx);
vfs_context_t vfs_context_create(vfs_context_t ctx);
vfs_context_t vfs_context_create_with_proc(proc_t proc);
int vfs_context_rele(vfs_context_t ctx);
vfs_context_t vfs_context_current(void) __pure2;
int     vfs_context_bind(vfs_context_t);
int     vfs_ctx_skipatime(vfs_context_t ctx);
int vfs_set_thread_fs_private(uint8_t tag, uint64_t fs_private);
int vfs_get_thread_fs_private(uint8_t tag, uint64_t *fs_private);
int     vflush(struct mount *mp, struct vnode *skipvp, int flags);
int     vnode_get(vnode_t);
int     vnode_getwithvid(vnode_t, uint32_t);
int vnode_getwithvid_drainok(vnode_t, uint32_t);
int     vnode_getwithref(vnode_t vp);
int vnode_getwithref_noblock(vnode_t vp);
int     vnode_put(vnode_t vp);
int     vnode_ref(vnode_t vp);
void    vnode_rele(vnode_t vp);
int     vnode_isinuse(vnode_t vp, int refcnt);
void     vnode_hold(vnode_t vp);
vnode_t  vnode_drop(vnode_t vp);
int     vnode_recycle(vnode_t vp);
int     vnode_ismonitored(vnode_t vp);
int     vnode_isdyldsharedcache(vnode_t vp);
int     vn_authorize_unlink(vnode_t dvp, vnode_t vp, struct componentname *cnp, vfs_context_t ctx, void *reserved);
int     vn_authorize_rmdir(vnode_t dvp, vnode_t vp, struct componentname *cnp, vfs_context_t ctx, void *reserved);
int     vn_getpath_fsenter(struct vnode *vp, char *pathbuf, int *len);
int     vn_getpath_no_firmlink(struct vnode *vp, char *pathbuf, int *len);
int     vn_getpath_fsenter_with_parent(struct vnode *dvp, struct vnode *vp, char *pathbuf, int *len);
int     vn_getpath_ext(struct vnode *vp, struct vnode *dvp, char *pathbuf, size_t *len, int flags);
int     vn_getpath_ext_with_mntlen(struct vnode *vp, struct vnode *dvp, char *pathbuf, size_t *len, size_t *mntlen, int flags);
void    vnode_update_identity(vnode_t vp, vnode_t dvp, const char *name, int name_len, uint32_t name_hashval, int flags);
int     vn_bwrite(struct vnop_bwrite_args *ap);
int     vnode_authorize(vnode_t vp, vnode_t dvp, kauth_action_t action, vfs_context_t ctx);
int     vnode_attr_authorize_init(struct vnode_attr *vap, struct vnode_attr *dvap, kauth_action_t action, vfs_context_t ctx);
int     vnode_attr_authorize(struct vnode_attr *vap, struct vnode_attr *dvap, mount_t mp, kauth_action_t action, vfs_context_t ctx);
int     vnode_authattr(vnode_t vp, struct vnode_attr *vap, kauth_action_t *actionp, vfs_context_t ctx);
int     vnode_authattr_new(vnode_t dvp, struct vnode_attr *vap, int noauth, vfs_context_t ctx);
errno_t vnode_close(vnode_t vp, int flags, vfs_context_t ctx);
int vn_getpath(struct vnode *vp, char *pathbuf, int *len);
int     vnode_notify(vnode_t vp, uint32_t events, struct vnode_attr *vap);
int     vfs_get_notify_attributes(struct vnode_attr *vap);
errno_t vnode_lookup(const char *path, int flags, vnode_t *vpp, vfs_context_t ctx);
errno_t vnode_lookupat(const char *path, int flags, vnode_t *vpp, vfs_context_t ctx, vnode_t start_dvp);
errno_t vnode_open(const char *path, int fmode, int cmode, int flags, vnode_t *vpp, vfs_context_t ctx);
int     vnode_iterate(struct mount *mp, int flags, int (*callout)(struct vnode *, void *), void *arg);
int     vn_revoke(vnode_t vp, int flags, vfs_context_t ctx);
int     cache_lookup(vnode_t dvp, vnode_t *vpp, struct componentname *cnp);
void    cache_enter(vnode_t dvp, vnode_t vp, struct componentname *cnp);
void    cache_purge(vnode_t vp);
void    cache_purge_negatives(vnode_t vp);
const char *vfs_addname(const char *name, uint32_t len, uint32_t nc_hash, uint32_t flags);
int   vfs_removename(const char *name);
int     vcount(vnode_t vp);
int vn_path_package_check(vnode_t vp, char *path, int pathlen, int *component);
int vn_searchfs_inappropriate_name(const char *name, int len);
int     vn_rdwr(enum uio_rw rw, struct vnode *vp, caddr_t base, int len, off_t offset, enum uio_seg segflg, int ioflg, kauth_cred_t cred, int *aresid, proc_t p);
int     vn_rdwr_64(enum uio_rw rw, struct vnode *vp, uint64_t base, int64_t len, off_t offset, enum uio_seg segflg, int ioflg, kauth_cred_t cred, int64_t *aresid, struct proc *p);
errno_t vnode_rdadvise(vnode_t vp, off_t offset, int len, vfs_context_t ctx);
const char      *vnode_getname(vnode_t vp);
void    vnode_putname(const char *name);
vnode_t vnode_getparent(vnode_t vp);
int     vnode_setdirty(vnode_t vp);
int     vnode_cleardirty(vnode_t vp);
int     vnode_isdirty(vnode_t vp);
int vnode_lookup_continue_needed(vnode_t vp, struct componentname *cnp);
boolean_t vnode_isonssd(vnode_t vp);
int vnode_istty(vnode_t vp);
int bdevvp(dev_t dev, struct vnode **vpp);
int vnode_getfromfd(vfs_context_t ctx, int fd, vnode_t *vpp);
vnode_t vnode_parent(vnode_t vp);
void vnode_getparent_and_name(vnode_t vp, vnode_t *out_pvp, const char **out_name);
thread_t vfs_context_thread(vfs_context_t ctx);
task_t vfs_context_task(vfs_context_t ctx);
int vnode_isauthfs(vnode_t vp);
int     vn_stat(struct vnode *vp, void * sb, kauth_filesec_t *xsec, int isstat64, int needsrealdev,
    vfs_context_t ctx);
int     vn_stat_noauth(struct vnode *vp, void * sb, kauth_filesec_t *xsec, int isstat64, int needsrealdev,
    vfs_context_t ctx, struct ucred *file_cred);
int     vaccess(mode_t file_mode, uid_t uid, gid_t gid,
    mode_t acc_mode, kauth_cred_t cred);
int     check_mountedon(dev_t dev, enum vtype type, int  *errorp);
int vn_getcdhash(struct vnode *vp, off_t offset, unsigned char *cdhash);
void    vnode_reclaim(vnode_t);
vnode_t current_workingdir(void);
void    *vnode_vfsfsprivate(vnode_t);
uint32_t vnode_vfsvisflags(vnode_t);
uint32_t vnode_vfscmdflags(vnode_t);
int     vnode_is_openevt(vnode_t);
void    vnode_set_openevt(vnode_t);
void    vnode_clear_openevt(vnode_t);
int     vnode_isstandard(vnode_t);
int     vnode_makeimode(int, int);
int     vnode_vttoif(enum vtype);
int     vnode_isshadow(vnode_t);
boolean_t vnode_on_reliable_media(vnode_t);
void vnode_setparent(vnode_t, vnode_t);
void vnode_setname(vnode_t, char *);
const char *vnode_getname_printable(vnode_t vp);
void vnode_putname_printable(const char *name);
int vnode_getbackingvnode(vnode_t in_vp, vnode_t* out_vpp);
errno_t vfs_setup_vattr_from_attrlist(struct attrlist *alp, struct vnode_attr *vap, enum vtype obj_vtype, ssize_t *attr_fixed_sizep, vfs_context_t ctx);
errno_t vfs_attr_pack(vnode_t vp, uio_t uio, struct attrlist *alp, uint64_t options, struct vnode_attr *vap, void *fndesc, vfs_context_t ctx);
errno_t vfs_attr_pack_ext(mount_t mp, vnode_t vp, uio_t uio, struct attrlist *alp, uint64_t options, struct vnode_attr *vap, void *fndesc, vfs_context_t ctx);
int vnode_cmp_chrtoblk(vnode_t vp, vnode_t blk_vp);
vm_offset_t kdebug_vnode(vnode_t vp);
int vn_pathconf(vnode_t, int, int32_t *, vfs_context_t);
int vnode_should_flush_after_write(vnode_t vp, int ioflag);
void vfs_setowner(mount_t mp, uid_t uid, gid_t gid);
uint64_t vfs_idle_time(mount_t mp);
int vnode_usecount(vnode_t vp);
int vnode_writecount(vnode_t vp);
int vnode_iocount(vnode_t vp);
void vnode_rele_ext(vnode_t, int, int);
int is_package_name(const char *name, int len);
int vfs_context_issuser(vfs_context_t);
int vfs_context_iskernel(vfs_context_t);
vfs_context_t vfs_context_kernel(void) __pure2;
vnode_t vfs_context_cwd(vfs_context_t);
vnode_t vfs_context_get_cwd(vfs_context_t);
int vnode_isnoflush(vnode_t);
void vnode_setnoflush(vnode_t);
void vnode_clearnoflush(vnode_t);
memory_object_control_t vnode_memoryobject(vnode_t);
int vnode_issubdir(vnode_t vp, vnode_t dvp, int *is_subdir, vfs_context_t ctx);
errno_t vnio_openfd(int fd, vniodesc_t *vniop);
errno_t vnio_close(vniodesc_t);
errno_t vnio_read(vniodesc_t, uio_t);
vnode_t vnio_vnode(vniodesc_t);
int     cache_lookup_ext(vnode_t dvp, vnode_t *vpp, struct componentname *cnp,
    int flags);
int     build_path(vnode_t first_vp, char *buff, int buflen, int *outlen, int flags, vfs_context_t ctx);
boolean_t vnode_is_rsr(vnode_t);
extern int fs_buffer_cache_gc_register(void (* callout)(int, void *), void *);
extern int fs_buffer_cache_gc_unregister(void (* callout)(int, void *), void *);
__BEGIN_DECLS




extern errno_t VNOP_LOOKUP(vnode_t, vnode_t *, struct componentname *, vfs_context_t);
extern errno_t VNOP_CREATE(vnode_t, vnode_t *, struct componentname *, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_WHITEOUT(vnode_t, struct componentname *, int, vfs_context_t);
extern errno_t VNOP_MKNOD(vnode_t, vnode_t *, struct componentname *, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_OPEN(vnode_t, int, vfs_context_t);
extern int VNOP_COMPOUND_OPEN(vnode_t dvp, vnode_t *vpp, struct nameidata *ndp, int32_t flags, int32_t fmode, uint32_t *status, struct vnode_attr *vap, vfs_context_t ctx);
extern errno_t VNOP_CLOSE(vnode_t, int, vfs_context_t);
extern errno_t VNOP_ACCESS(vnode_t, int, vfs_context_t);
extern errno_t VNOP_GETATTR(vnode_t, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_SETATTR(vnode_t, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_READ(vnode_t vp, struct uio *uio, int ioflag, vfs_context_t ctx);
extern errno_t VNOP_WRITE(vnode_t vp, struct uio *uio, int ioflag, vfs_context_t ctx);
extern errno_t VNOP_IOCTL(vnode_t vp, u_long command, caddr_t data, int fflag, vfs_context_t ctx);
extern errno_t VNOP_SELECT(vnode_t, int, int, void *, vfs_context_t);
extern errno_t VNOP_EXCHANGE(vnode_t, vnode_t, int, vfs_context_t);
extern errno_t VNOP_REVOKE(vnode_t, int, vfs_context_t);
extern errno_t VNOP_MMAP_CHECK(vnode_t, int, vfs_context_t);
extern errno_t VNOP_MMAP(vnode_t, int, vfs_context_t);
extern errno_t VNOP_MNOMAP(vnode_t, vfs_context_t);
extern errno_t VNOP_FSYNC(vnode_t vp, int waitfor, vfs_context_t ctx);
extern errno_t VNOP_REMOVE(vnode_t, vnode_t, struct componentname *, int, vfs_context_t);
extern errno_t VNOP_COMPOUND_REMOVE(vnode_t, vnode_t*, struct nameidata *, int32_t flags, struct vnode_attr *vap, vfs_context_t);
extern errno_t VNOP_LINK(vnode_t, vnode_t, struct componentname *, vfs_context_t);
extern errno_t VNOP_RENAME(vnode_t, vnode_t, struct componentname *, vnode_t, vnode_t, struct componentname *, vfs_context_t);
extern errno_t VNOP_RENAMEX(vnode_t, vnode_t, struct componentname *, vnode_t, vnode_t, struct componentname *, vfs_rename_flags_t, vfs_context_t);
errno_t
VNOP_COMPOUND_RENAME(
	struct vnode *fdvp, struct vnode **fvpp, struct componentname *fcnp, struct vnode_attr *fvap,
	struct vnode *tdvp, struct vnode **tvpp, struct componentname *tcnp, struct vnode_attr *tvap,
	uint32_t flags, vfs_context_t ctx);
extern errno_t VNOP_MKDIR(vnode_t, vnode_t *, struct componentname *, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_COMPOUND_MKDIR(vnode_t, vnode_t *, struct nameidata *, struct vnode_attr *, vfs_context_t);
extern errno_t VNOP_RMDIR(vnode_t, vnode_t, struct componentname *, vfs_context_t);
extern errno_t VNOP_COMPOUND_RMDIR(vnode_t, vnode_t*, struct nameidata *, struct vnode_attr *vap, vfs_context_t);
extern errno_t VNOP_SYMLINK(vnode_t, vnode_t *, struct componentname *, struct vnode_attr *, char *, vfs_context_t);
extern errno_t VNOP_READDIR(vnode_t, struct uio *, int, int *, int *, vfs_context_t);
extern errno_t VNOP_READDIRATTR(vnode_t, struct attrlist *, struct uio *, uint32_t, uint32_t, uint32_t *, int *, uint32_t *, vfs_context_t);
extern errno_t VNOP_GETATTRLISTBULK(vnode_t, struct attrlist *, struct vnode_attr *, uio_t, void *, uint64_t, int32_t *, int32_t *, vfs_context_t);
extern errno_t VNOP_READLINK(vnode_t, struct uio *, vfs_context_t);
extern errno_t VNOP_INACTIVE(vnode_t, vfs_context_t);
extern errno_t VNOP_RECLAIM(vnode_t, vfs_context_t);
extern errno_t VNOP_PATHCONF(vnode_t, int, int32_t *, vfs_context_t);
extern errno_t VNOP_ADVLOCK(vnode_t, caddr_t, int, struct flock *, int, vfs_context_t, struct timespec *);
extern errno_t VNOP_ALLOCATE(vnode_t, off_t, u_int32_t, off_t *, off_t, vfs_context_t);
extern errno_t VNOP_PAGEIN(vnode_t, upl_t, upl_offset_t, off_t, size_t, int, vfs_context_t);
extern errno_t VNOP_PAGEOUT(vnode_t, upl_t, upl_offset_t, off_t, size_t, int, vfs_context_t);
extern errno_t VNOP_SEARCHFS(vnode_t, void *, void *, struct attrlist *, uint32_t, struct timeval *, struct attrlist *, uint32_t *, uint32_t, uint32_t, struct uio *, struct searchstate *, vfs_context_t);
extern errno_t VNOP_COPYFILE(vnode_t, vnode_t, vnode_t, struct componentname *, int, int, vfs_context_t);
extern errno_t VNOP_CLONEFILE(vnode_t, vnode_t, vnode_t *, struct componentname *, struct vnode_attr *, uint32_t, vfs_context_t);
extern errno_t VNOP_GETXATTR(vnode_t vp, const char *name, uio_t uio, size_t *size, int options, vfs_context_t ctx);
extern errno_t VNOP_SETXATTR(vnode_t vp, const char *name, uio_t uio, int options, vfs_context_t ctx);
extern errno_t VNOP_REMOVEXATTR(vnode_t, const char *, int, vfs_context_t);
extern errno_t VNOP_LISTXATTR(vnode_t, uio_t, size_t *, int, vfs_context_t);
extern errno_t VNOP_BLKTOOFF(vnode_t, daddr64_t, off_t *);
extern errno_t VNOP_OFFTOBLK(vnode_t, off_t, daddr64_t *);
extern errno_t VNOP_BLOCKMAP(vnode_t, off_t, size_t, daddr64_t *, size_t *, void *,
    int, vfs_context_t);
extern errno_t VNOP_STRATEGY(struct buf *bp);
extern errno_t VNOP_BWRITE(buf_t bp);
extern errno_t VNOP_KQFILT_ADD(vnode_t, struct knote *, vfs_context_t);
errno_t VNOP_KQFILT_REMOVE(vnode_t, uintptr_t, vfs_context_t);
errno_t VNOP_MONITOR(vnode_t vp, uint32_t events, uint32_t flags, void *handle, vfs_context_t ctx);
errno_t VNOP_SETLABEL(vnode_t, struct label *, vfs_context_t);
LIST_HEAD(buflists, buf);
void    cvtstat(struct stat *st, struct ostat *ost);
void    vprint(const char *label, struct vnode *vp);
__private_extern__ int set_package_extensions_table(user_addr_t data, int nentries, int maxwidth);
int     vn_setlabel(struct vnode *vp, struct label *intlabel,
    vfs_context_t context);
void    fifo_printinfo(struct vnode *vp);
int     vn_open(struct nameidata *ndp, int fmode, int cmode);
int     vn_open_modflags(struct nameidata *ndp, int *fmode, int cmode);
int     vn_open_auth(struct nameidata *ndp, int *fmode, struct vnode_attr *, vnode_t authvp);
int     vn_close(vnode_t, int flags, vfs_context_t ctx);
errno_t vn_remove(vnode_t dvp, vnode_t *vpp, struct nameidata *ndp, int32_t flags, struct vnode_attr *vap, vfs_context_t ctx);
errno_t vn_rename(struct vnode *fdvp, struct vnode **fvpp, struct componentname *fcnp, struct vnode_attr *fvap,
    struct vnode *tdvp, struct vnode **tvpp, struct componentname *tcnp, struct vnode_attr *tvap,
    uint32_t flags, vfs_context_t ctx);
void    lock_vnode_and_post(vnode_t, int);
int     vn_authorize_open_existing(vnode_t vp, struct componentname *cnp, int fmode, vfs_context_t ctx, void *reserved);
int     vn_authorize_create(vnode_t, struct componentname *, struct vnode_attr *, vfs_context_t, void*);
int     vn_attribute_prepare(vnode_t dvp, struct vnode_attr *vap, uint32_t *defaulted_fieldsp, vfs_context_t ctx);
void    vn_attribute_cleanup(struct vnode_attr *vap, uint32_t defaulted_fields);
int     vn_authorize_rename(struct vnode *fdvp, struct vnode *fvp, struct componentname *fcnp,
    struct vnode *tdvp, struct vnode *tvp, struct componentname *tcnp,
    vfs_context_t ctx, void *reserved);
int vn_authorize_renamex(struct vnode *fdvp, struct vnode *fvp, struct componentname *fcnp,
    struct vnode *tdvp, struct vnode *tvp, struct componentname *tcnp,
    vfs_context_t ctx, vfs_rename_flags_t flags, void *reserved);
int vn_authorize_renamex_with_paths(struct vnode *fdvp, struct vnode *fvp, struct componentname *fcnp, const char *from_path,
    struct vnode *tdvp, struct vnode *tvp, struct componentname *tcnp, const char *to_path,
    vfs_context_t ctx, vfs_rename_flags_t flags, void *reserved);
int vn_authorize_mkdir(vnode_t, struct componentname *, struct vnode_attr *, vfs_context_t, void*);
int vn_authorize_null(vnode_t, struct componentname *, struct vnode_attr *, vfs_context_t, void*);
int vnode_attr_authorize_dir_clone(struct vnode_attr *vap, kauth_action_t action,
    struct vnode_attr *dvap, vnode_t sdvp, mount_t mp, dir_clone_authorizer_op_t vattr_op,
    uint32_t flags, vfs_context_t ctx, void *reserved);
void vnode_attr_handle_uid_and_gid(struct vnode_attr *vap, mount_t mp, vfs_context_t ctx);
errno_t vn_create(vnode_t, vnode_t *, struct nameidata *, struct vnode_attr *, uint32_t, int, uint32_t*, vfs_context_t);
int     vn_mkdir(vnode_t dvp, vnode_t *vpp, struct nameidata *ndp, struct vnode_attr *vap, vfs_context_t ctx);
int     vn_rmdir(vnode_t dvp, vnode_t *vpp, struct nameidata *ndp, struct vnode_attr *vap, vfs_context_t ctx);
int     vn_getxattr(vnode_t, const char *, uio_t, size_t *, int, vfs_context_t);
int     vn_setxattr(vnode_t, const char *, uio_t, int, vfs_context_t);
int     vn_removexattr(vnode_t, const char *, int, vfs_context_t);
int     vn_listxattr(vnode_t, uio_t, size_t *, int, vfs_context_t);
void    nchinit(void);
int     resize_namecache(int newsize);
void    name_cache_lock_shared(void);
boolean_t    name_cache_lock_shared_to_exclusive(void);
void    name_cache_lock(void);
void    name_cache_unlock(void);
void    cache_enter_with_gen(vnode_t dvp, vnode_t vp, struct componentname *cnp, int gen);
const char *cache_enter_create(vnode_t dvp, vnode_t vp, struct componentname *cnp);
void    vnode_lock_spin(vnode_t);
void    vnode_list_lock(void);
void    vnode_list_unlock(void);
int     vnode_ref_ext(vnode_t, int, int);
void    vnode_rele_internal(vnode_t, int, int, int);
int     vnode_getalways(vnode_t);
int     vnode_getalways_from_pager(vnode_t);
int     vget_internal(vnode_t, int, int);
errno_t vnode_getiocount(vnode_t, unsigned int, int);
vnode_t vnode_getparent_if_different(vnode_t, vnode_t);
void    vnode_link_lock(vnode_t);
void    vnode_link_unlock(vnode_t);
int     vnode_get_locked(vnode_t);
int     vnode_put_locked(vnode_t);
int     vnode_put_from_pager(vnode_t);
vnode_t vnode_drop_and_unlock(vnode_t);
int     vnode_issock(vnode_t);
int     vnode_isaliased(vnode_t);
void    unlock_fsnode(vnode_t, int *);
int     lock_fsnode(vnode_t, int *);
errno_t vnode_resume(vnode_t);
errno_t vnode_suspend(vnode_t);
errno_t vnode_mtime(vnode_t, struct timespec *, vfs_context_t);
errno_t vnode_flags(vnode_t, uint32_t *, vfs_context_t);
errno_t vnode_size(vnode_t, off_t *, vfs_context_t);
errno_t vnode_setsize(vnode_t, off_t, int ioflag, vfs_context_t);
int     vnode_setattr_fallback(vnode_t vp, struct vnode_attr *vap, vfs_context_t ctx);
int     vnode_isspec(vnode_t vp);
int     vnode_compound_rename_available(vnode_t vp);
int     vnode_compound_rmdir_available(vnode_t vp);
int     vnode_compound_mkdir_available(vnode_t vp);
int     vnode_compound_remove_available(vnode_t vp);
int     vnode_compound_open_available(vnode_t vp);
int     vnode_compound_op_available(vnode_t, compound_vnop_id_t);
void vn_setunionwait(vnode_t);
void vn_checkunionwait(vnode_t);
void vn_clearunionwait(vnode_t, int);
void SPECHASH_LOCK(void);
void SPECHASH_UNLOCK(void);
lck_mtx_t * SPECHASH_LOCK_ADDR(void);
void    vnode_authorize_init(void);
void    vfsinit(void);
void vnode_lock(vnode_t);
void vnode_unlock(vnode_t);
void vn_print_state(vnode_t , const char * , ...)
__printflike(2, 3);
void    vfs_op_init(void);
void    vfs_opv_init(void);
void vnode_setneedinactive(vnode_t);
int     vnode_hasnamedstreams(vnode_t);
errno_t
vnode_readdir64(struct vnode *vp, struct uio *uio, int flags, int *eofflag,
    int *numdirent, vfs_context_t ctxp);
void vnode_setswapmount(vnode_t);
int64_t vnode_getswappin_avail(vnode_t);
int vnode_get_snapdir(vnode_t, vnode_t *, vfs_context_t);
int     build_path_with_parent(vnode_t, vnode_t , char *, int, int *, size_t *, int, vfs_context_t);
void    nspace_resolver_init(void);
void    nspace_resolver_exited(struct proc *);
int     vnode_isinuse_locked(vnode_t, int, int );
kauth_cred_t vnode_cred(vnode_t);
int     fsgetpath_internal(vfs_context_t, int, uint64_t, vm_size_t, caddr_t, uint32_t options, int *);
bool vfs_context_allow_fs_blksize_nocache_write(vfs_context_t);
bool vnode_hold_smr(vnode_t);
extern int vsock_add_transport(struct vsock_transport *transport);
extern int vsock_remove_transport(struct vsock_transport *transport);
extern int vsock_reset_transport(struct vsock_transport *transport);
extern int vsock_put_message(struct vsock_address src, struct vsock_address dst,
    enum vsock_operation op, uint32_t buf_alloc, uint32_t fwd_cnt, mbuf_t m);
int     __work_interval_ctl(uint32_t operation, uint64_t work_interval_id, void *arg, size_t len);
__BEGIN_DECLS
int  xattr_protected(const char *);
int  xattr_validatename(const char *);
int      select(int, fd_set * __restrict, fd_set * __restrict,
    fd_set * __restrict, struct timeval * __restrict)

;
UUID_DEFINE(UUID_NULL, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
void uuid_clear(uuid_t uu);
int uuid_compare(const uuid_t uu1, const uuid_t uu2);
void uuid_copy(uuid_t dst, const uuid_t src);
void uuid_generate(uuid_t out);
void uuid_generate_random(uuid_t out);
void uuid_generate_time(uuid_t out);
void uuid_generate_early_random(uuid_t out);
int uuid_is_null(const uuid_t uu);
int uuid_parse(const uuid_string_t in, uuid_t uu);
void uuid_unparse(const uuid_t uu, uuid_string_t out);
void uuid_unparse_lower(const uuid_t uu, uuid_string_t out);
void uuid_unparse_upper(const uuid_t uu, uuid_string_t out);
int disk_conditioner_get_info(mount_t, disk_conditioner_info *);
int disk_conditioner_set_info(mount_t, disk_conditioner_info *);
boolean_t disk_conditioner_mount_is_ssd(mount_t);
void vfs_exclave_fs_stop(void);
int vfs_exclave_fs_register(uint32_t fs_tag, vnode_t vp);
int vfs_exclave_fs_unregister(vnode_t vp);
int vfs_exclave_fs_get_base_dirs(void *buf, uint32_t *count);
int vfs_exclave_fs_register_path(uint32_t fs_tag, const char *base_path);
int vfs_exclave_fs_root(const char *exclave_id, uint64_t *root_id);
int vfs_exclave_fs_root_ex(uint32_t fs_tag, const char *exclave_id, uint64_t *root_id);
int vfs_exclave_fs_open(uint32_t fs_tag, uint64_t root_id, const char *name, uint64_t *file_id);
int vfs_exclave_fs_close(uint32_t fs_tag, uint64_t file_id);
int vfs_exclave_fs_create(uint32_t fs_tag, uint64_t root_id, const char *name, uint64_t *file_id);
int vfs_exclave_fs_read(uint32_t fs_tag, uint64_t file_id, uint64_t file_offset, uint64_t length, void *data);
int vfs_exclave_fs_write(uint32_t fs_tag, uint64_t file_id, uint64_t file_offset, uint64_t length, void *data);
int vfs_exclave_fs_remove(uint32_t fs_tag, uint64_t root_id, const char *name);
int vfs_exclave_fs_sync(uint32_t fs_tag, uint64_t file_id, uint64_t sync_op);
int vfs_exclave_fs_readdir(uint32_t fs_tag, uint64_t file_id, void *dirent_buf,
    uint32_t buf_size, int32_t *count);
int vfs_exclave_fs_getsize(uint32_t fs_tag, uint64_t file_id, uint64_t *size);
int vfs_exclave_fs_sealstate(uint32_t fs_tag, bool *sealed);
int vfs_exclave_fs_query_volume_group(const uuid_string_t vguuid, bool *exists);
void io_compression_stats_init(void);
void io_compression_stats(buf_t bp);
__BEGIN_DECLS
extern int nop_create(struct vnop_create_args *ap);
extern int err_create(struct vnop_create_args *ap);
extern int nop_whiteout(struct vnop_whiteout_args *ap);
extern int err_whiteout(struct vnop_whiteout_args *ap);
extern int nop_mknod(struct vnop_mknod_args *ap);
extern int err_mknod(struct vnop_mknod_args *ap);
extern int nop_open(struct vnop_open_args *ap);
extern int err_open(struct vnop_open_args *ap);
extern int nop_close(struct vnop_close_args *ap);
extern int err_close(struct vnop_close_args *ap);
extern int nop_access(struct vnop_access_args *ap);
extern int err_access(struct vnop_access_args *ap);
extern int nop_getattr(struct vnop_getattr_args *ap);
extern int err_getattr(struct vnop_getattr_args *ap);
extern int nop_setattr(struct vnop_setattr_args *ap);
extern int err_setattr(struct vnop_setattr_args *ap);
extern int nop_read(struct vnop_read_args *ap);
extern int err_read(struct vnop_read_args *ap);
extern int nop_write(struct vnop_write_args *ap);
extern int err_write(struct vnop_write_args *ap);
extern int nop_ioctl(struct vnop_ioctl_args *ap);
extern int err_ioctl(struct vnop_ioctl_args *ap);
extern int nop_select(struct vnop_select_args *ap);
extern int err_select(struct vnop_select_args *ap);
extern int nop_exchange(struct vnop_exchange_args *ap);
extern int err_exchange(struct vnop_exchange_args *ap);
extern int nop_revoke(struct vnop_revoke_args *ap);
extern int err_revoke(struct vnop_revoke_args *ap);
extern int nop_mmap(struct vnop_mmap_args *ap);
extern int err_mmap(struct vnop_mmap_args *ap);
extern int nop_fsync(struct vnop_fsync_args *ap);
extern int err_fsync(struct vnop_fsync_args *ap);
extern int nop_remove(struct vnop_remove_args *ap);
extern int err_remove(struct vnop_remove_args *ap);
extern int nop_link(struct vnop_link_args *ap);
extern int err_link(struct vnop_link_args *ap);
extern int nop_rename(struct vnop_rename_args *ap);
extern int err_rename(struct vnop_rename_args *ap);
extern int nop_mkdir(struct vnop_mkdir_args *ap);
extern int err_mkdir(struct vnop_mkdir_args *ap);
extern int nop_rmdir(struct vnop_rmdir_args *ap);
extern int err_rmdir(struct vnop_rmdir_args *ap);
extern int nop_symlink(struct vnop_symlink_args *ap);
extern int err_symlink(struct vnop_symlink_args *ap);
extern int nop_readdir(struct vnop_readdir_args *ap);
extern int err_readdir(struct vnop_readdir_args *ap);
extern int nop_readdirattr(struct vnop_readdirattr_args *ap);
extern int err_readdirattr(struct vnop_readdirattr_args *ap);
extern int nop_readlink(struct vnop_readlink_args *ap);
extern int err_readlink(struct vnop_readlink_args *ap);
extern int nop_inactive(struct vnop_inactive_args *ap);
extern int err_inactive(struct vnop_inactive_args *ap);
extern int nop_reclaim(struct vnop_reclaim_args *ap);
extern int err_reclaim(struct vnop_reclaim_args *ap);
extern int nop_strategy(struct vnop_strategy_args *ap);
extern int err_strategy(struct vnop_strategy_args *ap);
extern int nop_pathconf(struct vnop_pathconf_args *ap);
extern int err_pathconf(struct vnop_pathconf_args *ap);
extern int nop_advlock(struct vnop_advlock_args *ap);
extern int err_advlock(struct vnop_advlock_args *ap);
extern int nop_allocate(struct vnop_allocate_args *ap);
extern int err_allocate(struct vnop_allocate_args *ap);
extern int nop_bwrite(struct vnop_bwrite_args *ap);
extern int err_bwrite(struct vnop_bwrite_args *ap);
extern int nop_pagein(struct vnop_pagein_args *ap);
extern int err_pagein(struct vnop_pagein_args *ap);
extern int nop_pageout(struct vnop_pageout_args *ap);
extern int err_pageout(struct vnop_pageout_args *ap);
extern int nop_searchfs(struct vnop_searchfs_args *ap);
extern int err_searchfs(struct vnop_searchfs_args *ap);
extern int nop_copyfile(struct vnop_copyfile_args *ap);
extern int err_copyfile(struct vnop_copyfile_args *ap);
extern int nop_blktooff(struct vnop_blktooff_args *ap);
extern int err_blktooff(struct vnop_blktooff_args *ap);
extern int nop_offtoblk(struct vnop_offtoblk_args *ap);
extern int err_offtoblk(struct vnop_offtoblk_args *ap);
extern int nop_blockmap(struct vnop_blockmap_args *ap);
extern int err_blockmap(struct vnop_blockmap_args *ap);
extern int nop_monitor(struct vnop_monitor_args *ap);
extern int err_monitor(struct vnop_monitor_args *ap);
static const u_int32_t __UniCharDecompositionTableLength =
    (sizeof(__CFUniCharDecompositionTable) / (sizeof(u_int16_t) * 2));
static const u_int32_t __CFUniCharPrecompositionTableLength =
    (sizeof(__CFUniCharPrecompSourceTable) / (sizeof(u_int32_t) * 2));
__attribute__((always_inline))
static inline slot_idx_t
KR_SLOT_INDEX(const struct __kern_channel_ring *kr,
    const struct __slot_desc *slot)
{
	ASSERT(slot >= kr->ckr_ksds && slot <= kr->ckr_ksds_last);
	return (slot_idx_t)(slot - kr->ckr_ksds);
}



































__attribute__((always_inline))
static inline uint32_t
kr_available_slots(struct __kern_channel_ring *kr)
{
	uint32_t space;

	space = kr->ckr_lim - (kr->ckr_num_slots - kr->ckr_khead);
	return space;
}


__attribute__((always_inline))
static inline uint32_t
kr_available_slots_rxring(struct __kern_channel_ring *rxkring)
{
	int busy;
	uint32_t space;

	
	busy = (int)(rxkring->ckr_ktail - rxkring->ckr_khead);
	if (busy < 0) {
		busy += rxkring->ckr_num_slots;
	}

	
	space = rxkring->ckr_lim - (uint32_t)busy;
	return space;
}

extern kern_allocation_name_t skmem_tag_ch_key;
__BEGIN_DECLS
extern int channel_init(void);
extern void channel_fini(void);
extern struct kern_channel *ch_open(struct ch_init *, struct proc *,
    int, int *);
extern struct kern_channel *ch_open_special(struct kern_nexus *,
    struct chreq *, boolean_t, int *);
extern void ch_close(struct kern_channel *, boolean_t);
extern void ch_close_special(struct kern_channel *);
extern int ch_kqfilter(struct kern_channel *, struct knote *,
    struct kevent_qos_s *kev);
extern boolean_t ch_is_multiplex(struct kern_channel *, enum txrx);
extern int ch_select(struct kern_channel *, int, void *, struct proc *);
extern int ch_get_opt(struct kern_channel *, struct sockopt *);
extern int ch_set_opt(struct kern_channel *, struct sockopt *);
extern void ch_deactivate(struct kern_channel *);
extern void ch_retain(struct kern_channel *);
extern void ch_retain_locked(struct kern_channel *);
extern int ch_release(struct kern_channel *);
extern int ch_release_locked(struct kern_channel *);
extern void ch_dtor(struct kern_channel *);
extern void csi_init(struct ch_selinfo *, boolean_t, uint64_t);
extern void csi_destroy(struct ch_selinfo *);
extern void csi_selrecord_one(struct __kern_channel_ring *, struct proc *,
    void *);
extern void csi_selrecord_all(struct nexus_adapter *, enum txrx, struct proc *,
    void *);
extern void csi_selwakeup_one(struct __kern_channel_ring *, boolean_t,
    boolean_t, boolean_t, uint32_t);
extern void csi_selwakeup_all(struct nexus_adapter *, enum txrx, boolean_t,
    boolean_t, boolean_t, uint32_t);
extern void kr_init_to_mhints(struct __kern_channel_ring *, uint32_t);
extern int kr_enter(struct __kern_channel_ring *, boolean_t);
extern void kr_exit(struct __kern_channel_ring *);
extern void kr_start(struct __kern_channel_ring *);
extern void kr_stop(struct __kern_channel_ring *kr, uint32_t state);
extern void kr_update_stats(struct __kern_channel_ring *kring,
    uint32_t slot_count, uint32_t byte_count);
extern boolean_t kr_txempty(struct __kern_channel_ring *kring);
extern uint32_t kr_reclaim(struct __kern_channel_ring *kr);
extern slot_idx_t kr_txsync_prologue(struct kern_channel *,
    struct __kern_channel_ring *, struct proc *);
extern int kr_txprologue(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, uint32_t *, uint64_t *,
    struct proc *);
extern int kr_txprologue_upp(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, uint32_t *, uint64_t *,
    struct proc *);
extern void kr_txsync_finalize(struct kern_channel *,
    struct __kern_channel_ring *, struct proc *);
extern void kr_txfinalize(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, struct proc *p);
extern void kr_txfinalize_upp(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, struct proc *p);
extern slot_idx_t kr_rxsync_prologue(struct kern_channel *ch,
    struct __kern_channel_ring *kring, struct proc *p);
extern int kr_rxprologue(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, uint32_t *, uint64_t *,
    struct proc *);
extern int kr_rxprologue_nodetach(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, uint32_t *, uint64_t *,
    struct proc *);
extern int kr_rxprologue_upp(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, uint32_t *, uint64_t *,
    struct proc *);
extern void kr_rxsync_finalize(struct kern_channel *ch,
    struct __kern_channel_ring *kring, struct proc *p);
extern void kr_rxfinalize(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, struct proc *p);
extern void kr_rxfinalize_upp(struct kern_channel *,
    struct __kern_channel_ring *, const slot_idx_t, struct proc *p);
extern void kr_txkring_reclaim_and_refill(struct __kern_channel_ring *kring,
    slot_idx_t index);
extern slot_idx_t kr_alloc_sync_prologue(struct __kern_channel_ring *kring,
    struct proc *p);
extern slot_idx_t kr_free_sync_prologue(struct __kern_channel_ring *kring,
    struct proc *p);
extern void kr_alloc_sync_finalize(struct __kern_channel_ring *kring,
    struct proc *p);
extern void kr_free_sync_finalize(struct __kern_channel_ring *kring,
    struct proc *p);
extern int kr_internalize_metadata(struct kern_channel *,
    struct __kern_channel_ring *, const uint32_t, struct __kern_quantum *,
    struct proc *);
extern void kr_externalize_metadata(struct __kern_channel_ring *,
    const uint32_t, struct __kern_quantum *, struct proc *);
extern slot_idx_t kr_event_sync_prologue(struct __kern_channel_ring *kring,
    struct proc *p);
extern void kr_event_sync_finalize(struct kern_channel *ch,
    struct __kern_channel_ring *kring, struct proc *p);
__BEGIN_DECLS
extern errno_t kern_channel_event_transmit_status_with_packet(
	const kern_packet_t, const ifnet_t);
extern void kern_channel_event_notify(struct __kern_channel_ring *);
extern int kern_channel_event_sync(struct __kern_channel_ring *, struct proc *,
    uint32_t);
__END_DECLS

__BEGIN_DECLS

extern errno_t kern_channel_event_transmit_status(const ifnet_t,
    os_channel_event_packet_transmit_status_t *, uint32_t);
extern errno_t kern_channel_event_transmit_status_with_nexus(const uuid_t,
    os_channel_event_packet_transmit_status_t *, uint32_t);
extern errno_t kern_channel_event_transmit_expired(const ifnet_t,
    os_channel_event_packet_transmit_expired_t *, uint32_t);
extern errno_t kern_channel_event_transmit_expired_with_nexus(const uuid_t,
    os_channel_event_packet_transmit_expired_t *, uint32_t);
__attribute__((always_inline))
static inline void
__sk_copy64_8(uint64_t *src, uint64_t *dst)
{
	*dst = *src;            
}


__attribute__((always_inline))
static inline void
__sk_copy32_8(uint32_t *__counted_by(2)src, uint32_t *__counted_by(2)dst)
{
}


static inline void
__sk_copy64_16(uint64_t *__counted_by(2) src, uint64_t *__counted_by(2) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
}


__attribute__((always_inline))
static inline void
__sk_copy32_16(uint32_t *__counted_by(4) src, uint32_t *__counted_by(4) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	dst[2] = src[2]; 
	dst[3] = src[3]; 
}


__attribute__((always_inline))
static inline void
__sk_copy64_20(uint64_t *__sized_by(20) src, uint64_t *__sized_by(20) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	*(uint32_t *)(dst + 2) = *(uint32_t *)(src + 2); 
}


__attribute__((always_inline))
static inline void
__sk_copy64_24(uint64_t *__counted_by(3) src, uint64_t *__counted_by(3) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	dst[2] = src[2]; 
}


__attribute__((always_inline))
static inline void
__sk_copy64_32(uint64_t *__counted_by(4) src, uint64_t *__counted_by(4) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	dst[2] = src[2]; 
	dst[3] = src[3]; 
}


__attribute__((always_inline))
static inline void
__sk_copy32_32(uint32_t *__counted_by(8) src, uint32_t *__counted_by(8) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	dst[2] = src[2]; 
	dst[3] = src[3]; 
	dst[4] = src[4]; 
	dst[5] = src[5]; 
	dst[6] = src[6]; 
	dst[7] = src[7]; 
}


__attribute__((always_inline))
static inline void
__sk_copy64_40(uint64_t *__sized_by(40) src, uint64_t *__sized_by(40) dst)
{
	dst[0] = src[0]; 
	dst[1] = src[1]; 
	dst[2] = src[2]; 
	dst[3] = src[3]; 
	dst[4] = src[4]; 
}


__attribute__((always_inline))
static inline void
__sk_vcopy64_16(uint64_t *__counted_by(2) src, uint64_t *__counted_by(2) dst)
{
	
	
	__asm__ __volatile__ (
                "ldr	q0, [%[src]]		\n\t"
                "str	q0, [%[dst]]		\n\t"
                :
                : [src] "r" ((uint64_t *__unsafe_indexable)src), [dst] "r" ((uint64_t *__unsafe_indexable)dst)
                : "v0", "memory"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_vcopy32_16(uint32_t *__counted_by(4) src, uint32_t *__counted_by(4) dst)
{
	
	__sk_vcopy64_16((uint64_t *)(void *)src, (uint64_t *)(void *)dst);
}


__attribute__((always_inline))
static inline void
__sk_vcopy64_20(uint64_t *__sized_by(20) src, uint64_t *__sized_by(20) dst)
{
	
	
	__asm__ __volatile__ (
                "ldr	q0, [%[src]]		\n\t"
                "str	q0, [%[dst]]		\n\t"
                "ldr	s0, [%[src], #16]	\n\t"
                "str	s0, [%[dst], #16]	\n\t"
                :
                : [src] "r" ((uint64_t *__unsafe_indexable)src), [dst] "r" ((uint64_t *__unsafe_indexable)dst)
                : "v0", "memory"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_vcopy64_24(uint64_t *__counted_by(3) src, uint64_t *__counted_by(3) dst)
{
	
	
	__asm__ __volatile__ (
                "ldr	q0, [%[src]]		\n\t"
                "str	q0, [%[dst]]		\n\t"
                "ldr	d0, [%[src], #16]	\n\t"
                "str	d0, [%[dst], #16]	\n\t"
                :
                : [src] "r" ((uint64_t *__unsafe_indexable)src), [dst] "r" ((uint64_t *__unsafe_indexable)dst)
                : "v0", "memory"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_vcopy64_32(uint64_t *__counted_by(4) src, uint64_t *__counted_by(4) dst)
{
	
	
	__asm__ __volatile__ (
                "ldp	q0, q1, [%[src]]	\n\t"
                "stp	q0, q1, [%[dst]]	\n\t"
                :
                : [src] "r" ((uint64_t *__unsafe_indexable)src), [dst] "r" ((uint64_t *__unsafe_indexable)dst)
                : "v0", "v1", "memory"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_vcopy32_32(uint32_t *__counted_by(8) src, uint32_t *__counted_by(8) dst)
{
	
	__sk_vcopy64_32((uint64_t *)(void *)src, (uint64_t *)(void *)dst);
}


__attribute__((always_inline))
static inline void
__sk_vcopy64_40(uint64_t *__sized_by(40) src, uint64_t *__sized_by(40) dst)
{
	
	
	__asm__ __volatile__ (
                "ldp	q0, q1, [%[src]]	\n\t"
                "stp	q0, q1, [%[dst]]	\n\t"
                "ldr	d0, [%[src], #32]	\n\t"
                "str	d0, [%[dst], #32]	\n\t"
                :
                : [src] "r" ((uint64_t *__unsafe_indexable)src), [dst] "r" ((uint64_t *__unsafe_indexable)dst)
                : "v0", "v1", "memory"
        );
	
}




__attribute__((always_inline))
static inline void
__sk_zero_16(void *p)
{
	
	
	__asm__ __volatile__ (
                "stp	xzr, xzr, [%[p]]	\n\t"
                :
                : [p] "r" (p)
                : "memory"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_zero_32(void *p)
{
	
	
	__asm__ __volatile__ (
                "eor.16b v0, v0, v0		\n\t"
                "stp	 q0, q0, [%[p]]		\n\t"
                :
                : [p] "r" (p)
                : "v0", "memory", "cc"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_zero_48(void *p)
{
	
	
	__asm__ __volatile__ (
                "eor.16b v0, v0, v0		\n\t"
                "stp	 q0, q0, [%[p]]		\n\t"
                "str	 q0, [%[p], #32]	\n\t"
                :
                : [p] "r" (p)
                : "v0", "memory", "cc"
        );
	
}


__attribute__((always_inline))
static inline void
__sk_zero_128(void *p)
{
	
	
	__asm__ __volatile__ (
                "eor.16b v0, v0, v0		\n\t"
                "stp	 q0, q0, [%[p]]		\n\t"
                "stp	 q0, q0, [%[p], #32]	\n\t"
                "stp	 q0, q0, [%[p], #64]	\n\t"
                "stp	 q0, q0, [%[p], #96]	\n\t"
                :
                : [p] "r" (p)
                : "v0", "memory", "cc"
        );
	
}







__attribute__((always_inline))
static inline void
sk_copy64_4x(uint32_t *__sized_by(l)src, uint32_t *__sized_by(l)dst, size_t l)
{
		int i;

		
		uint32_t *__unsafe_indexable src_unsafe = src;
		uint32_t *__unsafe_indexable dst_unsafe = dst;
		for (i = 0; i < l / 4; i++) {
			dst_unsafe[i] = src_unsafe[i]; 
		}
	} else {
		(void) memcpy((void *)dst, (void *)src, l);
	}
}


__attribute__((always_inline))
static inline void
sk_copy64_8x(uint64_t *__sized_by(l)src, uint64_t *__sized_by(l)dst, size_t l)
{
		int i;

		
		uint64_t *__unsafe_indexable src_unsafe = src;
		uint64_t *__unsafe_indexable dst_unsafe = dst;
		for (i = 0; i < l / 8; i++) {
			dst_unsafe[i] = src_unsafe[i]; 
		}
	} else {
		(void) memcpy((void *)dst, (void *)src, l);
	}
}


__attribute__((always_inline))
static inline void
sk_copy64_32x(uint64_t *__sized_by(l)src, uint64_t *__sized_by(l)dst, size_t l)
{
		int n, i;

		
		uint64_t *__unsafe_indexable src_unsafe = src;
		uint64_t *__unsafe_indexable dst_unsafe = dst;
		for (n = 0; n < l / 32; n++) {
			i = n * 4;
			dst_unsafe[i] = src_unsafe[i];         
			dst_unsafe[i + 1] = src_unsafe[i + 1]; 
			dst_unsafe[i + 2] = src_unsafe[i + 2]; 
			dst_unsafe[i + 3] = src_unsafe[i + 3]; 
		}
	} else {
		(void) memcpy((void *)dst, (void *)src, l);
	}
}


__attribute__((always_inline))
static inline void
sk_copy64_64x(uint64_t *__sized_by(l)src, uint64_t *__sized_by(l)dst, size_t l)
{
		int n, i;

		
		uint64_t *__unsafe_indexable src_unsafe = src;
		uint64_t *__unsafe_indexable dst_unsafe = dst;
		for (n = 0; n < l / 64; n++) {
			i = n * 8;
			dst_unsafe[i] = src_unsafe[i];         
			dst_unsafe[i + 1] = src_unsafe[i + 1]; 
			dst_unsafe[i + 2] = src_unsafe[i + 2]; 
			dst_unsafe[i + 3] = src_unsafe[i + 3]; 
			dst_unsafe[i + 4] = src_unsafe[i + 4]; 
			dst_unsafe[i + 5] = src_unsafe[i + 5]; 
			dst_unsafe[i + 6] = src_unsafe[i + 6]; 
			dst_unsafe[i + 7] = src_unsafe[i + 7]; 
		}
	} else {
		(void) memcpy((void *)dst, (void *)src, l);
	}
}














































struct sk_tag_spec {
	kern_allocation_name_t *skt_var;
	const char             *skt_name;
};
extern void __sk_tag_make(const struct sk_tag_spec *spec);
static inline int
__sk_memcmp_mask_scalar(const uint8_t *__counted_by(n)src1,
    const uint8_t *__counted_by(n)src2,
    const uint8_t *__counted_by(n)byte_mask, size_t n)
{
	uint32_t result = 0;
	for (size_t i = 0; i < n; i++) {
		result |= (src1[i] ^ src2[i]) & byte_mask[i];
	}
	return result;
}

static inline int
__sk_memcmp_mask_16B_scalar(const uint8_t *__counted_by(16)src1,
    const uint8_t *__counted_by(16)src2,
    const uint8_t *__counted_by(16)byte_mask)
{
	return __sk_memcmp_mask_scalar(src1, src2, byte_mask, 16);
}

static inline int
__sk_memcmp_mask_32B_scalar(const uint8_t *__counted_by(32)src1,
    const uint8_t *__counted_by(32)src2,
    const uint8_t *__counted_by(32)byte_mask)
{
	return __sk_memcmp_mask_scalar(src1, src2, byte_mask, 32);
}

static inline int
__sk_memcmp_mask_48B_scalar(const uint8_t *__counted_by(48)src1,
    const uint8_t *__counted_by(48)src2,
    const uint8_t *__counted_by(48)byte_mask)
{
	return __sk_memcmp_mask_scalar(src1, src2, byte_mask, 48);
}

static inline int
__sk_memcmp_mask_64B_scalar(const uint8_t *__counted_by(64)src1,
    const uint8_t *__counted_by(64)src2,
    const uint8_t *__counted_by(64)byte_mask)
{
	return __sk_memcmp_mask_scalar(src1, src2, byte_mask, 64);
}

static inline int
__sk_memcmp_mask_80B_scalar(const uint8_t *__counted_by(80)src1,
    const uint8_t *__counted_by(80)src2,
    const uint8_t *__counted_by(80)byte_mask)
{
	return __sk_memcmp_mask_scalar(src1, src2, byte_mask, 80);
}

extern int os_memcmp_mask_16B(const uint8_t *__counted_by(16)src1,
    const uint8_t *__counted_by(16)src2,
    const uint8_t *__counted_by(16)byte_mask);
extern int os_memcmp_mask_32B(const uint8_t *__counted_by(32)src1,
    const uint8_t *__counted_by(32)src2,
    const uint8_t *__counted_by(32)byte_mask);
extern int os_memcmp_mask_48B(const uint8_t *__counted_by(48)src1,
    const uint8_t *__counted_by(48)src2,
    const uint8_t *__counted_by(48)byte_mask);
extern int os_memcmp_mask_64B(const uint8_t *__counted_by(64)src1,
    const uint8_t *__counted_by(64)src2,
    const uint8_t *__counted_by(64)byte_mask);
extern int os_memcmp_mask_80B(const uint8_t *__counted_by(80)src1,
    const uint8_t *__counted_by(80)src2,
    const uint8_t *__counted_by(80)byte_mask);
SYSCTL_DECL(_kern_skywalk);
SYSCTL_DECL(_kern_skywalk_stats);
decl_lck_mtx_data(extern, sk_lock);
__attribute__((always_inline))
static inline const char *
sk_ring2str(enum txrx t)
{
	switch (t) {
	case NR_TX:
		return "TX";
	case NR_RX:
		return "RX";
	case NR_A:
		return "ALLOC";
	case NR_F:
		return "FREE";
	case NR_EV:
		return "EVENT";
	case NR_LBA:
		return "LARGE ALLOC";
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}

__attribute__((always_inline))
static inline enum txrx
sk_txrx_swap(enum txrx t)
{
	return t == NR_RX ? NR_TX : NR_RX;
}



__attribute__((always_inline))
static inline uint32_t
SLOT_NEXT(uint32_t i, uint32_t lim)
{
	return __improbable(i == lim) ? 0 : i + 1;
}


__attribute__((always_inline))
static inline uint32_t
SLOT_PREV(uint32_t i, uint32_t lim)
{
	return __improbable(i == 0) ? lim : i - 1;
}


static inline uint32_t
SLOT_INCREMENT(uint32_t i, uint32_t n, uint32_t lim)
{
	i += n;
	return __improbable(i > lim) ? i - lim - 1 : i;
}




























struct nexus_adapter;
__attribute__((always_inline))
static inline boolean_t
sk_is_sync_protected(void)
{
	return net_thread_is_marked(NET_THREAD_CHANNEL_SYNC) != 0;
}

__attribute__((always_inline))
static inline sk_protect_t
sk_sync_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_CHANNEL_SYNC);
}


__attribute__((always_inline))
static inline boolean_t
sk_is_rx_notify_protected(void)
{
	return net_thread_is_marked(NET_THREAD_RX_NOTIFY) != 0;
}

__attribute__((always_inline))
static inline sk_protect_t
sk_rx_notify_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_RX_NOTIFY);
}

__attribute__((always_inline))
static inline sk_protect_t
sk_tx_notify_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_TX_NOTIFY);
}

__attribute__((always_inline))
static inline boolean_t
sk_is_tx_notify_protected(void)
{
	return net_thread_is_marked(NET_THREAD_TX_NOTIFY) != 0;
}

__attribute__((always_inline))
static inline boolean_t
sk_is_cache_update_protected(void)
{
	return net_thread_is_marked(NET_THREAD_CACHE_UPDATE) != 0;
}

__attribute__((always_inline))
static inline sk_protect_t
sk_cache_update_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_CACHE_UPDATE);
}

__attribute__((always_inline))
static inline boolean_t
sk_is_region_update_protected(void)
{
	return net_thread_is_marked(NET_THREAD_REGION_UPDATE) != 0;
}

__attribute__((always_inline))
static inline sk_protect_t
sk_region_update_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_REGION_UPDATE);
}

__attribute__((always_inline))
static inline boolean_t
sk_is_async_transmit_protected(void)
{
	return net_thread_is_marked(NET_THREAD_AYSYNC_TX) != 0;
}

__attribute__((always_inline))
static inline sk_protect_t
sk_async_transmit_protect(void)
{
	return (sk_protect_t)(const void *)
	       net_thread_marks_push(NET_THREAD_AYSYNC_TX);
}


__attribute__((always_inline))
static inline void
sk_unprotect(sk_protect_t protect)
{
	net_thread_marks_pop((net_thread_marks_t)(const void*)protect);
}






__BEGIN_DECLS
extern int skywalk_init(void);
extern int skywalk_priv_check_cred(proc_t, kauth_cred_t, int);
extern int skywalk_priv_check_proc_cred(proc_t, int);
extern int skywalk_mac_system_check_proc_cred(proc_t, const char *);
extern int skywalk_nxctl_check_privileges(proc_t, kauth_cred_t);
extern boolean_t skywalk_check_platform_binary(proc_t);
extern boolean_t skywalk_netif_direct_allowed(const char *);
extern boolean_t skywalk_netif_direct_enabled(void);
extern void sk_gen_guard_id(boolean_t, const uuid_t, guardid_t *);
extern char *__counted_by(sizeof(uuid_string_t)) sk_uuid_unparse(const uuid_t, uuid_string_t);
extern bool sk_sa_has_addr(struct sockaddr *sa);
extern bool sk_sa_has_port(struct sockaddr *sa);
extern uint16_t sk_sa_get_port(struct sockaddr *sa);
extern void skywalk_kill_process(struct proc *, uint64_t);
extern const char *proc_name_address(void *p);
extern void skoid_init(void);
extern void skoid_create(struct skoid *skoid, struct sysctl_oid_list *parent,
    const char *name, int kind);
extern void skoid_add_int(struct skoid *skoid, const char *name, int flags,
    int *ptr);
extern void skoid_add_uint(struct skoid *skoid, const char *name, int flags,
    unsigned int *ptr);
extern void skoid_add_handler(struct skoid *skoid, const char *name, int kind,
    int (*handler)SYSCTL_HANDLER_ARGS, void *arg1, int arg2);
extern void skoid_destroy(struct skoid *skoid);
SYSCTL_DECL(_kern_skywalk_libcuckoo);
__BEGIN_DECLS

struct cuckoo_hashtable * cuckoo_hashtable_create(
	struct cuckoo_hashtable_params *p);
void cuckoo_hashtable_free(struct cuckoo_hashtable *ht);
size_t cuckoo_hashtable_entries(struct cuckoo_hashtable *h);
size_t cuckoo_hashtable_capacity(struct cuckoo_hashtable *h);
uint32_t cuckoo_hashtable_load_factor(struct cuckoo_hashtable *h);
size_t cuckoo_hashtable_memory_footprint(struct cuckoo_hashtable *h);
void cuckoo_hashtable_try_shrink(struct cuckoo_hashtable *h);
int cuckoo_hashtable_add_with_hash(struct cuckoo_hashtable *h, struct cuckoo_node *node,
    uint32_t key);
int cuckoo_hashtable_del(struct cuckoo_hashtable *h, struct cuckoo_node *node,
    uint32_t key);
void cuckoo_hashtable_foreach(struct cuckoo_hashtable *ht,
    void (^handler)(struct cuckoo_node *node, uint32_t hv));
void
net_filter_event_mark(enum net_filter_event_subsystems subsystem, bool compatible);
void
net_filter_event_register(net_filter_event_callback_t callback);
__attribute__((always_inline))
static inline struct skmem_arena_nexus *
skmem_arena_nexus(struct skmem_arena *ar)
{
	if (__improbable(ar->ar_type != SKMEM_ARENA_TYPE_NEXUS)) {
		return NULL;
	}

	return (struct skmem_arena_nexus *)ar;
}







struct kern_nexus_advisory;
__BEGIN_DECLS
extern struct skmem_arena *skmem_arena_create_for_nexus(
	const struct nexus_adapter *, struct skmem_region_params[SKMEM_REGIONS],
	struct kern_pbufpool **, struct kern_pbufpool **, boolean_t, boolean_t,
	struct kern_nexus_advisory *, int *);
extern void skmem_arena_nexus_sd_set_noidle(struct skmem_arena_nexus *, int);
extern boolean_t skmem_arena_nexus_sd_idle(struct skmem_arena_nexus *);
extern struct skmem_arena *skmem_arena_create_for_necp(const char *,
    struct skmem_region_params *, struct skmem_region_params *, int *);
extern struct skmem_arena_necp *skmem_arena_necp(struct skmem_arena *);
extern struct skmem_arena *skmem_arena_create_for_system(const char *, int *);
extern struct skmem_arena_system *skmem_arena_system(struct skmem_arena *);
extern void *skmem_arena_system_sysctls_obj_addr(struct skmem_arena *);
extern size_t skmem_arena_system_sysctls_obj_size(struct skmem_arena *);
extern void skmem_arena_retain(struct skmem_arena *);
extern boolean_t skmem_arena_release(struct skmem_arena *);
extern int skmem_arena_mmap(struct skmem_arena *, struct proc *,
    struct skmem_arena_mmap_info *);
extern void skmem_arena_munmap(struct skmem_arena *,
    struct skmem_arena_mmap_info *);
extern void skmem_arena_munmap_channel(struct skmem_arena *,
    struct kern_channel *);
extern int skmem_arena_mredirect(struct skmem_arena *,
    struct skmem_arena_mmap_info *, struct proc *, boolean_t *);
extern int skmem_arena_defunct(struct skmem_arena *);
extern void skmem_arena_get_stats(struct skmem_arena *, uint64_t *,
    uint64_t *);
extern mach_vm_offset_t skmem_arena_get_region_offset(struct skmem_arena *,
    skmem_region_id_t);
extern void skmem_arena_reap(struct skmem_arena *, boolean_t);
__BEGIN_DECLS

__attribute__((always_inline))
static inline void
skmem_bufctl_use(struct skmem_bufctl *bc)
{
	uint32_t old, new;

	os_atomic_rmw_loop(&bc->bc_usecnt, old, new, relaxed, {
		new = old + 1;
		VERIFY(new != 0);
		ASSERT(new == 1 || (bc->bc_flags & SKMEM_BUFCTL_SHAREOK));
	});
}


__attribute__((always_inline))
static inline uint32_t
skmem_bufctl_unuse(struct skmem_bufctl *bc)
{
	uint32_t old, new;

	os_atomic_rmw_loop(&bc->bc_usecnt, old, new, relaxed, {
		new = old - 1;
		VERIFY(old != 0);
		ASSERT(old == 1 || (bc->bc_flags & SKMEM_BUFCTL_SHAREOK));
	});

	return new;
}

extern struct skmem_cache *skmem_slab_cache;
extern int skmem_slab_alloc_locked(struct skmem_cache *,
    struct skmem_obj_info *, struct skmem_obj_info *, uint32_t);
extern void skmem_slab_free_locked(struct skmem_cache *, void *);
extern int skmem_slab_alloc_pseudo_locked(struct skmem_cache *,
    struct skmem_obj_info *, struct skmem_obj_info *, uint32_t);
extern void skmem_slab_free_pseudo_locked(struct skmem_cache *, void *);
extern void skmem_slab_free(struct skmem_cache *, void *);
extern void skmem_slab_batch_free(struct skmem_cache *, struct skmem_obj *);
extern uint32_t skmem_slab_batch_alloc(struct skmem_cache *, struct skmem_obj **,
    uint32_t, uint32_t);
extern int skmem_slab_alloc(struct skmem_cache *, struct skmem_obj_info *,
    struct skmem_obj_info *, uint32_t);
extern void skmem_audit_bufctl(struct skmem_bufctl *);
extern void skmem_cache_pre_init(void);
extern void skmem_cache_init(void);
extern void skmem_cache_fini(void);
extern struct skmem_cache *skmem_cache_create(const char *, size_t, size_t,
    skmem_ctor_fn_t, skmem_dtor_fn_t, skmem_reclaim_fn_t, void *,
    struct skmem_region *, uint32_t);
extern void skmem_cache_destroy(struct skmem_cache *);
extern uint32_t skmem_cache_batch_alloc(struct skmem_cache *,
    struct skmem_obj **list, size_t objsize, uint32_t, uint32_t);
static inline void *__header_indexable
skmem_cache_alloc(struct skmem_cache *skm, uint32_t skmflag)
{
	struct skmem_obj *__single buf;

	(void) skmem_cache_batch_alloc(skm, &buf, skm->skm_objsize, 1, skmflag);

	
	return __unsafe_forge_bidi_indexable(void *, buf, buf ? skm->skm_objsize : 0);
}

extern void skmem_cache_free(struct skmem_cache *, void *);
extern void skmem_cache_free_nocache(struct skmem_cache *, void *);
extern void skmem_cache_batch_free(struct skmem_cache *, struct skmem_obj *);
extern void skmem_cache_batch_free_nocache(struct skmem_cache *, struct skmem_obj *);
extern void skmem_cache_reap_now(struct skmem_cache *, boolean_t);
extern void skmem_cache_reap(void);
extern void skmem_reap_caches(boolean_t);
extern void skmem_cache_get_obj_info(struct skmem_cache *, void *,
    struct skmem_obj_info *, struct skmem_obj_info *);
extern uint32_t skmem_cache_magazine_max(uint32_t);
extern boolean_t skmem_allow_magazines(void);
__BEGIN_DECLS
extern void skmem_region_init(void);
extern void skmem_region_fini(void);
extern void skmem_region_reap_caches(boolean_t);
extern void skmem_region_params_config(struct skmem_region_params *);
extern struct skmem_region *skmem_region_create(const char *,
    struct skmem_region_params *, sksegment_ctor_fn_t, sksegment_dtor_fn_t,
    void *);
extern void skmem_region_mirror(struct skmem_region *, struct skmem_region *);
extern void skmem_region_slab_config(struct skmem_region *,
    struct skmem_cache *, bool);
extern void *__sized_by(objsize) skmem_region_alloc(struct skmem_region *,
    void *__sized_by(*msize) *, struct sksegment **, struct sksegment **,
    uint32_t, uint32_t objsize, uint32_t *msize);
extern void skmem_region_free(struct skmem_region *, void *, void *);
extern void skmem_region_retain(struct skmem_region *);
extern boolean_t skmem_region_release(struct skmem_region *);
extern mach_vm_address_t skmem_region_obj_lookup(struct skmem_region *,
    uint32_t);
extern int skmem_region_get_info(struct skmem_region *, uint32_t *,
    struct sksegment **);
extern boolean_t skmem_region_for_pp(skmem_region_id_t);
extern void skmem_region_get_stats(struct skmem_region *,
    struct sk_stats_region *);
__BEGIN_DECLS
extern void skmem_init(void);
extern void skmem_fini(void);
extern const struct skmem_region_params *skmem_get_default(skmem_region_id_t);
extern uint32_t skmem_cpu_cache_line_size(void);
extern void skmem_dispatch(thread_call_t, void (*func)(void), uint64_t);
extern struct skmem_region *skmem_get_sysctls_region(void);
extern char *skmem_dump(struct skmem_region *);
extern boolean_t skmem_lowmem_check(void);
extern int flowidns_init(void);
extern void flowidns_fini(void);
extern void flowidns_allocate_flowid(flowidns_domain_id_t domain,
    struct flowidns_flow_key *flow_key, flowidns_flowid_t *flowid);
extern void flowidns_release_flowid(flowidns_flowid_t flowid);
extern int netns_init(void);
extern void netns_uninit(void);
extern void netns_reap_caches(boolean_t);
extern boolean_t netns_is_enabled(void);
extern int netns_reserve(netns_token * token, uint32_t *__sized_by(addr_len) addr,
    uint8_t addr_len, uint8_t proto, in_port_t port, uint32_t flags,
    struct ns_flow_info *nfi);
extern int netns_reserve_ephemeral(netns_token * token,
    uint32_t *__sized_by(addr_len) addr, uint8_t addr_len, uint8_t proto,
    in_port_t *port, uint32_t flags, struct ns_flow_info *nfi);
extern void netns_release(netns_token *token);
extern void netns_half_close(netns_token *token);
extern void netns_withdraw(netns_token *token);
extern int netns_get_flow_info(netns_token *token, struct ns_flow_info *nfi);
extern int netns_change_addr(netns_token * token,
    uint32_t *__sized_by(new_addr_len) new_addr, uint8_t new_addr_len);
extern void netns_set_ifnet(netns_token *token, ifnet_t ifp);
extern void netns_ifnet_detach(ifnet_t ifp);
extern void netns_change_flags(netns_token *token, uint32_t set_flags,
    uint32_t clear_flags);
extern errno_t
netns_get_local_ports(ifnet_t ifp, protocol_family_t protocol,
    u_int32_t flags, u_int8_t bitfield[IP_PORTRANGE_BITFIELD_LEN]);
extern uint32_t
netns_find_anyres_byaddr(struct ifaddr *ifa, uint8_t proto);
extern uint32_t
netns_lookup_reservations_count_in(struct in_addr addr, uint8_t proto);
extern uint32_t
netns_lookup_reservations_count_in6(struct in6_addr addr, uint8_t proto);
extern int protons_init(void);
extern void protons_fini(void);
extern int protons_reserve(struct protons_token **ptp, pid_t pid, pid_t epid,
    uint8_t proto);
extern void protons_release(struct protons_token **ptp);
extern int protons_token_get_use_count(struct protons_token *pt);
extern bool protons_token_is_valid(struct protons_token *pt);
extern bool protons_token_has_matching_pid(struct protons_token *pt, pid_t pid,
    pid_t epid);
__attribute__((always_inline))
static inline uint32_t
na_get_nslots(const struct nexus_adapter *na, enum txrx t)
{
	switch (t) {
	case NR_TX:
		return na->na_num_tx_slots;
	case NR_RX:
		return na->na_num_rx_slots;
	case NR_A:
	case NR_F:
		return na->na_num_allocator_slots;
	case NR_EV:
		return na->na_num_event_slots;
	case NR_LBA:
		return na->na_num_large_buf_alloc_slots;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}

__attribute__((always_inline))
static inline void
na_set_nslots(struct nexus_adapter *na, enum txrx t, uint32_t v)
{
	switch (t) {
	case NR_TX:
		na->na_num_tx_slots = v;
		break;
	case NR_RX:
		na->na_num_rx_slots = v;
		break;
	case NR_A:
	case NR_F:
		na->na_num_allocator_slots = v;
		break;
	case NR_EV:
		na->na_num_event_slots = v;
		break;
	case NR_LBA:
		na->na_num_large_buf_alloc_slots = v;
		break;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}

__attribute__((always_inline))
static inline uint32_t
na_get_nrings(const struct nexus_adapter *na, enum txrx t)
{
	switch (t) {
	case NR_TX:
		return na->na_num_tx_rings;
	case NR_RX:
		return na->na_num_rx_rings;
	case NR_A:
	case NR_F:
		return na->na_num_allocator_ring_pairs;
	case NR_EV:
		return na->na_num_event_rings;
	case NR_LBA:
		return na->na_num_large_buf_alloc_rings;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}

__attribute__((always_inline))
static inline void
na_set_nrings(struct nexus_adapter *na, enum txrx t, uint32_t v)
{
	switch (t) {
	case NR_TX:
		na->na_num_tx_rings = v;
		break;
	case NR_RX:
		na->na_num_rx_rings = v;
		break;
	case NR_A:
	case NR_F:
		na->na_num_allocator_ring_pairs = v;
		break;
	case NR_EV:
		na->na_num_event_rings = v;
		break;
	case NR_LBA:
		
		ASSERT(v <= 1);
		na->na_num_large_buf_alloc_rings = v;
		break;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}

__attribute__((always_inline))
static inline struct __kern_channel_ring *__header_indexable
NAKR(struct nexus_adapter *na, enum txrx t)
{
	switch (t) {
	case NR_TX:
		return na->na_tx_rings;
	case NR_RX:
		return na->na_rx_rings;
	case NR_A:
		return na->na_alloc_rings;
	case NR_F:
		return na->na_free_rings;
	case NR_EV:
		return na->na_event_rings;
	case NR_LBA:
		return na->na_large_buf_alloc_rings;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
}






__attribute__((always_inline))
static inline boolean_t
na_reject_channel(struct kern_channel *ch, struct nexus_adapter *na)
{
	boolean_t reject;

	ASSERT(ch->ch_na == NULL || ch->ch_na == na);

	if ((na->na_flags & NAF_REJECT) || NX_REJECT_ACT(na->na_nx)) {
		
		if (!(na->na_flags & NAF_REJECT)) {
			SK_ERR("%s(%d) marked as non-permissive",
			    ch->ch_name, ch->ch_pid);
			os_atomic_or(&na->na_flags, NAF_REJECT, relaxed);
			ch_deactivate(ch);
		}
		reject = TRUE;
	} else {
		reject = FALSE;
	}

	return reject;
}


__BEGIN_DECLS
extern void na_init(void);
extern void na_fini(void);
extern int na_bind_channel(struct nexus_adapter *na, struct kern_channel *ch,
    struct chreq *);
extern void na_unbind_channel(struct kern_channel *ch);
extern void na_attach_common(struct nexus_adapter *,
    struct kern_nexus *, struct kern_nexus_domain_provider *);
extern int na_update_config(struct nexus_adapter *na);
extern int na_rings_mem_setup(struct nexus_adapter *, boolean_t,
    struct kern_channel *);
extern void na_rings_mem_teardown(struct nexus_adapter *,
    struct kern_channel *, boolean_t);
extern void na_ch_rings_defunct(struct kern_channel *, struct proc *);
extern void na_disable_all_rings(struct nexus_adapter *);
extern void na_enable_all_rings(struct nexus_adapter *);
extern void na_lock_all_rings(struct nexus_adapter *);
extern void na_unlock_all_rings(struct nexus_adapter *);
extern int na_interp_ringid(struct nexus_adapter *, ring_id_t, ring_set_t,
    uint32_t[NR_TXRX], uint32_t[NR_TXRX]);
extern struct kern_pbufpool *na_kr_get_pp(struct nexus_adapter *, enum txrx);
extern int na_find(struct kern_channel *, struct kern_nexus *,
    struct chreq *, struct kern_channel *, struct nxbind *,
    struct proc *, struct nexus_adapter **, boolean_t);
extern void na_retain_locked(struct nexus_adapter *na);
extern int na_release_locked(struct nexus_adapter *na);
extern int na_connect(struct kern_nexus *, struct kern_channel *,
    struct chreq *, struct kern_channel *, struct nxbind *, struct proc *);
extern void na_disconnect(struct kern_nexus *, struct kern_channel *);
extern void na_defunct(struct kern_nexus *, struct kern_channel *,
    struct nexus_adapter *, boolean_t);
extern int na_connect_spec(struct kern_nexus *, struct kern_channel *,
    struct chreq *, struct proc *);
extern void na_disconnect_spec(struct kern_nexus *, struct kern_channel *);
extern void na_start_spec(struct kern_nexus *, struct kern_channel *);
extern void na_stop_spec(struct kern_nexus *, struct kern_channel *);
extern int na_pseudo_create(struct kern_nexus *, struct chreq *,
    struct nexus_adapter **);
extern void na_kr_drop(struct nexus_adapter *, boolean_t);
extern void na_flowadv_entry_alloc(const struct nexus_adapter *, uuid_t,
    const flowadv_idx_t, const uint32_t);
extern void na_flowadv_entry_free(const struct nexus_adapter *, uuid_t,
    const flowadv_idx_t, const uint32_t);
extern bool na_flowadv_set(const struct kern_channel *,
    const flowadv_idx_t, const flowadv_token_t);
extern bool na_flowadv_clear(const struct kern_channel *,
    const flowadv_idx_t, const flowadv_token_t);
extern int na_flowadv_report_ce_event(const struct kern_channel *ch,
    const flowadv_idx_t fe_idx, const flowadv_token_t flow_token,
    uint32_t ce_cnt, uint32_t total_pkt_cnt);
extern void na_flowadv_event(struct __kern_channel_ring *);
extern void na_post_event(struct __kern_channel_ring *, boolean_t, boolean_t,
    boolean_t, uint32_t);
extern void na_drain(struct nexus_adapter *, boolean_t);
__attribute__((always_inline))
static inline void
nx_mbq_lock(struct nx_mbq *q)
{
	lck_mtx_lock(&q->nx_mbq_lock);
}

__attribute__((always_inline))
static inline void
nx_mbq_lock_spin(struct nx_mbq *q)
{
	lck_mtx_lock_spin(&q->nx_mbq_lock);
}

__attribute__((always_inline))
static inline void
nx_mbq_convert_spin(struct nx_mbq *q)
{
	lck_mtx_convert_spin(&q->nx_mbq_lock);
}

__attribute__((always_inline))
static inline void
nx_mbq_unlock(struct nx_mbq *q)
{
	lck_mtx_unlock(&q->nx_mbq_lock);
}

__attribute__((always_inline))
static inline struct mbuf *
nx_mbq_peek(struct nx_mbq *q)
{
	return qhead(&q->nx_mbq_q);
}

__attribute__((always_inline))
static inline unsigned int
nx_mbq_len(struct nx_mbq *q)
{
	return qlen(&q->nx_mbq_q);
}

__attribute__((always_inline))
static inline size_t
nx_mbq_size(struct nx_mbq *q)
{
	u_int64_t qsize = qsize(&q->nx_mbq_q);
	VERIFY(qsize <= UINT_MAX);
	return (size_t)qsize;
}

__attribute__((always_inline))
static inline unsigned int
nx_mbq_limit(struct nx_mbq *q)
{
	return qlimit(&q->nx_mbq_q);
}

__attribute__((always_inline))
static inline void
__nx_mbq_enq(struct nx_mbq *q, struct mbuf *m)
{
	classq_pkt_t pkt;

	CLASSQ_PKT_INIT_MBUF(&pkt, m);
	_addq(&q->nx_mbq_q, &pkt);
}

__attribute__((always_inline))
static inline void
nx_mbq_safe_enq(struct nx_mbq *q, struct mbuf *m)
{
	nx_mbq_lock(q);
	__nx_mbq_enq(q, m);
	nx_mbq_unlock(q);
}

__attribute__((always_inline))
static inline void
nx_mbq_enq(struct nx_mbq *q, struct mbuf *m)
{
	__nx_mbq_enq(q, m);
}

__attribute__((always_inline))
static inline void
__nx_mbq_enq_multi(struct nx_mbq *q, struct mbuf *m_head, struct mbuf *m_tail,
    uint32_t cnt, uint32_t size)
{
	classq_pkt_t head, tail;

	CLASSQ_PKT_INIT_MBUF(&head, m_head);
	CLASSQ_PKT_INIT_MBUF(&tail, m_tail);
	_addq_multi(&q->nx_mbq_q, &head, &tail, cnt, size);
}

__attribute__((always_inline))
static inline void
nx_mbq_safe_enq_multi(struct nx_mbq *q, struct mbuf *m_head,
    struct mbuf *m_tail, uint32_t cnt, uint32_t size)
{
	nx_mbq_lock(q);
	__nx_mbq_enq_multi(q, m_head, m_tail, cnt, size);
	nx_mbq_unlock(q);
}

__attribute__((always_inline))
static inline void
nx_mbq_enq_multi(struct nx_mbq *q, struct mbuf *m_head, struct mbuf *m_tail,
    uint32_t cnt, uint32_t size)
{
	__nx_mbq_enq_multi(q, m_head, m_tail, cnt, size);
}

__attribute__((always_inline))
static inline struct mbuf *
__mbq_deq(struct nx_mbq *q)
{
	classq_pkt_t pkt = CLASSQ_PKT_INITIALIZER(pkt);

	_getq(&q->nx_mbq_q, &pkt);
	ASSERT((pkt.cp_mbuf == NULL) || (pkt.cp_ptype == QP_MBUF));
	return pkt.cp_mbuf;
}

__attribute__((always_inline))
static inline struct mbuf *
nx_mbq_safe_deq(struct nx_mbq *q)
{
	struct mbuf *ret;

	nx_mbq_lock(q);
	ret = __mbq_deq(q);
	nx_mbq_unlock(q);

	return ret;
}

__attribute__((always_inline))
static inline struct mbuf *
nx_mbq_deq(struct nx_mbq *q)
{
	return __mbq_deq(q);
}

__attribute__((always_inline))
static inline struct mbuf *
__mbq_deq_all(struct nx_mbq *q, struct mbuf **mlast, uint32_t *qlenp,
    uint64_t *qsizep)
{
	classq_pkt_t first = CLASSQ_PKT_INITIALIZER(first);
	classq_pkt_t last = CLASSQ_PKT_INITIALIZER(last);

	_getq_all(&q->nx_mbq_q, &first, &last, qlenp, qsizep);
	*mlast = last.cp_mbuf;
	ASSERT((first.cp_mbuf == NULL) || (first.cp_ptype == QP_MBUF));
	return first.cp_mbuf;
}

__attribute__((always_inline))
static inline struct mbuf *
nx_mbq_safe_deq_all(struct nx_mbq *q, struct mbuf **last, uint32_t *qlenp,
    uint64_t *qsizep)
{
	struct mbuf *ret;

	nx_mbq_lock(q);
	ret = __mbq_deq_all(q, last, qlenp, qsizep);
	nx_mbq_unlock(q);

	return ret;
}

__attribute__((always_inline))
static inline struct mbuf *
nx_mbq_deq_all(struct nx_mbq *q, struct mbuf **last, uint32_t *qlenp,
    uint64_t *qsizep)
{
	return __mbq_deq_all(q, last, qlenp, qsizep);
}

__BEGIN_DECLS
extern void nx_mbq_init(struct nx_mbq *q, uint32_t lim);
extern void nx_mbq_concat(struct nx_mbq *, struct nx_mbq *);
extern boolean_t nx_mbq_empty(struct nx_mbq *);
extern void nx_mbq_destroy(struct nx_mbq *q);
extern void nx_mbq_purge(struct nx_mbq *q);
extern void nx_mbq_safe_init(struct __kern_channel_ring *kr, struct nx_mbq *q,
    uint32_t lim, lck_grp_t *lck_grp, lck_attr_t *lck_attr);
extern void nx_mbq_safe_destroy(struct nx_mbq *q);
extern void nx_mbq_safe_purge(struct nx_mbq *q);
__attribute__((always_inline))
static inline void
nx_pktq_lock(struct nx_pktq *q)
{
	lck_mtx_lock(&q->nx_pktq_lock);
}

__attribute__((always_inline))
static inline void
nx_pktq_lock_spin(struct nx_pktq *q)
{
	lck_mtx_lock_spin(&q->nx_pktq_lock);
}

__attribute__((always_inline))
static inline void
nx_pktq_convert_spin(struct nx_pktq *q)
{
	lck_mtx_convert_spin(&q->nx_pktq_lock);
}

__attribute__((always_inline))
static inline void
nx_pktq_unlock(struct nx_pktq *q)
{
	lck_mtx_unlock(&q->nx_pktq_lock);
}

__attribute__((always_inline))
static inline struct __kern_packet *
nx_pktq_peek(struct nx_pktq *q)
{
	return qhead(&q->nx_pktq_q);
}

__attribute__((always_inline))
static inline unsigned int
nx_pktq_len(struct nx_pktq *q)
{
	return qlen(&q->nx_pktq_q);
}

__attribute__((always_inline))
static inline size_t
nx_pktq_size(struct nx_pktq *q)
{
	u_int64_t qsize = qsize(&q->nx_pktq_q);
	VERIFY(qsize <= UINT_MAX);
	return (size_t)qsize;
}

__attribute__((always_inline))
static inline unsigned int
nx_pktq_limit(struct nx_pktq *q)
{
	return qlimit(&q->nx_pktq_q);
}

__attribute__((always_inline))
static inline void
__nx_pktq_enq(struct nx_pktq *q, struct __kern_packet *p)
{
	classq_pkt_t pkt;

	CLASSQ_PKT_INIT_PACKET(&pkt, p);
	_addq(&q->nx_pktq_q, &pkt);
}

__attribute__((always_inline))
static inline void
nx_pktq_safe_enq(struct nx_pktq *q, struct __kern_packet *p)
{
	nx_pktq_lock(q);
	__nx_pktq_enq(q, p);
	nx_pktq_unlock(q);
}

__attribute__((always_inline))
static inline void
nx_pktq_enq(struct nx_pktq *q, struct __kern_packet *p)
{
	__nx_pktq_enq(q, p);
}

__attribute__((always_inline))
static inline void
__nx_pktq_enq_multi(struct nx_pktq *q, struct __kern_packet *p_head,
    struct __kern_packet *p_tail,
    uint32_t cnt, uint32_t size)
{
	classq_pkt_t head, tail;

	CLASSQ_PKT_INIT_PACKET(&head, p_head);
	CLASSQ_PKT_INIT_PACKET(&tail, p_tail);
	_addq_multi(&q->nx_pktq_q, &head, &tail, cnt, size);
}

__attribute__((always_inline))
static inline void
nx_pktq_safe_enq_multi(struct nx_pktq *q, struct __kern_packet *p_head,
    struct __kern_packet *p_tail, uint32_t cnt, uint32_t size)
{
	nx_pktq_lock(q);
	__nx_pktq_enq_multi(q, p_head, p_tail, cnt, size);
	nx_pktq_unlock(q);
}

__attribute__((always_inline))
static inline void
nx_pktq_enq_multi(struct nx_pktq *q, struct __kern_packet *p_head,
    struct __kern_packet *p_tail, uint32_t cnt, uint32_t size)
{
	__nx_pktq_enq_multi(q, p_head, p_tail, cnt, size);
}

__attribute__((always_inline))
static inline struct __kern_packet *
__pktq_deq(struct nx_pktq *q)
{
	classq_pkt_t pkt = CLASSQ_PKT_INITIALIZER(pkt);

	_getq(&q->nx_pktq_q, &pkt);
	ASSERT((pkt.cp_kpkt == NULL) || (pkt.cp_ptype == QP_PACKET));
	return pkt.cp_kpkt;
}

__attribute__((always_inline))
static inline struct __kern_packet *
nx_pktq_safe_deq(struct nx_pktq *q)
{
	struct __kern_packet *__single ret;

	nx_pktq_lock(q);
	ret = __pktq_deq(q);
	nx_pktq_unlock(q);

	return ret;
}

__attribute__((always_inline))
static inline struct __kern_packet *
nx_pktq_deq(struct nx_pktq *q)
{
	return __pktq_deq(q);
}

__attribute__((always_inline))
static inline struct __kern_packet *
__pktq_deq_all(struct nx_pktq *q, struct __kern_packet **plast, uint32_t *qlenp,
    uint64_t *qsizep)
{
	classq_pkt_t first = CLASSQ_PKT_INITIALIZER(first);
	classq_pkt_t last = CLASSQ_PKT_INITIALIZER(last);

	_getq_all(&q->nx_pktq_q, &first, &last, qlenp, qsizep);
	*plast = last.cp_kpkt;
	ASSERT((first.cp_kpkt == NULL) || (first.cp_ptype == QP_PACKET));
	return first.cp_kpkt;
}

__attribute__((always_inline))
static inline struct __kern_packet *
nx_pktq_safe_deq_all(struct nx_pktq *q, struct __kern_packet **last,
    uint32_t *qlenp, uint64_t *qsizep)
{
	struct __kern_packet *__single ret;

	nx_pktq_lock(q);
	ret = __pktq_deq_all(q, last, qlenp, qsizep);
	nx_pktq_unlock(q);

	return ret;
}

__attribute__((always_inline))
static inline struct __kern_packet *
nx_pktq_deq_all(struct nx_pktq *q, struct __kern_packet **last, uint32_t *qlenp,
    uint64_t *qsizep)
{
	return __pktq_deq_all(q, last, qlenp, qsizep);
}

__BEGIN_DECLS
extern void nx_pktq_init(struct nx_pktq *q, uint32_t lim);
extern void nx_pktq_concat(struct nx_pktq *q1, struct nx_pktq *q2);
extern boolean_t nx_pktq_empty(struct nx_pktq *q);
extern void nx_pktq_destroy(struct nx_pktq *q);
extern void nx_pktq_purge(struct nx_pktq *q);
extern void nx_pktq_safe_init(struct __kern_channel_ring *kr, struct nx_pktq *q,
    uint32_t lim, lck_grp_t *lck_grp, lck_attr_t *lck_attr);
extern void nx_pktq_safe_destroy(struct nx_pktq *q);
extern void nx_pktq_safe_purge(struct nx_pktq *q);
__BEGIN_DECLS
extern int nexus_init(void);
extern void nexus_fini(void);
extern struct kern_nexus *nx_create(struct nxctl *, const uuid_t,
    const nexus_type_t, const void *, nexus_ctx_release_fn_t,
    struct kern_pbufpool *, struct kern_pbufpool *, int *);
extern void nx_retain(struct kern_nexus *);
extern void nx_retain_locked(struct kern_nexus *);
extern int nx_release(struct kern_nexus *);
extern int nx_release_locked(struct kern_nexus *);
extern void nx_detach(struct kern_nexus *);
extern void nx_stop(struct kern_nexus *nx);
extern int nx_close(struct kern_nexus *, boolean_t);
extern int nx_destroy(struct nxctl *, const uuid_t);
extern struct kern_nexus *nx_find(const uuid_t, boolean_t);
extern int nx_advisory_alloc(struct kern_nexus *, const char *,
    struct skmem_region_params *, nexus_advisory_type_t);
extern void nx_advisory_free(struct kern_nexus *);
extern int nx_port_find(struct kern_nexus *, nexus_port_t,
    nexus_port_t, nexus_port_t *);
extern int nx_port_alloc(struct kern_nexus *, nexus_port_t,
    struct nxbind *, struct nexus_adapter **, struct proc *);
extern int nx_port_bind(struct kern_nexus *, nexus_port_t,
    struct nxbind *);
extern int nx_port_bind_info(struct kern_nexus *, nexus_port_t,
    struct nxbind *, void *);
extern int nx_port_unbind(struct kern_nexus *, nexus_port_t);
extern struct nexus_adapter *nx_port_get_na(struct kern_nexus *,
    nexus_port_t);
extern int nx_port_get_info(struct kern_nexus *, nexus_port_t,
    nx_port_info_type_t, void *__sized_by(len), uint32_t len);
extern void nx_port_defunct(struct kern_nexus *, nexus_port_t);
extern void nx_port_free(struct kern_nexus *, nexus_port_t);
extern void nx_port_free_all(struct kern_nexus *);
extern bool nx_port_is_valid(struct kern_nexus *, nexus_port_t);
extern bool nx_port_is_defunct(struct kern_nexus *, nexus_port_t);
extern void nx_port_foreach(struct kern_nexus *, void (^)(nexus_port_t));
extern void nx_interface_advisory_notify(struct kern_nexus *);
extern struct nxctl *nxctl_create(struct proc *, struct fileproc *,
    const uuid_t, int *);
extern void nxctl_close(struct nxctl *);
extern void nxctl_traffic_rule_clean(struct nxctl *);
extern void nxctl_traffic_rule_init(void);
extern void nxctl_traffic_rule_fini(void);
extern int nxctl_inet_traffic_rule_find_qset_id_with_pkt(const char *,
    struct __kern_packet *, uint64_t *);
extern int nxctl_inet_traffic_rule_find_qset_id(const char *,
    struct ifnet_traffic_descriptor_inet *, uint64_t *);
extern int nxctl_inet_traffic_rule_get_count(const char *, uint32_t *);
extern int nxctl_get_opt(struct nxctl *, struct sockopt *);
extern int nxctl_set_opt(struct nxctl *, struct sockopt *);
extern void nxctl_retain(struct nxctl *);
extern int nxctl_release(struct nxctl *);
extern void nxctl_dtor(struct nxctl *);
extern int nxprov_advise_connect(struct kern_nexus *, struct kern_channel *,
    struct proc *p);
extern void nxprov_advise_disconnect(struct kern_nexus *,
    struct kern_channel *);
extern struct kern_nexus_provider *nxprov_create(struct proc *,
    struct nxctl *, struct nxprov_reg *, int *);
extern struct kern_nexus_provider *nxprov_create_kern(struct nxctl *,
    struct kern_nexus_domain_provider *, struct nxprov_reg *,
    const struct kern_nexus_provider_init *init, int *err);
extern int nxprov_close(struct kern_nexus_provider *, boolean_t);
extern int nxprov_destroy(struct nxctl *, const uuid_t);
extern void nxprov_retain(struct kern_nexus_provider *);
extern int nxprov_release(struct kern_nexus_provider *);
extern struct nxprov_params *nxprov_params_alloc(zalloc_flags_t);
extern void nxprov_params_free(struct nxprov_params *);
extern int nxprov_params_adjust(struct kern_nexus_domain_provider *,
    const uint32_t, const struct nxprov_params *, struct nxprov_params *,
    struct skmem_region_params[SKMEM_REGIONS], const struct nxdom *,
    const struct nxdom *, const struct nxdom *, uint32_t,
    int (*adjust_fn)(const struct kern_nexus_domain_provider *,
    const struct nxprov_params *, struct nxprov_adjusted_params *));
extern void nxdom_attach_all(void);
extern void nxdom_detach_all(void);
extern struct nxdom *nxdom_find(nexus_type_t);
extern struct kern_nexus_domain_provider *nxdom_prov_find(
	const struct nxdom *, const char *);
extern struct kern_nexus_domain_provider *nxdom_prov_find_uuid(const uuid_t);
extern int nxdom_prov_add(struct nxdom *, struct kern_nexus_domain_provider *);
extern void nxdom_prov_del(struct kern_nexus_domain_provider *);
extern void nxdom_prov_retain_locked(struct kern_nexus_domain_provider *);
extern void nxdom_prov_retain(struct kern_nexus_domain_provider *);
extern boolean_t nxdom_prov_release_locked(struct kern_nexus_domain_provider *);
extern boolean_t nxdom_prov_release(struct kern_nexus_domain_provider *);
extern int nxdom_prov_validate_params(struct kern_nexus_domain_provider *,
    const struct nxprov_reg *, struct nxprov_params *,
    struct skmem_region_params[SKMEM_REGIONS], const uint32_t, uint32_t);
extern struct nxbind *nxb_alloc(zalloc_flags_t);
extern void nxb_free(struct nxbind *);
extern boolean_t nxb_is_equal(struct nxbind *, struct nxbind *);
extern void nxb_move(struct nxbind *, struct nxbind *);
extern void kern_nexus_walktree(kern_nexus_walktree_f_t *, void *, boolean_t);
extern int kern_nexus_get_pbufpool_info(const uuid_t nx_uuid,
    struct kern_pbufpool_memory_info *rx_pool,
    struct kern_pbufpool_memory_info *tx_pool);
__BEGIN_DECLS

extern struct mbuf *kern_packet_get_mbuf(const kern_packet_t);
__attribute__((always_inline))
static inline int
__packet_set_headroom(const uint64_t ph, const uint8_t headroom)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	if (__probable(headroom < PKT_ADDR(ph)->pkt_qum_buf.buf_dlim)) {
		PKT_ADDR(ph)->pkt_headroom = headroom;
		return 0;
	}
	return ERANGE;
}

__attribute__((always_inline))
static inline uint8_t
__packet_get_headroom(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	return PKT_ADDR(ph)->pkt_headroom;
}

__attribute__((always_inline))
static inline int
__packet_set_link_header_length(const uint64_t ph, const uint8_t len)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if (__probable(len <= PKT_ADDR(ph)->pkt_qum_buf.buf_dlim)) {
		PKT_ADDR(ph)->pkt_l2_len = len;
		return 0;
	}
	return ERANGE;
}

__attribute__((always_inline))
static inline uint8_t
__packet_get_link_header_length(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return PKT_ADDR(ph)->pkt_l2_len;
}

__attribute__((always_inline))
static inline int
__packet_set_link_broadcast(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	PKT_ADDR(ph)->pkt_link_flags |= PKT_LINKF_BCAST;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_link_broadcast(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	return (PKT_ADDR(ph)->pkt_link_flags & PKT_LINKF_BCAST) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_link_multicast(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	PKT_ADDR(ph)->pkt_link_flags |= PKT_LINKF_MCAST;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_link_multicast(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	return (PKT_ADDR(ph)->pkt_link_flags & PKT_LINKF_MCAST) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_link_ethfcs(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	PKT_ADDR(ph)->pkt_link_flags |= PKT_LINKF_ETHFCS;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_link_ethfcs(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	return (PKT_ADDR(ph)->pkt_link_flags & PKT_LINKF_ETHFCS) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_transport_traffic_background(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_BACKGROUND;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_transport_traffic_background(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_BACKGROUND) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_transport_traffic_realtime(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_REALTIME;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_transport_traffic_realtime(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_REALTIME) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_transport_retransmit(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_REXMT;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_transport_retransmit(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_REXMT) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_transport_last_packet(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_LAST_PKT;
	return 0;
}

__attribute__((always_inline))
static inline int
__packet_set_group_start(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_GROUP_START;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_group_start(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_GROUP_START) != 0;
}

__attribute__((always_inline))
static inline int
__packet_set_group_end(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_GROUP_END;
	return 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_group_end(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_GROUP_END) != 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_expire_time(const uint64_t ph, uint64_t *ts)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_EXPIRE_TS) == 0) {
		return ENOENT;
	}
	if (ts == NULL) {
		return EINVAL;
	}
	*ts = po->__po_expire_ts;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_expire_time(const uint64_t ph, const uint64_t ts)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if (ts != 0) {
		po->__po_expire_ts = ts;
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_EXPIRE_TS;
	} else {
		po->__po_expire_ts = 0;
		PKT_ADDR(ph)->pkt_pflags &= ~PKT_F_OPT_EXPIRE_TS;
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_expiry_action(const uint64_t ph, packet_expiry_action_t *pea)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_EXP_ACTION) == 0) {
		return ENOENT;
	}
	if (pea == NULL) {
		return EINVAL;
	}
	*pea = po->__po_expiry_action;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_expiry_action(const uint64_t ph, packet_expiry_action_t pea)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if (pea != PACKET_EXPIRY_ACTION_NONE) {
		po->__po_expiry_action = (uint8_t)pea;
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_EXP_ACTION;
	} else {
		po->__po_expiry_action = 0;
		PKT_ADDR(ph)->pkt_pflags &= ~PKT_F_OPT_EXP_ACTION;
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_opt_get_token(const struct __packet_opt *po,
    void *__sized_by(*len)token,
    uint16_t *len, uint8_t *type)
{
	uint16_t tlen = po->__po_token_len;
	uint8_t ttype;

	if (token == NULL || len == NULL || type == NULL || tlen > *len) {
		return EINVAL;
	}
	ttype = (uint8_t)po->__po_token_type;

	ASSERT(tlen <= PKT_OPT_MAX_TOKEN_SIZE);
	_CASSERT((__builtin_offsetof(struct __packet_opt, __po_token) % 8) == 0);
	bcopy(po->__po_token, token, tlen);
	
	*len = tlen;
	*type = ttype;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_token(const uint64_t ph,
    void *__sized_by(*len)token, uint16_t *len)
{
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	uint8_t type;
	errno_t err;

	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_TOKEN) == 0) {
		return ENOENT;
	}
	err = __packet_opt_get_token(po, token, len, &type);
	if ((err == 0) && (type != PKT_OPT_TOKEN_TYPE_OPAQUE)) {
		err = ENOENT;
	}
	return err;
}

__attribute__((always_inline))
static inline errno_t
__packet_opt_set_token(struct __packet_opt *po,
    const void *__sized_by(PKT_OPT_MAX_TOKEN_SIZE)token,
    const uint16_t len, const uint8_t type, volatile uint64_t *pflags)
{
	_CASSERT((__builtin_offsetof(struct __packet_opt, __po_token) % 8) == 0);
	if (len != 0) {
		if (token == NULL || len > PKT_OPT_MAX_TOKEN_SIZE ||
		    type == 0) {
			return EINVAL;
		}
		if (__probable(IS_P2ALIGNED(token, 8))) {
			uint64_t *token64 = __DECONST(void *, token);
			po->__po_token_data[0] = *token64;
			po->__po_token_data[1] = *(token64 + 1);
		} else {
			bcopy(token, po->__po_token, len);
		}
		po->__po_token_len = len;
		po->__po_token_type = type;
		*pflags |= PKT_F_OPT_TOKEN;
	} else {
		_CASSERT(sizeof(po->__po_token_data[0]) == 8);
		_CASSERT(sizeof(po->__po_token_data[1]) == 8);
		_CASSERT(sizeof(po->__po_token) == 16);
		po->__po_token_data[0] = 0;
		po->__po_token_data[1] = 0;
		po->__po_token_len = 0;
		po->__po_token_type = 0;
		*pflags &= ~PKT_F_OPT_TOKEN;
	}
	return 0;
}

__attribute__((always_inline))
static inline void
__packet_set_tx_timestamp(const uint64_t ph, const uint64_t ts)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;

	if (po != NULL) {
		po->__po_pkt_tx_time = ts;
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_TX_TIMESTAMP;
	}
}

__attribute__((always_inline))
static inline uint64_t
__packet_get_tx_timestamp(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if (po == NULL || (PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_TX_TIMESTAMP) == 0) {
		return 0;
	}

	return po->__po_pkt_tx_time;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_token(const uint64_t ph,
    const void *__sized_by(len)token, const uint16_t len)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return __packet_opt_set_token(PKT_ADDR(ph)->pkt_com_opt, token, len,
	           PKT_OPT_TOKEN_TYPE_OPAQUE, &PKT_ADDR(ph)->pkt_pflags);
}

__attribute__((always_inline))
static inline errno_t
__packet_get_packetid(const uint64_t ph, packet_id_t *pktid)
{
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	uint16_t len = sizeof(packet_id_t);
	uint8_t type;
	errno_t err;


	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_TOKEN) == 0) {
		return ENOENT;
	}
	err = __packet_opt_get_token(po, (packet_id_t * __header_indexable)pktid,
	    &len, &type);
	if ((err == 0) && ((type != PKT_OPT_TOKEN_TYPE_PACKET_ID) ||
	    (len != sizeof(packet_id_t)))) {
		err = ENOENT;
	}
	return err;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_packetid(const uint64_t ph, const packet_id_t *pktid)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return __packet_opt_set_token(PKT_ADDR(ph)->pkt_com_opt, pktid,
	           sizeof(packet_id_t), PKT_OPT_TOKEN_TYPE_PACKET_ID,
	           &PKT_ADDR(ph)->pkt_pflags);
}

__attribute__((always_inline))
static inline errno_t
__packet_get_vlan_tag(const uint64_t ph, uint16_t *vlan_tag,
    boolean_t *tag_in_pkt)
{
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	uint64_t pflags;

	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	pflags = PKT_ADDR(ph)->pkt_pflags;
	if ((pflags & PKT_F_OPT_VLTAG) == 0) {
		return ENOENT;
	}
	if (vlan_tag != NULL) {
		*vlan_tag = po->__po_vlan_tag;
	}
	if (tag_in_pkt != NULL) {
		*tag_in_pkt = ((pflags & PKT_F_OPT_VLTAG_IN_PKT) != 0);
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_vlan_tag(const uint64_t ph, const uint16_t vlan_tag,
    const boolean_t tag_in_pkt)
{
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;

	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_VLTAG;
	po->__po_vlan_tag = vlan_tag;

	if (tag_in_pkt) {
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_VLTAG_IN_PKT;
	}
	return 0;
}

__attribute__((always_inline))
static inline uint16_t
__packet_get_vlan_id(const uint16_t vlan_tag)
{
	return EVL_VLANOFTAG(vlan_tag);
}

__attribute__((always_inline))
static inline uint8_t
__packet_get_vlan_priority(const uint16_t vlan_tag)
{
	return EVL_PRIOFTAG(vlan_tag);
}

__attribute__((always_inline))
static inline errno_t
__packet_get_app_metadata(const uint64_t ph,
    packet_app_metadata_type_t *app_type, uint8_t *app_metadata)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if (app_type == NULL || app_metadata == NULL) {
		return EINVAL;
	}
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_APP_METADATA) == 0) {
		return ENOENT;
	}
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if (po->__po_app_type == PACKET_APP_METADATA_TYPE_UNSPECIFIED) {
		return ENOENT;
	}
	*app_type = po->__po_app_type;
	*app_metadata = po->__po_app_metadata;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_app_metadata(const uint64_t ph,
    const packet_app_metadata_type_t app_type, const uint8_t app_metadata)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
	if (app_type < PACKET_APP_METADATA_TYPE_MIN ||
	    app_type > PACKET_APP_METADATA_TYPE_MAX) {
		po->__po_app_type = PACKET_APP_METADATA_TYPE_UNSPECIFIED;
		PKT_ADDR(ph)->pkt_pflags &= ~PKT_F_OPT_APP_METADATA;
		return EINVAL;
	}
	po->__po_app_type = app_type;
	po->__po_app_metadata = app_metadata;
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_OPT_APP_METADATA;
	return 0;
}

__attribute__((always_inline))
static inline void
__packet_set_wake_flag(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_WAKE_PKT;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_wake_flag(const uint64_t ph)
{
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_WAKE_PKT) != 0;
}

__attribute__((always_inline))
static inline void
__packet_set_keep_alive(const uint64_t ph, const boolean_t is_keep_alive)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if (is_keep_alive) {
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_KEEPALIVE;
	} else {
		PKT_ADDR(ph)->pkt_pflags &= ~PKT_F_KEEPALIVE;
	}
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_keep_alive(const uint64_t ph)
{
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_KEEPALIVE) != 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_truncated(const uint64_t ph)
{
	PKT_SUBTYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET, NEXUS_META_SUBTYPE_RAW);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_TRUNCATED) != 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_transport_new_flow(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_NEW_FLOW) != 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_transport_last_packet(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_LAST_PKT) != 0;
}

__attribute__((always_inline))
static inline boolean_t
__packet_get_l4s_flag(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return (PKT_ADDR(ph)->pkt_pflags & PKT_F_L4S) != 0;
}

__attribute__((always_inline))
static inline void
__packet_set_l4s_flag(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_L4S;
}

__attribute__((always_inline))
static inline int
__packet_set_service_class(const uint64_t ph, const uint32_t sc)
{
	int err = 0;

	_CASSERT(sizeof(QUM_ADDR(ph)->qum_svc_class == sizeof(uint32_t)));

	switch (sc) {
	case PKT_SC_BE:
	case PKT_SC_BK_SYS:
	case PKT_SC_BK:
	case PKT_SC_RD:
	case PKT_SC_OAM:
	case PKT_SC_AV:
	case PKT_SC_RV:
	case PKT_SC_VI:
	case PKT_SC_SIG:
	case PKT_SC_VO:
	case PKT_SC_CTL:
		QUM_ADDR(ph)->qum_svc_class = sc;
		break;

	default:
		err = EINVAL;
		break;
	}

	return err;
}

__attribute__((always_inline))
static inline uint32_t
__packet_get_service_class(const uint64_t ph)
{
	uint32_t sc;

	_CASSERT(sizeof(QUM_ADDR(ph)->qum_svc_class == sizeof(uint32_t)));

	switch (QUM_ADDR(ph)->qum_svc_class) {
	case PKT_SC_BE:         
	case PKT_SC_BK_SYS:
	case PKT_SC_BK:
	case PKT_SC_RD:
	case PKT_SC_OAM:
	case PKT_SC_AV:
	case PKT_SC_RV:
	case PKT_SC_VI:
	case PKT_SC_SIG:
	case PKT_SC_VO:
	case PKT_SC_CTL:
		sc = QUM_ADDR(ph)->qum_svc_class;
		break;

	default:
		sc = PKT_SC_BE;
		break;
	}

	return sc;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_comp_gencnt(const uint64_t ph, const uint32_t gencnt)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_comp_gencnt == sizeof(uint32_t)));
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	PKT_ADDR(ph)->pkt_comp_gencnt = gencnt;

	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_comp_gencnt(const uint64_t ph, uint32_t *pgencnt)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_comp_gencnt == sizeof(uint32_t)));
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	if (pgencnt == NULL) {
		return EINVAL;
	}

	if (PKT_ADDR(ph)->pkt_comp_gencnt == 0) {
		return ENOENT;
	}

	*pgencnt = PKT_ADDR(ph)->pkt_comp_gencnt;
	return 0;
}

__attribute__((always_inline))
static inline int
__packet_set_traffic_class(const uint64_t ph, const uint32_t tc)
{
	uint32_t val = PKT_TC2SCVAL(tc);        
	uint32_t sc;

	switch (val) {
	case PKT_SCVAL_BK_SYS:
		sc = PKT_SC_BK_SYS;
		break;
	case PKT_SCVAL_BK:
		sc = PKT_SC_BK;
		break;
	case PKT_SCVAL_BE:
		sc = PKT_SC_BE;
		break;
	case PKT_SCVAL_RD:
		sc = PKT_SC_RD;
		break;
	case PKT_SCVAL_OAM:
		sc = PKT_SC_OAM;
		break;
	case PKT_SCVAL_AV:
		sc = PKT_SC_AV;
		break;
	case PKT_SCVAL_RV:
		sc = PKT_SC_RV;
		break;
	case PKT_SCVAL_VI:
		sc = PKT_SC_VI;
		break;
	case PKT_SCVAL_SIG:
		sc = PKT_SC_SIG;
		break;
	case PKT_SCVAL_VO:
		sc = PKT_SC_VO;
		break;
	case PKT_SCVAL_CTL:
		sc = PKT_SC_CTL;
		break;
	default:
		sc = PKT_SC_BE;
		break;
	}

	return __packet_set_service_class(ph, sc);
}

__attribute__((always_inline))
static inline uint32_t
__packet_get_traffic_class(const uint64_t ph)
{
	return PKT_SC2TC(__packet_get_service_class(ph));
}

__attribute__((always_inline))
static inline int
__packet_set_inet_checksum(const uint64_t ph, const packet_csum_flags_t flags,
    const uint16_t start, const uint16_t stuff_val, boolean_t tx)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	PKT_ADDR(ph)->pkt_csum_flags = flags & PACKET_CSUM_FLAGS;

	if (tx) {
		PKT_ADDR(ph)->pkt_csum_tx_start_off = start;
		PKT_ADDR(ph)->pkt_csum_tx_stuff_off = stuff_val;
	} else {
		PKT_ADDR(ph)->pkt_csum_rx_start_off = start;
		PKT_ADDR(ph)->pkt_csum_rx_value = stuff_val;
	}
	return 0;
}

__attribute__((always_inline))
static inline void
__packet_add_inet_csum_flags(const uint64_t ph, const packet_csum_flags_t flags)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	PKT_ADDR(ph)->pkt_csum_flags |= flags & PACKET_CSUM_FLAGS;
}

__attribute__((always_inline))
static inline packet_csum_flags_t
__packet_get_inet_checksum(const uint64_t ph, uint16_t *start,
    uint16_t *stuff_val, boolean_t tx)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	if (tx) {
		if (__probable(start != NULL)) {
			*start = PKT_ADDR(ph)->pkt_csum_tx_start_off;
		}
		if (__probable(stuff_val != NULL)) {
			*stuff_val = PKT_ADDR(ph)->pkt_csum_tx_stuff_off;
		}
	} else {
		if (__probable(start != NULL)) {
			*start = PKT_ADDR(ph)->pkt_csum_rx_start_off;
		}
		if (__probable(stuff_val != NULL)) {
			*stuff_val = PKT_ADDR(ph)->pkt_csum_rx_value;
		}
	}
	return PKT_ADDR(ph)->pkt_csum_flags & PACKET_CSUM_FLAGS;
}

__attribute__((always_inline))
static inline void
__packet_set_flow_uuid(const uint64_t ph, const uuid_t flow_uuid)
{
	struct __quantum *q = &QUM_ADDR(ph)->qum_com;

	
	if (__probable(IS_P2ALIGNED(flow_uuid, sizeof(uint64_t)))) {
		const uint64_t *id_64 = (const uint64_t *)(const void *)flow_uuid;
		q->__q_flow_id_val64[0] = id_64[0];
		q->__q_flow_id_val64[1] = id_64[1];
	} else if (__probable(IS_P2ALIGNED(flow_uuid, sizeof(uint32_t)))) {
		const uint32_t *id_32 = (const uint32_t *)(const void *)flow_uuid;
		q->__q_flow_id_val32[0] = id_32[0];
		q->__q_flow_id_val32[1] = id_32[1];
		q->__q_flow_id_val32[2] = id_32[2];
		q->__q_flow_id_val32[3] = id_32[3];
	} else {
		bcopy(flow_uuid, q->__q_flow_id, sizeof(uuid_t));
	}
}

__attribute__((always_inline))
static inline void
__packet_get_flow_uuid(const uint64_t ph, uuid_t flow_uuid)
{
	struct __quantum *q = &QUM_ADDR(ph)->qum_com;

	
	if (__probable(IS_P2ALIGNED(flow_uuid, sizeof(uint64_t)))) {
		uint64_t *id_64 = (uint64_t *)(void *)flow_uuid;
		id_64[0] = q->__q_flow_id_val64[0];
		id_64[1] = q->__q_flow_id_val64[1];
	} else if (__probable(IS_P2ALIGNED(flow_uuid, sizeof(uint32_t)))) {
		uint32_t *id_32 = (uint32_t *)(void *)flow_uuid;
		id_32[0] = q->__q_flow_id_val32[0];
		id_32[1] = q->__q_flow_id_val32[1];
		id_32[2] = q->__q_flow_id_val32[2];
		id_32[3] = q->__q_flow_id_val32[3];
	} else {
		bcopy(q->__q_flow_id, flow_uuid, sizeof(uuid_t));
	}
}

__attribute__((always_inline))
static inline void
__packet_clear_flow_uuid(const uint64_t ph)
{
	struct __quantum *q = &QUM_ADDR(ph)->qum_com;
	q->__q_flow_id_val64[0] = 0;
	q->__q_flow_id_val64[1] = 0;
}

__attribute__((always_inline))
static inline uint8_t
__packet_get_aggregation_type(const uint64_t ph)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_aggr_type == sizeof(uint8_t)));
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	return PKT_ADDR(ph)->pkt_aggr_type;
}

__attribute__((always_inline))
static inline uint32_t
__packet_get_data_length(const uint64_t ph)
{
	return QUM_ADDR(ph)->qum_len;
}


__attribute__((always_inline))
static inline uint32_t
__packet_get_real_data_length(const struct __kern_packet *pkt)
{
	uint32_t pkt_len;

	if (pkt->pkt_pflags & PKT_F_TRUNCATED) {
		struct __kern_buflet *bft;

		bft = kern_packet_get_next_buflet(SK_PKT2PH(pkt), NULL);
		pkt_len = kern_buflet_get_data_length(bft);
	} else {
		pkt_len = pkt->pkt_length;
	}
	return pkt_len;
}

__attribute__((always_inline))
static inline uint16_t
__packet_get_buflet_count(const uint64_t ph)
{
	uint16_t bcnt = 0;

	switch (SK_PTR_TYPE(ph)) {
	case NEXUS_META_TYPE_PACKET:
		bcnt = PKT_ADDR(ph)->pkt_bufs_cnt;
		VERIFY(bcnt != 0 ||
		    PP_HAS_BUFFER_ON_DEMAND(PKT_ADDR(ph)->pkt_qum.qum_pp));
		break;
	case NEXUS_META_TYPE_QUANTUM:
		bcnt = 1;
		break;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
		break;
	}
	return bcnt;
}

__attribute__((always_inline))
static inline int
__packet_add_buflet(const uint64_t ph, const void *bprev0, const void *bnew0)
{
	uint16_t bcnt;

	kern_buflet_t bprev = __DECONST(kern_buflet_t, bprev0);
	kern_buflet_t bnew = __DECONST(kern_buflet_t, bnew0);

	VERIFY(PKT_ADDR(ph) && bnew && (bnew != bprev));
	VERIFY(PP_HAS_BUFFER_ON_DEMAND(PKT_ADDR(ph)->pkt_qum.qum_pp));

	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	bcnt = PKT_ADDR(ph)->pkt_bufs_cnt;

	VERIFY((bprev != NULL || bcnt == 0) &&
	    (bcnt < PKT_ADDR(ph)->pkt_bufs_max));


	if (bprev == NULL) {
		bprev = &PKT_ADDR(ph)->pkt_qum_buf;
	}
	KBUF_LINK(bprev, bnew);

	*(uint16_t *)(uintptr_t)&PKT_ADDR(ph)->pkt_bufs_cnt = ++bcnt;
	return 0;
}

__attribute__((always_inline))
static inline struct __kern_buflet *
__packet_get_next_buflet(const uint64_t ph, const void *bprev0)
{
	kern_buflet_t bprev = __DECONST(kern_buflet_t, bprev0);
	struct __kern_buflet *__single bcur = NULL;

	switch (SK_PTR_TYPE(ph)) {
	case NEXUS_META_TYPE_PACKET: {
		uint32_t bcnt = PKT_ADDR(ph)->pkt_bufs_cnt;
		ASSERT(bcnt != 0 ||
		    PP_HAS_BUFFER_ON_DEMAND(PKT_ADDR(ph)->pkt_qum.qum_pp));
		PKT_GET_NEXT_BUFLET(PKT_ADDR(ph), bcnt, BLT_ADDR(bprev), bcur);
		break;
	}
	case NEXUS_META_TYPE_QUANTUM:
		QUM_GET_NEXT_BUFLET(QUM_ADDR(ph), BLT_ADDR(bprev), bcur);
		break;
	default:
		VERIFY(0);
		
		__builtin_unreachable();
		break;
	}
	return bcur;
}

__attribute__((always_inline))
static inline uint8_t
__packet_get_segment_count(const uint64_t ph)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_seg_cnt == sizeof(uint8_t)));
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	return PKT_ADDR(ph)->pkt_seg_cnt;
}

__attribute__((always_inline))
static inline void
__packet_set_segment_count(const uint64_t ph, uint8_t segcount)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_seg_cnt == sizeof(uint8_t)));
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	PKT_ADDR(ph)->pkt_seg_cnt = segcount;
}

__attribute__((always_inline))
static inline uint16_t
__packet_get_protocol_segment_size(const uint64_t ph)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	return PKT_ADDR(ph)->pkt_proto_seg_sz;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_protocol_segment_size(const uint64_t ph, uint16_t proto_seg_sz)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_proto_seg_sz = proto_seg_sz;
	return 0;
}

__attribute__((always_inline))
static inline void
__packet_get_tso_flags(const uint64_t ph, packet_tso_flags_t *flags)
{
	_CASSERT(sizeof(PKT_ADDR(ph)->pkt_proto_seg_sz == sizeof(uint16_t)));

	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	*flags = PKT_ADDR(ph)->pkt_csum_flags & (PACKET_CSUM_TSO_FLAGS);
}

__attribute__((always_inline))
static inline void
__packet_set_tso_flags(const uint64_t ph, packet_tso_flags_t flags)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	PKT_ADDR(ph)->pkt_csum_flags |= flags & (PACKET_CSUM_TSO_FLAGS);
}

__attribute__((always_inline))
static inline uint32_t
__buflet_get_data_limit(const void *buf)
{
	return BLT_ADDR(buf)->buf_dlim;
}

__attribute__((always_inline))
static inline errno_t
__buflet_set_data_limit(const void *buf, const uint32_t dlim)
{
	
	ASSERT(BLT_ADDR(buf)->buf_ctl->bc_flags & SKMEM_BUFCTL_SHAREOK);

	
	if (__probable((uint32_t)dlim <= BLT_ADDR(buf)->buf_objlim)) {
		_CASSERT(sizeof(BLT_ADDR(buf)->buf_dlim) == sizeof(uint32_t));
		
		*(uint32_t *)(uintptr_t)&BLT_ADDR(buf)->buf_dlim = dlim;
		return 0;
	}
	return ERANGE;
}

__attribute__((always_inline))
static inline uint32_t
__buflet_get_data_offset(const void *buf)
{
	return BLT_ADDR(buf)->buf_doff;
}


__attribute__((always_inline))
static inline int
__packet_finalize(const uint64_t ph)
{
	void *__single bcur = NULL, *__single bprev = NULL;
	uint32_t len, bcnt, bdoff0, bdlim0;
	int err = 0;

	ASSERT(QUM_ADDR(ph)->qum_qflags & QUM_F_INTERNALIZED);
	QUM_ADDR(ph)->qum_qflags &= ~(QUM_F_DROPPED | QUM_F_FINALIZED);

	bcnt = __packet_get_buflet_count(ph);
	len = QUM_ADDR(ph)->qum_len = 0;

	while (bcnt--) {
		bcur = __packet_get_next_buflet(ph, bprev);

		ASSERT(bcur != NULL);
		ASSERT(BLT_ADDR(bcur)->buf_addr != 0);

		
		if (bprev == NULL) {
			bdoff0 = __buflet_get_data_offset(bcur);
			bdlim0 = __buflet_get_data_limit(bcur);
		}

		len += BLT_ADDR(bcur)->buf_dlen;
		bprev = bcur;
	}

	if (__improbable(err != 0)) {
		goto done;
	}

	switch (SK_PTR_TYPE(ph)) {
	case NEXUS_META_TYPE_PACKET:
		if (__improbable(bdoff0 > UINT8_MAX)) {
			err = ERANGE;
			goto done;
		}
		
		PKT_ADDR(ph)->pkt_headroom = (uint8_t)bdoff0;
		
		switch (SK_PTR_SUBTYPE(ph)) {
		case NEXUS_META_SUBTYPE_RAW:
			break;
		case NEXUS_META_SUBTYPE_PAYLOAD:
			
			if (__improbable((PKT_ADDR(ph)->pkt_headroom != 0) ||
			    (PKT_ADDR(ph)->pkt_l2_len != 0))) {
				err = ERANGE;
				goto done;
			}
			break;
		default:
			VERIFY(0);
			
			__builtin_unreachable();
			break;
		}

		if (__improbable(PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_DATA)) {
			struct __packet_opt *po = PKT_ADDR(ph)->pkt_com_opt;
			if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_EXPIRE_TS) &&
			    po->__po_expire_ts == 0) {
				err = EINVAL;
				goto done;
			}
			if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_OPT_TOKEN) &&
			    po->__po_token_len == 0) {
				err =  EINVAL;
				goto done;
			}
			ASSERT(err == 0);
		}

		
		break;

	default:
		
		break;
	}

done:
	if (__probable(err == 0)) {
		QUM_ADDR(ph)->qum_len = len;
		QUM_ADDR(ph)->qum_qflags |= QUM_F_FINALIZED;
	} else {
		QUM_ADDR(ph)->qum_len = 0;
		QUM_ADDR(ph)->qum_qflags |= QUM_F_DROPPED;
	}

	return err;
}

__attribute__((always_inline))
static inline boolean_t
__packet_is_finalized(const uint64_t ph)
{
	return QUM_ADDR(ph)->qum_qflags & QUM_F_FINALIZED;
}


__attribute__((always_inline))
static inline int
__packet_initialize_with_mbufchain(struct __kern_packet *pkt, struct mbuf *mbuf,
    uint8_t headroom, uint8_t l2len)
{
	VERIFY(METADATA_TYPE(pkt) == NEXUS_META_TYPE_PACKET);
	VERIFY(pkt->pkt_qum.qum_qflags & QUM_F_INTERNALIZED);
	VERIFY((pkt->pkt_pflags & PKT_F_MBUF_MASK) == 0);
	VERIFY((pkt->pkt_pflags & PKT_F_PKT_DATA) == 0);
	VERIFY(pkt->pkt_mbuf == NULL);

	pkt->pkt_qum.qum_qflags &= ~(QUM_F_DROPPED | QUM_F_FINALIZED);
	pkt->pkt_mbuf = mbuf;
	pkt->pkt_pflags |= (PKT_F_MBUF_DATA | PKT_F_TRUNCATED);
	pkt->pkt_headroom = headroom;
	pkt->pkt_l2_len = l2len;
	pkt->pkt_length = m_pktlen(mbuf);
	pkt->pkt_qum_buf.buf_dlen = 0;
	pkt->pkt_qum_buf.buf_doff = 0;
	pkt->pkt_qum.qum_qflags |= QUM_F_FINALIZED;
	return 0;
}

__attribute__((always_inline))
static inline int
__packet_initialize_with_mbuf(struct __kern_packet *pkt, struct mbuf *mbuf,
    uint8_t headroom, uint8_t l2len)
{
	__packet_initialize_with_mbufchain(pkt, mbuf, headroom, l2len);
	VERIFY(mbuf->m_nextpkt == NULL);
	return 0;
}


__attribute__((always_inline))
static inline int
__packet_finalize_with_mbuf(struct __kern_packet *pkt)
{
	uint32_t bdlen, bdoff, bdlim;
	struct __kern_buflet *buf;
	int err = 0;

	VERIFY(METADATA_TYPE(pkt) == NEXUS_META_TYPE_PACKET);
	VERIFY((pkt->pkt_pflags & (PKT_F_MBUF_DATA | PKT_F_PKT_DATA)) ==
	    PKT_F_MBUF_DATA);
	VERIFY(pkt->pkt_mbuf != NULL);
	ASSERT(pkt->pkt_qum.qum_qflags & QUM_F_INTERNALIZED);
	VERIFY(pkt->pkt_bufs_cnt == 1);
	PKT_GET_FIRST_BUFLET(pkt, pkt->pkt_bufs_cnt, buf);
	ASSERT(buf->buf_addr != 0);

	pkt->pkt_qum.qum_qflags &= ~(QUM_F_DROPPED | QUM_F_FINALIZED);
	pkt->pkt_pflags &= ~PKT_F_TRUNCATED;
	bdlen = buf->buf_dlen;
	bdlim = buf->buf_dlim;
	bdoff = buf->buf_doff;
	if (__improbable(!BUF_IN_RANGE(buf))) {
		err = ERANGE;
		goto done;
	}

	
	switch (METADATA_SUBTYPE(pkt)) {
	case NEXUS_META_SUBTYPE_RAW:
		if (__improbable((pkt->pkt_headroom != bdoff) ||
		    (pkt->pkt_headroom >= bdlim))) {
			err = ERANGE;
			goto done;
		}
		if (__improbable((pkt->pkt_headroom +
		    pkt->pkt_l2_len) >= bdlim)) {
			err = ERANGE;
			goto done;
		}
		break;

	case NEXUS_META_SUBTYPE_PAYLOAD:
		
		if (__improbable((pkt->pkt_headroom != 0) || (bdoff != 0) ||
		    (pkt->pkt_l2_len != 0))) {
			err = ERANGE;
			goto done;
		}
		break;

	default:
		VERIFY(0);
		
		__builtin_unreachable();
		break;
	}


	if (__improbable(pkt->pkt_pflags & PKT_F_OPT_DATA)) {
		struct __packet_opt *po = pkt->pkt_com_opt;

		if ((pkt->pkt_pflags & PKT_F_OPT_EXPIRE_TS) &&
		    po->__po_expire_ts == 0) {
			err = EINVAL;
			goto done;
		}
		if ((pkt->pkt_pflags & PKT_F_OPT_TOKEN) &&
		    po->__po_token_len == 0) {
			err =  EINVAL;
			goto done;
		}
	}
	ASSERT(err == 0);

done:
	if (__probable(err == 0)) {
		pkt->pkt_length = (uint32_t)m_pktlen(pkt->pkt_mbuf);
		if (bdlen < pkt->pkt_length) {
			pkt->pkt_pflags |= PKT_F_TRUNCATED;
		}
		pkt->pkt_qum.qum_qflags |= QUM_F_FINALIZED;
	} else {
		pkt->pkt_length = 0;
		pkt->pkt_qum.qum_qflags |= QUM_F_DROPPED;
	}

	return err;
}

__attribute__((always_inline))
static inline uint32_t
__packet_get_object_index(const uint64_t ph)
{
	return METADATA_IDX(QUM_ADDR(ph));
}

__attribute__((always_inline))
static inline errno_t
__packet_get_timestamp(const uint64_t ph, uint64_t *ts, boolean_t *valid)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_TS_VALID) != 0) {
		if (valid != NULL) {
			*valid = TRUE;
		}
		*ts = PKT_ADDR(ph)->pkt_timestamp;
	} else {
		if (valid != NULL) {
			*valid = FALSE;
		}
		*ts = 0;
	}

	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_timestamp(const uint64_t ph, uint64_t ts, boolean_t valid)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);

	if (valid) {
		PKT_ADDR(ph)->pkt_timestamp = ts;
		PKT_ADDR(ph)->pkt_pflags |= PKT_F_TS_VALID;
	} else {
		PKT_ADDR(ph)->pkt_pflags &= ~PKT_F_TS_VALID;
		PKT_ADDR(ph)->pkt_timestamp = 0;
	}

	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_tx_completion_data(const uint64_t ph, uintptr_t *cb_arg,
    uintptr_t *cb_data)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_COMPL_DATA) != 0) {
		ASSERT((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_COMPL_ALLOC));
		*cb_arg = PKT_ADDR(ph)->pkt_tx_compl_cb_arg;
		*cb_data = PKT_ADDR(ph)->pkt_tx_compl_cb_data;
	} else {
		*cb_arg = 0;
		*cb_data = 0;
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_tx_completion_data(const uint64_t ph, uintptr_t cb_arg,
    uintptr_t cb_data)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	_KPKT_INIT_TX_COMPL_DATA(PKT_ADDR(ph));
	PKT_ADDR(ph)->pkt_tx_compl_cb_arg = cb_arg;
	PKT_ADDR(ph)->pkt_tx_compl_cb_data = cb_data;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_timestamp_requested(const uint64_t ph, boolean_t *requested)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_COMPL_TS_REQ) != 0) {
		*requested = TRUE;
	} else {
		*requested = FALSE;
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_tx_completion_status(const uint64_t ph, kern_return_t *status)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_COMPL_DATA) != 0) {
		ASSERT((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_COMPL_ALLOC));
		*status = (kern_return_t)PKT_ADDR(ph)->pkt_tx_compl_status;
	} else {
		*status = 0;
	}
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_tx_completion_status(const uint64_t ph, kern_return_t status)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	_KPKT_INIT_TX_COMPL_DATA(PKT_ADDR(ph));
	PKT_ADDR(ph)->pkt_tx_compl_status = (uint32_t)status;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_set_tx_nx_port(const uint64_t ph, nexus_port_t nx_port,
    uint16_t vpna_gencnt)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	PKT_ADDR(ph)->pkt_nx_port = nx_port;
	PKT_ADDR(ph)->pkt_vpna_gencnt = vpna_gencnt;
	PKT_ADDR(ph)->pkt_pflags |= PKT_F_TX_PORT_DATA;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_tx_nx_port(const uint64_t ph, nexus_port_t *nx_port,
    uint16_t *vpna_gencnt)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_TX_PORT_DATA) == 0) {
		return ENOTSUP;
	}

	*nx_port = PKT_ADDR(ph)->pkt_nx_port;
	*vpna_gencnt = PKT_ADDR(ph)->pkt_vpna_gencnt;
	return 0;
}

__attribute__((always_inline))
static inline errno_t
__packet_get_tx_nx_port_id(const uint64_t ph, uint32_t *nx_port_id)
{
	errno_t err;
	nexus_port_t nx_port;
	uint16_t vpna_gencnt;

	_CASSERT(sizeof(nx_port) == sizeof(uint16_t));

	err = __packet_get_tx_nx_port(ph, &nx_port, &vpna_gencnt);
	if (err == 0) {
		*nx_port_id = PKT_COMPOSE_NX_PORT_ID(nx_port, vpna_gencnt);
	}
	return err;
}


__attribute__((always_inline))
static inline errno_t
__packet_get_flowid(const uint64_t ph, packet_flowid_t *pflowid)
{
	PKT_TYPE_ASSERT(ph, NEXUS_META_TYPE_PACKET);
	if ((PKT_ADDR(ph)->pkt_pflags & PKT_F_FLOW_ID) == 0) {
		return ENOENT;
	}
	*pflowid = PKT_ADDR(ph)->pkt_flow_token;
	return 0;
}

extern uint32_t os_cpu_in_cksum(const void *, uint32_t, uint32_t);
__attribute__((always_inline))
static inline uint16_t
__packet_fold_sum(uint32_t sum)
{
	sum = (sum >> 16) + (sum & 0xffff);     
	sum = (sum >> 16) + (sum & 0xffff);     
	sum = (sum >> 16) + (sum & 0xffff);     
	return sum & 0xffff;
}

__attribute__((always_inline))
static inline uint16_t
__packet_fold_sum_final(uint32_t sum)
{
	sum = (sum >> 16) + (sum & 0xffff);     
	sum = (sum >> 16) + (sum & 0xffff);     
	sum = (sum >> 16) + (sum & 0xffff);     
	return ~sum & 0xffff;
}

__attribute__((always_inline))
static inline uint32_t
__packet_cksum(const void *data, uint32_t len, uint32_t sum0)
{
	return os_cpu_in_cksum(data, len, sum0);
}

extern uint32_t os_cpu_copy_in_cksum(const void *__sized_by(len), void *__sized_by(len),
    uint32_t len, uint32_t);
KPKTQ_HEAD(pktq);
__attribute__((always_inline))
inline boolean_t
_UUID_MATCH(uuid_t u1, uuid_t u2)
{
	uint64_t *a = (uint64_t *)(void *) u1;
	uint64_t *b = (uint64_t *)(void *) u2;
	bool first_same = (a[0] == b[0]);
	bool second_same = (a[1] == b[1]);

	return first_same && second_same;
}














































__attribute__((always_inline))
static inline kern_packet_t
SD_GET_TAGGED_METADATA(const struct __kern_slot_desc *ksd)
{
	return __improbable(ksd->sd_md == NULL) ? 0 :
	       SK_PTR_ENCODE(ksd->sd_md, METADATA_TYPE(ksd->sd_qum),
	           METADATA_SUBTYPE(ksd->sd_qum));
}

__attribute__((always_inline))
static inline errno_t
KR_SLOT_ATTACH_METADATA(const kern_channel_ring_t kring,
    struct __kern_slot_desc *ksd, struct __kern_quantum *kqum)
{
	obj_idx_t idx = KR_SLOT_INDEX(kring,
	    (struct __slot_desc *)(void *)ksd);

	
	ASSERT(sk_is_sync_protected());
	ASSERT(kqum->qum_pp == kring->ckr_pp);
	ASSERT(kqum->qum_ksd == NULL);
	
	ASSERT(kqum->qum_qflags & QUM_F_INTERNALIZED);
	ASSERT(((kqum->qum_qflags & QUM_F_FINALIZED) != 0) ^
	    ((kqum->qum_qflags & QUM_F_DROPPED) != 0));

	kqum->qum_ksd = ksd;

	KSD_ATTACH_METADATA(ksd, kqum);
	if (!KR_KERNEL_ONLY(kring)) {
		USD_ATTACH_METADATA(KR_USD(kring, idx), METADATA_IDX(kqum));
	}

	return 0;
}

__attribute__((always_inline))
static inline struct __kern_quantum *
KR_SLOT_DETACH_METADATA(const kern_channel_ring_t kring,
    struct __kern_slot_desc *ksd)
{
	struct __kern_quantum *kqum = ksd->sd_qum;
	obj_idx_t idx = KR_SLOT_INDEX(kring,
	    (struct __slot_desc *)(void *)ksd);

	
	ASSERT(sk_is_sync_protected());
	ASSERT(KSD_VALID_METADATA(ksd));
	ASSERT(kqum->qum_ksd == ksd);
	ASSERT(kqum->qum_pp == kring->ckr_pp);
	
	ASSERT((kqum->qum_qflags & (QUM_F_INTERNALIZED)) ||
	    ((((kqum->qum_qflags & QUM_F_FINALIZED) != 0) ^
	    ((kqum->qum_qflags & QUM_F_DROPPED) != 0))));

	
	kqum->qum_qflags &= ~QUM_F_FINALIZED;
	kqum->qum_ksd = NULL;

	KSD_DETACH_METADATA(ksd);
	if (!KR_KERNEL_ONLY(kring)) {
		USD_DETACH_METADATA(KR_USD(kring, idx));
	}

	return kqum;
}

__attribute__((always_inline))
static inline errno_t
KR_SLOT_ATTACH_BUF_METADATA(const kern_channel_ring_t kring,
    struct __kern_slot_desc *ksd, struct __kern_buflet *kbuf)
{
	obj_idx_t idx = KR_SLOT_INDEX(kring,
	    (struct __slot_desc *)(void *)ksd);

	
	ASSERT(sk_is_sync_protected());

	KSD_ATTACH_METADATA(ksd, kbuf);
	
	ASSERT(!KR_KERNEL_ONLY(kring));
	ASSERT(kring->ckr_tx == CR_KIND_ALLOC);
	USD_ATTACH_METADATA(KR_USD(kring, idx), kbuf->buf_bft_idx_reg);
	return 0;
}


typedef void (pkt_copy_from_pkt_t)(const enum txrx, kern_packet_t,
    const uint16_t, kern_packet_t, const uint16_t, const uint32_t,
    const boolean_t, const uint16_t, const uint16_t, const boolean_t);
__BEGIN_DECLS
extern void pkt_subtype_assert_fail(const kern_packet_t, uint64_t, uint64_t);
extern void pkt_type_assert_fail(const kern_packet_t, uint64_t);
extern void pkt_copypkt_sum(kern_packet_t, uint16_t, kern_packet_t,
    uint16_t, uint16_t, uint32_t *, boolean_t);
extern uint32_t
    pkt_copyaddr_sum(kern_packet_t sph, uint16_t soff, uint8_t *__sized_by(len) dbaddr,
    uint32_t len, boolean_t do_csum, uint32_t initial_sum, boolean_t *odd_start);
extern uint32_t pkt_sum(kern_packet_t, uint16_t, uint16_t);
extern uint32_t pkt_mcopypkt_sum(mbuf_t, int, kern_packet_t, uint16_t,
    uint16_t, boolean_t);
extern uint32_t
    m_copydata_sum(struct mbuf *m, int off, int len, void *__sized_by(len) vp, uint32_t initial_sum,
    boolean_t *odd_start);
extern void pkt_copy(void *__sized_by(len) src, void *__sized_by(len) dst,
    size_t len);
__BEGIN_DECLS
extern int pp_init(void);
extern void pp_fini(void);
extern void pp_close(struct kern_pbufpool *);
extern struct kern_pbufpool *pp_create(const char *name,
    struct skmem_region_params srp_array[SKMEM_REGIONS], pbuf_seg_ctor_fn_t buf_seg_ctor,
    pbuf_seg_dtor_fn_t buf_seg_dtor, const void *ctx,
    pbuf_ctx_retain_fn_t ctx_retain, pbuf_ctx_release_fn_t ctx_release,
    uint32_t ppcreatef);
extern void pp_destroy(struct kern_pbufpool *);
extern int pp_init_upp(struct kern_pbufpool *, boolean_t);
extern void pp_insert_upp(struct kern_pbufpool *, struct __kern_quantum *,
    pid_t);
extern void pp_insert_upp_locked(struct kern_pbufpool *,
    struct __kern_quantum *, pid_t);
extern void pp_insert_upp_batch(struct kern_pbufpool *pp, pid_t pid,
    uint64_t *__counted_by(num) array, uint32_t num);
extern struct __kern_quantum *pp_remove_upp(struct kern_pbufpool *, obj_idx_t,
    int *);
extern struct __kern_quantum *pp_remove_upp_locked(struct kern_pbufpool *,
    obj_idx_t, int *);
extern struct __kern_quantum *pp_find_upp(struct kern_pbufpool *, obj_idx_t);
extern void pp_purge_upp(struct kern_pbufpool *, pid_t);
extern struct __kern_buflet *pp_remove_upp_bft(struct kern_pbufpool *,
    obj_idx_t, int *);
extern void pp_insert_upp_bft(struct kern_pbufpool *, struct __kern_buflet *,
    pid_t);
extern boolean_t pp_isempty_upp(struct kern_pbufpool *);
extern void pp_retain_locked(struct kern_pbufpool *);
extern void pp_retain(struct kern_pbufpool *);
extern boolean_t pp_release_locked(struct kern_pbufpool *);
extern boolean_t pp_release(struct kern_pbufpool *);
extern void pp_regions_params_adjust(struct skmem_region_params srp_array[SKMEM_REGIONS],
    nexus_meta_type_t, nexus_meta_subtype_t, uint32_t, uint16_t, uint32_t,
    uint32_t, uint32_t, uint32_t, uint32_t);
extern uint64_t pp_alloc_packet(struct kern_pbufpool *, uint16_t, uint32_t);
extern uint64_t pp_alloc_packet_by_size(struct kern_pbufpool *, uint32_t,
    uint32_t);
extern int pp_alloc_packet_batch(struct kern_pbufpool *, uint16_t,
    uint64_t *__counted_by(*size), uint32_t *size, boolean_t, alloc_cb_func_t,
    const void *, uint32_t);
extern int pp_alloc_pktq(struct kern_pbufpool *, uint16_t, struct pktq *,
    uint32_t, alloc_cb_func_t, const void *, uint32_t);
extern void pp_free_packet(struct kern_pbufpool *, uint64_t);
extern void pp_free_packet_batch(struct kern_pbufpool *, uint64_t *__counted_by(size) array, uint32_t size);
extern void pp_free_packet_single(struct __kern_packet *);
extern void pp_drop_packet_single(struct __kern_packet *, struct ifnet *, uint16_t,
    drop_reason_t, const char *, uint16_t);
extern void pp_free_packet_chain(struct __kern_packet *, int *);
extern void pp_free_pktq(struct pktq *);
extern void pp_drop_pktq(struct pktq *, struct ifnet *, uint16_t,
    drop_reason_t, const char *, uint16_t);
extern errno_t pp_alloc_buffer(const kern_pbufpool_t, mach_vm_address_t *,
    kern_segment_t *, kern_obj_idx_seg_t *, uint32_t);
extern void pp_free_buffer(const kern_pbufpool_t, mach_vm_address_t);
extern errno_t pp_alloc_buflet(struct kern_pbufpool *pp, kern_buflet_t *kbft,
    uint32_t skmflag, bool large);
extern errno_t pp_alloc_buflet_batch(struct kern_pbufpool *pp,
    uint64_t *__counted_by(*size) array, uint32_t *size, uint32_t skmflag,
    bool large);
extern void pp_free_buflet(const kern_pbufpool_t, kern_buflet_t);
extern void pp_reap_caches(boolean_t);
__BEGIN_DECLS


extern void fsw_init(void);
extern void fsw_uninit(void);
extern struct nx_flowswitch * fsw_alloc(zalloc_flags_t);
extern void fsw_free(struct nx_flowswitch *fsw);
extern int fsw_grow(struct nx_flowswitch *fsw, uint32_t grow);
extern int fsw_port_find(struct nx_flowswitch *fsw, nexus_port_t first,
    nexus_port_t last, nexus_port_t *nx_port);
extern int fsw_port_bind(struct nx_flowswitch *fsw, nexus_port_t nx_port,
    struct nxbind *nxb0);
extern int fsw_port_unbind(struct nx_flowswitch *fsw, nexus_port_t nx_port);
extern int fsw_port_na_defunct(struct nx_flowswitch *fsw,
    struct nexus_vp_adapter *vpna);
extern size_t fsw_mib_get(struct nx_flowswitch *fsw,
    struct nexus_mib_filter *filter, void *__sized_by(len)out, size_t len, struct proc *p);
extern int fsw_attach_vp(struct kern_nexus *nx, struct kern_channel *ch,
    struct chreq *chr, struct nxbind *nxb, struct proc *p,
    struct nexus_vp_adapter **vpna);
extern int fsw_ctl(struct kern_nexus *nx, nxcfg_cmd_t nc_cmd, struct proc *p,
    void *data);
extern int fsw_ctl_detach(struct kern_nexus *nx, struct proc *p,
    struct nx_spec_req *nsr);
extern boolean_t fsw_should_drop_packet(boolean_t is_input, sa_family_t af,
    uint8_t proto, const char *ifname);
extern int fsw_port_alloc(struct nx_flowswitch *fsw, struct nxbind *nxb,
    struct nexus_vp_adapter **vpna, nexus_port_t nx_port, struct proc *p,
    boolean_t ifattach, boolean_t host);
extern void fsw_port_free(struct nx_flowswitch *fsw,
    struct nexus_vp_adapter *vpna, nexus_port_t nx_port, boolean_t defunct);
extern int fsw_port_grow(struct nx_flowswitch *fsw, uint32_t num_ports);
extern int fsw_port_na_activate(struct nx_flowswitch *fsw,
    struct nexus_vp_adapter *vpna, na_activate_mode_t mode);
extern boolean_t fsw_detach_barrier_add(struct nx_flowswitch *fsw);
extern void fsw_detach_barrier_remove(struct nx_flowswitch *fsw);
extern int fsw_vp_na_activate(struct nexus_adapter *na,
    na_activate_mode_t mode);
extern int fsw_vp_na_krings_create(struct nexus_adapter *na,
    struct kern_channel *ch);
extern void fsw_vp_na_krings_delete(struct nexus_adapter *na,
    struct kern_channel *ch, boolean_t defunct);
extern int fsw_vp_na_txsync(struct __kern_channel_ring *kring,
    struct proc *p, uint32_t flags);
extern int fsw_vp_na_rxsync(struct __kern_channel_ring *kring,
    struct proc *p, uint32_t flags);
extern int fsw_vp_na_create(struct kern_nexus *nx, struct chreq *chr,
    struct proc *p, struct nexus_vp_adapter **ret);
extern void fsw_vp_channel_error_stats_fold(struct fsw_stats *fs,
    struct __nx_stats_channel_errors *es);
extern errno_t fsw_vp_na_channel_event(struct nx_flowswitch *fsw,
    uint32_t nx_port_id, struct __kern_channel_event *__sized_by(event_len)event,
    uint16_t event_len);
extern void fsw_classq_setup(struct nx_flowswitch *fsw,
    struct nexus_adapter *hostna);
extern void fsw_classq_teardown(struct nx_flowswitch *fsw,
    struct nexus_adapter *hostna);
extern struct mbuf * fsw_classq_kpkt_to_mbuf(struct nx_flowswitch *fsw,
    struct __kern_packet *pkt);
extern int fsw_generic_resolve(struct nx_flowswitch *fsw, struct flow_route *fr,
    struct __kern_packet *pkt);
extern int fsw_dp_init(void);
extern void fsw_dp_uninit(void);
extern int fsw_dp_ctor(struct nx_flowswitch *fsw);
extern void fsw_dp_dtor(struct nx_flowswitch *fsw);
extern void fsw_ring_flush(struct nx_flowswitch *fsw,
    struct __kern_channel_ring *skring, struct proc *p);
extern void fsw_ring_enqueue_tail_drop(struct nx_flowswitch *fsw,
    struct __kern_channel_ring *ring, struct pktq *pktq);
extern boolean_t fsw_detach_barrier_add(struct nx_flowswitch *fsw);
extern void fsw_detach_barrier_remove(struct nx_flowswitch *fsw);
extern void fsw_linger_insert(struct flow_entry *fsw);
extern void fsw_linger_purge(struct nx_flowswitch *fsw);
extern void fsw_reap_sched(struct nx_flowswitch *fsw);
extern int fsw_dev_input_netem_dequeue(void *handle,
    pktsched_pkt_t *__counted_by(n_pkts) pkts, uint32_t n_pkts);
extern void fsw_snoop(struct nx_flowswitch *fsw, struct flow_entry *fe,
    struct pktq *pktq, bool input);
extern void fsw_receive(struct nx_flowswitch *fsw, struct pktq *pktq);
extern void dp_flow_tx_process(struct nx_flowswitch *fsw,
    struct flow_entry *fe, uint32_t flags);
extern void dp_flow_rx_process(struct nx_flowswitch *fsw,
    struct flow_entry *fe, struct pktq *rx_pkts, uint32_t rx_bytes,
    uint32_t flags);
extern struct flow_owner * fsw_flow_add(struct nx_flowswitch *fsw,
    struct nx_flow_req *req0, int *error);
extern int fsw_flow_del(struct nx_flowswitch *fsw, struct nx_flow_req *req,
    bool nolinger, void *params);
extern int fsw_flow_config(struct nx_flowswitch *fsw, struct nx_flow_req *req);
extern void fsw_flow_abort_tcp(struct nx_flowswitch *fsw, struct flow_entry *fe,
    struct __kern_packet *pkt);
extern void fsw_flow_abort_quic(struct flow_entry *fe, uint8_t *token);
extern struct __kern_channel_ring * fsw_flow_get_rx_ring(struct nx_flowswitch *fsw,
    struct flow_entry *fe);
extern bool dp_flow_rx_route_process(struct nx_flowswitch *fsw,
    struct flow_entry *fe);
extern void fsw_fold_stats(struct nx_flowswitch *fsw, void *data,
    nexus_stats_type_t type);
extern int fsw_netagent_add_remove(struct kern_nexus *nx, boolean_t add);
extern void fsw_netagent_update(struct kern_nexus *nx);
extern int fsw_netagent_register(struct nx_flowswitch *fsw, struct ifnet *ifp);
extern void fsw_netagent_unregister(struct nx_flowswitch *fsw,
    struct ifnet *ifp);
extern int fsw_ip_setup(struct nx_flowswitch *fsw, struct ifnet *ifp);
extern int fsw_cellular_setup(struct nx_flowswitch *fsw, struct ifnet *ifp);
extern int fsw_ethernet_setup(struct nx_flowswitch *fsw, struct ifnet *ifp);
extern void fsw_classq_setup(struct nx_flowswitch *fsw,
    struct nexus_adapter *hostna);
extern void fsw_classq_teardown(struct nx_flowswitch *fsw,
    struct nexus_adapter *hostna);
extern void fsw_qos_mark(struct nx_flowswitch *fsw, struct flow_entry *fe,
    struct __kern_packet *pkt);
extern boolean_t fsw_qos_default_restricted(void);
extern struct mbuf * fsw_classq_kpkt_to_mbuf(struct nx_flowswitch *fsw,
    struct __kern_packet *pkt);
extern sa_family_t fsw_ip_demux(struct nx_flowswitch *, struct __kern_packet *);
extern struct fsw_ip_frag_mgr * fsw_ip_frag_mgr_create(
	struct nx_flowswitch *fsw, struct ifnet *ifp, size_t f_limit);
extern void fsw_ip_frag_mgr_destroy(struct fsw_ip_frag_mgr *mgr);
extern int fsw_ip_frag_reass_v4(struct fsw_ip_frag_mgr *mgr,
    struct __kern_packet **pkt, struct ip *ip4, uint16_t *nfrags,
    uint16_t *tlen);
extern int fsw_ip_frag_reass_v6(struct fsw_ip_frag_mgr *mgr,
    struct __kern_packet **pkt, struct ip6_hdr *ip6, struct ip6_frag *ip6f,
    uint16_t *nfrags, uint16_t *tlen);
__BEGIN_DECLS
extern int nx_netif_host_na_activate(struct nexus_adapter *,
    na_activate_mode_t);
extern int nx_netif_host_krings_create(struct nexus_adapter *,
    struct kern_channel *);
extern void nx_netif_host_krings_delete(struct nexus_adapter *,
    struct kern_channel *, boolean_t);
extern int nx_netif_host_na_rxsync(struct __kern_channel_ring *,
    struct proc *, uint32_t);
extern int nx_netif_host_na_txsync(struct __kern_channel_ring *,
    struct proc *, uint32_t);
extern int nx_netif_host_na_special(struct nexus_adapter *,
    struct kern_channel *, struct chreq *, nxspec_cmd_t);
extern int nx_netif_host_output(struct ifnet *, struct mbuf *);
extern boolean_t netif_chain_enqueue_enabled(struct ifnet *);
RB_HEAD(flow_owner_tree, flow_owner);
RB_HEAD(flow_entry_id_tree, flow_entry);
RB_PROTOTYPE_SC_PREV(__private_extern__, flow_owner_tree, flow_owner,
    fo_link, fo_cmp);
RB_PROTOTYPE_SC_PREV(__private_extern__, flow_entry_id_tree, flow_entry,
    fe_id_link, fe_id_cmp);
TAILQ_HEAD(flow_entry_list, flow_entry);
TAILQ_HEAD(flow_entry_linger_head, flow_entry);
RB_HEAD(flow_route_tree, flow_route);
RB_PROTOTYPE_SC_PREV(__private_extern__, flow_route_tree, flow_route,
    fr_link, fr_cmp);
RB_HEAD(flow_route_id_tree, flow_route);
RB_PROTOTYPE_SC_PREV(__private_extern__, flow_route_id_tree, flow_route,
    fr_id_link, fr_id_cmp);
static inline int
flow_key_cmp(const struct flow_key *match, const struct flow_key *key)
{

	FK_CMP(fk_ipver, FKMASK_IPVER);
	FK_CMP(fk_proto, FKMASK_PROTO);
	FK_CMP(fk_src, FKMASK_SRC);
	FK_CMP(fk_dst, FKMASK_DST);
	FK_CMP(fk_sport, FKMASK_SPORT);
	FK_CMP(fk_dport, FKMASK_DPORT);

	return 0;
}


static inline int
flow_key_cmp_mask(const struct flow_key *match,
    const struct flow_key *key, const struct flow_key *mask)
{
	_CASSERT(FLOW_KEY_LEN == 48);
	_CASSERT(FLOW_KEY_LEN == sizeof(struct flow_key));
	_CASSERT((sizeof(struct flow_entry) % 16) == 0);
	_CASSERT((offsetof(struct flow_entry, fe_key) % 16) == 0);

	
	const struct flow_key *match_idx = match;
	const struct flow_key *key_idx = key;
	const struct flow_key *mask_idx = mask;

	return sk_memcmp_mask_48B((const uint8_t *)match_idx,
	           (const uint8_t *)key_idx, (const uint8_t *)mask_idx);
}

static inline uint32_t
flow_key_hash(const struct flow_key *key)
{
	uint32_t hash = FK_HASH_SEED;

	FK_HASH(fk_ipver, FKMASK_IPVER);
	FK_HASH(fk_proto, FKMASK_PROTO);
	FK_HASH(fk_src, FKMASK_SRC);
	FK_HASH(fk_dst, FKMASK_DST);
	FK_HASH(fk_sport, FKMASK_SPORT);
	FK_HASH(fk_dport, FKMASK_DPORT);

	return hash;
}

__attribute__((always_inline))
static inline void
flow_key_unpack(const struct flow_key *key, union sockaddr_in_4_6 *laddr,
    union sockaddr_in_4_6 *faddr, uint8_t *protocol)
{
	*protocol = key->fk_proto;
	if (key->fk_ipver == IPVERSION) {
		laddr->sa.sa_family = AF_INET;
		laddr->sin.sin_addr = key->fk_src4;
		laddr->sin.sin_port = key->fk_sport;
		faddr->sa.sa_family = AF_INET;
		faddr->sin.sin_addr = key->fk_dst4;
		faddr->sin.sin_port = key->fk_dport;
	} else if (key->fk_ipver == IPV6_VERSION) {
		laddr->sa.sa_family = AF_INET6;
		laddr->sin6.sin6_addr = key->fk_src6;
		laddr->sin6.sin6_port = key->fk_sport;
		faddr->sa.sa_family = AF_INET6;
		faddr->sin6.sin6_addr = key->fk_dst6;
		faddr->sin6.sin6_port = key->fk_dport;
	}
}

__attribute__((always_inline))
static inline int
flow_req2key(struct nx_flow_req *req, struct flow_key *key)
{
	FLOW_KEY_CLEAR(key);

	if (req->nfr_saddr.sa.sa_family == AF_INET) {
		key->fk_ipver = IPVERSION;
		key->fk_proto = req->nfr_ip_protocol;
		key->fk_mask |= FKMASK_PROTO;
		if (sk_sa_has_addr(SA(&req->nfr_saddr))) {
			key->fk_src4 = req->nfr_saddr.sin.sin_addr;
			key->fk_mask |= (FKMASK_IPVER | FKMASK_SRC);
		}
		if (sk_sa_has_addr(SA(&req->nfr_daddr))) {
			key->fk_dst4 = req->nfr_daddr.sin.sin_addr;
			key->fk_mask |= (FKMASK_IPVER | FKMASK_DST);
		}
		if (sk_sa_has_port(SA(&req->nfr_saddr))) {
			key->fk_sport = req->nfr_saddr.sin.sin_port;
			key->fk_mask |= FKMASK_SPORT;
		}
		if (sk_sa_has_port(SA(&req->nfr_daddr))) {
			key->fk_dport = req->nfr_daddr.sin.sin_port;
			key->fk_mask |= FKMASK_DPORT;
		}
	} else if (req->nfr_saddr.sa.sa_family == AF_INET6) {
		key->fk_ipver = IPV6_VERSION;
		key->fk_proto = req->nfr_ip_protocol;
		key->fk_mask |= FKMASK_PROTO;
		if (sk_sa_has_addr(SA(&req->nfr_saddr))) {
			key->fk_src6 = req->nfr_saddr.sin6.sin6_addr;
			key->fk_mask |= (FKMASK_IPVER | FKMASK_SRC);
		}
		if (sk_sa_has_addr(SA(&req->nfr_daddr))) {
			key->fk_dst6 = req->nfr_daddr.sin6.sin6_addr;
			key->fk_mask |= (FKMASK_IPVER | FKMASK_DST);
		}
		if (sk_sa_has_port(SA(&req->nfr_saddr))) {
			key->fk_sport = req->nfr_saddr.sin6.sin6_port;
			key->fk_mask |= FKMASK_SPORT;
		}
		if (sk_sa_has_port(SA(&req->nfr_daddr))) {
			key->fk_dport = req->nfr_daddr.sin6.sin6_port;
			key->fk_mask |= FKMASK_DPORT;
		}
	} else {
		SK_ERR("unknown AF %d", req->nfr_saddr.sa.sa_family);
		return ENOTSUP;
	}

	switch (key->fk_mask) {
	case FKMASK_5TUPLE:
	case FKMASK_4TUPLE:
	case FKMASK_3TUPLE:
	case FKMASK_2TUPLE:
	case FKMASK_IPFLOW3:
	case FKMASK_IPFLOW2:
	case FKMASK_IPFLOW1:
		break;
	default:
		SK_ERR("unknown flow key mask 0x%04x", key->fk_mask);
		return ENOTSUP;
	}

	return 0;
}

__attribute__((always_inline))
static inline void
flow_pkt2key(struct __kern_packet *pkt, boolean_t input,
    struct flow_key *key)
{
	struct __flow *flow = pkt->pkt_flow;

	FLOW_KEY_CLEAR(key);

	if (__improbable((pkt->pkt_qum_qflags & QUM_F_FLOW_CLASSIFIED) == 0)) {
		return;
	}

	ASSERT(flow->flow_l3._l3_ip_ver != 0);

	key->fk_ipver = flow->flow_l3._l3_ip_ver;
	key->fk_proto = flow->flow_ip_proto;
	if (input) {
		if (flow->flow_ip_ver == IPVERSION) {
			key->fk_src4 = flow->flow_ipv4_dst;
			key->fk_sport = flow->flow_tcp_dst;
			key->fk_dst4 = flow->flow_ipv4_src;
			key->fk_dport = flow->flow_tcp_src;
		} else {
			key->fk_src6 = flow->flow_ipv6_dst;
			key->fk_sport = flow->flow_tcp_dst;
			key->fk_dst6 = flow->flow_ipv6_src;
			key->fk_dport = flow->flow_tcp_src;
		}
	} else {
		if (flow->flow_ip_ver == IPVERSION) {
			key->fk_src4 = flow->flow_ipv4_src;
			key->fk_sport = flow->flow_tcp_src;
			key->fk_dst4 = flow->flow_ipv4_dst;
			key->fk_dport = flow->flow_tcp_dst;
		} else {
			key->fk_src6 = flow->flow_ipv6_src;
			key->fk_sport = flow->flow_tcp_src;
			key->fk_dst6 = flow->flow_ipv6_dst;
			key->fk_dport = flow->flow_tcp_dst;
		}
	}
}

__attribute__((always_inline))
static inline int
flow_ip_cmp(const void *a0, const void *b0, size_t alen)
{
	struct flow_ip_addr *a = __DECONST(struct flow_ip_addr *, a0),
	    *b = __DECONST(struct flow_ip_addr *, b0);

	switch (alen) {
	case sizeof(struct in_addr):
		if (a->_addr32[0] > b->_addr32[0]) {
			return 1;
		}
		if (a->_addr32[0] < b->_addr32[0]) {
			return -1;
		}
		break;

	case sizeof(struct in6_addr):
		if (a->_addr64[1] > b->_addr64[1]) {
			return 1;
		}
		if (a->_addr64[1] < b->_addr64[1]) {
			return -1;
		}
		if (a->_addr64[0] > b->_addr64[0]) {
			return 1;
		}
		if (a->_addr64[0] < b->_addr64[0]) {
			return -1;
		}
		break;

	default:
		VERIFY(0);
		
		__builtin_unreachable();
	}
	return 0;
}

__attribute__((always_inline))
static inline struct flow_owner_bucket *
flow_mgr_get_fob_at_idx(struct flow_mgr *fm, uint32_t idx)
{
	char *buckets = fm->fm_owner_buckets;
	void *bucket = buckets + (idx * fm->fm_owner_bucket_sz);
	return bucket;
}

__attribute__((always_inline))
static inline struct flow_route_bucket *
flow_mgr_get_frb_at_idx(struct flow_mgr *fm, uint32_t idx)
{
	char *buckets = fm->fm_route_buckets;
	void *bucket = buckets + (idx * fm->fm_route_bucket_sz);
	return bucket;
}

__attribute__((always_inline))
static inline struct flow_route_id_bucket *
flow_mgr_get_frib_at_idx(struct flow_mgr *fm, uint32_t idx)
{
	char *buckets = fm->fm_route_id_buckets;
	void *bucket = buckets + (idx * fm->fm_route_id_bucket_sz);
	return bucket;
}

__attribute__((always_inline))
static inline uint32_t
flow_mgr_get_fob_idx(struct flow_mgr *fm,
    struct flow_owner_bucket *bkt)
{
	ASSERT(((intptr_t)bkt - (intptr_t)fm->fm_owner_buckets) %
	    fm->fm_owner_bucket_sz == 0);
	return (uint32_t)(((intptr_t)bkt - (intptr_t)fm->fm_owner_buckets) /
	       fm->fm_owner_bucket_sz);
}

__attribute__((always_inline))
static inline size_t
flow_mgr_get_num_flows(struct flow_mgr *mgr)
{
	ASSERT(mgr->fm_flow_table != NULL);
	return cuckoo_hashtable_entries(mgr->fm_flow_table);
}

extern unsigned int sk_fo_size;
extern int flow_init(void);
extern void flow_fini(void);
extern void flow_mgr_init(void);
extern void flow_mgr_fini(void);
extern struct flow_mgr *flow_mgr_find_lock(uuid_t);
extern void flow_mgr_unlock(void);
extern struct flow_mgr * flow_mgr_create(size_t, size_t, size_t, size_t);
extern void flow_mgr_destroy(struct flow_mgr *);
extern void flow_mgr_terminate(struct flow_mgr *);
extern int flow_mgr_flow_add(struct kern_nexus *nx, struct flow_mgr *fm,
    struct flow_owner *fo, struct ifnet *ifp, struct nx_flow_req *req,
    flow_route_ctor_fn_t fr_ctor, flow_route_resolve_fn_t fr_resolve, void *fr_arg);
extern struct flow_owner_bucket *flow_mgr_get_fob_by_pid(
	struct flow_mgr *, pid_t);
extern struct flow_entry *flow_mgr_get_fe_by_uuid_rlock(
	struct flow_mgr *, uuid_t);
extern struct flow_route_bucket *flow_mgr_get_frb_by_addr(
	struct flow_mgr *, union sockaddr_in_4_6 *);
extern struct flow_route_id_bucket *flow_mgr_get_frib_by_uuid(
	struct flow_mgr *, uuid_t);
extern int flow_mgr_flow_hash_mask_add(struct flow_mgr *fm, uint32_t mask);
extern int flow_mgr_flow_hash_mask_del(struct flow_mgr *fm, uint32_t mask);
extern struct flow_entry * fe_alloc(boolean_t can_block);
extern int flow_namespace_create(union sockaddr_in_4_6 *, uint8_t protocol,
    netns_token *, uint16_t, struct ns_flow_info *);
extern void flow_namespace_half_close(netns_token *token);
extern void flow_namespace_withdraw(netns_token *);
extern void flow_namespace_destroy(netns_token *);
extern void flow_owner_buckets_free(struct flow_owner_bucket *, size_t);
extern void flow_owner_bucket_init(struct flow_owner_bucket *);
extern void flow_owner_bucket_destroy(struct flow_owner_bucket *);
extern void flow_owner_bucket_purge_all(struct flow_owner_bucket *);
extern void flow_owner_attach_nexus_port(struct flow_mgr *, boolean_t,
    pid_t, nexus_port_t);
extern uint32_t flow_owner_detach_nexus_port(struct flow_mgr *,
    boolean_t, pid_t, nexus_port_t, boolean_t);
extern struct flow_owner *flow_owner_alloc(struct flow_owner_bucket *,
    struct proc *, nexus_port_t, bool, bool, struct nx_flowswitch*,
    struct nexus_adapter *, void *, bool);
extern void flow_owner_free(struct flow_owner_bucket *, struct flow_owner *);
extern struct flow_entry *flow_owner_create_entry(struct flow_owner *,
    struct nx_flow_req *, boolean_t, uint32_t, boolean_t,
    struct flow_route *, int *);
extern int flow_owner_destroy_entry(struct flow_owner *, uuid_t, bool, void *);
extern struct flow_owner *flow_owner_find_by_pid(struct flow_owner_bucket *,
    pid_t, void *, bool);
extern int flow_owner_flowadv_index_alloc(struct flow_owner *, flowadv_idx_t *);
extern void flow_owner_flowadv_index_free(struct flow_owner *, flowadv_idx_t);
extern uint32_t flow_owner_activate_nexus_port(struct flow_mgr *,
    boolean_t, pid_t, nexus_port_t, struct nexus_adapter *,
    na_activate_mode_t);
extern struct flow_entry *flow_mgr_find_fe_by_key(struct flow_mgr *,
    struct flow_key *);
extern struct flow_entry * flow_mgr_find_conflicting_fe(struct flow_mgr *fm,
    struct flow_key *fe_key);
extern void flow_mgr_foreach_flow(struct flow_mgr *fm,
    void (^flow_handler)(struct flow_entry *fe));
extern struct flow_entry *flow_entry_find_by_uuid(struct flow_owner *,
    uuid_t);
extern struct flow_entry * flow_entry_alloc(struct flow_owner *fo,
    struct nx_flow_req *req, int *perr);
extern void flow_entry_teardown(struct flow_owner *, struct flow_entry *);
extern void flow_entry_destroy(struct flow_owner *, struct flow_entry *, bool,
    void *);
extern void flow_entry_retain(struct flow_entry *fe);
extern void flow_entry_release(struct flow_entry **pfe);
extern uint32_t flow_entry_refcnt(struct flow_entry *fe);
extern bool rx_flow_demux_match(struct nx_flowswitch *, struct flow_entry *, struct __kern_packet *);
extern struct flow_entry *rx_lookup_child_flow(struct nx_flowswitch *fsw,
    struct flow_entry *, struct __kern_packet *);
extern struct flow_entry *tx_lookup_child_flow(struct flow_entry *, uuid_t);
extern struct flow_entry_dead *flow_entry_dead_alloc(zalloc_flags_t);
extern void flow_entry_dead_free(struct flow_entry_dead *);
extern void flow_entry_stats_get(struct flow_entry *, struct sk_stats_flow *);
extern int flow_pkt_classify(struct __kern_packet *pkt, struct ifnet *ifp,
    sa_family_t af, bool input);
extern void flow_track_stats(struct flow_entry *, uint64_t, uint64_t,
    bool, bool);
extern int flow_pkt_track(struct flow_entry *, struct __kern_packet *, bool);
extern boolean_t flow_track_tcp_want_abort(struct flow_entry *);
extern void flow_track_abort_tcp( struct flow_entry *fe,
    struct __kern_packet *in_pkt, struct __kern_packet *rst_pkt);
extern void flow_track_abort_quic(struct flow_entry *fe,
    uint8_t *__counted_by(QUIC_STATELESS_RESET_TOKEN_SIZE)token);
extern void fsw_host_rx(struct nx_flowswitch *, struct pktq *);
extern void fsw_host_sendup(struct ifnet *, struct mbuf *, struct mbuf *,
    uint32_t, uint32_t);
extern void flow_rx_agg_tcp(struct nx_flowswitch *fsw, struct flow_entry *fe,
    struct pktq *rx_pkts, uint32_t rx_bytes, uint32_t flags);
extern void flow_route_init(void);
extern void flow_route_fini(void);
extern void flow_route_buckets_free(struct flow_route_bucket *, size_t);
extern void flow_route_bucket_init(struct flow_route_bucket *);
extern void flow_route_bucket_destroy(struct flow_route_bucket *);
extern void flow_route_bucket_purge_all(struct flow_route_bucket *);
extern void flow_route_id_buckets_free(struct flow_route_id_bucket *, size_t);
extern void flow_route_id_bucket_init(struct flow_route_id_bucket *);
extern void flow_route_id_bucket_destroy(struct flow_route_id_bucket *);
extern int flow_route_select_laddr(union sockaddr_in_4_6 *,
    union sockaddr_in_4_6 *, struct ifnet *, struct rtentry *, uint32_t *, int);
extern int flow_route_find(struct kern_nexus *, struct flow_mgr *,
    struct ifnet *, struct nx_flow_req *, flow_route_ctor_fn_t,
    flow_route_resolve_fn_t, void *, struct flow_route **);
extern int flow_route_configure(struct flow_route *, struct ifnet *, struct nx_flow_req *);
extern void flow_route_retain(struct flow_route *);
extern void flow_route_release(struct flow_route *);
extern uint32_t flow_route_prune(struct flow_mgr *, struct ifnet *,
    uint32_t *);
extern void flow_route_cleanup(struct flow_route *);
extern boolean_t flow_route_laddr_validate(union sockaddr_in_4_6 *,
    struct ifnet *, uint32_t *);
extern boolean_t flow_route_key_validate(struct flow_key *, struct ifnet *,
    uint32_t *);
extern void flow_qset_select_dynamic(struct nx_flowswitch *,
    struct flow_entry *, boolean_t);
extern void flow_stats_init(void);
extern void flow_stats_fini(void);
extern struct flow_stats *flow_stats_alloc(boolean_t cansleep);
int                      kau_will_audit(void);
void                     audit_init(void);
void                     audit_shutdown(void);
void                     audit_syscall_enter(unsigned int code,
    struct proc *proc, struct uthread *uthread);
void    audit_syscall_exit(unsigned int code, int error,
    struct proc *proc, struct uthread *uthread);
void    audit_mach_syscall_enter(unsigned short audit_event);
void    audit_mach_syscall_exit(int retval, struct uthread *uthread);
void                    audit_subcall_enter(au_event_t event,
    struct proc *proc, struct uthread *uthread);
void                    audit_subcall_exit(int error,
    struct uthread *uthread);
void     audit_arg_addr(struct kaudit_record *ar, user_addr_t addr);
void     audit_arg_exit(struct kaudit_record *ar, int status, int retval);
void     audit_arg_len(struct kaudit_record *ar, user_size_t len);
void     audit_arg_fd(struct kaudit_record *ar, int fd);
void     audit_arg_fd2(struct kaudit_record *ar, int fd);
void     audit_arg_fflags(struct kaudit_record *ar, int fflags);
void     audit_arg_gid(struct kaudit_record *ar, gid_t gid);
void     audit_arg_uid(struct kaudit_record *ar, uid_t uid);
void     audit_arg_egid(struct kaudit_record *ar, gid_t egid);
void     audit_arg_euid(struct kaudit_record *ar, uid_t euid);
void     audit_arg_rgid(struct kaudit_record *ar, gid_t rgid);
void     audit_arg_ruid(struct kaudit_record *ar, uid_t ruid);
void     audit_arg_sgid(struct kaudit_record *ar, gid_t sgid);
void     audit_arg_suid(struct kaudit_record *ar, uid_t suid);
void     audit_arg_groupset(struct kaudit_record *ar, const gid_t *gidset,
    u_int gidset_size);
void     audit_arg_login(struct kaudit_record *ar, const char *login);
void     audit_arg_ctlname(struct kaudit_record *ar, const int *name,
    int namelen);
void     audit_arg_mask(struct kaudit_record *ar, int mask);
void     audit_arg_mode(struct kaudit_record *ar, mode_t mode);
void     audit_arg_value32(struct kaudit_record *ar, uint32_t value32);
void     audit_arg_value64(struct kaudit_record *ar, uint64_t value64);
void     audit_arg_owner(struct kaudit_record *ar, uid_t uid, gid_t gid);
void     audit_arg_pid(struct kaudit_record *ar, pid_t pid);
void     audit_arg_process(struct kaudit_record *ar, proc_t p);
void     audit_arg_signum(struct kaudit_record *ar, u_int signum);
void     audit_arg_socket(struct kaudit_record *ar, int sodomain, int sotype,
    int soprotocol);
void     audit_arg_sockaddr(struct kaudit_record *ar, struct vnode *cwd_vp,
    struct sockaddr *so);
void     audit_arg_auid(struct kaudit_record *ar, uid_t auid);
void     audit_arg_auditinfo(struct kaudit_record *ar,
    const struct auditinfo *au_info);
void     audit_arg_auditinfo_addr(struct kaudit_record *ar,
    const struct auditinfo_addr *au_info);
void     audit_arg_upath(struct kaudit_record *ar, struct vnode *cwd_vp,
    const char *upath, u_int64_t flags);
void     audit_arg_kpath(struct kaudit_record *ar,
    const char *kpath, u_int64_t flags);
void     audit_arg_vnpath(struct kaudit_record *ar, struct vnode *vp,
    u_int64_t flags);
void     audit_arg_vnpath_withref(struct kaudit_record *ar, struct vnode *vp,
    u_int64_t flags);
void     audit_arg_text(struct kaudit_record *ar, const char *text);
void     audit_arg_opaque(struct kaudit_record *ar, const void *data,
    size_t size);
void     audit_arg_data(struct kaudit_record *ar, const void *data, size_t size,
    size_t number);
void     audit_arg_cmd(struct kaudit_record *ar, int cmd);
void     audit_arg_svipc_cmd(struct kaudit_record *ar, int cmd);
void     audit_arg_svipc_perm(struct kaudit_record *ar,
    const struct ipc_perm *perm);
void     audit_arg_svipc_id(struct kaudit_record *ar, int id);
void     audit_arg_svipc_addr(struct kaudit_record *ar, user_addr_t addr);
void     audit_arg_posix_ipc_perm(struct kaudit_record *ar, uid_t uid,
    gid_t gid, mode_t mode);
void     audit_arg_auditon(struct kaudit_record *ar,
    const union auditon_udata *udata);
void     audit_arg_file(struct kaudit_record *ar, struct proc *p,
    struct fileproc *fp);
void     audit_arg_argv(struct kaudit_record *ar, const char *argv, int argc,
    size_t length);
void     audit_arg_envv(struct kaudit_record *ar, const char *envv, int envc,
    size_t length);
void    audit_arg_identity(struct kaudit_record *ar);
void     audit_arg_mach_port1(struct kaudit_record *ar, mach_port_name_t port);
void     audit_arg_mach_port2(struct kaudit_record *ar, mach_port_name_t port);
void     audit_sysclose(struct kaudit_record *ar, struct proc *p, int fd);
void     audit_proc_coredump(proc_t proc, const char *path, int errcode);
void     audit_session_ref(kauth_cred_t cred);
void     audit_session_unref(kauth_cred_t cred);
void     audit_session_procnew(proc_t p);
void     audit_session_procexit(proc_t p);
int      audit_session_spawnjoin(proc_t p, ipc_port_t port);
void     audit_sdev_submit(au_id_t auid, au_asid_t asid, void *record,
    u_int record_len);
int      audit_mac_data(int type, int len, u_char *data);
void     audit_arg_mac_string(struct kaudit_record *ar, char *string);
void    _audit_cv_init(struct cv *cvp, const char *desc);
void    _audit_cv_destroy(struct cv *cvp);
void    _audit_cv_signal(struct cv *cvp);
void    _audit_cv_broadcast(struct cv *cvp);
void    _audit_cv_wait(struct cv *cvp, lck_mtx_t *mp, const char *desc);
int     _audit_cv_wait_sig(struct cv *cvp, lck_mtx_t *mp, const char *desc);
int     _audit_cv_wait_continuation(struct cv *cvp, lck_mtx_t *mp,
    thread_continue_t function);
void    _audit_mtx_init(struct mtx *mp, const char *name);
void    _audit_mtx_destroy(struct mtx *mp);
void            _audit_slck_init(struct slck *lp, const char *grpname);
wait_result_t   _audit_slck_lock(struct slck *lp, int intr);
void            _audit_slck_unlock(struct slck *lp);
int             _audit_slck_trylock(struct slck *lp);
void            _audit_slck_assert(struct slck *lp, u_int assert);
void            _audit_slck_destroy(struct slck *lp);
void            _audit_rlck_init(struct rlck *lp, const char *grpname);
void            _audit_rlck_lock(struct rlck *lp);
void            _audit_rlck_unlock(struct rlck *lp);
void            _audit_rlck_assert(struct rlck *lp, u_int assert);
void            _audit_rlck_destroy(struct rlck *lp);
void    _audit_rw_init(struct rwlock *lp, const char *name);
void    _audit_rw_destroy(struct rwlock *lp);
int _audit_ppsratecheck(struct timeval *lasttime, int *curpps, int maxpps);
TAILQ_HEAD(kaudit_queue, kaudit_record);
void                     audit_abort(struct kaudit_record *ar);
void                     audit_commit(struct kaudit_record *ar, int error,
    int retval);
int      kaudit_to_bsm(struct kaudit_record *kar, struct au_record **pau);
int      bsm_rec_verify(void *rec, int length, boolean_t kern_events_allowed);
void     kau_free(struct au_record *rec);
void     kau_init(void);
token_t         *kau_to_socket(struct socket_au_info *soi);
int              au_preselect(au_event_t event, au_class_t class,
    au_mask_t *mask_p, int sorf);
void             au_evclassmap_init(void);
void             au_evclassmap_insert(au_event_t event, au_class_t class);
au_class_t       au_event_class(au_event_t event);
au_event_t       audit_ctlname_to_sysctlevent(int name[], uint64_t valid_arg);
au_event_t       audit_flags_and_error_to_openevent(int oflags, int error);
au_event_t       audit_flags_and_error_to_openextendedevent(int oflags,
    int error);
au_event_t       audit_flags_and_error_to_openatevent(int oflags,
    int error);
au_event_t       audit_flags_and_error_to_openbyidevent(int oflags,
    int error);
au_event_t       audit_msgctl_to_event(int cmd);
au_event_t       audit_semctl_to_event(int cmr);
int              audit_canon_path(struct vnode *cwd_vp, const char *path,
    char *cpath);
au_event_t       auditon_command_event(int cmd);
au_event_t       audit_fcntl_command_event(int cmd, int oflags, int error);
int              audit_send_trigger(unsigned int trigger);
int              audit_send_analytics(char* id, char* name);
void            audit_set_kinfo(struct auditinfo_addr *);
void            audit_get_kinfo(struct auditinfo_addr *);
void                     audit_free(struct kaudit_record *ar);
void                     audit_rotate_vnode(struct ucred *cred,
    struct vnode *vp);
void                     audit_worker_init(void);
void                     audit_identity_info_construct(
	struct au_identity_info *id_info);
void                     audit_identity_info_destruct(
	struct au_identity_info *id_info);
int      audit_pipe_init(void);
int      audit_pipe_shutdown(void);
int      audit_pipe_preselect(au_id_t auid, au_event_t event,
    au_class_t class, int sorf, int trail_select);
void     audit_pipe_submit(au_id_t auid, au_event_t event, au_class_t class,
    int sorf, int trail_select, void *record, u_int record_len);
void     audit_pipe_submit_user(void *record, u_int record_len);
int     audit_mac_new(proc_t p, struct kaudit_record *ar);
void    audit_mac_free(struct kaudit_record *ar);
int     audit_mac_syscall_enter(unsigned short code, proc_t p,
    struct uthread *uthread, kauth_cred_t my_cred, au_event_t event);
int     audit_mac_syscall_exit(unsigned short code, struct uthread *uthread,
    int error, int retval);
void    audit_session_init(void);
int     audit_session_setaia(proc_t p, auditinfo_addr_t *aia_p);
auditinfo_addr_t *audit_session_update(auditinfo_addr_t *new_aia);
int     audit_session_lookup(au_asid_t asid, auditinfo_addr_t *ret_aia);
static_assert(sizeof(struct hdr_tok_partial) == 8);
static_assert(sizeof(struct trl_tok_partial) == 7);
uint32_t
    gss_krb5_get_mic_mbuf(uint32_t *,   
    gss_ctx_id_t,                       
    gss_qop_t,                          
    mbuf_t,                             
    size_t,                             
    size_t,                             
    gss_buffer_t                        
    );
uint32_t
    gss_krb5_get_mic(uint32_t *, 
    gss_ctx_id_t,               
    gss_qop_t,                  
    gss_buffer_t,               
    gss_buffer_t                
    );
uint32_t
    gss_krb5_verify_mic_mbuf(uint32_t *,        
    gss_ctx_id_t,                               
    mbuf_t,                                     
    size_t,                                     
    size_t,                                     
    gss_buffer_t,                               
    gss_qop_t *                                 
    );
uint32_t
    gss_krb5_wrap_mbuf(uint32_t *,      
    gss_ctx_id_t,                       
    int,                                
    gss_qop_t,                          
    mbuf_t *,                           
    size_t,                             
    size_t,                             
    int *                               
    );
uint32_t
    gss_krb5_unwrap_mbuf(uint32_t *,    
    gss_ctx_id_t,                       
    mbuf_t *,                           
    size_t,                             
    size_t,                             
    int *,                              
    gss_qop_t *                         
    );
void gss_krb5_destroy_context(gss_ctx_id_t);
gss_ctx_id_t gss_krb5_make_context(void *, uint32_t);
void gss_krb5_mech_init(void);
int corecrypto_available(void);
errno_t gss_normalize_mbuf(mbuf_t, size_t, size_t *, mbuf_t *, mbuf_t *, int);
mbuf_t gss_join_mbuf(mbuf_t, mbuf_t, mbuf_t);
void hmac_update(const struct ccdigest_info *, hmac_ctx_t, size_t, void *);
void hmac_final(const struct ccdigest_info *, hmac_ctx_t, uint8_t *);
void printmbuf(const char *, mbuf_t, uint32_t, uint32_t);
void printgbuf(const char *, gss_buffer_t);
SYSCTL_DECL(_net_classq);
extern void _qinit(class_queue_t *, int, int, classq_pkt_type_t);
extern void _addq(class_queue_t *, classq_pkt_t *);
extern void _addq_multi(class_queue_t *, classq_pkt_t *, classq_pkt_t *,
    u_int32_t, u_int64_t);
extern void _getq(class_queue_t *, classq_pkt_t *);
extern void _getq_all(class_queue_t *, classq_pkt_t *, classq_pkt_t *,
    u_int32_t *, u_int64_t *);
extern void _getq_tail(class_queue_t *, classq_pkt_t *);
extern void _getq_random(class_queue_t *, classq_pkt_t *);
extern void _getq_flow(class_queue_t *, classq_pkt_t *, u_int32_t);
extern void _getq_scidx_lt(class_queue_t *, classq_pkt_t *, u_int32_t);
extern void _removeq(class_queue_t *, classq_pkt_t *);
extern void _flushq(class_queue_t *);
extern void _flushq_flow(class_queue_t *, u_int32_t, u_int32_t *, u_int32_t *);
extern void classq_init(void);
extern void fq_codel_init(void);
extern fq_t *fq_alloc(classq_pkt_type_t);
extern void fq_destroy(fq_t *, classq_pkt_type_t);
extern int fq_addq(struct fq_codel_sched_data *, fq_if_group_t *,
    pktsched_pkt_t *, struct fq_if_classq *);
extern void fq_getq_flow(struct fq_codel_sched_data *, fq_t *,
    pktsched_pkt_t *, uint64_t now);
extern void fq_codel_dequeue(fq_if_t *fqs, fq_t *fq,
    pktsched_pkt_t *pkt, uint64_t now);
extern void fq_getq_flow_internal(struct fq_codel_sched_data *,
    fq_t *, pktsched_pkt_t *);
extern void fq_head_drop(struct fq_codel_sched_data *, fq_t *);
extern boolean_t fq_tx_time_ready(fq_if_t *fqs, fq_t *fq, uint64_t now,
    uint64_t *ready_time);
extern void sfb_destroy(struct sfb *);
extern int sfb_addq(struct sfb *, class_queue_t *, pktsched_pkt_t *,
    struct pf_mtag *);
extern void sfb_getq(struct sfb *, class_queue_t *, pktsched_pkt_t *);
extern void sfb_purgeq(struct sfb *, class_queue_t *, u_int32_t,
    u_int32_t *, u_int32_t *);
extern void sfb_getstats(struct sfb *, struct sfb_stats *);
extern void sfb_updateq(struct sfb *, cqev_t);
extern int sfb_suspendq(struct sfb *, class_queue_t *, boolean_t);
extern int ifclassq_setup(struct ifclassq *, struct ifnet *, uint32_t);
extern void ifclassq_teardown(struct ifclassq *);
extern int ifclassq_pktsched_setup(struct ifclassq *);
extern void ifclassq_set_maxlen(struct ifclassq *, u_int32_t);
extern u_int32_t ifclassq_get_maxlen(struct ifclassq *);
extern int ifclassq_get_len(struct ifclassq *, mbuf_svc_class_t,
    u_int8_t, u_int32_t *, u_int32_t *);
extern errno_t ifclassq_enqueue(struct ifclassq *, classq_pkt_t *,
    classq_pkt_t *, u_int32_t, u_int32_t, boolean_t *);
extern errno_t ifclassq_dequeue(struct ifclassq *, u_int32_t, u_int32_t,
    classq_pkt_t *, classq_pkt_t *, u_int32_t *, u_int32_t *, u_int8_t);
extern errno_t ifclassq_dequeue_sc(struct ifclassq *, mbuf_svc_class_t,
    u_int32_t, u_int32_t, classq_pkt_t *, classq_pkt_t *, u_int32_t *,
    u_int32_t *, u_int8_t);
extern void *ifclassq_poll(struct ifclassq *, classq_pkt_type_t *);
extern void *ifclassq_poll_sc(struct ifclassq *, mbuf_svc_class_t,
    classq_pkt_type_t *);
extern void ifclassq_update(struct ifclassq *, cqev_t);
extern int ifclassq_attach(struct ifclassq *, u_int32_t, void *);
extern void ifclassq_detach(struct ifclassq *);
extern int ifclassq_getqstats(struct ifclassq *, u_int8_t, u_int32_t,
    void *, u_int32_t *);
extern const char *__null_terminated ifclassq_ev2str(cqev_t);
extern int ifclassq_tbr_set(struct ifclassq *, struct tb_profile *, boolean_t);
extern void ifclassq_tbr_dequeue(struct ifclassq *, classq_pkt_t *, u_int8_t);
extern void ifclassq_tbr_dequeue_sc(struct ifclassq *, mbuf_svc_class_t,
    classq_pkt_t *, u_int8_t);
extern void ifclassq_calc_target_qdelay(struct ifnet *ifp,
    uint64_t *if_target_qdelay, uint32_t flags);
extern void ifclassq_calc_update_interval(uint64_t *update_interval,
    uint32_t flags);
extern void ifclassq_set_packet_metadata(struct ifclassq *ifq,
    struct ifnet *ifp, classq_pkt_t *p);
extern struct ifclassq *ifclassq_alloc(void);
extern void ifclassq_retain(struct ifclassq *);
extern void ifclassq_release(struct ifclassq **);
extern int ifclassq_setup_group(struct ifclassq *ifcq, uint8_t grp_idx,
    uint8_t flags);
extern void ifclassq_set_grp_combined(struct ifclassq *ifcq, uint8_t grp_idx);
extern void ifclassq_set_grp_separated(struct ifclassq *ifcq, uint8_t grp_idx);
static inline boolean_t
pktsched_bit_tst(u_int32_t ix, pktsched_bitmap_t *pData)
{
	return (boolean_t)(*pData & (1 << ix));
}

static inline void
pktsched_bit_set(u_int32_t ix, pktsched_bitmap_t *pData)
{
	*pData |= (1 << ix);
}

static inline void
pktsched_bit_clr(u_int32_t ix, pktsched_bitmap_t *pData)
{
	*pData &= ~(1 << ix);
}

static inline void
pktsched_bit_cpy(u_int32_t ix, pktsched_bitmap_t *pData_dst,
    pktsched_bitmap_t *pData_src)
{
	*pData_dst ^= (-(*pData_src & (1 << ix)) ^ *pData_dst) & (1 << ix);
}

static inline pktsched_bitmap_t
pktsched_ffs(pktsched_bitmap_t pData)
{
	return (pktsched_bitmap_t)ffs(pData);
}

static inline pktsched_bitmap_t
pktsched_fls(pktsched_bitmap_t pData)
{
	return (pktsched_bitmap_t)((sizeof(pktsched_bitmap_t) << 3) - (unsigned long)clz(pData));
}

static inline pktsched_bitmap_t
__fls(pktsched_bitmap_t word)
{
	VERIFY(word != 0);
	return pktsched_fls(word) - 1;
}

static inline uint32_t
pktsched_get_pkt_len(pktsched_pkt_t *pkt)
{
	return pkt->pktsched_plen;
}

static inline void
pktsched_bit_move(u_int32_t ix, pktsched_bitmap_t *pData_dst,
    pktsched_bitmap_t *pData_src)
{
	*pData_dst |= (*pData_src & (1 << ix));
}




extern uint32_t machclk_freq;
SYSCTL_DECL(_net_pktsched);
extern void pktsched_register_m_tag(void);
extern void pktsched_init(void);
extern int pktsched_setup(struct ifclassq *, u_int32_t, u_int32_t,
    classq_pkt_type_t);
extern void pktsched_teardown(struct ifclassq *);
extern int pktsched_getqstats(struct ifclassq *, u_int32_t, u_int32_t,
    struct if_ifclassq_stats *);
extern u_int64_t pktsched_abs_to_nsecs(u_int64_t);
extern u_int64_t pktsched_nsecs_to_abstime(u_int64_t);
extern void pktsched_free_pkt(pktsched_pkt_t *);
extern void pktsched_drop_pkt(pktsched_pkt_t *, struct ifnet *, uint32_t, const char *, uint16_t,
    uint16_t);
extern int pktsched_clone_pkt(pktsched_pkt_t *, pktsched_pkt_t *);
extern void pktsched_corrupt_packet(pktsched_pkt_t *pkt);
extern void pktsched_get_pkt_vars(pktsched_pkt_t *, volatile uint32_t **,
    uint64_t **, uint32_t *, uint8_t *, uint8_t *, uint32_t *, uint64_t *);
extern uint32_t *pktsched_get_pkt_sfb_vars(pktsched_pkt_t *, uint32_t **);
extern void pktsched_pkt_encap(pktsched_pkt_t *, classq_pkt_t *);
extern void pktsched_pkt_encap_chain(pktsched_pkt_t *, classq_pkt_t *,
    classq_pkt_t *, uint32_t, uint32_t);
extern mbuf_svc_class_t pktsched_get_pkt_svc(pktsched_pkt_t *);
extern struct flowadv_fcentry *pktsched_alloc_fcentry(pktsched_pkt_t *,
    struct ifnet *, int);
extern int pktsched_mark_ecn(pktsched_pkt_t *pkt);
extern boolean_t pktsched_is_pkt_l4s(pktsched_pkt_t *pkt);
_Static_assert(FQ_IF_MAX_CLASSES < 127,
    "maximum number of classes needs to fit in a single byte");
extern void pktsched_fq_init(void);
extern void fq_codel_scheduler_init(void);
extern int fq_if_enqueue_classq(struct ifclassq *ifq, classq_pkt_t *h,
    classq_pkt_t *t, uint32_t cnt, uint32_t bytes, boolean_t *pdrop);
extern void fq_if_dequeue_classq(struct ifclassq *ifq, classq_pkt_t *pkt,
    uint8_t grp_idx);
extern void fq_if_dequeue_sc_classq(struct ifclassq *ifq, mbuf_svc_class_t svc,
    classq_pkt_t *pkt, uint8_t grp_idx);
extern int fq_if_dequeue_classq_multi(struct ifclassq *ifq, u_int32_t maxpktcnt,
    u_int32_t maxbytecnt, classq_pkt_t *first_packet, classq_pkt_t *last_packet,
    u_int32_t *retpktcnt, u_int32_t *retbytecnt, uint8_t grp_idx);
extern int fq_if_dequeue_sc_classq_multi(struct ifclassq *ifq,
    mbuf_svc_class_t svc, u_int32_t maxpktcnt, u_int32_t maxbytecnt,
    classq_pkt_t *first_packet, classq_pkt_t *last_packet, u_int32_t *retpktcnt,
    u_int32_t *retbytecnt, uint8_t grp_idx);
extern int fq_if_request_classq(struct ifclassq *ifq, cqrq_t rq, void *arg);
extern struct flowq *fq_if_hash_pkt(fq_if_t *, fq_if_group_t *,
    u_int32_t, mbuf_svc_class_t, u_int64_t, bool, fq_tfc_type_t);
extern boolean_t fq_if_at_drop_limit(fq_if_t *);
extern boolean_t fq_if_almost_at_drop_limit(fq_if_t *fqs);
extern void fq_if_drop_packet(fq_if_t *, uint64_t);
extern void fq_if_is_flow_heavy(fq_if_t *, struct flowq *);
extern boolean_t fq_if_add_fcentry(fq_if_t *, pktsched_pkt_t *, uint8_t,
    struct flowq *, fq_if_classq_t *);
extern void fq_if_flow_feedback(fq_if_t *, struct flowq *, fq_if_classq_t *);
extern boolean_t fq_if_report_ce(fq_if_t *, pktsched_pkt_t *, uint32_t, uint32_t);
extern int fq_if_setup_ifclassq(struct ifclassq *ifq, u_int32_t flags,
    classq_pkt_type_t ptype);
extern void fq_if_teardown_ifclassq(struct ifclassq *ifq);
extern int fq_if_getqstats_ifclassq(struct ifclassq *ifq, uint8_t gid,
    u_int32_t qid, struct if_ifclassq_stats *ifqs);
extern void fq_if_destroy_flow(fq_if_t *, fq_if_classq_t *, struct flowq *);
extern void fq_if_move_to_empty_flow(fq_if_t *, fq_if_classq_t *,
    struct flowq *, uint64_t);
extern int fq_if_create_grp(struct ifclassq *ifcq, uint8_t qset_idx, uint8_t flags);
extern void fq_if_set_grp_combined(struct ifclassq *ifcq, uint8_t qset_idx);
extern void fq_if_set_grp_separated(struct ifclassq *ifcq, uint8_t qset_idx);
extern fq_if_group_t *fq_if_find_grp(fq_if_t *fqs, uint8_t grp_idx);
extern boolean_t fq_if_is_all_paced(struct ifclassq *ifq);
extern int netem_config(struct netem **ne, const char *__null_terminated name, struct ifnet *ifp,
    const struct if_netem_params *p, void *output_handle,
    netem_output_func_t *output_func, uint32_t output_max_batch_size);
extern void netem_get_params(struct netem *ne, struct if_netem_params *p);
extern void netem_destroy(struct netem *ne);
extern int netem_enqueue(struct netem *ne, classq_pkt_t *p, bool *pdrop);
__BEGIN_DECLS

int bindfs_init(struct vfsconf * vfsp);
int bindfs_destroy(void);
int bind_nodeget(
	struct mount * mp, struct vnode * lowervp, struct vnode * dvp, struct vnode ** vpp, struct componentname * cnp, int root);
int bind_hashget(struct mount * mp, struct vnode * lowervp, struct vnode ** vpp);
int bind_getnewvnode(
	struct mount * mp, struct vnode * lowervp, struct vnode * dvp, struct vnode ** vpp, struct componentname * cnp, int root);
void bind_hashrem(struct bind_node * xp);
int bindfs_getbackingvnode(vnode_t in_vp, vnode_t* out_vpp);
__BEGIN_DECLS


void *  devfs_make_node_clone(dev_t dev, int chrblk, uid_t uid, gid_t gid,
    int perms, int (*clone)(dev_t dev, int action),
    const char *fmt, ...) __printflike(7, 8);
void *  devfs_make_node(dev_t dev, int chrblk, uid_t uid, gid_t gid,
    int perms, const char *fmt, ...) __printflike(6, 7);
int     devfs_make_link(void * handle, char *fmt, ...) __printflike(2, 3);
void    devfs_remove(void * handle);
__BEGIN_DECLS







int     fifo_close_internal(vnode_t, int, vfs_context_t, int);
int fifo_freespace(struct vnode *vp, long *count);
int fifo_charcount(struct vnode *vp, int *count);
int     fifo_ebadf(void *);
int     fifo_lookup(struct vnop_lookup_args *);
int     fifo_open(struct vnop_open_args *);
int     fifo_close(struct vnop_close_args *);
int     fifo_read(struct vnop_read_args *);
int     fifo_write(struct vnop_write_args *);
int     fifo_ioctl(struct vnop_ioctl_args *);
int     fifo_select(struct vnop_select_args *);
int     fifo_inactive(struct  vnop_inactive_args *);
int     fifo_pathconf(struct vnop_pathconf_args *);
int     fifo_advlock(struct vnop_advlock_args *);
__BEGIN_DECLS

int nullfs_init(struct vfsconf * vfsp);
void nullfs_init_lck(lck_mtx_t * lck);
void nullfs_destroy_lck(lck_mtx_t * lck);
int nullfs_uninit(void);
int null_nodeget(
	struct mount * mp, struct vnode * lowervp, struct vnode * dvp, struct vnode ** vpp, struct componentname * cnp, int root);
int null_hashget(struct mount * mp, struct vnode * lowervp, struct vnode ** vpp);
int null_getnewvnode(
	struct mount * mp, struct vnode * lowervp, struct vnode * dvp, struct vnode ** vpp, struct componentname * cnp, int root);
void null_hashrem(struct null_node * xp);
int nullfs_getbackingvnode(vnode_t in_vp, vnode_t* out_vpp);
vfs_context_t nullfs_get_patched_context(struct null_mount * null_mp, vfs_context_t ctx);
void nullfs_cleanup_patched_context(struct null_mount * null_mp, vfs_context_t ctx);
__BEGIN_DECLS









int     routefs_kernel_mount(char * routepath);
extern void blist_destroy(blist_t blist);
extern daddr_t blist_alloc(blist_t blist, daddr_t count);
extern void blist_free(blist_t blist, daddr_t blkno, daddr_t count);
extern void blist_print(blist_t blist);
extern void blist_resize(blist_t *pblist, daddr_t count, int freenew);
void dtrace_xoroshiro128_plus_jump(uint64_t * const, uint64_t * const);
uint64_t dtrace_xoroshiro128_plus_next(uint64_t * const);
extern void systrace_stub(dtrace_id_t, uint64_t, uint64_t,
    uint64_t, uint64_t, uint64_t);
extern int32_t dtrace_systrace_syscall(struct proc *, void *, int *);
extern void dtrace_systrace_syscall_return(unsigned short, int, int *);
